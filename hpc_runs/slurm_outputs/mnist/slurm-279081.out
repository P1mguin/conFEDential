ctit088
2024-04-05 18:40:11.175597: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-05 18:40:16.930132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-05 18:40:42.668688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 18:42:51,538 | batch_run_simulation.py:63 | Loaded 140 configs, running...
INFO flwr 2024-04-05 18:42:51,538 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 18:50:18,814 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-05 18:50:21,334	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 18:50:29,780	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 18:50:30,090	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f4205761b2e4b5d0.zip' (7.45MiB) to Ray cluster...
2024-04-05 18:50:30,114	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f4205761b2e4b5d0.zip'.
INFO flwr 2024-04-05 18:50:41,810 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 165877064295.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 75375884697.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 18:50:41,810 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 18:50:41,810 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 18:50:41,830 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 18:50:41,831 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 18:50:41,831 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 18:50:41,832 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=445771)[0m 2024-04-05 18:50:47.782839: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=445771)[0m 2024-04-05 18:50:47.874208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=445771)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=445771)[0m 2024-04-05 18:50:50.005120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 18:50:57,276 | server.py:94 | initial parameters (loss, other metrics): 2.3022384643554688, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-05 18:50:57,277 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 18:50:57,277 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=445771)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=445771)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=445781)[0m 2024-04-05 18:50:48.266873: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=445781)[0m 2024-04-05 18:50:48.362923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=445781)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=445781)[0m 2024-04-05 18:50:50.795022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 18:51:18,164 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 18:51:21,758 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 18:51:25,487 | server.py:125 | fit progress: (1, 2.3022372722625732, {'accuracy': 0.1221, 'data_size': 10000}, 28.210164702002658)
INFO flwr 2024-04-05 18:51:25,487 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:25,488 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:51:34,608 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 18:51:48,499 | server.py:125 | fit progress: (2, 2.3022372722625732, {'accuracy': 0.1222, 'data_size': 10000}, 51.221644924007705)
INFO flwr 2024-04-05 18:51:48,499 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 18:51:48,499 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:51:56,728 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:16,166 | server.py:125 | fit progress: (3, 2.302236318588257, {'accuracy': 0.1224, 'data_size': 10000}, 78.88930068499758)
INFO flwr 2024-04-05 18:52:16,167 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:16,167 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:23,644 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 18:52:49,183 | server.py:125 | fit progress: (4, 2.3022356033325195, {'accuracy': 0.1222, 'data_size': 10000}, 111.90612149999652)
INFO flwr 2024-04-05 18:52:49,183 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 18:52:49,184 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:52:57,574 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 18:53:31,336 | server.py:125 | fit progress: (5, 2.3022348880767822, {'accuracy': 0.1222, 'data_size': 10000}, 154.05918198199652)
INFO flwr 2024-04-05 18:53:31,336 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 18:53:31,337 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:53:39,790 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 18:54:17,535 | server.py:125 | fit progress: (6, 2.302234411239624, {'accuracy': 0.1222, 'data_size': 10000}, 200.25776348701038)
INFO flwr 2024-04-05 18:54:17,535 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 18:54:17,536 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:54:25,875 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 18:55:10,623 | server.py:125 | fit progress: (7, 2.3022336959838867, {'accuracy': 0.1221, 'data_size': 10000}, 253.34583206400566)
INFO flwr 2024-04-05 18:55:10,623 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 18:55:10,623 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:55:18,182 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 18:56:16,741 | server.py:125 | fit progress: (8, 2.3022329807281494, {'accuracy': 0.1221, 'data_size': 10000}, 319.4644714550086)
INFO flwr 2024-04-05 18:56:16,742 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 18:56:16,742 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:56:24,676 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 18:57:34,512 | server.py:125 | fit progress: (9, 2.302231788635254, {'accuracy': 0.1221, 'data_size': 10000}, 397.2346620820026)
INFO flwr 2024-04-05 18:57:34,512 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 18:57:34,512 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 18:57:42,479 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 18:58:56,585 | server.py:125 | fit progress: (10, 2.302231550216675, {'accuracy': 0.1221, 'data_size': 10000}, 479.3081709280086)
INFO flwr 2024-04-05 18:58:56,586 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 18:58:56,587 | server.py:153 | FL finished in 479.30963666200114
INFO flwr 2024-04-05 18:58:56,589 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 18:58:56,589 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 18:58:56,589 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 18:58:56,589 | app.py:229 | app_fit: losses_centralized [(0, 2.3022384643554688), (1, 2.3022372722625732), (2, 2.3022372722625732), (3, 2.302236318588257), (4, 2.3022356033325195), (5, 2.3022348880767822), (6, 2.302234411239624), (7, 2.3022336959838867), (8, 2.3022329807281494), (9, 2.302231788635254), (10, 2.302231550216675)]
INFO flwr 2024-04-05 18:58:56,589 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.1221), (2, 0.1222), (3, 0.1224), (4, 0.1222), (5, 0.1222), (6, 0.1222), (7, 0.1221), (8, 0.1221), (9, 0.1221), (10, 0.1221)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1221
wandb:     loss 2.30223
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_185018-9zyph7kx
wandb: Find logs at: ./wandb/offline-run-20240405_185018-9zyph7kx/logs
INFO flwr 2024-04-05 18:59:00,166 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:06:19,014 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=445768)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=445768)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:06:23,837	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:06:24,121	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:06:24,428	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8157a5673ba4da58.zip' (7.49MiB) to Ray cluster...
2024-04-05 19:06:24,444	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8157a5673ba4da58.zip'.
INFO flwr 2024-04-05 19:06:34,608 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 71332502323.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 156442505421.0}
INFO flwr 2024-04-05 19:06:34,608 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:06:34,608 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:06:34,624 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:06:34,625 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:06:34,625 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:06:34,625 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=455951)[0m 2024-04-05 19:06:39.760301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=455951)[0m 2024-04-05 19:06:39.850691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=455951)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=455949)[0m 2024-04-05 19:06:42.036924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 19:06:42,611 | server.py:94 | initial parameters (loss, other metrics): 2.3024933338165283, {'accuracy': 0.1005, 'data_size': 10000}
INFO flwr 2024-04-05 19:06:42,612 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:06:42,612 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=455953)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=455953)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=455958)[0m 2024-04-05 19:06:40.675942: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=455958)[0m 2024-04-05 19:06:40.765334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=455958)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=455953)[0m 2024-04-05 19:06:43.073282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=455946)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=455946)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:06:56,931 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:06:59,881 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:07:03,615 | server.py:125 | fit progress: (1, 2.3024604320526123, {'accuracy': 0.0984, 'data_size': 10000}, 21.002481751012965)
INFO flwr 2024-04-05 19:07:03,615 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:07:03,615 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:07:11,952 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:07:24,313 | server.py:125 | fit progress: (2, 2.3024368286132812, {'accuracy': 0.0992, 'data_size': 10000}, 41.70098997101013)
INFO flwr 2024-04-05 19:07:24,313 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:07:24,314 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:07:32,279 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:07:50,608 | server.py:125 | fit progress: (3, 2.3024020195007324, {'accuracy': 0.0988, 'data_size': 10000}, 67.99552021900308)
INFO flwr 2024-04-05 19:07:50,608 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:07:50,608 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:07:57,899 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:08:21,497 | server.py:125 | fit progress: (4, 2.302375078201294, {'accuracy': 0.0983, 'data_size': 10000}, 98.8851901730086)
INFO flwr 2024-04-05 19:08:21,498 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:08:21,498 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:08:29,296 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:08:58,651 | server.py:125 | fit progress: (5, 2.302345037460327, {'accuracy': 0.1001, 'data_size': 10000}, 136.03912088400102)
INFO flwr 2024-04-05 19:08:58,651 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:08:58,652 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:09:06,065 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:09:47,060 | server.py:125 | fit progress: (6, 2.302314043045044, {'accuracy': 0.099, 'data_size': 10000}, 184.44833410200954)
INFO flwr 2024-04-05 19:09:47,061 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:09:47,061 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:09:54,887 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:10:45,313 | server.py:125 | fit progress: (7, 2.3022844791412354, {'accuracy': 0.0996, 'data_size': 10000}, 242.70114052500867)
INFO flwr 2024-04-05 19:10:45,313 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:10:45,314 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:10:53,514 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:11:41,870 | server.py:125 | fit progress: (8, 2.3022501468658447, {'accuracy': 0.1007, 'data_size': 10000}, 299.2576313710015)
INFO flwr 2024-04-05 19:11:41,870 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:11:41,870 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:11:50,340 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:12:45,604 | server.py:125 | fit progress: (9, 2.3022162914276123, {'accuracy': 0.1007, 'data_size': 10000}, 362.99157330801245)
INFO flwr 2024-04-05 19:12:45,604 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:12:45,604 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:12:53,420 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:13:50,158 | server.py:125 | fit progress: (10, 2.3021843433380127, {'accuracy': 0.1014, 'data_size': 10000}, 427.5457841460011)
INFO flwr 2024-04-05 19:13:50,158 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:13:50,158 | server.py:153 | FL finished in 427.5463851500099
INFO flwr 2024-04-05 19:13:50,159 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:13:50,159 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:13:50,159 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:13:50,159 | app.py:229 | app_fit: losses_centralized [(0, 2.3024933338165283), (1, 2.3024604320526123), (2, 2.3024368286132812), (3, 2.3024020195007324), (4, 2.302375078201294), (5, 2.302345037460327), (6, 2.302314043045044), (7, 2.3022844791412354), (8, 2.3022501468658447), (9, 2.3022162914276123), (10, 2.3021843433380127)]
INFO flwr 2024-04-05 19:13:50,159 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1005), (1, 0.0984), (2, 0.0992), (3, 0.0988), (4, 0.0983), (5, 0.1001), (6, 0.099), (7, 0.0996), (8, 0.1007), (9, 0.1007), (10, 0.1014)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1014
wandb:     loss 2.30218
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_190618-31j4wg9t
wandb: Find logs at: ./wandb/offline-run-20240405_190618-31j4wg9t/logs
INFO flwr 2024-04-05 19:13:53,734 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:21:18,786 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-05 19:21:24,355	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:21:24,741	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:21:25,051	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9191e105a6256fcc.zip' (7.52MiB) to Ray cluster...
2024-04-05 19:21:25,066	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9191e105a6256fcc.zip'.
INFO flwr 2024-04-05 19:21:35,904 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 159524877312.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 72653518848.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 19:21:35,905 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:21:35,906 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:21:35,927 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:21:35,928 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:21:35,929 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:21:35,929 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=467363)[0m 2024-04-05 19:21:41.973048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=467363)[0m 2024-04-05 19:21:42.070233: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=467363)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 19:21:43,290 | server.py:94 | initial parameters (loss, other metrics): 2.3026363849639893, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-05 19:21:43,290 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:21:43,291 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=467371)[0m 2024-04-05 19:21:44.114661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=467363)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=467363)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=467370)[0m 2024-04-05 19:21:42.417046: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=467370)[0m 2024-04-05 19:21:42.515195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=467370)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=467370)[0m 2024-04-05 19:21:44.815682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 19:22:54,795 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:22:58,240 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:23:02,172 | server.py:125 | fit progress: (1, 2.3025662899017334, {'accuracy': 0.096, 'data_size': 10000}, 78.88118209299864)
INFO flwr 2024-04-05 19:23:02,172 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:02,172 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:10,600 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:23,475 | server.py:125 | fit progress: (2, 2.3024823665618896, {'accuracy': 0.0999, 'data_size': 10000}, 100.18460785600473)
INFO flwr 2024-04-05 19:23:23,475 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:23,475 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:31,445 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:23:50,514 | server.py:125 | fit progress: (3, 2.302401065826416, {'accuracy': 0.0976, 'data_size': 10000}, 127.2234559589997)
INFO flwr 2024-04-05 19:23:50,514 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:23:50,514 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:23:58,510 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:24:23,249 | server.py:125 | fit progress: (4, 2.302333116531372, {'accuracy': 0.0955, 'data_size': 10000}, 159.95856251499208)
INFO flwr 2024-04-05 19:24:23,249 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:24:23,249 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:24:31,448 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:25:02,719 | server.py:125 | fit progress: (5, 2.302260637283325, {'accuracy': 0.0976, 'data_size': 10000}, 199.42809506600315)
INFO flwr 2024-04-05 19:25:02,719 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:25:02,719 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:25:10,656 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:25:47,090 | server.py:125 | fit progress: (6, 2.3021645545959473, {'accuracy': 0.0957, 'data_size': 10000}, 243.79989111000032)
INFO flwr 2024-04-05 19:25:47,091 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:25:47,091 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:25:55,010 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:26:34,457 | server.py:125 | fit progress: (7, 2.3020975589752197, {'accuracy': 0.0958, 'data_size': 10000}, 291.16659933399933)
INFO flwr 2024-04-05 19:26:34,458 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:26:34,458 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:26:42,583 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:27:40,001 | server.py:125 | fit progress: (8, 2.3020098209381104, {'accuracy': 0.0959, 'data_size': 10000}, 356.71047364900005)
INFO flwr 2024-04-05 19:27:40,001 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:27:40,002 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:27:47,870 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:28:50,608 | server.py:125 | fit progress: (9, 2.301931619644165, {'accuracy': 0.1012, 'data_size': 10000}, 427.31802558599156)
INFO flwr 2024-04-05 19:28:50,609 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:28:50,609 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:28:58,636 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:30:08,527 | server.py:125 | fit progress: (10, 2.3018710613250732, {'accuracy': 0.1015, 'data_size': 10000}, 505.23618643499503)
INFO flwr 2024-04-05 19:30:08,527 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:30:08,527 | server.py:153 | FL finished in 505.23662945900287
INFO flwr 2024-04-05 19:30:08,527 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:30:08,527 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:30:08,527 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:30:08,528 | app.py:229 | app_fit: losses_centralized [(0, 2.3026363849639893), (1, 2.3025662899017334), (2, 2.3024823665618896), (3, 2.302401065826416), (4, 2.302333116531372), (5, 2.302260637283325), (6, 2.3021645545959473), (7, 2.3020975589752197), (8, 2.3020098209381104), (9, 2.301931619644165), (10, 2.3018710613250732)]
INFO flwr 2024-04-05 19:30:08,528 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.096), (2, 0.0999), (3, 0.0976), (4, 0.0955), (5, 0.0976), (6, 0.0957), (7, 0.0958), (8, 0.0959), (9, 0.1012), (10, 0.1015)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1015
wandb:     loss 2.30187
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_192118-t3o7rtgn
wandb: Find logs at: ./wandb/offline-run-20240405_192118-t3o7rtgn/logs
INFO flwr 2024-04-05 19:30:12,124 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:37:32,128 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=467379)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=467379)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 19:37:36,617	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:37:36,910	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:37:37,284	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_be95f86752e965c6.zip' (7.55MiB) to Ray cluster...
2024-04-05 19:37:37,299	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_be95f86752e965c6.zip'.
INFO flwr 2024-04-05 19:37:48,122 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70071837081.0, 'memory': 153500953191.0, 'CPU': 64.0}
INFO flwr 2024-04-05 19:37:48,123 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:37:48,123 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:37:48,139 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:37:48,140 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:37:48,141 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:37:48,141 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=476255)[0m 2024-04-05 19:37:53.269836: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=476255)[0m 2024-04-05 19:37:53.372905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=476255)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 19:37:56,075 | server.py:94 | initial parameters (loss, other metrics): 2.302727460861206, {'accuracy': 0.1025, 'data_size': 10000}
INFO flwr 2024-04-05 19:37:56,076 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:37:56,076 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=476258)[0m 2024-04-05 19:37:56.115856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=476264)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=476264)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=476260)[0m 2024-04-05 19:37:54.517905: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=476260)[0m 2024-04-05 19:37:54.616570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=476260)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=476260)[0m 2024-04-05 19:37:56.808307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=476253)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=476253)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-05 19:38:10,755 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:38:14,051 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:38:17,983 | server.py:125 | fit progress: (1, 2.3026199340820312, {'accuracy': 0.1051, 'data_size': 10000}, 21.907044709994807)
INFO flwr 2024-04-05 19:38:17,984 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:38:17,984 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:38:26,622 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:38:39,785 | server.py:125 | fit progress: (2, 2.302501678466797, {'accuracy': 0.1014, 'data_size': 10000}, 43.708931845991174)
INFO flwr 2024-04-05 19:38:39,786 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:38:39,786 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:38:47,729 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:39:06,711 | server.py:125 | fit progress: (3, 2.3023579120635986, {'accuracy': 0.1137, 'data_size': 10000}, 70.63444250299654)
INFO flwr 2024-04-05 19:39:06,711 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:39:06,711 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:39:14,273 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:39:38,863 | server.py:125 | fit progress: (4, 2.3022186756134033, {'accuracy': 0.1122, 'data_size': 10000}, 102.78708906599786)
INFO flwr 2024-04-05 19:39:38,864 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:39:38,864 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:39:46,824 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:40:17,485 | server.py:125 | fit progress: (5, 2.3020875453948975, {'accuracy': 0.1062, 'data_size': 10000}, 141.40847759399912)
INFO flwr 2024-04-05 19:40:17,485 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:40:17,485 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:40:25,547 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:41:01,584 | server.py:125 | fit progress: (6, 2.3019425868988037, {'accuracy': 0.1499, 'data_size': 10000}, 185.50797539199993)
INFO flwr 2024-04-05 19:41:01,585 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:41:01,585 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:41:09,806 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:41:49,934 | server.py:125 | fit progress: (7, 2.3018252849578857, {'accuracy': 0.1559, 'data_size': 10000}, 233.85781326699362)
INFO flwr 2024-04-05 19:41:49,934 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:41:49,934 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:41:57,243 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:42:59,846 | server.py:125 | fit progress: (8, 2.301678419113159, {'accuracy': 0.1558, 'data_size': 10000}, 303.7702078290022)
INFO flwr 2024-04-05 19:42:59,847 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:42:59,847 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:43:07,929 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:44:14,793 | server.py:125 | fit progress: (9, 2.301563024520874, {'accuracy': 0.1619, 'data_size': 10000}, 378.7172395569942)
INFO flwr 2024-04-05 19:44:14,794 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:44:14,794 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:44:22,960 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 19:45:23,649 | server.py:125 | fit progress: (10, 2.3014204502105713, {'accuracy': 0.1353, 'data_size': 10000}, 447.57313270599116)
INFO flwr 2024-04-05 19:45:23,650 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 19:45:23,650 | server.py:153 | FL finished in 447.5735907079943
INFO flwr 2024-04-05 19:45:23,650 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 19:45:23,650 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 19:45:23,650 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 19:45:23,650 | app.py:229 | app_fit: losses_centralized [(0, 2.302727460861206), (1, 2.3026199340820312), (2, 2.302501678466797), (3, 2.3023579120635986), (4, 2.3022186756134033), (5, 2.3020875453948975), (6, 2.3019425868988037), (7, 2.3018252849578857), (8, 2.301678419113159), (9, 2.301563024520874), (10, 2.3014204502105713)]
INFO flwr 2024-04-05 19:45:23,650 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1025), (1, 0.1051), (2, 0.1014), (3, 0.1137), (4, 0.1122), (5, 0.1062), (6, 0.1499), (7, 0.1559), (8, 0.1558), (9, 0.1619), (10, 0.1353)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1353
wandb:     loss 2.30142
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_193731-h5e6kd2r
wandb: Find logs at: ./wandb/offline-run-20240405_193731-h5e6kd2r/logs
INFO flwr 2024-04-05 19:45:27,157 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 19:52:48,265 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=476251)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=476251)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 19:52:52,789	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 19:52:52,997	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 19:52:53,387	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_22ff53cf539e8691.zip' (7.58MiB) to Ray cluster...
2024-04-05 19:52:53,402	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_22ff53cf539e8691.zip'.
INFO flwr 2024-04-05 19:53:04,345 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 153668707328.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 70143731712.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 19:53:04,346 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 19:53:04,346 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 19:53:04,360 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 19:53:04,361 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 19:53:04,361 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 19:53:04,361 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=487452)[0m 2024-04-05 19:53:09.789771: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=487452)[0m 2024-04-05 19:53:09.941489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=487452)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=487452)[0m 2024-04-05 19:53:12.114297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-05 19:53:12,655 | server.py:94 | initial parameters (loss, other metrics): 2.302785634994507, {'accuracy': 0.0847, 'data_size': 10000}
INFO flwr 2024-04-05 19:53:12,655 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 19:53:12,655 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=487459)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=487459)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=487450)[0m 2024-04-05 19:53:10.728142: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=487450)[0m 2024-04-05 19:53:10.821702: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=487450)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=487460)[0m 2024-04-05 19:53:12.978082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=487453)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=487453)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-05 19:53:30,412 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 19:53:33,708 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 19:53:37,577 | server.py:125 | fit progress: (1, 2.302654266357422, {'accuracy': 0.0959, 'data_size': 10000}, 24.921636173996376)
INFO flwr 2024-04-05 19:53:37,577 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:37,577 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:53:46,414 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 19:53:59,476 | server.py:125 | fit progress: (2, 2.3025081157684326, {'accuracy': 0.0999, 'data_size': 10000}, 46.82103175600059)
INFO flwr 2024-04-05 19:53:59,477 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 19:53:59,477 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:54:07,211 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 19:54:26,164 | server.py:125 | fit progress: (3, 2.302358627319336, {'accuracy': 0.0972, 'data_size': 10000}, 73.50892568999552)
INFO flwr 2024-04-05 19:54:26,165 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 19:54:26,165 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:54:33,997 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 19:54:59,060 | server.py:125 | fit progress: (4, 2.3022115230560303, {'accuracy': 0.0981, 'data_size': 10000}, 106.40501953900093)
INFO flwr 2024-04-05 19:54:59,061 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 19:54:59,061 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:55:06,982 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 19:55:37,503 | server.py:125 | fit progress: (5, 2.302063465118408, {'accuracy': 0.0982, 'data_size': 10000}, 144.84793864999665)
INFO flwr 2024-04-05 19:55:37,504 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 19:55:37,504 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:55:45,527 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 19:56:18,713 | server.py:125 | fit progress: (6, 2.301920175552368, {'accuracy': 0.1062, 'data_size': 10000}, 186.05718341299507)
INFO flwr 2024-04-05 19:56:18,713 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 19:56:18,713 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:56:26,738 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 19:57:17,445 | server.py:125 | fit progress: (7, 2.301739454269409, {'accuracy': 0.1069, 'data_size': 10000}, 244.78926223199232)
INFO flwr 2024-04-05 19:57:17,445 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 19:57:17,445 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:57:25,288 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 19:58:11,586 | server.py:125 | fit progress: (8, 2.3015849590301514, {'accuracy': 0.0984, 'data_size': 10000}, 298.9304353560001)
INFO flwr 2024-04-05 19:58:11,586 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 19:58:11,586 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:58:19,591 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 19:59:22,357 | server.py:125 | fit progress: (9, 2.3014137744903564, {'accuracy': 0.0982, 'data_size': 10000}, 369.70159178599715)
INFO flwr 2024-04-05 19:59:22,357 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 19:59:22,357 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 19:59:30,772 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:00:40,800 | server.py:125 | fit progress: (10, 2.30123233795166, {'accuracy': 0.1173, 'data_size': 10000}, 448.1447193299973)
INFO flwr 2024-04-05 20:00:40,801 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:00:40,801 | server.py:153 | FL finished in 448.1453395319986
INFO flwr 2024-04-05 20:00:40,801 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:00:40,801 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:00:40,801 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:00:40,801 | app.py:229 | app_fit: losses_centralized [(0, 2.302785634994507), (1, 2.302654266357422), (2, 2.3025081157684326), (3, 2.302358627319336), (4, 2.3022115230560303), (5, 2.302063465118408), (6, 2.301920175552368), (7, 2.301739454269409), (8, 2.3015849590301514), (9, 2.3014137744903564), (10, 2.30123233795166)]
INFO flwr 2024-04-05 20:00:40,801 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0847), (1, 0.0959), (2, 0.0999), (3, 0.0972), (4, 0.0981), (5, 0.0982), (6, 0.1062), (7, 0.1069), (8, 0.0984), (9, 0.0982), (10, 0.1173)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1173
wandb:     loss 2.30123
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_195247-arg3i8t8
wandb: Find logs at: ./wandb/offline-run-20240405_195247-arg3i8t8/logs
INFO flwr 2024-04-05 20:00:44,382 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:08:04,413 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=487450)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=487450)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 20:08:09,110	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:08:09,490	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:08:09,829	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2728bc1a2c75891a.zip' (7.61MiB) to Ray cluster...
2024-04-05 20:08:09,847	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2728bc1a2c75891a.zip'.
INFO flwr 2024-04-05 20:08:20,566 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 152812386509.0, 'object_store_memory': 69776737075.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 20:08:20,567 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:08:20,567 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:08:20,580 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:08:20,581 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:08:20,581 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:08:20,582 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=495966)[0m 2024-04-05 20:08:26.297679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=495966)[0m 2024-04-05 20:08:26.357229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=495966)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 20:08:28,156 | server.py:94 | initial parameters (loss, other metrics): 2.302922248840332, {'accuracy': 0.1152, 'data_size': 10000}
INFO flwr 2024-04-05 20:08:28,156 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:08:28,157 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=495966)[0m 2024-04-05 20:08:28.494053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=495972)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=495972)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=495968)[0m 2024-04-05 20:08:27.060222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=495963)[0m 2024-04-05 20:08:27.154422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=495963)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=495968)[0m 2024-04-05 20:08:29.238424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:08:42,458 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:08:45,338 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:08:49,230 | server.py:125 | fit progress: (1, 2.302781343460083, {'accuracy': 0.1325, 'data_size': 10000}, 21.07303510599013)
INFO flwr 2024-04-05 20:08:49,230 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:08:49,230 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:08:57,884 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:09:10,488 | server.py:125 | fit progress: (2, 2.3025870323181152, {'accuracy': 0.1257, 'data_size': 10000}, 42.331760759989265)
INFO flwr 2024-04-05 20:09:10,489 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:09:10,489 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:09:18,241 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:09:37,163 | server.py:125 | fit progress: (3, 2.3024210929870605, {'accuracy': 0.0982, 'data_size': 10000}, 69.00669310899684)
INFO flwr 2024-04-05 20:09:37,164 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:09:37,164 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:09:44,869 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:10:09,411 | server.py:125 | fit progress: (4, 2.3022620677948, {'accuracy': 0.1068, 'data_size': 10000}, 101.25400722799532)
INFO flwr 2024-04-05 20:10:09,411 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:10:09,411 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:10:17,430 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:10:48,240 | server.py:125 | fit progress: (5, 2.30204439163208, {'accuracy': 0.1167, 'data_size': 10000}, 140.0838932619954)
INFO flwr 2024-04-05 20:10:48,241 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:10:48,241 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:10:56,165 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:11:32,412 | server.py:125 | fit progress: (6, 2.301884412765503, {'accuracy': 0.1695, 'data_size': 10000}, 184.25582051399397)
INFO flwr 2024-04-05 20:11:32,413 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:11:32,413 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:11:40,438 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:12:21,801 | server.py:125 | fit progress: (7, 2.3017032146453857, {'accuracy': 0.2114, 'data_size': 10000}, 233.64484053300112)
INFO flwr 2024-04-05 20:12:21,802 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:12:21,802 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:12:29,686 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:13:26,862 | server.py:125 | fit progress: (8, 2.3015778064727783, {'accuracy': 0.2115, 'data_size': 10000}, 298.7057642909931)
INFO flwr 2024-04-05 20:13:26,863 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:13:26,863 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:13:35,303 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:14:29,636 | server.py:125 | fit progress: (9, 2.301406145095825, {'accuracy': 0.2543, 'data_size': 10000}, 361.47969987599936)
INFO flwr 2024-04-05 20:14:29,637 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:14:29,637 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:14:37,860 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:15:35,317 | server.py:125 | fit progress: (10, 2.3012161254882812, {'accuracy': 0.2889, 'data_size': 10000}, 427.16059994399257)
INFO flwr 2024-04-05 20:15:35,318 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:15:35,318 | server.py:153 | FL finished in 427.16111868599546
INFO flwr 2024-04-05 20:15:35,320 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:15:35,321 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:15:35,321 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:15:35,321 | app.py:229 | app_fit: losses_centralized [(0, 2.302922248840332), (1, 2.302781343460083), (2, 2.3025870323181152), (3, 2.3024210929870605), (4, 2.3022620677948), (5, 2.30204439163208), (6, 2.301884412765503), (7, 2.3017032146453857), (8, 2.3015778064727783), (9, 2.301406145095825), (10, 2.3012161254882812)]
INFO flwr 2024-04-05 20:15:35,321 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1152), (1, 0.1325), (2, 0.1257), (3, 0.0982), (4, 0.1068), (5, 0.1167), (6, 0.1695), (7, 0.2114), (8, 0.2115), (9, 0.2543), (10, 0.2889)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2889
wandb:     loss 2.30122
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_200803-yu3n1xpk
wandb: Find logs at: ./wandb/offline-run-20240405_200803-yu3n1xpk/logs
INFO flwr 2024-04-05 20:15:38,925 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:22:58,872 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=495957)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=495957)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:23:04,637	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:23:04,931	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:23:05,257	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6675901bfbd9b17b.zip' (7.64MiB) to Ray cluster...
2024-04-05 20:23:05,277	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6675901bfbd9b17b.zip'.
INFO flwr 2024-04-05 20:23:16,259 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 69945918259.0, 'node:10.20.240.18': 1.0, 'memory': 153207142605.0, 'CPU': 64.0}
INFO flwr 2024-04-05 20:23:16,260 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:23:16,260 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:23:16,278 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:23:16,280 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:23:16,280 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:23:16,280 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=507110)[0m 2024-04-05 20:23:22.136374: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=507110)[0m 2024-04-05 20:23:22.237496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=507110)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 20:23:24,183 | server.py:94 | initial parameters (loss, other metrics): 2.3025691509246826, {'accuracy': 0.1089, 'data_size': 10000}
INFO flwr 2024-04-05 20:23:24,184 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:23:24,184 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=507110)[0m 2024-04-05 20:23:24.396906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=507118)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=507118)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=507106)[0m 2024-04-05 20:23:22.594177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=507106)[0m 2024-04-05 20:23:22.688292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=507106)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=507106)[0m 2024-04-05 20:23:24.834291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:23:39,085 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:23:42,510 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:23:46,425 | server.py:125 | fit progress: (1, 2.302391290664673, {'accuracy': 0.1098, 'data_size': 10000}, 22.24088627700985)
INFO flwr 2024-04-05 20:23:46,426 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:23:46,426 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:23:55,203 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:24:08,353 | server.py:125 | fit progress: (2, 2.302257537841797, {'accuracy': 0.1634, 'data_size': 10000}, 44.168993827013765)
INFO flwr 2024-04-05 20:24:08,353 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:24:08,353 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:24:16,215 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:24:35,220 | server.py:125 | fit progress: (3, 2.3021419048309326, {'accuracy': 0.1762, 'data_size': 10000}, 71.03613969500293)
INFO flwr 2024-04-05 20:24:35,220 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:24:35,220 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:24:42,834 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:25:07,312 | server.py:125 | fit progress: (4, 2.301997661590576, {'accuracy': 0.1236, 'data_size': 10000}, 103.12807701301062)
INFO flwr 2024-04-05 20:25:07,312 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:25:07,312 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:25:15,329 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:25:45,630 | server.py:125 | fit progress: (5, 2.301819324493408, {'accuracy': 0.2292, 'data_size': 10000}, 141.4456598340039)
INFO flwr 2024-04-05 20:25:45,630 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:25:45,630 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:25:53,548 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:26:32,716 | server.py:125 | fit progress: (6, 2.3016276359558105, {'accuracy': 0.214, 'data_size': 10000}, 188.53229964600177)
INFO flwr 2024-04-05 20:26:32,716 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:26:32,717 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:26:40,903 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:27:28,243 | server.py:125 | fit progress: (7, 2.301499605178833, {'accuracy': 0.1979, 'data_size': 10000}, 244.05920930301363)
INFO flwr 2024-04-05 20:27:28,243 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:27:28,243 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:27:36,002 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:28:36,040 | server.py:125 | fit progress: (8, 2.301286220550537, {'accuracy': 0.1844, 'data_size': 10000}, 311.85634550500254)
INFO flwr 2024-04-05 20:28:36,040 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:28:36,041 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:28:43,960 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:29:47,670 | server.py:125 | fit progress: (9, 2.3011069297790527, {'accuracy': 0.2208, 'data_size': 10000}, 383.48629315401195)
INFO flwr 2024-04-05 20:29:47,670 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:29:47,671 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:29:55,299 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:31:06,260 | server.py:125 | fit progress: (10, 2.300903797149658, {'accuracy': 0.2364, 'data_size': 10000}, 462.07606262101035)
INFO flwr 2024-04-05 20:31:06,260 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:31:06,260 | server.py:153 | FL finished in 462.0766472680116
INFO flwr 2024-04-05 20:31:06,261 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:31:06,261 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:31:06,261 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:31:06,261 | app.py:229 | app_fit: losses_centralized [(0, 2.3025691509246826), (1, 2.302391290664673), (2, 2.302257537841797), (3, 2.3021419048309326), (4, 2.301997661590576), (5, 2.301819324493408), (6, 2.3016276359558105), (7, 2.301499605178833), (8, 2.301286220550537), (9, 2.3011069297790527), (10, 2.300903797149658)]
INFO flwr 2024-04-05 20:31:06,261 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1089), (1, 0.1098), (2, 0.1634), (3, 0.1762), (4, 0.1236), (5, 0.2292), (6, 0.214), (7, 0.1979), (8, 0.1844), (9, 0.2208), (10, 0.2364)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2364
wandb:     loss 2.3009
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_202258-65z6d2uu
wandb: Find logs at: ./wandb/offline-run-20240405_202258-65z6d2uu/logs
INFO flwr 2024-04-05 20:31:09,811 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:38:29,712 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=507106)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=507106)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:38:36,157	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:38:36,469	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:38:36,786	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8b9f7f1885c55143.zip' (7.67MiB) to Ray cluster...
2024-04-05 20:38:36,804	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8b9f7f1885c55143.zip'.
INFO flwr 2024-04-05 20:38:47,530 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 69668234035.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 152559212749.0, 'CPU': 64.0}
INFO flwr 2024-04-05 20:38:47,531 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:38:47,531 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:38:47,556 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:38:47,562 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:38:47,563 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:38:47,563 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=515598)[0m 2024-04-05 20:38:53.593027: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=515598)[0m 2024-04-05 20:38:53.669228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=515598)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 20:38:55,023 | server.py:94 | initial parameters (loss, other metrics): 2.302553415298462, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-05 20:38:55,024 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:38:55,024 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=515606)[0m 2024-04-05 20:38:55.707824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=515606)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=515606)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=515600)[0m 2024-04-05 20:38:53.928797: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=515600)[0m 2024-04-05 20:38:54.028339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=515600)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=515602)[0m 2024-04-05 20:38:56.299292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:39:09,917 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:39:13,176 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:39:17,129 | server.py:125 | fit progress: (1, 2.3025498390197754, {'accuracy': 0.1028, 'data_size': 10000}, 22.105024331001914)
INFO flwr 2024-04-05 20:39:17,130 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:39:17,130 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:39:25,937 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:39:38,791 | server.py:125 | fit progress: (2, 2.3025457859039307, {'accuracy': 0.1028, 'data_size': 10000}, 43.76708240399603)
INFO flwr 2024-04-05 20:39:38,792 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:39:38,792 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:39:46,675 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:40:05,433 | server.py:125 | fit progress: (3, 2.3025429248809814, {'accuracy': 0.1028, 'data_size': 10000}, 70.40829414900509)
INFO flwr 2024-04-05 20:40:05,433 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:40:05,433 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:40:12,993 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:40:38,067 | server.py:125 | fit progress: (4, 2.3025383949279785, {'accuracy': 0.1028, 'data_size': 10000}, 103.04229839600157)
INFO flwr 2024-04-05 20:40:38,067 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:40:38,067 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:40:46,120 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:41:16,464 | server.py:125 | fit progress: (5, 2.302534580230713, {'accuracy': 0.1028, 'data_size': 10000}, 141.43985207400692)
INFO flwr 2024-04-05 20:41:16,464 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:41:16,465 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:41:24,334 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:42:00,742 | server.py:125 | fit progress: (6, 2.3025295734405518, {'accuracy': 0.1028, 'data_size': 10000}, 185.71782516299572)
INFO flwr 2024-04-05 20:42:00,743 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:00,743 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:42:09,318 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:42:54,481 | server.py:125 | fit progress: (7, 2.3025269508361816, {'accuracy': 0.1028, 'data_size': 10000}, 239.4567984210007)
INFO flwr 2024-04-05 20:42:54,481 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:42:54,482 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:02,642 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:43:50,972 | server.py:125 | fit progress: (8, 2.3025224208831787, {'accuracy': 0.1028, 'data_size': 10000}, 295.9480347810022)
INFO flwr 2024-04-05 20:43:50,973 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:43:50,973 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:43:59,282 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 20:44:56,068 | server.py:125 | fit progress: (9, 2.302517890930176, {'accuracy': 0.1028, 'data_size': 10000}, 361.0433677819965)
INFO flwr 2024-04-05 20:44:56,068 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 20:44:56,068 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:45:04,312 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 20:46:05,483 | server.py:125 | fit progress: (10, 2.302513837814331, {'accuracy': 0.1028, 'data_size': 10000}, 430.4590718859981)
INFO flwr 2024-04-05 20:46:05,484 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 20:46:05,484 | server.py:153 | FL finished in 430.4596257850062
INFO flwr 2024-04-05 20:46:05,484 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 20:46:05,484 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 20:46:05,484 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 20:46:05,485 | app.py:229 | app_fit: losses_centralized [(0, 2.302553415298462), (1, 2.3025498390197754), (2, 2.3025457859039307), (3, 2.3025429248809814), (4, 2.3025383949279785), (5, 2.302534580230713), (6, 2.3025295734405518), (7, 2.3025269508361816), (8, 2.3025224208831787), (9, 2.302517890930176), (10, 2.302513837814331)]
INFO flwr 2024-04-05 20:46:05,485 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1028), (2, 0.1028), (3, 0.1028), (4, 0.1028), (5, 0.1028), (6, 0.1028), (7, 0.1028), (8, 0.1028), (9, 0.1028), (10, 0.1028)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1028
wandb:     loss 2.30251
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_203829-z59zrwrf
wandb: Find logs at: ./wandb/offline-run-20240405_203829-z59zrwrf/logs
INFO flwr 2024-04-05 20:46:09,063 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 20:53:29,032 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=515596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=515596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 20:53:34,576	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 20:53:35,588	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 20:53:35,873	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_56cce4e603364d55.zip' (7.71MiB) to Ray cluster...
2024-04-05 20:53:35,891	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_56cce4e603364d55.zip'.
INFO flwr 2024-04-05 20:53:46,757 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 151528462951.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69226484121.0}
INFO flwr 2024-04-05 20:53:46,757 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 20:53:46,757 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 20:53:46,771 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 20:53:46,771 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 20:53:46,772 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 20:53:46,772 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=526782)[0m 2024-04-05 20:53:52.901928: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=526781)[0m 2024-04-05 20:53:52.927195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=526781)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 20:53:53,732 | server.py:94 | initial parameters (loss, other metrics): 2.3024802207946777, {'accuracy': 0.0622, 'data_size': 10000}
INFO flwr 2024-04-05 20:53:53,732 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 20:53:53,733 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=526782)[0m 2024-04-05 20:53:55.059382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=526782)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=526782)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=526779)[0m 2024-04-05 20:53:53.741508: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=526779)[0m 2024-04-05 20:53:53.835505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=526779)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=526779)[0m 2024-04-05 20:53:55.979657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 20:54:09,953 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 20:54:13,321 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 20:54:17,372 | server.py:125 | fit progress: (1, 2.302246570587158, {'accuracy': 0.0776, 'data_size': 10000}, 23.639980079999077)
INFO flwr 2024-04-05 20:54:17,373 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 20:54:17,373 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:54:26,438 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 20:54:39,380 | server.py:125 | fit progress: (2, 2.302032470703125, {'accuracy': 0.0958, 'data_size': 10000}, 45.64788865900482)
INFO flwr 2024-04-05 20:54:39,381 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 20:54:39,381 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:54:47,231 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 20:55:06,247 | server.py:125 | fit progress: (3, 2.301791191101074, {'accuracy': 0.0974, 'data_size': 10000}, 72.51415056400583)
INFO flwr 2024-04-05 20:55:06,247 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 20:55:06,247 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:55:14,428 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 20:55:39,152 | server.py:125 | fit progress: (4, 2.301591634750366, {'accuracy': 0.1092, 'data_size': 10000}, 105.41940058300679)
INFO flwr 2024-04-05 20:55:39,152 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 20:55:39,152 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:55:47,595 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 20:56:18,693 | server.py:125 | fit progress: (5, 2.30137300491333, {'accuracy': 0.1822, 'data_size': 10000}, 144.96040897200874)
INFO flwr 2024-04-05 20:56:18,693 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 20:56:18,693 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:56:26,601 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 20:57:03,312 | server.py:125 | fit progress: (6, 2.301164150238037, {'accuracy': 0.23, 'data_size': 10000}, 189.57926615599717)
INFO flwr 2024-04-05 20:57:03,312 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 20:57:03,312 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:57:11,126 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 20:58:03,782 | server.py:125 | fit progress: (7, 2.3008546829223633, {'accuracy': 0.3279, 'data_size': 10000}, 250.04976248400635)
INFO flwr 2024-04-05 20:58:03,782 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 20:58:03,783 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:58:11,736 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 20:59:09,884 | server.py:125 | fit progress: (8, 2.300584077835083, {'accuracy': 0.2331, 'data_size': 10000}, 316.15182088600704)
INFO flwr 2024-04-05 20:59:09,884 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 20:59:09,885 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 20:59:18,066 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:00:25,447 | server.py:125 | fit progress: (9, 2.3002982139587402, {'accuracy': 0.3749, 'data_size': 10000}, 391.71465159600484)
INFO flwr 2024-04-05 21:00:25,447 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:00:25,448 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:00:33,374 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:01:50,533 | server.py:125 | fit progress: (10, 2.2999989986419678, {'accuracy': 0.3864, 'data_size': 10000}, 476.80099207400053)
INFO flwr 2024-04-05 21:01:50,534 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:01:50,534 | server.py:153 | FL finished in 476.8013857379992
INFO flwr 2024-04-05 21:01:50,534 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:01:50,534 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:01:50,534 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:01:50,534 | app.py:229 | app_fit: losses_centralized [(0, 2.3024802207946777), (1, 2.302246570587158), (2, 2.302032470703125), (3, 2.301791191101074), (4, 2.301591634750366), (5, 2.30137300491333), (6, 2.301164150238037), (7, 2.3008546829223633), (8, 2.300584077835083), (9, 2.3002982139587402), (10, 2.2999989986419678)]
INFO flwr 2024-04-05 21:01:50,534 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0622), (1, 0.0776), (2, 0.0958), (3, 0.0974), (4, 0.1092), (5, 0.1822), (6, 0.23), (7, 0.3279), (8, 0.2331), (9, 0.3749), (10, 0.3864)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3864
wandb:     loss 2.3
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_205328-fkkvu2h8
wandb: Find logs at: ./wandb/offline-run-20240405_205328-fkkvu2h8/logs
INFO flwr 2024-04-05 21:01:54,109 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:09:14,132 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=526771)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=526771)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:09:18,903	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:09:19,253	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:09:19,668	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5e33e8b987c9a12a.zip' (7.73MiB) to Ray cluster...
2024-04-05 21:09:19,693	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5e33e8b987c9a12a.zip'.
INFO flwr 2024-04-05 21:09:30,467 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 152062496154.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 69455355494.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 21:09:30,468 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:09:30,468 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:09:30,482 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:09:30,483 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:09:30,483 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:09:30,483 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=535312)[0m 2024-04-05 21:09:36.321796: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=535312)[0m 2024-04-05 21:09:36.419782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=535312)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 21:09:38,449 | server.py:94 | initial parameters (loss, other metrics): 2.3025336265563965, {'accuracy': 0.1267, 'data_size': 10000}
INFO flwr 2024-04-05 21:09:38,449 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:09:38,450 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=535321)[0m 2024-04-05 21:09:38.500360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=535322)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=535322)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=535316)[0m 2024-04-05 21:09:36.667567: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=535316)[0m 2024-04-05 21:09:36.760455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=535316)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=535316)[0m 2024-04-05 21:09:39.209556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=535312)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=535312)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-05 21:09:53,428 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:09:56,629 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:10:00,490 | server.py:125 | fit progress: (1, 2.302070379257202, {'accuracy': 0.1314, 'data_size': 10000}, 22.04031192799448)
INFO flwr 2024-04-05 21:10:00,490 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:10:00,490 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:10:09,504 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:10:23,321 | server.py:125 | fit progress: (2, 2.3014283180236816, {'accuracy': 0.1251, 'data_size': 10000}, 44.87169331399491)
INFO flwr 2024-04-05 21:10:23,322 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:10:23,322 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:10:31,486 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:10:53,441 | server.py:125 | fit progress: (3, 2.3007826805114746, {'accuracy': 0.1525, 'data_size': 10000}, 74.99183996400097)
INFO flwr 2024-04-05 21:10:53,442 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:10:53,442 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:11:01,421 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:11:30,537 | server.py:125 | fit progress: (4, 2.299929141998291, {'accuracy': 0.1071, 'data_size': 10000}, 112.08731094098766)
INFO flwr 2024-04-05 21:11:30,537 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:11:30,537 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:11:39,443 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:12:14,101 | server.py:125 | fit progress: (5, 2.2989492416381836, {'accuracy': 0.1853, 'data_size': 10000}, 155.65092233500036)
INFO flwr 2024-04-05 21:12:14,101 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:12:14,101 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:12:22,486 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:13:04,104 | server.py:125 | fit progress: (6, 2.2976772785186768, {'accuracy': 0.1009, 'data_size': 10000}, 205.65419175299758)
INFO flwr 2024-04-05 21:13:04,104 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:13:04,104 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:13:12,209 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:14:02,132 | server.py:125 | fit progress: (7, 2.2962374687194824, {'accuracy': 0.1009, 'data_size': 10000}, 263.6822617419966)
INFO flwr 2024-04-05 21:14:02,132 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:14:02,132 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:14:10,154 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:15:06,480 | server.py:125 | fit progress: (8, 2.2921502590179443, {'accuracy': 0.1973, 'data_size': 10000}, 328.030610582995)
INFO flwr 2024-04-05 21:15:06,481 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:15:06,481 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:15:14,631 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:16:15,151 | server.py:125 | fit progress: (9, 2.287545919418335, {'accuracy': 0.1534, 'data_size': 10000}, 396.70169556599285)
INFO flwr 2024-04-05 21:16:15,152 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:16:15,152 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:16:23,098 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:17:38,869 | server.py:125 | fit progress: (10, 2.2810466289520264, {'accuracy': 0.098, 'data_size': 10000}, 480.41891227399174)
INFO flwr 2024-04-05 21:17:38,869 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:17:38,869 | server.py:153 | FL finished in 480.419541108
INFO flwr 2024-04-05 21:17:38,869 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:17:38,869 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:17:38,869 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:17:38,870 | app.py:229 | app_fit: losses_centralized [(0, 2.3025336265563965), (1, 2.302070379257202), (2, 2.3014283180236816), (3, 2.3007826805114746), (4, 2.299929141998291), (5, 2.2989492416381836), (6, 2.2976772785186768), (7, 2.2962374687194824), (8, 2.2921502590179443), (9, 2.287545919418335), (10, 2.2810466289520264)]
INFO flwr 2024-04-05 21:17:38,870 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1267), (1, 0.1314), (2, 0.1251), (3, 0.1525), (4, 0.1071), (5, 0.1853), (6, 0.1009), (7, 0.1009), (8, 0.1973), (9, 0.1534), (10, 0.098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.098
wandb:     loss 2.28105
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_210913-bh5dnu6a
wandb: Find logs at: ./wandb/offline-run-20240405_210913-bh5dnu6a/logs
INFO flwr 2024-04-05 21:17:42,398 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:25:02,570 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=535309)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=535309)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 21:25:08,057	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:25:08,377	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:25:08,676	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8a39090e802745af.zip' (7.77MiB) to Ray cluster...
2024-04-05 21:25:08,700	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8a39090e802745af.zip'.
INFO flwr 2024-04-05 21:25:19,420 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 69345788313.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 151806839399.0}
INFO flwr 2024-04-05 21:25:19,421 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:25:19,421 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:25:19,440 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:25:19,441 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:25:19,441 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:25:19,442 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=546543)[0m 2024-04-05 21:25:25.371644: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=546543)[0m 2024-04-05 21:25:25.468250: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=546543)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 21:25:26,330 | server.py:94 | initial parameters (loss, other metrics): 2.3025062084198, {'accuracy': 0.0793, 'data_size': 10000}
INFO flwr 2024-04-05 21:25:26,331 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:25:26,331 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=546543)[0m 2024-04-05 21:25:27.588701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=546543)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=546543)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=546531)[0m 2024-04-05 21:25:25.934574: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=546531)[0m 2024-04-05 21:25:26.029767: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=546531)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=546531)[0m 2024-04-05 21:25:28.030863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:25:41,153 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:25:44,670 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:25:48,569 | server.py:125 | fit progress: (1, 2.3019914627075195, {'accuracy': 0.0618, 'data_size': 10000}, 22.23771507300262)
INFO flwr 2024-04-05 21:25:48,569 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:25:48,569 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:25:57,356 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:26:11,526 | server.py:125 | fit progress: (2, 2.301429510116577, {'accuracy': 0.0982, 'data_size': 10000}, 45.19475315400632)
INFO flwr 2024-04-05 21:26:11,526 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:26:11,526 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:26:19,150 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:26:39,794 | server.py:125 | fit progress: (3, 2.3006293773651123, {'accuracy': 0.2169, 'data_size': 10000}, 73.46274819600512)
INFO flwr 2024-04-05 21:26:39,794 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:26:39,794 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:26:47,442 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:27:15,430 | server.py:125 | fit progress: (4, 2.299459457397461, {'accuracy': 0.2866, 'data_size': 10000}, 109.09876634599641)
INFO flwr 2024-04-05 21:27:15,430 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:27:15,430 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:27:23,028 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:28:02,449 | server.py:125 | fit progress: (5, 2.2977709770202637, {'accuracy': 0.101, 'data_size': 10000}, 156.11792202100332)
INFO flwr 2024-04-05 21:28:02,449 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:28:02,449 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:28:10,604 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:28:57,193 | server.py:125 | fit progress: (6, 2.2950401306152344, {'accuracy': 0.2346, 'data_size': 10000}, 210.86231072200462)
INFO flwr 2024-04-05 21:28:57,194 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:28:57,194 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:29:06,090 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:29:53,645 | server.py:125 | fit progress: (7, 2.291334867477417, {'accuracy': 0.2783, 'data_size': 10000}, 267.314056275005)
INFO flwr 2024-04-05 21:29:53,645 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:29:53,646 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:30:00,990 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:31:02,883 | server.py:125 | fit progress: (8, 2.2831292152404785, {'accuracy': 0.098, 'data_size': 10000}, 336.5515481760085)
INFO flwr 2024-04-05 21:31:02,883 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:31:02,883 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:31:12,398 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:32:20,939 | server.py:125 | fit progress: (9, 2.266444683074951, {'accuracy': 0.1513, 'data_size': 10000}, 414.6076132150047)
INFO flwr 2024-04-05 21:32:20,939 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:32:20,939 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:32:30,755 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:33:50,868 | server.py:125 | fit progress: (10, 2.2445592880249023, {'accuracy': 0.2302, 'data_size': 10000}, 504.5369796890009)
INFO flwr 2024-04-05 21:33:50,869 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:33:50,869 | server.py:153 | FL finished in 504.5380791770003
INFO flwr 2024-04-05 21:33:50,869 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:33:50,870 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:33:50,870 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:33:50,870 | app.py:229 | app_fit: losses_centralized [(0, 2.3025062084198), (1, 2.3019914627075195), (2, 2.301429510116577), (3, 2.3006293773651123), (4, 2.299459457397461), (5, 2.2977709770202637), (6, 2.2950401306152344), (7, 2.291334867477417), (8, 2.2831292152404785), (9, 2.266444683074951), (10, 2.2445592880249023)]
INFO flwr 2024-04-05 21:33:50,870 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0793), (1, 0.0618), (2, 0.0982), (3, 0.2169), (4, 0.2866), (5, 0.101), (6, 0.2346), (7, 0.2783), (8, 0.098), (9, 0.1513), (10, 0.2302)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2302
wandb:     loss 2.24456
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_212502-nhxbhlkv
wandb: Find logs at: ./wandb/offline-run-20240405_212502-nhxbhlkv/logs
INFO flwr 2024-04-05 21:33:54,465 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:41:19,086 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=546533)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=546533)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 21:41:24,114	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:41:24,502	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:41:24,858	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6a6d640327619cad.zip' (7.79MiB) to Ray cluster...
2024-04-05 21:41:24,886	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6a6d640327619cad.zip'.
INFO flwr 2024-04-05 21:41:35,669 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 151666555904.0, 'object_store_memory': 69285666816.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 21:41:35,669 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:41:35,669 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:41:35,687 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:41:35,689 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:41:35,689 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:41:35,690 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=554935)[0m 2024-04-05 21:41:41.415327: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=554935)[0m 2024-04-05 21:41:41.543567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=554935)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 21:41:42,723 | server.py:94 | initial parameters (loss, other metrics): 2.3027071952819824, {'accuracy': 0.0545, 'data_size': 10000}
INFO flwr 2024-04-05 21:41:42,723 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:41:42,724 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=554927)[0m 2024-04-05 21:41:43.776884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=554935)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=554935)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=554925)[0m 2024-04-05 21:41:42.137443: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=554925)[0m 2024-04-05 21:41:42.229758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=554925)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=554929)[0m 2024-04-05 21:41:44.069154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=554928)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=554928)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-05 21:41:59,260 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:42:02,921 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:42:07,258 | server.py:125 | fit progress: (1, 2.3019917011260986, {'accuracy': 0.099, 'data_size': 10000}, 24.534460356997442)
INFO flwr 2024-04-05 21:42:07,259 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:42:07,259 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:42:17,673 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:42:31,895 | server.py:125 | fit progress: (2, 2.3005499839782715, {'accuracy': 0.0974, 'data_size': 10000}, 49.17116171600355)
INFO flwr 2024-04-05 21:42:31,895 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:42:31,895 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:42:40,427 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:02,934 | server.py:125 | fit progress: (3, 2.297797203063965, {'accuracy': 0.0974, 'data_size': 10000}, 80.21078273399326)
INFO flwr 2024-04-05 21:43:02,935 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:02,935 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:12,537 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:43:41,978 | server.py:125 | fit progress: (4, 2.294344425201416, {'accuracy': 0.1009, 'data_size': 10000}, 119.25462257600157)
INFO flwr 2024-04-05 21:43:41,979 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:43:41,979 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:43:51,498 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 21:44:26,104 | server.py:125 | fit progress: (5, 2.2899372577667236, {'accuracy': 0.2186, 'data_size': 10000}, 163.38070095899457)
INFO flwr 2024-04-05 21:44:26,105 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 21:44:26,105 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:44:35,554 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 21:45:18,559 | server.py:125 | fit progress: (6, 2.2780086994171143, {'accuracy': 0.1531, 'data_size': 10000}, 215.83500338799786)
INFO flwr 2024-04-05 21:45:18,559 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 21:45:18,559 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:45:28,410 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 21:46:13,137 | server.py:125 | fit progress: (7, 2.2520172595977783, {'accuracy': 0.2844, 'data_size': 10000}, 270.4130522600026)
INFO flwr 2024-04-05 21:46:13,137 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 21:46:13,137 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:46:21,732 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 21:47:24,354 | server.py:125 | fit progress: (8, 2.22350811958313, {'accuracy': 0.421, 'data_size': 10000}, 341.63042363899876)
INFO flwr 2024-04-05 21:47:24,354 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 21:47:24,355 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:47:33,317 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 21:48:41,980 | server.py:125 | fit progress: (9, 2.194972038269043, {'accuracy': 0.3694, 'data_size': 10000}, 419.256387454996)
INFO flwr 2024-04-05 21:48:41,980 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 21:48:41,981 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:48:50,584 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 21:50:06,637 | server.py:125 | fit progress: (10, 2.1404290199279785, {'accuracy': 0.3636, 'data_size': 10000}, 503.91335915899253)
INFO flwr 2024-04-05 21:50:06,637 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 21:50:06,638 | server.py:153 | FL finished in 503.91404033399886
INFO flwr 2024-04-05 21:50:06,638 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 21:50:06,638 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 21:50:06,638 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 21:50:06,638 | app.py:229 | app_fit: losses_centralized [(0, 2.3027071952819824), (1, 2.3019917011260986), (2, 2.3005499839782715), (3, 2.297797203063965), (4, 2.294344425201416), (5, 2.2899372577667236), (6, 2.2780086994171143), (7, 2.2520172595977783), (8, 2.22350811958313), (9, 2.194972038269043), (10, 2.1404290199279785)]
INFO flwr 2024-04-05 21:50:06,638 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0545), (1, 0.099), (2, 0.0974), (3, 0.0974), (4, 0.1009), (5, 0.2186), (6, 0.1531), (7, 0.2844), (8, 0.421), (9, 0.3694), (10, 0.3636)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3636
wandb:     loss 2.14043
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_214118-wjh79ato
wandb: Find logs at: ./wandb/offline-run-20240405_214118-wjh79ato/logs
INFO flwr 2024-04-05 21:50:10,170 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 21:57:35,032 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=554925)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=554925)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-05 21:57:40,462	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 21:57:40,860	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 21:57:41,200	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_48b4cf76f36537e1.zip' (7.83MiB) to Ray cluster...
2024-04-05 21:57:41,229	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_48b4cf76f36537e1.zip'.
INFO flwr 2024-04-05 21:57:51,958 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 151441635533.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69189272371.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 21:57:51,959 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 21:57:51,959 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 21:57:51,976 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 21:57:51,977 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 21:57:51,977 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 21:57:51,977 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=566716)[0m 2024-04-05 21:57:57.669529: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=566716)[0m 2024-04-05 21:57:57.768178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=566716)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 21:57:59,183 | server.py:94 | initial parameters (loss, other metrics): 2.3026270866394043, {'accuracy': 0.1033, 'data_size': 10000}
INFO flwr 2024-04-05 21:57:59,183 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 21:57:59,184 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=566716)[0m 2024-04-05 21:58:00.056276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=566718)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=566718)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=566718)[0m 2024-04-05 21:57:58.183508: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=566718)[0m 2024-04-05 21:57:58.275685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=566718)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=566717)[0m 2024-04-05 21:58:00.212031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=566711)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=566711)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 21:58:14,345 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 21:58:17,862 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 21:58:22,132 | server.py:125 | fit progress: (1, 2.3009419441223145, {'accuracy': 0.1232, 'data_size': 10000}, 22.948324498996953)
INFO flwr 2024-04-05 21:58:22,133 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 21:58:22,133 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:58:31,776 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 21:58:45,727 | server.py:125 | fit progress: (2, 2.2978901863098145, {'accuracy': 0.1846, 'data_size': 10000}, 46.543430577992694)
INFO flwr 2024-04-05 21:58:45,728 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 21:58:45,728 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:58:54,238 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 21:59:14,721 | server.py:125 | fit progress: (3, 2.2920982837677, {'accuracy': 0.1003, 'data_size': 10000}, 75.53717939299531)
INFO flwr 2024-04-05 21:59:14,721 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 21:59:14,721 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:59:23,849 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 21:59:50,131 | server.py:125 | fit progress: (4, 2.2788641452789307, {'accuracy': 0.1946, 'data_size': 10000}, 110.94745298300404)
INFO flwr 2024-04-05 21:59:50,132 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 21:59:50,132 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 21:59:58,644 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:00:37,002 | server.py:125 | fit progress: (5, 2.262003183364868, {'accuracy': 0.2477, 'data_size': 10000}, 157.81807216799643)
INFO flwr 2024-04-05 22:00:37,002 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:00:37,002 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:00:46,023 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:01:29,828 | server.py:125 | fit progress: (6, 2.2470641136169434, {'accuracy': 0.2481, 'data_size': 10000}, 210.64419656699465)
INFO flwr 2024-04-05 22:01:29,828 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:01:29,828 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:01:38,751 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:02:33,541 | server.py:125 | fit progress: (7, 2.2322592735290527, {'accuracy': 0.2016, 'data_size': 10000}, 274.35775278099754)
INFO flwr 2024-04-05 22:02:33,542 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:02:33,542 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:02:42,014 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:03:44,493 | server.py:125 | fit progress: (8, 2.189781427383423, {'accuracy': 0.3105, 'data_size': 10000}, 345.30890793299477)
INFO flwr 2024-04-05 22:03:44,493 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:03:44,493 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:03:53,405 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:04:52,756 | server.py:125 | fit progress: (9, 2.103619337081909, {'accuracy': 0.4216, 'data_size': 10000}, 413.57234930100094)
INFO flwr 2024-04-05 22:04:52,756 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:04:52,757 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:05:01,418 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:06:04,510 | server.py:125 | fit progress: (10, 2.0722122192382812, {'accuracy': 0.4462, 'data_size': 10000}, 485.3263979799958)
INFO flwr 2024-04-05 22:06:04,510 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:06:04,511 | server.py:153 | FL finished in 485.32691219399567
INFO flwr 2024-04-05 22:06:04,514 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:06:04,514 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:06:04,514 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:06:04,515 | app.py:229 | app_fit: losses_centralized [(0, 2.3026270866394043), (1, 2.3009419441223145), (2, 2.2978901863098145), (3, 2.2920982837677), (4, 2.2788641452789307), (5, 2.262003183364868), (6, 2.2470641136169434), (7, 2.2322592735290527), (8, 2.189781427383423), (9, 2.103619337081909), (10, 2.0722122192382812)]
INFO flwr 2024-04-05 22:06:04,515 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1033), (1, 0.1232), (2, 0.1846), (3, 0.1003), (4, 0.1946), (5, 0.2477), (6, 0.2481), (7, 0.2016), (8, 0.3105), (9, 0.4216), (10, 0.4462)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4462
wandb:     loss 2.07221
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_215734-lq8fg6v4
wandb: Find logs at: ./wandb/offline-run-20240405_215734-lq8fg6v4/logs
INFO flwr 2024-04-05 22:06:08,092 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:13:32,518 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-05 22:13:37,249	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:13:37,669	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:13:37,979	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf68eebb5ac79603.zip' (7.86MiB) to Ray cluster...
2024-04-05 22:13:37,998	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf68eebb5ac79603.zip'.
INFO flwr 2024-04-05 22:13:48,917 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 151458328372.0, 'object_store_memory': 69196426444.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 22:13:48,917 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:13:48,917 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:13:48,931 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:13:48,932 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:13:48,932 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:13:48,932 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=578269)[0m 2024-04-05 22:13:55.014338: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=578272)[0m 2024-04-05 22:13:55.128506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=578272)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 22:13:56,136 | server.py:94 | initial parameters (loss, other metrics): 2.302152156829834, {'accuracy': 0.1248, 'data_size': 10000}
INFO flwr 2024-04-05 22:13:56,137 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:13:56,138 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=578270)[0m 2024-04-05 22:13:57.137054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=578276)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=578276)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=578276)[0m 2024-04-05 22:13:55.300816: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=578276)[0m 2024-04-05 22:13:55.394092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=578276)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=578268)[0m 2024-04-05 22:13:57.596541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=578268)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=578268)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-05 22:14:12,543 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:14:16,131 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:14:20,416 | server.py:125 | fit progress: (1, 2.2997381687164307, {'accuracy': 0.0974, 'data_size': 10000}, 24.278256316989427)
INFO flwr 2024-04-05 22:14:20,416 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:14:20,416 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:14:30,262 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:14:44,339 | server.py:125 | fit progress: (2, 2.304246187210083, {'accuracy': 0.1009, 'data_size': 10000}, 48.20138166399556)
INFO flwr 2024-04-05 22:14:44,339 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:14:44,339 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:14:52,645 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:15:12,942 | server.py:125 | fit progress: (3, 2.29288911819458, {'accuracy': 0.2049, 'data_size': 10000}, 76.80490324099082)
INFO flwr 2024-04-05 22:15:12,942 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:15:12,943 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:15:21,384 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:15:47,800 | server.py:125 | fit progress: (4, 2.2836573123931885, {'accuracy': 0.1386, 'data_size': 10000}, 111.66316749498947)
INFO flwr 2024-04-05 22:15:47,801 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:15:47,801 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:15:56,370 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:16:27,900 | server.py:125 | fit progress: (5, 2.2702839374542236, {'accuracy': 0.269, 'data_size': 10000}, 151.76308723099646)
INFO flwr 2024-04-05 22:16:27,901 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:16:27,901 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:16:36,470 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:17:19,591 | server.py:125 | fit progress: (6, 2.2680654525756836, {'accuracy': 0.1837, 'data_size': 10000}, 203.45407831399643)
INFO flwr 2024-04-05 22:17:19,592 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:17:19,592 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:17:28,991 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:18:25,427 | server.py:125 | fit progress: (7, 2.249014377593994, {'accuracy': 0.2407, 'data_size': 10000}, 269.2895476329868)
INFO flwr 2024-04-05 22:18:25,427 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:18:25,427 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:18:34,123 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:19:34,913 | server.py:125 | fit progress: (8, 2.220151662826538, {'accuracy': 0.2749, 'data_size': 10000}, 338.7760559629969)
INFO flwr 2024-04-05 22:19:34,914 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:19:34,914 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:19:43,361 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:20:52,524 | server.py:125 | fit progress: (9, 2.2107324600219727, {'accuracy': 0.2849, 'data_size': 10000}, 416.38634833699325)
INFO flwr 2024-04-05 22:20:52,524 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:20:52,524 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:21:01,480 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:22:09,524 | server.py:125 | fit progress: (10, 2.1555371284484863, {'accuracy': 0.4736, 'data_size': 10000}, 493.38698033099354)
INFO flwr 2024-04-05 22:22:09,525 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:22:09,525 | server.py:153 | FL finished in 493.3876466429938
INFO flwr 2024-04-05 22:22:09,525 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:22:09,525 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:22:09,525 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:22:09,525 | app.py:229 | app_fit: losses_centralized [(0, 2.302152156829834), (1, 2.2997381687164307), (2, 2.304246187210083), (3, 2.29288911819458), (4, 2.2836573123931885), (5, 2.2702839374542236), (6, 2.2680654525756836), (7, 2.249014377593994), (8, 2.220151662826538), (9, 2.2107324600219727), (10, 2.1555371284484863)]
INFO flwr 2024-04-05 22:22:09,525 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1248), (1, 0.0974), (2, 0.1009), (3, 0.2049), (4, 0.1386), (5, 0.269), (6, 0.1837), (7, 0.2407), (8, 0.2749), (9, 0.2849), (10, 0.4736)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4736
wandb:     loss 2.15554
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_221332-lpci3sba
wandb: Find logs at: ./wandb/offline-run-20240405_221332-lpci3sba/logs
INFO flwr 2024-04-05 22:22:13,135 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:29:33,347 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=578263)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=578263)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 22:29:39,196	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:29:39,538	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:29:39,943	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c0c057a984ccb824.zip' (7.89MiB) to Ray cluster...
2024-04-05 22:29:39,965	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c0c057a984ccb824.zip'.
INFO flwr 2024-04-05 22:29:50,904 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 68997139660.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 150993325876.0}
INFO flwr 2024-04-05 22:29:50,904 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:29:50,904 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:29:50,920 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:29:50,922 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:29:50,922 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:29:50,922 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=586709)[0m 2024-04-05 22:29:56.836641: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=586709)[0m 2024-04-05 22:29:56.937091: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=586709)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 22:29:58,633 | server.py:94 | initial parameters (loss, other metrics): 2.302633762359619, {'accuracy': 0.0974, 'data_size': 10000}
INFO flwr 2024-04-05 22:29:58,633 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:29:58,633 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=586713)[0m 2024-04-05 22:29:59.031033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=586720)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=586720)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=586712)[0m 2024-04-05 22:29:57.060233: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=586712)[0m 2024-04-05 22:29:57.149454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=586712)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=586712)[0m 2024-04-05 22:29:59.213199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:30:13,844 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:30:17,330 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:30:21,203 | server.py:125 | fit progress: (1, 2.302626848220825, {'accuracy': 0.0974, 'data_size': 10000}, 22.570131249987753)
INFO flwr 2024-04-05 22:30:21,204 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:30:21,204 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:30:30,527 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:30:44,588 | server.py:125 | fit progress: (2, 2.3026206493377686, {'accuracy': 0.0974, 'data_size': 10000}, 45.95459739799844)
INFO flwr 2024-04-05 22:30:44,588 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:30:44,589 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:30:52,865 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:31:14,042 | server.py:125 | fit progress: (3, 2.3026130199432373, {'accuracy': 0.0974, 'data_size': 10000}, 75.40817537199473)
INFO flwr 2024-04-05 22:31:14,042 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:31:14,042 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:31:22,447 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:31:50,066 | server.py:125 | fit progress: (4, 2.3026068210601807, {'accuracy': 0.0974, 'data_size': 10000}, 111.43302263900114)
INFO flwr 2024-04-05 22:31:50,067 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:31:50,067 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:31:58,263 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:32:31,058 | server.py:125 | fit progress: (5, 2.3025991916656494, {'accuracy': 0.0974, 'data_size': 10000}, 152.42427036599838)
INFO flwr 2024-04-05 22:32:31,058 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:32:31,058 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:32:40,786 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:33:31,287 | server.py:125 | fit progress: (6, 2.3025927543640137, {'accuracy': 0.0975, 'data_size': 10000}, 212.65352021499712)
INFO flwr 2024-04-05 22:33:31,287 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:33:31,288 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:33:39,719 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:34:26,545 | server.py:125 | fit progress: (7, 2.3025851249694824, {'accuracy': 0.0975, 'data_size': 10000}, 267.9115543730004)
INFO flwr 2024-04-05 22:34:26,545 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:34:26,546 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:34:36,223 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:35:29,761 | server.py:125 | fit progress: (8, 2.302577257156372, {'accuracy': 0.0975, 'data_size': 10000}, 331.1274194449943)
INFO flwr 2024-04-05 22:35:29,761 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:35:29,761 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:35:39,097 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:36:35,756 | server.py:125 | fit progress: (9, 2.3025729656219482, {'accuracy': 0.0975, 'data_size': 10000}, 397.1221661140007)
INFO flwr 2024-04-05 22:36:35,756 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:36:35,756 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:36:44,895 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:38:04,632 | server.py:125 | fit progress: (10, 2.302563190460205, {'accuracy': 0.0975, 'data_size': 10000}, 485.99867912899936)
INFO flwr 2024-04-05 22:38:04,633 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:38:04,633 | server.py:153 | FL finished in 485.9996995399997
INFO flwr 2024-04-05 22:38:04,633 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:38:04,634 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:38:04,634 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:38:04,634 | app.py:229 | app_fit: losses_centralized [(0, 2.302633762359619), (1, 2.302626848220825), (2, 2.3026206493377686), (3, 2.3026130199432373), (4, 2.3026068210601807), (5, 2.3025991916656494), (6, 2.3025927543640137), (7, 2.3025851249694824), (8, 2.302577257156372), (9, 2.3025729656219482), (10, 2.302563190460205)]
INFO flwr 2024-04-05 22:38:04,634 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0974), (1, 0.0974), (2, 0.0974), (3, 0.0974), (4, 0.0974), (5, 0.0974), (6, 0.0975), (7, 0.0975), (8, 0.0975), (9, 0.0975), (10, 0.0975)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0975
wandb:     loss 2.30256
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_222932-lkh9lz4j
wandb: Find logs at: ./wandb/offline-run-20240405_222932-lkh9lz4j/logs
INFO flwr 2024-04-05 22:38:08,300 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 22:45:33,013 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=586709)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=586709)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-05 22:45:39,002	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 22:45:39,381	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 22:45:39,712	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6628b6f3b2e1802f.zip' (7.93MiB) to Ray cluster...
2024-04-05 22:45:39,731	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6628b6f3b2e1802f.zip'.
INFO flwr 2024-04-05 22:45:50,479 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 150614732186.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68834885222.0}
INFO flwr 2024-04-05 22:45:50,480 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 22:45:50,480 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 22:45:50,497 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 22:45:50,498 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 22:45:50,498 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 22:45:50,498 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=597955)[0m 2024-04-05 22:45:56.453716: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=597955)[0m 2024-04-05 22:45:56.543964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=597955)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 22:45:57,832 | server.py:94 | initial parameters (loss, other metrics): 2.302518606185913, {'accuracy': 0.0982, 'data_size': 10000}
INFO flwr 2024-04-05 22:45:57,833 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 22:45:57,834 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=597957)[0m 2024-04-05 22:45:58.582800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=597963)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=597963)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=597963)[0m 2024-04-05 22:45:57.009201: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=597963)[0m 2024-04-05 22:45:57.104308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=597963)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=597963)[0m 2024-04-05 22:45:58.904800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=597955)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=597955)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-05 22:46:13,244 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 22:46:16,635 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 22:46:20,986 | server.py:125 | fit progress: (1, 2.3022515773773193, {'accuracy': 0.0982, 'data_size': 10000}, 23.153320707002422)
INFO flwr 2024-04-05 22:46:20,987 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 22:46:20,987 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:46:31,311 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 22:46:45,637 | server.py:125 | fit progress: (2, 2.3020143508911133, {'accuracy': 0.0982, 'data_size': 10000}, 47.803978729993105)
INFO flwr 2024-04-05 22:46:45,638 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 22:46:45,638 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:46:54,350 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 22:47:16,343 | server.py:125 | fit progress: (3, 2.301654577255249, {'accuracy': 0.1816, 'data_size': 10000}, 78.50970422499813)
INFO flwr 2024-04-05 22:47:16,343 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 22:47:16,343 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:47:25,785 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 22:47:52,467 | server.py:125 | fit progress: (4, 2.301362991333008, {'accuracy': 0.1246, 'data_size': 10000}, 114.63398742899881)
INFO flwr 2024-04-05 22:47:52,468 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 22:47:52,468 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:48:01,663 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 22:48:34,234 | server.py:125 | fit progress: (5, 2.300894021987915, {'accuracy': 0.2431, 'data_size': 10000}, 156.40123111499997)
INFO flwr 2024-04-05 22:48:34,235 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 22:48:34,235 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:48:43,343 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 22:49:27,279 | server.py:125 | fit progress: (6, 2.300499677658081, {'accuracy': 0.0963, 'data_size': 10000}, 209.4461002219905)
INFO flwr 2024-04-05 22:49:27,280 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 22:49:27,280 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:49:36,380 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 22:50:30,278 | server.py:125 | fit progress: (7, 2.299966812133789, {'accuracy': 0.0966, 'data_size': 10000}, 272.4447241460002)
INFO flwr 2024-04-05 22:50:30,278 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 22:50:30,278 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:50:38,846 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 22:51:36,566 | server.py:125 | fit progress: (8, 2.2992327213287354, {'accuracy': 0.1737, 'data_size': 10000}, 338.73333573499986)
INFO flwr 2024-04-05 22:51:36,567 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 22:51:36,567 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:51:46,065 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 22:52:54,913 | server.py:125 | fit progress: (9, 2.298333168029785, {'accuracy': 0.0958, 'data_size': 10000}, 417.0793166549993)
INFO flwr 2024-04-05 22:52:54,913 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 22:52:54,914 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 22:53:03,515 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 22:54:35,744 | server.py:125 | fit progress: (10, 2.2969133853912354, {'accuracy': 0.101, 'data_size': 10000}, 517.9106720179989)
INFO flwr 2024-04-05 22:54:35,745 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 22:54:35,745 | server.py:153 | FL finished in 517.9115497619932
INFO flwr 2024-04-05 22:54:35,745 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 22:54:35,745 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 22:54:35,745 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 22:54:35,745 | app.py:229 | app_fit: losses_centralized [(0, 2.302518606185913), (1, 2.3022515773773193), (2, 2.3020143508911133), (3, 2.301654577255249), (4, 2.301362991333008), (5, 2.300894021987915), (6, 2.300499677658081), (7, 2.299966812133789), (8, 2.2992327213287354), (9, 2.298333168029785), (10, 2.2969133853912354)]
INFO flwr 2024-04-05 22:54:35,745 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0982), (1, 0.0982), (2, 0.0982), (3, 0.1816), (4, 0.1246), (5, 0.2431), (6, 0.0963), (7, 0.0966), (8, 0.1737), (9, 0.0958), (10, 0.101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.101
wandb:     loss 2.29691
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_224532-26bld6hc
wandb: Find logs at: ./wandb/offline-run-20240405_224532-26bld6hc/logs
INFO flwr 2024-04-05 22:54:39,451 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:01:59,052 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-05 23:02:05,493	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:02:05,840	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:02:06,168	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_99058bf60fffcf11.zip' (7.95MiB) to Ray cluster...
2024-04-05 23:02:06,185	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_99058bf60fffcf11.zip'.
INFO flwr 2024-04-05 23:02:17,143 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68647220428.0, 'memory': 150176847668.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-05 23:02:17,143 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:02:17,144 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:02:17,159 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:02:17,159 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:02:17,159 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:02:17,160 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=607124)[0m 2024-04-05 23:02:23.182152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=607124)[0m 2024-04-05 23:02:23.304158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=607124)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 23:02:24,932 | server.py:94 | initial parameters (loss, other metrics): 2.302527666091919, {'accuracy': 0.0673, 'data_size': 10000}
INFO flwr 2024-04-05 23:02:24,933 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:02:24,934 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=607124)[0m 2024-04-05 23:02:25.327297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=607124)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=607124)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=607088)[0m 2024-04-05 23:02:23.502024: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=607088)[0m 2024-04-05 23:02:23.606488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=607088)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=607088)[0m 2024-04-05 23:02:25.816287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=607085)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=607085)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=607083)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=607083)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-05 23:02:46,756 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:02:50,145 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:02:54,205 | server.py:125 | fit progress: (1, 2.301401376724243, {'accuracy': 0.0982, 'data_size': 10000}, 29.27192937899963)
INFO flwr 2024-04-05 23:02:54,206 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:02:54,206 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:03,677 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:03:16,737 | server.py:125 | fit progress: (2, 2.2992594242095947, {'accuracy': 0.1017, 'data_size': 10000}, 51.803919113997836)
INFO flwr 2024-04-05 23:03:16,738 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:16,738 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:25,242 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:03:45,807 | server.py:125 | fit progress: (3, 2.2936174869537354, {'accuracy': 0.1617, 'data_size': 10000}, 80.87415750299988)
INFO flwr 2024-04-05 23:03:45,808 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:03:45,808 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:03:53,815 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:04:21,833 | server.py:125 | fit progress: (4, 2.281764507293701, {'accuracy': 0.258, 'data_size': 10000}, 116.89987710400601)
INFO flwr 2024-04-05 23:04:21,833 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:04:21,834 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:04:30,498 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:05:05,815 | server.py:125 | fit progress: (5, 2.265624523162842, {'accuracy': 0.1108, 'data_size': 10000}, 160.8814943190082)
INFO flwr 2024-04-05 23:05:05,815 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:05:05,815 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:05:13,726 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:05:54,589 | server.py:125 | fit progress: (6, 2.2239367961883545, {'accuracy': 0.3166, 'data_size': 10000}, 209.65552024800854)
INFO flwr 2024-04-05 23:05:54,589 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:05:54,589 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:06:03,028 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:06:51,716 | server.py:125 | fit progress: (7, 2.2171778678894043, {'accuracy': 0.1814, 'data_size': 10000}, 266.7825373720116)
INFO flwr 2024-04-05 23:06:51,716 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:06:51,716 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:06:59,585 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:07:55,486 | server.py:125 | fit progress: (8, 2.132993221282959, {'accuracy': 0.3657, 'data_size': 10000}, 330.55305829900317)
INFO flwr 2024-04-05 23:07:55,487 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:07:55,487 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:08:03,989 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:09:04,539 | server.py:125 | fit progress: (9, 2.058239698410034, {'accuracy': 0.4137, 'data_size': 10000}, 399.6060934020061)
INFO flwr 2024-04-05 23:09:04,540 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:09:04,540 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:09:12,927 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:10:30,465 | server.py:125 | fit progress: (10, 2.0020668506622314, {'accuracy': 0.4923, 'data_size': 10000}, 485.5312877430115)
INFO flwr 2024-04-05 23:10:30,465 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:10:30,465 | server.py:153 | FL finished in 485.53171341199777
INFO flwr 2024-04-05 23:10:30,465 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:10:30,465 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:10:30,465 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:10:30,465 | app.py:229 | app_fit: losses_centralized [(0, 2.302527666091919), (1, 2.301401376724243), (2, 2.2992594242095947), (3, 2.2936174869537354), (4, 2.281764507293701), (5, 2.265624523162842), (6, 2.2239367961883545), (7, 2.2171778678894043), (8, 2.132993221282959), (9, 2.058239698410034), (10, 2.0020668506622314)]
INFO flwr 2024-04-05 23:10:30,465 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0673), (1, 0.0982), (2, 0.1017), (3, 0.1617), (4, 0.258), (5, 0.1108), (6, 0.3166), (7, 0.1814), (8, 0.3657), (9, 0.4137), (10, 0.4923)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4923
wandb:     loss 2.00207
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_230158-0o089uis
wandb: Find logs at: ./wandb/offline-run-20240405_230158-0o089uis/logs
INFO flwr 2024-04-05 23:10:34,045 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:17:54,102 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=607082)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=607082)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 23:17:59,581	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:17:59,868	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:18:00,181	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e481c0fc7c520b64.zip' (7.99MiB) to Ray cluster...
2024-04-05 23:18:00,197	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e481c0fc7c520b64.zip'.
INFO flwr 2024-04-05 23:18:11,255 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 68376074649.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'memory': 149544174183.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-05 23:18:11,255 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:18:11,255 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:18:11,270 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:18:11,271 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:18:11,271 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:18:11,271 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=618283)[0m 2024-04-05 23:18:17.141549: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=618283)[0m 2024-04-05 23:18:17.239363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=618283)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 23:18:19,095 | server.py:94 | initial parameters (loss, other metrics): 2.3026843070983887, {'accuracy': 0.1055, 'data_size': 10000}
INFO flwr 2024-04-05 23:18:19,096 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:18:19,096 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=618283)[0m 2024-04-05 23:18:19.374316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=618290)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=618290)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=618282)[0m 2024-04-05 23:18:17.651716: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=618282)[0m 2024-04-05 23:18:17.749974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=618282)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=618282)[0m 2024-04-05 23:18:19.816488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=618281)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=618281)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-05 23:18:34,349 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:18:37,925 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:18:41,773 | server.py:125 | fit progress: (1, 2.299661874771118, {'accuracy': 0.1887, 'data_size': 10000}, 22.677039734000573)
INFO flwr 2024-04-05 23:18:41,773 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:18:41,774 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:18:50,963 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:19:05,085 | server.py:125 | fit progress: (2, 2.2902133464813232, {'accuracy': 0.3052, 'data_size': 10000}, 45.988874210001086)
INFO flwr 2024-04-05 23:19:05,085 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:19:05,085 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:19:13,720 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:19:34,917 | server.py:125 | fit progress: (3, 2.2707698345184326, {'accuracy': 0.2627, 'data_size': 10000}, 75.82060067501152)
INFO flwr 2024-04-05 23:19:34,917 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:19:34,917 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:19:42,596 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:20:10,389 | server.py:125 | fit progress: (4, 2.225956916809082, {'accuracy': 0.2896, 'data_size': 10000}, 111.29274050600361)
INFO flwr 2024-04-05 23:20:10,389 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:20:10,389 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:20:18,750 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:20:52,909 | server.py:125 | fit progress: (5, 2.1842174530029297, {'accuracy': 0.3127, 'data_size': 10000}, 153.81293572401046)
INFO flwr 2024-04-05 23:20:52,909 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:20:52,909 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:21:00,715 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:21:47,964 | server.py:125 | fit progress: (6, 2.123839855194092, {'accuracy': 0.4927, 'data_size': 10000}, 208.86779449600726)
INFO flwr 2024-04-05 23:21:47,964 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:21:47,964 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:21:56,419 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:22:52,569 | server.py:125 | fit progress: (7, 2.0130534172058105, {'accuracy': 0.5211, 'data_size': 10000}, 273.4726046920114)
INFO flwr 2024-04-05 23:22:52,569 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:22:52,569 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:23:01,069 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:24:19,367 | server.py:125 | fit progress: (8, 1.9423017501831055, {'accuracy': 0.5477, 'data_size': 10000}, 360.2710236920102)
INFO flwr 2024-04-05 23:24:19,367 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:24:19,368 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:24:27,758 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:26:01,029 | server.py:125 | fit progress: (9, 1.859848976135254, {'accuracy': 0.6564, 'data_size': 10000}, 461.93324771100015)
INFO flwr 2024-04-05 23:26:01,030 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:26:01,030 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:26:10,730 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:27:50,997 | server.py:125 | fit progress: (10, 1.806256651878357, {'accuracy': 0.6926, 'data_size': 10000}, 571.9012146580062)
INFO flwr 2024-04-05 23:27:50,998 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:27:50,998 | server.py:153 | FL finished in 571.9023424370098
INFO flwr 2024-04-05 23:27:50,999 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:27:50,999 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:27:50,999 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:27:50,999 | app.py:229 | app_fit: losses_centralized [(0, 2.3026843070983887), (1, 2.299661874771118), (2, 2.2902133464813232), (3, 2.2707698345184326), (4, 2.225956916809082), (5, 2.1842174530029297), (6, 2.123839855194092), (7, 2.0130534172058105), (8, 1.9423017501831055), (9, 1.859848976135254), (10, 1.806256651878357)]
INFO flwr 2024-04-05 23:27:51,000 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1055), (1, 0.1887), (2, 0.3052), (3, 0.2627), (4, 0.2896), (5, 0.3127), (6, 0.4927), (7, 0.5211), (8, 0.5477), (9, 0.6564), (10, 0.6926)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6926
wandb:     loss 1.80626
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_231753-63u6vxdf
wandb: Find logs at: ./wandb/offline-run-20240405_231753-63u6vxdf/logs
INFO flwr 2024-04-05 23:27:54,560 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:35:19,764 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=618278)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=618278)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-05 23:35:26,286	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:35:26,693	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:35:26,995	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4a34570305e38aab.zip' (8.02MiB) to Ray cluster...
2024-04-05 23:35:27,016	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4a34570305e38aab.zip'.
INFO flwr 2024-04-05 23:35:38,056 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 149701927527.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 68443683225.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-05 23:35:38,056 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:35:38,056 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:35:38,072 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:35:38,074 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:35:38,075 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:35:38,075 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=629865)[0m 2024-04-05 23:35:44.160817: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=629864)[0m 2024-04-05 23:35:44.234632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=629864)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 23:35:46,032 | server.py:94 | initial parameters (loss, other metrics): 2.302581548690796, {'accuracy': 0.0973, 'data_size': 10000}
INFO flwr 2024-04-05 23:35:46,033 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:35:46,034 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=629864)[0m 2024-04-05 23:35:46.337586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=629865)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=629865)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=629862)[0m 2024-04-05 23:35:44.530876: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=629862)[0m 2024-04-05 23:35:44.625630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=629862)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=629862)[0m 2024-04-05 23:35:46.903569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=629860)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=629860)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-05 23:36:02,052 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:36:05,662 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:36:10,056 | server.py:125 | fit progress: (1, 2.301199197769165, {'accuracy': 0.1028, 'data_size': 10000}, 24.02216553800099)
INFO flwr 2024-04-05 23:36:10,056 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:36:10,057 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:36:19,061 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:36:32,576 | server.py:125 | fit progress: (2, 2.2959818840026855, {'accuracy': 0.1011, 'data_size': 10000}, 46.542115485994145)
INFO flwr 2024-04-05 23:36:32,576 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:36:32,577 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:36:40,932 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:37:02,205 | server.py:125 | fit progress: (3, 2.287222385406494, {'accuracy': 0.19, 'data_size': 10000}, 76.17194176999328)
INFO flwr 2024-04-05 23:37:02,206 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:37:02,206 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:37:10,335 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:37:43,117 | server.py:125 | fit progress: (4, 2.259185552597046, {'accuracy': 0.2444, 'data_size': 10000}, 117.08359937799105)
INFO flwr 2024-04-05 23:37:43,117 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:37:43,118 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:37:51,533 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:38:35,708 | server.py:125 | fit progress: (5, 2.244926691055298, {'accuracy': 0.1377, 'data_size': 10000}, 169.6749885219906)
INFO flwr 2024-04-05 23:38:35,709 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:38:35,709 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:38:44,281 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:39:54,265 | server.py:125 | fit progress: (6, 2.174791097640991, {'accuracy': 0.5073, 'data_size': 10000}, 248.2321003049874)
INFO flwr 2024-04-05 23:39:54,266 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:39:54,266 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:40:02,685 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:41:07,345 | server.py:125 | fit progress: (7, 2.0950698852539062, {'accuracy': 0.4604, 'data_size': 10000}, 321.3114160799887)
INFO flwr 2024-04-05 23:41:07,345 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:41:07,345 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:41:15,427 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:42:16,376 | server.py:125 | fit progress: (8, 2.0749475955963135, {'accuracy': 0.399, 'data_size': 10000}, 390.3425884509925)
INFO flwr 2024-04-05 23:42:16,376 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:42:16,376 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:42:24,768 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:43:26,712 | server.py:125 | fit progress: (9, 1.9455113410949707, {'accuracy': 0.5731, 'data_size': 10000}, 460.67831840999133)
INFO flwr 2024-04-05 23:43:26,712 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:43:26,712 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:43:34,426 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-05 23:44:45,254 | server.py:125 | fit progress: (10, 1.8171181678771973, {'accuracy': 0.6943, 'data_size': 10000}, 539.2204801629996)
INFO flwr 2024-04-05 23:44:45,254 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-05 23:44:45,254 | server.py:153 | FL finished in 539.2209875789995
INFO flwr 2024-04-05 23:44:45,257 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-05 23:44:45,257 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-05 23:44:45,257 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-05 23:44:45,257 | app.py:229 | app_fit: losses_centralized [(0, 2.302581548690796), (1, 2.301199197769165), (2, 2.2959818840026855), (3, 2.287222385406494), (4, 2.259185552597046), (5, 2.244926691055298), (6, 2.174791097640991), (7, 2.0950698852539062), (8, 2.0749475955963135), (9, 1.9455113410949707), (10, 1.8171181678771973)]
INFO flwr 2024-04-05 23:44:45,257 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0973), (1, 0.1028), (2, 0.1011), (3, 0.19), (4, 0.2444), (5, 0.1377), (6, 0.5073), (7, 0.4604), (8, 0.399), (9, 0.5731), (10, 0.6943)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6943
wandb:     loss 1.81712
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_233519-zp01anz6
wandb: Find logs at: ./wandb/offline-run-20240405_233519-zp01anz6/logs
INFO flwr 2024-04-05 23:44:48,865 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-05 23:52:09,144 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=629859)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=629859)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-05 23:52:14,368	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-05 23:52:14,669	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-05 23:52:14,953	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_db2b266f02e1324a.zip' (8.05MiB) to Ray cluster...
2024-04-05 23:52:14,970	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_db2b266f02e1324a.zip'.
INFO flwr 2024-04-05 23:52:25,898 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 149537548084.0, 'node:__internal_head__': 1.0, 'object_store_memory': 68373234892.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-05 23:52:25,898 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-05 23:52:25,898 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-05 23:52:25,913 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-05 23:52:25,914 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-05 23:52:25,914 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-05 23:52:25,914 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=638000)[0m 2024-04-05 23:52:31.781445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=638000)[0m 2024-04-05 23:52:31.878324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=638000)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-05 23:52:33,972 | server.py:94 | initial parameters (loss, other metrics): 2.302260637283325, {'accuracy': 0.1242, 'data_size': 10000}
INFO flwr 2024-04-05 23:52:33,972 | server.py:104 | FL starting
DEBUG flwr 2024-04-05 23:52:33,973 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=638007)[0m 2024-04-05 23:52:33.906843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=638004)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=638004)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=638003)[0m 2024-04-05 23:52:31.927909: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=638003)[0m 2024-04-05 23:52:32.018422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=638003)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=637997)[0m 2024-04-05 23:52:34.376175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=637996)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=637996)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-05 23:52:50,251 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-05 23:52:53,703 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-05 23:52:57,582 | server.py:125 | fit progress: (1, 2.2988383769989014, {'accuracy': 0.1334, 'data_size': 10000}, 23.609350459999405)
INFO flwr 2024-04-05 23:52:57,582 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-05 23:52:57,583 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:53:06,720 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-05 23:53:21,911 | server.py:125 | fit progress: (2, 2.2981984615325928, {'accuracy': 0.1009, 'data_size': 10000}, 47.938210753010935)
INFO flwr 2024-04-05 23:53:21,912 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-05 23:53:21,912 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:53:30,370 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-05 23:53:51,637 | server.py:125 | fit progress: (3, 2.2911477088928223, {'accuracy': 0.1229, 'data_size': 10000}, 77.66405660599412)
INFO flwr 2024-04-05 23:53:51,637 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-05 23:53:51,637 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:53:59,969 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-05 23:54:27,297 | server.py:125 | fit progress: (4, 2.2649502754211426, {'accuracy': 0.2037, 'data_size': 10000}, 113.32408802201098)
INFO flwr 2024-04-05 23:54:27,297 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-05 23:54:27,297 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:54:35,608 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-05 23:55:11,843 | server.py:125 | fit progress: (5, 2.208251714706421, {'accuracy': 0.348, 'data_size': 10000}, 157.86991307900462)
INFO flwr 2024-04-05 23:55:11,843 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-05 23:55:11,843 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:55:19,939 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-05 23:56:01,124 | server.py:125 | fit progress: (6, 2.2122998237609863, {'accuracy': 0.2297, 'data_size': 10000}, 207.15126315200177)
INFO flwr 2024-04-05 23:56:01,124 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-05 23:56:01,124 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:56:09,408 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-05 23:56:58,900 | server.py:125 | fit progress: (7, 2.1682190895080566, {'accuracy': 0.2306, 'data_size': 10000}, 264.9278113779874)
INFO flwr 2024-04-05 23:56:58,901 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-05 23:56:58,901 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:57:06,477 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-05 23:57:57,763 | server.py:125 | fit progress: (8, 2.0156657695770264, {'accuracy': 0.54, 'data_size': 10000}, 323.790776164984)
INFO flwr 2024-04-05 23:57:57,764 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-05 23:57:57,764 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:58:05,425 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-05 23:59:14,433 | server.py:125 | fit progress: (9, 1.9448349475860596, {'accuracy': 0.5218, 'data_size': 10000}, 400.4605097820022)
INFO flwr 2024-04-05 23:59:14,433 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-05 23:59:14,434 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-05 23:59:22,904 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:00:36,764 | server.py:125 | fit progress: (10, 1.8447763919830322, {'accuracy': 0.665, 'data_size': 10000}, 482.791018066011)
INFO flwr 2024-04-06 00:00:36,764 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:00:36,764 | server.py:153 | FL finished in 482.7914049469837
INFO flwr 2024-04-06 00:00:36,767 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:00:36,767 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:00:36,767 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:00:36,767 | app.py:229 | app_fit: losses_centralized [(0, 2.302260637283325), (1, 2.2988383769989014), (2, 2.2981984615325928), (3, 2.2911477088928223), (4, 2.2649502754211426), (5, 2.208251714706421), (6, 2.2122998237609863), (7, 2.1682190895080566), (8, 2.0156657695770264), (9, 1.9448349475860596), (10, 1.8447763919830322)]
INFO flwr 2024-04-06 00:00:36,767 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1242), (1, 0.1334), (2, 0.1009), (3, 0.1229), (4, 0.2037), (5, 0.348), (6, 0.2297), (7, 0.2306), (8, 0.54), (9, 0.5218), (10, 0.665)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.665
wandb:     loss 1.84478
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240405_235208-7e5377s6
wandb: Find logs at: ./wandb/offline-run-20240405_235208-7e5377s6/logs
INFO flwr 2024-04-06 00:00:40,324 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:08:00,785 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=637993)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=637993)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 00:08:05,628	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:08:05,902	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:08:06,222	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0c15c48816db41a3.zip' (8.09MiB) to Ray cluster...
2024-04-06 00:08:06,243	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0c15c48816db41a3.zip'.
INFO flwr 2024-04-06 00:08:17,240 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 149031742464.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68156461056.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 00:08:17,241 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:08:17,241 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:08:17,263 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:08:17,264 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:08:17,264 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:08:17,265 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=650122)[0m 2024-04-06 00:08:22.587584: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=650122)[0m 2024-04-06 00:08:22.679336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=650122)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=650123)[0m 2024-04-06 00:08:24.876607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 00:08:25,590 | server.py:94 | initial parameters (loss, other metrics): 2.3028411865234375, {'accuracy': 0.0599, 'data_size': 10000}
INFO flwr 2024-04-06 00:08:25,590 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:08:25,591 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=650123)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=650123)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=650119)[0m 2024-04-06 00:08:23.461646: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=650119)[0m 2024-04-06 00:08:23.533313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=650119)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=650121)[0m 2024-04-06 00:08:25.583486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=650115)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=650115)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 00:08:39,973 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:08:43,237 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:08:47,132 | server.py:125 | fit progress: (1, 2.299748659133911, {'accuracy': 0.1009, 'data_size': 10000}, 21.541729124000994)
INFO flwr 2024-04-06 00:08:47,133 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:08:47,133 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:08:55,957 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:09:09,873 | server.py:125 | fit progress: (2, 2.2920327186584473, {'accuracy': 0.1718, 'data_size': 10000}, 44.282434557011584)
INFO flwr 2024-04-06 00:09:09,874 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:09:09,874 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:09:17,818 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:09:38,922 | server.py:125 | fit progress: (3, 2.3187718391418457, {'accuracy': 0.1009, 'data_size': 10000}, 73.33158417002414)
INFO flwr 2024-04-06 00:09:38,923 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:09:38,923 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:09:46,725 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:10:14,682 | server.py:125 | fit progress: (4, 2.2479922771453857, {'accuracy': 0.1171, 'data_size': 10000}, 109.09178773200256)
INFO flwr 2024-04-06 00:10:14,683 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:10:14,683 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:10:22,502 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:10:56,586 | server.py:125 | fit progress: (5, 2.1269471645355225, {'accuracy': 0.3848, 'data_size': 10000}, 150.99488969601225)
INFO flwr 2024-04-06 00:10:56,586 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:10:56,586 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:11:04,265 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:11:50,747 | server.py:125 | fit progress: (6, 2.0075478553771973, {'accuracy': 0.4769, 'data_size': 10000}, 205.15624787201523)
INFO flwr 2024-04-06 00:11:50,747 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:11:50,747 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:11:58,893 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:12:53,225 | server.py:125 | fit progress: (7, 1.8342006206512451, {'accuracy': 0.6915, 'data_size': 10000}, 267.63441766100004)
INFO flwr 2024-04-06 00:12:53,225 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:12:53,226 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:13:01,585 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:14:02,037 | server.py:125 | fit progress: (8, 1.7442783117294312, {'accuracy': 0.7595, 'data_size': 10000}, 336.44682162901154)
INFO flwr 2024-04-06 00:14:02,038 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:14:02,038 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:14:10,478 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:15:28,248 | server.py:125 | fit progress: (9, 1.6859911680221558, {'accuracy': 0.8224, 'data_size': 10000}, 422.6570122310077)
INFO flwr 2024-04-06 00:15:28,248 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:15:28,248 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:15:36,622 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:16:49,834 | server.py:125 | fit progress: (10, 1.660292387008667, {'accuracy': 0.8311, 'data_size': 10000}, 504.2431272860267)
INFO flwr 2024-04-06 00:16:49,834 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:16:49,834 | server.py:153 | FL finished in 504.2436564040254
INFO flwr 2024-04-06 00:16:49,834 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:16:49,835 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:16:49,835 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:16:49,835 | app.py:229 | app_fit: losses_centralized [(0, 2.3028411865234375), (1, 2.299748659133911), (2, 2.2920327186584473), (3, 2.3187718391418457), (4, 2.2479922771453857), (5, 2.1269471645355225), (6, 2.0075478553771973), (7, 1.8342006206512451), (8, 1.7442783117294312), (9, 1.6859911680221558), (10, 1.660292387008667)]
INFO flwr 2024-04-06 00:16:49,835 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0599), (1, 0.1009), (2, 0.1718), (3, 0.1009), (4, 0.1171), (5, 0.3848), (6, 0.4769), (7, 0.6915), (8, 0.7595), (9, 0.8224), (10, 0.8311)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8311
wandb:     loss 1.66029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_000800-0plo6akh
wandb: Find logs at: ./wandb/offline-run-20240406_000800-0plo6akh/logs
INFO flwr 2024-04-06 00:16:53,411 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:24:13,152 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=650113)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=650113)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 00:24:19,679	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:24:20,068	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:24:20,441	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0dbaa2e576f21812.zip' (8.11MiB) to Ray cluster...
2024-04-06 00:24:20,460	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0dbaa2e576f21812.zip'.
INFO flwr 2024-04-06 00:24:31,296 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 153321082266.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69994749542.0}
INFO flwr 2024-04-06 00:24:31,296 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:24:31,297 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:24:31,315 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:24:31,316 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:24:31,317 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:24:31,317 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 00:24:37,081 | server.py:94 | initial parameters (loss, other metrics): 2.3027243614196777, {'accuracy': 0.075, 'data_size': 10000}
INFO flwr 2024-04-06 00:24:37,082 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:24:37,082 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=658505)[0m 2024-04-06 00:24:43.002812: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=658505)[0m 2024-04-06 00:24:43.113742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=658505)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=658505)[0m 2024-04-06 00:24:45.674991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=658491)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=658491)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=658500)[0m 2024-04-06 00:24:43.038321: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=658508)[0m 2024-04-06 00:24:43.182449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=658508)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=658507)[0m 2024-04-06 00:24:45.726292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 00:25:01,453 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:25:04,780 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:25:08,662 | server.py:125 | fit progress: (1, 2.302699565887451, {'accuracy': 0.076, 'data_size': 10000}, 31.57967450999422)
INFO flwr 2024-04-06 00:25:08,662 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:08,662 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:17,593 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:25:31,244 | server.py:125 | fit progress: (2, 2.302680730819702, {'accuracy': 0.0767, 'data_size': 10000}, 54.162348498008214)
INFO flwr 2024-04-06 00:25:31,245 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:25:31,245 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:25:39,651 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:26:00,525 | server.py:125 | fit progress: (3, 2.302656412124634, {'accuracy': 0.0758, 'data_size': 10000}, 83.44322201699833)
INFO flwr 2024-04-06 00:26:00,526 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:26:00,526 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:08,533 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:26:36,538 | server.py:125 | fit progress: (4, 2.302635431289673, {'accuracy': 0.0771, 'data_size': 10000}, 119.45599749599933)
INFO flwr 2024-04-06 00:26:36,538 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:26:36,539 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:26:44,815 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:27:19,966 | server.py:125 | fit progress: (5, 2.3026182651519775, {'accuracy': 0.0787, 'data_size': 10000}, 162.88373658500495)
INFO flwr 2024-04-06 00:27:19,966 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:27:19,966 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:27:27,620 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:28:06,380 | server.py:125 | fit progress: (6, 2.3025999069213867, {'accuracy': 0.0813, 'data_size': 10000}, 209.2982189450122)
INFO flwr 2024-04-06 00:28:06,381 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:28:06,381 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:28:14,493 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:29:08,938 | server.py:125 | fit progress: (7, 2.302577257156372, {'accuracy': 0.0814, 'data_size': 10000}, 271.85610637499485)
INFO flwr 2024-04-06 00:29:08,938 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:29:08,939 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:29:16,800 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:30:19,512 | server.py:125 | fit progress: (8, 2.302560329437256, {'accuracy': 0.0817, 'data_size': 10000}, 342.42965947600896)
INFO flwr 2024-04-06 00:30:19,512 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:30:19,512 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:30:27,568 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:31:30,464 | server.py:125 | fit progress: (9, 2.302537202835083, {'accuracy': 0.081, 'data_size': 10000}, 413.3816191359947)
INFO flwr 2024-04-06 00:31:30,464 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:31:30,464 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:31:38,451 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:32:40,907 | server.py:125 | fit progress: (10, 2.3025219440460205, {'accuracy': 0.0834, 'data_size': 10000}, 483.82488359100535)
INFO flwr 2024-04-06 00:32:40,907 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:32:40,907 | server.py:153 | FL finished in 483.82533876399975
INFO flwr 2024-04-06 00:32:40,908 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:32:40,908 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:32:40,908 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:32:40,908 | app.py:229 | app_fit: losses_centralized [(0, 2.3027243614196777), (1, 2.302699565887451), (2, 2.302680730819702), (3, 2.302656412124634), (4, 2.302635431289673), (5, 2.3026182651519775), (6, 2.3025999069213867), (7, 2.302577257156372), (8, 2.302560329437256), (9, 2.302537202835083), (10, 2.3025219440460205)]
INFO flwr 2024-04-06 00:32:40,908 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.075), (1, 0.076), (2, 0.0767), (3, 0.0758), (4, 0.0771), (5, 0.0787), (6, 0.0813), (7, 0.0814), (8, 0.0817), (9, 0.081), (10, 0.0834)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0834
wandb:     loss 2.30252
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_002412-09g1v56x
wandb: Find logs at: ./wandb/offline-run-20240406_002412-09g1v56x/logs
INFO flwr 2024-04-06 00:32:44,469 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:40:04,967 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=658503)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=658503)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 00:40:10,296	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:40:10,660	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:40:10,957	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a058bb2d0ce0b006.zip' (8.15MiB) to Ray cluster...
2024-04-06 00:40:10,978	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a058bb2d0ce0b006.zip'.
INFO flwr 2024-04-06 00:40:21,723 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 148584244224.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 67964676096.0}
INFO flwr 2024-04-06 00:40:21,723 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:40:21,723 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:40:21,737 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:40:21,737 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:40:21,738 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:40:21,738 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=669808)[0m 2024-04-06 00:40:27.204563: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=669806)[0m 2024-04-06 00:40:27.306155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=669806)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=669808)[0m 2024-04-06 00:40:29.577296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 00:40:29,878 | server.py:94 | initial parameters (loss, other metrics): 2.302785873413086, {'accuracy': 0.1018, 'data_size': 10000}
INFO flwr 2024-04-06 00:40:29,879 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:40:29,879 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=669820)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=669820)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=669810)[0m 2024-04-06 00:40:28.135541: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=669810)[0m 2024-04-06 00:40:28.279915: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=669810)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=669810)[0m 2024-04-06 00:40:30.737606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=669810)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=669810)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 00:40:45,637 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:40:49,080 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:40:52,922 | server.py:125 | fit progress: (1, 2.3016748428344727, {'accuracy': 0.1504, 'data_size': 10000}, 23.042974964977475)
INFO flwr 2024-04-06 00:40:52,923 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:40:52,923 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:41:01,968 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:41:16,001 | server.py:125 | fit progress: (2, 2.2986369132995605, {'accuracy': 0.1009, 'data_size': 10000}, 46.12144282099325)
INFO flwr 2024-04-06 00:41:16,001 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:41:16,001 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:41:24,180 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:41:45,481 | server.py:125 | fit progress: (3, 2.309006929397583, {'accuracy': 0.1009, 'data_size': 10000}, 75.60141388498596)
INFO flwr 2024-04-06 00:41:45,481 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:41:45,481 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:41:53,569 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:42:21,117 | server.py:125 | fit progress: (4, 2.2858893871307373, {'accuracy': 0.1137, 'data_size': 10000}, 111.23769620098756)
INFO flwr 2024-04-06 00:42:21,117 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:42:21,117 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:42:29,101 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 00:43:03,760 | server.py:125 | fit progress: (5, 2.2471399307250977, {'accuracy': 0.1885, 'data_size': 10000}, 153.88093102100538)
INFO flwr 2024-04-06 00:43:03,760 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 00:43:03,761 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:43:12,206 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 00:44:08,592 | server.py:125 | fit progress: (6, 2.1487772464752197, {'accuracy': 0.3218, 'data_size': 10000}, 218.7129147920059)
INFO flwr 2024-04-06 00:44:08,593 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 00:44:08,593 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:44:17,009 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 00:45:43,320 | server.py:125 | fit progress: (7, 2.0685787200927734, {'accuracy': 0.4462, 'data_size': 10000}, 313.440894172003)
INFO flwr 2024-04-06 00:45:43,320 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 00:45:43,321 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:45:51,414 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 00:47:04,963 | server.py:125 | fit progress: (8, 1.9915649890899658, {'accuracy': 0.4929, 'data_size': 10000}, 395.0840064029908)
INFO flwr 2024-04-06 00:47:04,964 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 00:47:04,964 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:47:13,320 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 00:48:16,701 | server.py:125 | fit progress: (9, 1.9278329610824585, {'accuracy': 0.5626, 'data_size': 10000}, 466.8219392489991)
INFO flwr 2024-04-06 00:48:16,701 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 00:48:16,702 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:48:26,049 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 00:49:45,168 | server.py:125 | fit progress: (10, 1.8904982805252075, {'accuracy': 0.6027, 'data_size': 10000}, 555.2884793340054)
INFO flwr 2024-04-06 00:49:45,168 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 00:49:45,168 | server.py:153 | FL finished in 555.2889254259935
INFO flwr 2024-04-06 00:49:45,171 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 00:49:45,171 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 00:49:45,171 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 00:49:45,171 | app.py:229 | app_fit: losses_centralized [(0, 2.302785873413086), (1, 2.3016748428344727), (2, 2.2986369132995605), (3, 2.309006929397583), (4, 2.2858893871307373), (5, 2.2471399307250977), (6, 2.1487772464752197), (7, 2.0685787200927734), (8, 1.9915649890899658), (9, 1.9278329610824585), (10, 1.8904982805252075)]
INFO flwr 2024-04-06 00:49:45,171 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1018), (1, 0.1504), (2, 0.1009), (3, 0.1009), (4, 0.1137), (5, 0.1885), (6, 0.3218), (7, 0.4462), (8, 0.4929), (9, 0.5626), (10, 0.6027)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6027
wandb:     loss 1.8905
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_004004-v5wf1bha
wandb: Find logs at: ./wandb/offline-run-20240406_004004-v5wf1bha/logs
INFO flwr 2024-04-06 00:49:48,965 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 00:57:10,886 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=669806)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=669806)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 00:57:15,766	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 00:57:16,096	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 00:57:16,394	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_88784a856960ace1.zip' (8.18MiB) to Ray cluster...
2024-04-06 00:57:16,413	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_88784a856960ace1.zip'.
INFO flwr 2024-04-06 00:57:27,608 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 150878738228.0, 'CPU': 64.0, 'object_store_memory': 68948030668.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 00:57:27,608 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 00:57:27,608 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 00:57:27,627 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 00:57:27,629 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 00:57:27,629 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 00:57:27,629 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=680918)[0m 2024-04-06 00:57:33.352287: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=680918)[0m 2024-04-06 00:57:33.442590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=680918)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=680906)[0m 2024-04-06 00:57:35.518181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 00:57:36,039 | server.py:94 | initial parameters (loss, other metrics): 2.302762031555176, {'accuracy': 0.0875, 'data_size': 10000}
INFO flwr 2024-04-06 00:57:36,040 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 00:57:36,040 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=680918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=680918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=680913)[0m 2024-04-06 00:57:33.817115: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=680913)[0m 2024-04-06 00:57:33.907714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=680913)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=680910)[0m 2024-04-06 00:57:36.253481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=680908)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=680908)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 00:57:51,145 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 00:57:54,601 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 00:57:58,585 | server.py:125 | fit progress: (1, 2.3018407821655273, {'accuracy': 0.0974, 'data_size': 10000}, 22.545517108985223)
INFO flwr 2024-04-06 00:57:58,586 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 00:57:58,586 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:08,222 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 00:58:22,575 | server.py:125 | fit progress: (2, 2.2996582984924316, {'accuracy': 0.1113, 'data_size': 10000}, 46.53504576298292)
INFO flwr 2024-04-06 00:58:22,575 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 00:58:22,575 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:30,912 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 00:58:51,482 | server.py:125 | fit progress: (3, 2.294848680496216, {'accuracy': 0.1795, 'data_size': 10000}, 75.44258808199083)
INFO flwr 2024-04-06 00:58:51,483 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 00:58:51,483 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:58:59,785 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 00:59:31,075 | server.py:125 | fit progress: (4, 2.2831873893737793, {'accuracy': 0.1073, 'data_size': 10000}, 115.03504897397943)
INFO flwr 2024-04-06 00:59:31,075 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 00:59:31,076 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 00:59:39,076 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:00:15,324 | server.py:125 | fit progress: (5, 2.256396770477295, {'accuracy': 0.2073, 'data_size': 10000}, 159.28409455300425)
INFO flwr 2024-04-06 01:00:15,324 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:00:15,324 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:00:23,192 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:01:05,634 | server.py:125 | fit progress: (6, 2.2052688598632812, {'accuracy': 0.3464, 'data_size': 10000}, 209.59437908499967)
INFO flwr 2024-04-06 01:01:05,635 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:01:05,635 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:01:13,625 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:02:01,730 | server.py:125 | fit progress: (7, 2.0822913646698, {'accuracy': 0.4179, 'data_size': 10000}, 265.69045393698616)
INFO flwr 2024-04-06 01:02:01,730 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:02:01,731 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:02:10,004 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:03:08,101 | server.py:125 | fit progress: (8, 1.9640663862228394, {'accuracy': 0.5338, 'data_size': 10000}, 332.06149777499377)
INFO flwr 2024-04-06 01:03:08,101 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:03:08,102 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:03:16,190 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:04:24,015 | server.py:125 | fit progress: (9, 1.8465466499328613, {'accuracy': 0.6418, 'data_size': 10000}, 407.9748072490038)
INFO flwr 2024-04-06 01:04:24,015 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:04:24,015 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:04:31,806 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:05:49,462 | server.py:125 | fit progress: (10, 1.7925490140914917, {'accuracy': 0.6795, 'data_size': 10000}, 493.42188351397635)
INFO flwr 2024-04-06 01:05:49,462 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:05:49,462 | server.py:153 | FL finished in 493.42231951697613
INFO flwr 2024-04-06 01:05:49,462 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:05:49,462 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:05:49,462 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:05:49,463 | app.py:229 | app_fit: losses_centralized [(0, 2.302762031555176), (1, 2.3018407821655273), (2, 2.2996582984924316), (3, 2.294848680496216), (4, 2.2831873893737793), (5, 2.256396770477295), (6, 2.2052688598632812), (7, 2.0822913646698), (8, 1.9640663862228394), (9, 1.8465466499328613), (10, 1.7925490140914917)]
INFO flwr 2024-04-06 01:05:49,463 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0875), (1, 0.0974), (2, 0.1113), (3, 0.1795), (4, 0.1073), (5, 0.2073), (6, 0.3464), (7, 0.4179), (8, 0.5338), (9, 0.6418), (10, 0.6795)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6795
wandb:     loss 1.79255
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_005710-7hehb42k
wandb: Find logs at: ./wandb/offline-run-20240406_005710-7hehb42k/logs
INFO flwr 2024-04-06 01:05:53,013 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:13:21,011 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=680907)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=680907)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 01:13:25,692	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:13:26,097	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:13:26,495	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cf45a1144d3577d5.zip' (8.21MiB) to Ray cluster...
2024-04-06 01:13:26,520	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cf45a1144d3577d5.zip'.
INFO flwr 2024-04-06 01:13:53,602 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 148659313255.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 67996848537.0}
INFO flwr 2024-04-06 01:13:53,603 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:13:53,603 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:13:53,623 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:13:53,625 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:13:53,626 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:13:53,626 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 01:13:59,229 | server.py:94 | initial parameters (loss, other metrics): 2.30273699760437, {'accuracy': 0.0609, 'data_size': 10000}
INFO flwr 2024-04-06 01:13:59,230 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:13:59,230 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=690043)[0m 2024-04-06 01:14:20.051482: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=690043)[0m 2024-04-06 01:14:20.149017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=690043)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=690043)[0m 2024-04-06 01:14:32.547740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=690037)[0m 2024-04-06 01:14:20.163393: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=690037)[0m 2024-04-06 01:14:20.237972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=690037)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=690041)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=690041)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=690044)[0m 2024-04-06 01:14:32.558854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 01:15:42,579 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:15:50,311 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:15:54,446 | server.py:125 | fit progress: (1, 2.3000195026397705, {'accuracy': 0.1712, 'data_size': 10000}, 115.21592705498915)
INFO flwr 2024-04-06 01:15:54,446 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:15:54,446 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:16:04,183 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:16:23,180 | server.py:125 | fit progress: (2, 2.2816267013549805, {'accuracy': 0.0963, 'data_size': 10000}, 143.9496699970041)
INFO flwr 2024-04-06 01:16:23,181 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:16:23,181 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:16:32,490 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:16:57,394 | server.py:125 | fit progress: (3, 2.261159658432007, {'accuracy': 0.2865, 'data_size': 10000}, 178.16414820498903)
INFO flwr 2024-04-06 01:16:57,394 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:16:57,395 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:17:06,017 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:17:30,787 | server.py:125 | fit progress: (4, 2.100303888320923, {'accuracy': 0.4458, 'data_size': 10000}, 211.55682703197817)
INFO flwr 2024-04-06 01:17:30,787 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:17:30,787 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:17:40,074 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:18:10,289 | server.py:125 | fit progress: (5, 1.9047505855560303, {'accuracy': 0.6025, 'data_size': 10000}, 251.05914125198615)
INFO flwr 2024-04-06 01:18:10,289 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:18:10,290 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:18:19,290 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:19:02,488 | server.py:125 | fit progress: (6, 1.8342159986495972, {'accuracy': 0.6274, 'data_size': 10000}, 303.2576308719872)
INFO flwr 2024-04-06 01:19:02,488 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:19:02,488 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:19:11,705 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:19:52,473 | server.py:125 | fit progress: (7, 1.7275558710098267, {'accuracy': 0.7599, 'data_size': 10000}, 353.2430147069972)
INFO flwr 2024-04-06 01:19:52,473 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:19:52,474 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:20:01,279 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:20:56,821 | server.py:125 | fit progress: (8, 1.6373640298843384, {'accuracy': 0.8656, 'data_size': 10000}, 417.59085344098276)
INFO flwr 2024-04-06 01:20:56,821 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:20:56,821 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:21:05,331 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:22:08,234 | server.py:125 | fit progress: (9, 1.6141804456710815, {'accuracy': 0.8682, 'data_size': 10000}, 489.004236726003)
INFO flwr 2024-04-06 01:22:08,235 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:22:08,235 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:22:16,854 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:23:34,046 | server.py:125 | fit progress: (10, 1.5996439456939697, {'accuracy': 0.8826, 'data_size': 10000}, 574.815640744986)
INFO flwr 2024-04-06 01:23:34,046 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:23:34,046 | server.py:153 | FL finished in 574.8161082919978
INFO flwr 2024-04-06 01:23:34,046 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:23:34,046 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:23:34,046 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:23:34,047 | app.py:229 | app_fit: losses_centralized [(0, 2.30273699760437), (1, 2.3000195026397705), (2, 2.2816267013549805), (3, 2.261159658432007), (4, 2.100303888320923), (5, 1.9047505855560303), (6, 1.8342159986495972), (7, 1.7275558710098267), (8, 1.6373640298843384), (9, 1.6141804456710815), (10, 1.5996439456939697)]
INFO flwr 2024-04-06 01:23:34,047 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0609), (1, 0.1712), (2, 0.0963), (3, 0.2865), (4, 0.4458), (5, 0.6025), (6, 0.6274), (7, 0.7599), (8, 0.8656), (9, 0.8682), (10, 0.8826)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8826
wandb:     loss 1.59964
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_011319-bzrpkur2
wandb: Find logs at: ./wandb/offline-run-20240406_011319-bzrpkur2/logs
INFO flwr 2024-04-06 01:23:37,709 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:30:58,186 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=690040)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=690040)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 01:31:04,162	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:31:04,509	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:31:04,785	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b8b59ccc940d95ac.zip' (8.23MiB) to Ray cluster...
2024-04-06 01:31:04,804	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b8b59ccc940d95ac.zip'.
INFO flwr 2024-04-06 01:31:15,804 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 147690242663.0, 'object_store_memory': 67581532569.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 01:31:15,804 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:31:15,804 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:31:15,826 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:31:15,827 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:31:15,827 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:31:15,827 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=698610)[0m 2024-04-06 01:31:21.455693: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=698610)[0m 2024-04-06 01:31:21.551256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=698610)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=698610)[0m 2024-04-06 01:31:23.627937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 01:31:24,912 | server.py:94 | initial parameters (loss, other metrics): 2.302586555480957, {'accuracy': 0.1748, 'data_size': 10000}
INFO flwr 2024-04-06 01:31:24,912 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:31:24,913 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=698616)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=698616)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=698601)[0m 2024-04-06 01:31:22.263821: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=698601)[0m 2024-04-06 01:31:22.364913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=698601)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=698601)[0m 2024-04-06 01:31:24.707511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=698604)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=698604)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 01:31:40,540 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:31:43,999 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:31:47,920 | server.py:125 | fit progress: (1, 2.293015241622925, {'accuracy': 0.1571, 'data_size': 10000}, 23.007400766015053)
INFO flwr 2024-04-06 01:31:47,920 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:31:47,920 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:31:57,072 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:32:10,293 | server.py:125 | fit progress: (2, 2.2451820373535156, {'accuracy': 0.2962, 'data_size': 10000}, 45.3809787069913)
INFO flwr 2024-04-06 01:32:10,294 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:32:10,294 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:32:18,982 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:32:38,313 | server.py:125 | fit progress: (3, 2.136190176010132, {'accuracy': 0.3031, 'data_size': 10000}, 73.40092057999573)
INFO flwr 2024-04-06 01:32:38,314 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:32:38,314 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:32:46,473 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:33:11,703 | server.py:125 | fit progress: (4, 2.0188047885894775, {'accuracy': 0.5501, 'data_size': 10000}, 106.79019637301099)
INFO flwr 2024-04-06 01:33:11,703 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:33:11,703 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:33:20,150 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:33:50,980 | server.py:125 | fit progress: (5, 1.8554589748382568, {'accuracy': 0.625, 'data_size': 10000}, 146.06714465501136)
INFO flwr 2024-04-06 01:33:50,980 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:33:50,980 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:34:00,326 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:34:38,140 | server.py:125 | fit progress: (6, 1.7132195234298706, {'accuracy': 0.7815, 'data_size': 10000}, 193.22726873800275)
INFO flwr 2024-04-06 01:34:38,140 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:34:38,141 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:34:47,003 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:35:38,505 | server.py:125 | fit progress: (7, 1.6667253971099854, {'accuracy': 0.8196, 'data_size': 10000}, 253.59265847399365)
INFO flwr 2024-04-06 01:35:38,505 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:35:38,506 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:35:47,452 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:36:38,961 | server.py:125 | fit progress: (8, 1.6231411695480347, {'accuracy': 0.87, 'data_size': 10000}, 314.0483720219927)
INFO flwr 2024-04-06 01:36:38,961 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:36:38,961 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:36:48,015 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:37:46,130 | server.py:125 | fit progress: (9, 1.591173529624939, {'accuracy': 0.8913, 'data_size': 10000}, 381.21789468300994)
INFO flwr 2024-04-06 01:37:46,131 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:37:46,131 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:37:55,394 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:38:55,855 | server.py:125 | fit progress: (10, 1.5640299320220947, {'accuracy': 0.9135, 'data_size': 10000}, 450.9424435030087)
INFO flwr 2024-04-06 01:38:55,855 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:38:55,855 | server.py:153 | FL finished in 450.9429419559892
INFO flwr 2024-04-06 01:38:55,856 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:38:55,856 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:38:55,856 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:38:55,856 | app.py:229 | app_fit: losses_centralized [(0, 2.302586555480957), (1, 2.293015241622925), (2, 2.2451820373535156), (3, 2.136190176010132), (4, 2.0188047885894775), (5, 1.8554589748382568), (6, 1.7132195234298706), (7, 1.6667253971099854), (8, 1.6231411695480347), (9, 1.591173529624939), (10, 1.5640299320220947)]
INFO flwr 2024-04-06 01:38:55,856 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1748), (1, 0.1571), (2, 0.2962), (3, 0.3031), (4, 0.5501), (5, 0.625), (6, 0.7815), (7, 0.8196), (8, 0.87), (9, 0.8913), (10, 0.9135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9135
wandb:     loss 1.56403
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_013057-nhtsp0vh
wandb: Find logs at: ./wandb/offline-run-20240406_013057-nhtsp0vh/logs
INFO flwr 2024-04-06 01:38:59,453 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 01:46:21,811 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=698601)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=698601)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 01:46:26,825	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 01:46:27,165	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 01:46:27,446	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_de949edff3e5e24c.zip' (8.26MiB) to Ray cluster...
2024-04-06 01:46:27,466	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_de949edff3e5e24c.zip'.
INFO flwr 2024-04-06 01:46:38,659 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 68470402252.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 149764271924.0}
INFO flwr 2024-04-06 01:46:38,660 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 01:46:38,660 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 01:46:38,683 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 01:46:38,684 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 01:46:38,684 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 01:46:38,684 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=709714)[0m 2024-04-06 01:46:44.724301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=709714)[0m 2024-04-06 01:46:44.825216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=709714)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 01:46:46,038 | server.py:94 | initial parameters (loss, other metrics): 2.3028831481933594, {'accuracy': 0.0765, 'data_size': 10000}
INFO flwr 2024-04-06 01:46:46,038 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 01:46:46,039 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=709716)[0m 2024-04-06 01:46:46.750479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=709714)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=709714)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=709712)[0m 2024-04-06 01:46:45.420341: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=709712)[0m 2024-04-06 01:46:45.532406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=709712)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=709710)[0m 2024-04-06 01:46:48.010769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=709710)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=709710)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 01:47:02,550 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 01:47:05,876 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 01:47:09,784 | server.py:125 | fit progress: (1, 2.279123067855835, {'accuracy': 0.1015, 'data_size': 10000}, 23.745123917004094)
INFO flwr 2024-04-06 01:47:09,784 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 01:47:09,784 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:47:18,793 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 01:47:32,926 | server.py:125 | fit progress: (2, 2.1875882148742676, {'accuracy': 0.5318, 'data_size': 10000}, 46.8875664640218)
INFO flwr 2024-04-06 01:47:32,926 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 01:47:32,926 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:47:41,221 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 01:48:00,701 | server.py:125 | fit progress: (3, 1.9315555095672607, {'accuracy': 0.6131, 'data_size': 10000}, 74.6629486020247)
INFO flwr 2024-04-06 01:48:00,702 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 01:48:00,702 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:48:08,979 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 01:48:34,325 | server.py:125 | fit progress: (4, 1.7741751670837402, {'accuracy': 0.7427, 'data_size': 10000}, 108.28653388199746)
INFO flwr 2024-04-06 01:48:34,325 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 01:48:34,326 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:48:43,005 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 01:49:14,438 | server.py:125 | fit progress: (5, 1.7140570878982544, {'accuracy': 0.7559, 'data_size': 10000}, 148.39957759302342)
INFO flwr 2024-04-06 01:49:14,438 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 01:49:14,439 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:49:23,836 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 01:50:01,451 | server.py:125 | fit progress: (6, 1.6547796726226807, {'accuracy': 0.8246, 'data_size': 10000}, 195.4123609160015)
INFO flwr 2024-04-06 01:50:01,451 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 01:50:01,451 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:50:10,213 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 01:50:52,201 | server.py:125 | fit progress: (7, 1.5779014825820923, {'accuracy': 0.9072, 'data_size': 10000}, 246.1621499210014)
INFO flwr 2024-04-06 01:50:52,201 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 01:50:52,201 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:51:00,336 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 01:51:55,955 | server.py:125 | fit progress: (8, 1.5583746433258057, {'accuracy': 0.9165, 'data_size': 10000}, 309.916221499996)
INFO flwr 2024-04-06 01:51:55,955 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 01:51:55,955 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:52:04,700 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 01:53:08,070 | server.py:125 | fit progress: (9, 1.549323320388794, {'accuracy': 0.9233, 'data_size': 10000}, 382.0320395970193)
INFO flwr 2024-04-06 01:53:08,071 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 01:53:08,071 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 01:53:16,216 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 01:54:26,297 | server.py:125 | fit progress: (10, 1.5425984859466553, {'accuracy': 0.9275, 'data_size': 10000}, 460.2582463330182)
INFO flwr 2024-04-06 01:54:26,297 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 01:54:26,297 | server.py:153 | FL finished in 460.2586756310193
INFO flwr 2024-04-06 01:54:26,300 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 01:54:26,300 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 01:54:26,300 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 01:54:26,300 | app.py:229 | app_fit: losses_centralized [(0, 2.3028831481933594), (1, 2.279123067855835), (2, 2.1875882148742676), (3, 1.9315555095672607), (4, 1.7741751670837402), (5, 1.7140570878982544), (6, 1.6547796726226807), (7, 1.5779014825820923), (8, 1.5583746433258057), (9, 1.549323320388794), (10, 1.5425984859466553)]
INFO flwr 2024-04-06 01:54:26,300 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0765), (1, 0.1015), (2, 0.5318), (3, 0.6131), (4, 0.7427), (5, 0.7559), (6, 0.8246), (7, 0.9072), (8, 0.9165), (9, 0.9233), (10, 0.9275)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9275
wandb:     loss 1.5426
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_014621-lajb69z2
wandb: Find logs at: ./wandb/offline-run-20240406_014621-lajb69z2/logs
INFO flwr 2024-04-06 01:54:29,927 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:01:50,587 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=709706)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=709706)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 02:01:56,473	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:01:56,773	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:01:57,070	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f60d37f31e8d6b60.zip' (8.30MiB) to Ray cluster...
2024-04-06 02:01:57,088	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f60d37f31e8d6b60.zip'.
INFO flwr 2024-04-06 02:02:08,089 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 147453001933.0, 'node:__internal_head__': 1.0, 'object_store_memory': 67479857971.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 02:02:08,090 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:02:08,090 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:02:08,110 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:02:08,111 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:02:08,111 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:02:08,111 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=718247)[0m 2024-04-06 02:02:14.004436: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=718256)[0m 2024-04-06 02:02:14.154015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=718256)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=718256)[0m 2024-04-06 02:02:16.172094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 02:02:16,560 | server.py:94 | initial parameters (loss, other metrics): 2.3028082847595215, {'accuracy': 0.0883, 'data_size': 10000}
INFO flwr 2024-04-06 02:02:16,561 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:02:16,561 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=718256)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=718256)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=718245)[0m 2024-04-06 02:02:14.493070: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=718245)[0m 2024-04-06 02:02:14.647651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=718245)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=718245)[0m 2024-04-06 02:02:16.973505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=718245)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=718245)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 02:02:32,009 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:02:35,275 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:02:39,157 | server.py:125 | fit progress: (1, 2.300015687942505, {'accuracy': 0.1013, 'data_size': 10000}, 22.595380432991078)
INFO flwr 2024-04-06 02:02:39,157 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:02:39,157 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:02:48,356 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:03:01,283 | server.py:125 | fit progress: (2, 2.2287204265594482, {'accuracy': 0.2954, 'data_size': 10000}, 44.72189913201146)
INFO flwr 2024-04-06 02:03:01,283 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:03:01,284 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:03:09,236 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:03:28,465 | server.py:125 | fit progress: (3, 2.174553155899048, {'accuracy': 0.4395, 'data_size': 10000}, 71.90379507601028)
INFO flwr 2024-04-06 02:03:28,465 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:03:28,466 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:03:36,954 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:04:01,857 | server.py:125 | fit progress: (4, 2.025724172592163, {'accuracy': 0.4419, 'data_size': 10000}, 105.29562700199313)
INFO flwr 2024-04-06 02:04:01,857 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:04:01,857 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:04:10,209 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:04:40,621 | server.py:125 | fit progress: (5, 1.849297285079956, {'accuracy': 0.6178, 'data_size': 10000}, 144.0599041270034)
INFO flwr 2024-04-06 02:04:40,621 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:04:40,622 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:04:49,170 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:05:28,317 | server.py:125 | fit progress: (6, 1.691879153251648, {'accuracy': 0.7841, 'data_size': 10000}, 191.75540870599798)
INFO flwr 2024-04-06 02:05:28,317 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:05:28,317 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:05:36,687 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:06:19,119 | server.py:125 | fit progress: (7, 1.6514824628829956, {'accuracy': 0.8194, 'data_size': 10000}, 242.55823764100205)
INFO flwr 2024-04-06 02:06:19,120 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:06:19,120 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:06:27,084 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:07:27,229 | server.py:125 | fit progress: (8, 1.5728157758712769, {'accuracy': 0.9077, 'data_size': 10000}, 310.6681328859995)
INFO flwr 2024-04-06 02:07:27,230 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:07:27,230 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:07:35,526 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:08:38,365 | server.py:125 | fit progress: (9, 1.5575389862060547, {'accuracy': 0.9159, 'data_size': 10000}, 381.80356167801074)
INFO flwr 2024-04-06 02:08:38,365 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:08:38,365 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:08:47,867 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:10:08,723 | server.py:125 | fit progress: (10, 1.5493758916854858, {'accuracy': 0.9233, 'data_size': 10000}, 472.16173081699526)
INFO flwr 2024-04-06 02:10:08,723 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:10:08,723 | server.py:153 | FL finished in 472.1622016970068
INFO flwr 2024-04-06 02:10:08,724 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:10:08,724 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:10:08,724 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:10:08,724 | app.py:229 | app_fit: losses_centralized [(0, 2.3028082847595215), (1, 2.300015687942505), (2, 2.2287204265594482), (3, 2.174553155899048), (4, 2.025724172592163), (5, 1.849297285079956), (6, 1.691879153251648), (7, 1.6514824628829956), (8, 1.5728157758712769), (9, 1.5575389862060547), (10, 1.5493758916854858)]
INFO flwr 2024-04-06 02:10:08,724 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0883), (1, 0.1013), (2, 0.2954), (3, 0.4395), (4, 0.4419), (5, 0.6178), (6, 0.7841), (7, 0.8194), (8, 0.9077), (9, 0.9159), (10, 0.9233)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9233
wandb:     loss 1.54938
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_020150-9mjlyupq
wandb: Find logs at: ./wandb/offline-run-20240406_020150-9mjlyupq/logs
INFO flwr 2024-04-06 02:10:12,345 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:17:32,529 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=718242)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=718242)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 02:17:38,255	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:17:38,580	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:17:38,866	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c86d06a416fef589.zip' (8.32MiB) to Ray cluster...
2024-04-06 02:17:38,892	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c86d06a416fef589.zip'.
INFO flwr 2024-04-06 02:17:49,819 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 67386288537.0, 'node:10.20.240.18': 1.0, 'memory': 147234673255.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 02:17:49,819 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:17:49,819 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:17:49,834 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:17:49,834 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:17:49,835 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:17:49,835 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=726362)[0m 2024-04-06 02:17:55.159837: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=726355)[0m 2024-04-06 02:17:55.330774: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=726355)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=726355)[0m 2024-04-06 02:17:57.589561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 02:17:58,217 | server.py:94 | initial parameters (loss, other metrics): 2.302983283996582, {'accuracy': 0.0686, 'data_size': 10000}
INFO flwr 2024-04-06 02:17:58,218 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:17:58,218 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=726362)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=726362)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=726353)[0m 2024-04-06 02:17:56.508408: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=726353)[0m 2024-04-06 02:17:56.611211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=726353)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=726357)[0m 2024-04-06 02:17:58.599813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=726353)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=726353)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 02:18:14,668 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:18:17,487 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:18:21,356 | server.py:125 | fit progress: (1, 2.302541732788086, {'accuracy': 0.1149, 'data_size': 10000}, 23.138307883986272)
INFO flwr 2024-04-06 02:18:21,357 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:18:21,357 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:18:32,171 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:18:46,432 | server.py:125 | fit progress: (2, 2.301990270614624, {'accuracy': 0.1679, 'data_size': 10000}, 48.21424419397954)
INFO flwr 2024-04-06 02:18:46,432 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:18:46,433 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:18:56,745 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:19:15,672 | server.py:125 | fit progress: (3, 2.301501750946045, {'accuracy': 0.1439, 'data_size': 10000}, 77.45371849398362)
INFO flwr 2024-04-06 02:19:15,672 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:19:15,673 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:19:25,617 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:19:53,572 | server.py:125 | fit progress: (4, 2.3008217811584473, {'accuracy': 0.1271, 'data_size': 10000}, 115.3545482569898)
INFO flwr 2024-04-06 02:19:53,573 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:19:53,573 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:20:03,641 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:20:33,838 | server.py:125 | fit progress: (5, 2.299973726272583, {'accuracy': 0.1093, 'data_size': 10000}, 155.6198658309877)
INFO flwr 2024-04-06 02:20:33,838 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:20:33,838 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:20:43,474 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:21:25,336 | server.py:125 | fit progress: (6, 2.299071788787842, {'accuracy': 0.098, 'data_size': 10000}, 207.11825312499423)
INFO flwr 2024-04-06 02:21:25,337 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:21:25,337 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:21:36,509 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:22:25,187 | server.py:125 | fit progress: (7, 2.2969648838043213, {'accuracy': 0.1799, 'data_size': 10000}, 266.9687499649881)
INFO flwr 2024-04-06 02:22:25,187 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:22:25,187 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:22:34,487 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:23:29,160 | server.py:125 | fit progress: (8, 2.2958366870880127, {'accuracy': 0.1882, 'data_size': 10000}, 330.941769291996)
INFO flwr 2024-04-06 02:23:29,160 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:23:29,160 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:23:39,706 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:24:42,721 | server.py:125 | fit progress: (9, 2.2939164638519287, {'accuracy': 0.1029, 'data_size': 10000}, 404.50284007398295)
INFO flwr 2024-04-06 02:24:42,721 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:24:42,721 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:24:52,501 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:26:02,388 | server.py:125 | fit progress: (10, 2.290353775024414, {'accuracy': 0.105, 'data_size': 10000}, 484.17013346997555)
INFO flwr 2024-04-06 02:26:02,388 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:26:02,389 | server.py:153 | FL finished in 484.17079846098204
INFO flwr 2024-04-06 02:26:02,389 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:26:02,389 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:26:02,389 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:26:02,389 | app.py:229 | app_fit: losses_centralized [(0, 2.302983283996582), (1, 2.302541732788086), (2, 2.301990270614624), (3, 2.301501750946045), (4, 2.3008217811584473), (5, 2.299973726272583), (6, 2.299071788787842), (7, 2.2969648838043213), (8, 2.2958366870880127), (9, 2.2939164638519287), (10, 2.290353775024414)]
INFO flwr 2024-04-06 02:26:02,389 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0686), (1, 0.1149), (2, 0.1679), (3, 0.1439), (4, 0.1271), (5, 0.1093), (6, 0.098), (7, 0.1799), (8, 0.1882), (9, 0.1029), (10, 0.105)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.105
wandb:     loss 2.29035
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_021732-ca8mh0p6
wandb: Find logs at: ./wandb/offline-run-20240406_021732-ca8mh0p6/logs
INFO flwr 2024-04-06 02:26:06,022 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:33:26,117 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 02:33:30,737	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:33:31,068	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:33:31,381	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_81a5086db08ad4b7.zip' (8.35MiB) to Ray cluster...
2024-04-06 02:33:31,401	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_81a5086db08ad4b7.zip'.
INFO flwr 2024-04-06 02:33:42,559 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69237851750.0, 'memory': 151554987418.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 02:33:42,560 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:33:42,560 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:33:42,575 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:33:42,576 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:33:42,576 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:33:42,576 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=735431)[0m 2024-04-06 02:33:48.547682: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=735431)[0m 2024-04-06 02:33:48.642655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=735431)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 02:33:49,206 | server.py:94 | initial parameters (loss, other metrics): 2.3026816844940186, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 02:33:49,206 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:33:49,207 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=735422)[0m 2024-04-06 02:33:50.959840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=735431)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=735431)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=735424)[0m 2024-04-06 02:33:49.319154: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=735424)[0m 2024-04-06 02:33:49.417768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=735424)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=735424)[0m 2024-04-06 02:33:51.409622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=735422)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=735422)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 02:34:06,603 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:34:09,444 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:34:13,352 | server.py:125 | fit progress: (1, 2.259925365447998, {'accuracy': 0.1816, 'data_size': 10000}, 24.14546442500432)
INFO flwr 2024-04-06 02:34:13,352 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:34:13,352 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:34:24,204 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:34:37,130 | server.py:125 | fit progress: (2, 1.9237042665481567, {'accuracy': 0.5739, 'data_size': 10000}, 47.92360755300615)
INFO flwr 2024-04-06 02:34:37,130 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:34:37,131 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:34:47,204 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:35:06,197 | server.py:125 | fit progress: (3, 1.7137503623962402, {'accuracy': 0.7459, 'data_size': 10000}, 76.99035981102497)
INFO flwr 2024-04-06 02:35:06,197 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:35:06,197 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:35:15,893 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:35:44,002 | server.py:125 | fit progress: (4, 1.5908727645874023, {'accuracy': 0.8807, 'data_size': 10000}, 114.79542314700666)
INFO flwr 2024-04-06 02:35:44,002 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:35:44,002 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:35:53,702 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:36:27,915 | server.py:125 | fit progress: (5, 1.5388892889022827, {'accuracy': 0.9272, 'data_size': 10000}, 158.7089593160199)
INFO flwr 2024-04-06 02:36:27,916 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:36:27,916 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:36:37,592 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:37:21,033 | server.py:125 | fit progress: (6, 1.5338940620422363, {'accuracy': 0.9323, 'data_size': 10000}, 211.82617088902043)
INFO flwr 2024-04-06 02:37:21,033 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:37:21,033 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:37:31,139 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:38:12,158 | server.py:125 | fit progress: (7, 1.5213401317596436, {'accuracy': 0.9423, 'data_size': 10000}, 262.9515481190174)
INFO flwr 2024-04-06 02:38:12,158 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:38:12,158 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:38:22,203 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:39:18,123 | server.py:125 | fit progress: (8, 1.5196465253829956, {'accuracy': 0.9425, 'data_size': 10000}, 328.91647734100115)
INFO flwr 2024-04-06 02:39:18,123 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:39:18,124 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:39:28,481 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:40:23,535 | server.py:125 | fit progress: (9, 1.520499587059021, {'accuracy': 0.9415, 'data_size': 10000}, 394.3281872200023)
INFO flwr 2024-04-06 02:40:23,535 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:40:23,535 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:40:33,404 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:41:44,754 | server.py:125 | fit progress: (10, 1.5202038288116455, {'accuracy': 0.9408, 'data_size': 10000}, 475.5475931040128)
INFO flwr 2024-04-06 02:41:44,754 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:41:44,754 | server.py:153 | FL finished in 475.54798622301314
INFO flwr 2024-04-06 02:41:44,757 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:41:44,757 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:41:44,757 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:41:44,757 | app.py:229 | app_fit: losses_centralized [(0, 2.3026816844940186), (1, 2.259925365447998), (2, 1.9237042665481567), (3, 1.7137503623962402), (4, 1.5908727645874023), (5, 1.5388892889022827), (6, 1.5338940620422363), (7, 1.5213401317596436), (8, 1.5196465253829956), (9, 1.520499587059021), (10, 1.5202038288116455)]
INFO flwr 2024-04-06 02:41:44,757 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.1816), (2, 0.5739), (3, 0.7459), (4, 0.8807), (5, 0.9272), (6, 0.9323), (7, 0.9423), (8, 0.9425), (9, 0.9415), (10, 0.9408)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9408
wandb:     loss 1.5202
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_023325-chg343gf
wandb: Find logs at: ./wandb/offline-run-20240406_023325-chg343gf/logs
INFO flwr 2024-04-06 02:41:48,391 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 02:49:08,917 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 02:49:14,408	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 02:49:14,683	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 02:49:14,958	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4493ae6b088a59ef.zip' (8.37MiB) to Ray cluster...
2024-04-06 02:49:14,984	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4493ae6b088a59ef.zip'.
INFO flwr 2024-04-06 02:49:25,755 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151077836596.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 69033358540.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 02:49:25,755 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 02:49:25,755 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 02:49:25,773 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 02:49:25,774 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 02:49:25,774 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 02:49:25,774 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=743632)[0m 2024-04-06 02:49:31.599645: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=743632)[0m 2024-04-06 02:49:31.697570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=743632)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=743622)[0m 2024-04-06 02:49:33.735544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 02:49:33,988 | server.py:94 | initial parameters (loss, other metrics): 2.302488327026367, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 02:49:33,989 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 02:49:33,989 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=743632)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=743632)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=743633)[0m 2024-04-06 02:49:32.196842: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=743633)[0m 2024-04-06 02:49:32.293510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=743633)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=743633)[0m 2024-04-06 02:49:34.436460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=743624)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=743624)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 02:49:50,721 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 02:49:54,086 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 02:49:57,949 | server.py:125 | fit progress: (1, 2.2425577640533447, {'accuracy': 0.3654, 'data_size': 10000}, 23.95968342697597)
INFO flwr 2024-04-06 02:49:57,949 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 02:49:57,949 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:50:07,962 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 02:50:21,006 | server.py:125 | fit progress: (2, 1.839011549949646, {'accuracy': 0.6623, 'data_size': 10000}, 47.01671765098581)
INFO flwr 2024-04-06 02:50:21,006 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 02:50:21,006 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:50:30,052 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 02:50:49,212 | server.py:125 | fit progress: (3, 1.7071595191955566, {'accuracy': 0.7686, 'data_size': 10000}, 75.22319332399638)
INFO flwr 2024-04-06 02:50:49,212 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 02:50:49,212 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:50:59,125 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 02:51:24,304 | server.py:125 | fit progress: (4, 1.6850258111953735, {'accuracy': 0.7784, 'data_size': 10000}, 110.3155200399924)
INFO flwr 2024-04-06 02:51:24,305 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 02:51:24,305 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:51:33,921 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 02:52:08,433 | server.py:125 | fit progress: (5, 1.5797921419143677, {'accuracy': 0.884, 'data_size': 10000}, 154.44401688099606)
INFO flwr 2024-04-06 02:52:08,433 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 02:52:08,433 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:52:17,977 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 02:53:02,573 | server.py:125 | fit progress: (6, 1.618367075920105, {'accuracy': 0.8442, 'data_size': 10000}, 208.58459558099275)
INFO flwr 2024-04-06 02:53:02,574 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 02:53:02,574 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:53:12,588 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 02:53:55,940 | server.py:125 | fit progress: (7, 1.6876369714736938, {'accuracy': 0.7731, 'data_size': 10000}, 261.95136326897773)
INFO flwr 2024-04-06 02:53:55,941 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 02:53:55,941 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:54:05,901 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 02:55:06,600 | server.py:125 | fit progress: (8, 1.7280685901641846, {'accuracy': 0.7325, 'data_size': 10000}, 332.61096576898126)
INFO flwr 2024-04-06 02:55:06,600 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 02:55:06,600 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:55:16,015 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 02:56:22,653 | server.py:125 | fit progress: (9, 1.5653436183929443, {'accuracy': 0.8959, 'data_size': 10000}, 408.66447618298116)
INFO flwr 2024-04-06 02:56:22,654 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 02:56:22,654 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 02:56:32,265 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 02:57:49,969 | server.py:125 | fit progress: (10, 1.5758920907974243, {'accuracy': 0.8842, 'data_size': 10000}, 495.98034033799195)
INFO flwr 2024-04-06 02:57:49,970 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 02:57:49,970 | server.py:153 | FL finished in 495.98086059698835
INFO flwr 2024-04-06 02:57:49,972 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 02:57:49,973 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 02:57:49,973 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 02:57:49,973 | app.py:229 | app_fit: losses_centralized [(0, 2.302488327026367), (1, 2.2425577640533447), (2, 1.839011549949646), (3, 1.7071595191955566), (4, 1.6850258111953735), (5, 1.5797921419143677), (6, 1.618367075920105), (7, 1.6876369714736938), (8, 1.7280685901641846), (9, 1.5653436183929443), (10, 1.5758920907974243)]
INFO flwr 2024-04-06 02:57:49,973 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.3654), (2, 0.6623), (3, 0.7686), (4, 0.7784), (5, 0.884), (6, 0.8442), (7, 0.7731), (8, 0.7325), (9, 0.8959), (10, 0.8842)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8842
wandb:     loss 1.57589
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_024908-97sjphgy
wandb: Find logs at: ./wandb/offline-run-20240406_024908-97sjphgy/logs
INFO flwr 2024-04-06 02:57:53,355 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:05:16,390 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=743622)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=743622)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 03:05:22,326	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:05:22,565	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:05:22,887	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a00aa671d35fb662.zip' (8.40MiB) to Ray cluster...
2024-04-06 03:05:22,912	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a00aa671d35fb662.zip'.
INFO flwr 2024-04-06 03:05:34,326 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 151249739572.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69107031244.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 03:05:34,327 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:05:34,327 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:05:34,342 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:05:34,343 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:05:34,343 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:05:34,343 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=754998)[0m 2024-04-06 03:05:40.629946: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=754998)[0m 2024-04-06 03:05:40.725583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=754998)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 03:05:42,061 | server.py:94 | initial parameters (loss, other metrics): 2.302541494369507, {'accuracy': 0.0926, 'data_size': 10000}
INFO flwr 2024-04-06 03:05:42,062 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:05:42,062 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=755000)[0m 2024-04-06 03:05:43.391245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=755001)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=755001)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=754996)[0m 2024-04-06 03:05:41.168433: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=754996)[0m 2024-04-06 03:05:41.268016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=754996)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=754996)[0m 2024-04-06 03:05:44.686457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=754992)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=754992)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:05:59,375 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:06:02,246 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:06:06,127 | server.py:125 | fit progress: (1, 2.1945250034332275, {'accuracy': 0.3464, 'data_size': 10000}, 24.06487110897433)
INFO flwr 2024-04-06 03:06:06,127 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:06:06,127 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:06:16,773 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:06:30,527 | server.py:125 | fit progress: (2, 1.9422460794448853, {'accuracy': 0.5833, 'data_size': 10000}, 48.46579651199863)
INFO flwr 2024-04-06 03:06:30,528 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:06:30,528 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:06:39,866 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:07:01,454 | server.py:125 | fit progress: (3, 1.7930700778961182, {'accuracy': 0.6864, 'data_size': 10000}, 79.39222350899945)
INFO flwr 2024-04-06 03:07:01,454 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:07:01,454 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:07:11,616 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:07:40,009 | server.py:125 | fit progress: (4, 1.8377074003219604, {'accuracy': 0.6201, 'data_size': 10000}, 117.94693746298435)
INFO flwr 2024-04-06 03:07:40,009 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:07:40,009 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:07:49,752 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:08:25,720 | server.py:125 | fit progress: (5, 1.9045562744140625, {'accuracy': 0.5532, 'data_size': 10000}, 163.65853645198513)
INFO flwr 2024-04-06 03:08:25,720 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:08:25,721 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:08:35,368 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:09:15,759 | server.py:125 | fit progress: (6, 1.8885656595230103, {'accuracy': 0.5715, 'data_size': 10000}, 213.6977688309853)
INFO flwr 2024-04-06 03:09:15,760 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:09:15,760 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:09:25,930 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:10:21,805 | server.py:125 | fit progress: (7, 1.7578473091125488, {'accuracy': 0.7032, 'data_size': 10000}, 279.7428513339837)
INFO flwr 2024-04-06 03:10:21,805 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:10:21,805 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:10:31,968 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:11:29,824 | server.py:125 | fit progress: (8, 1.8336162567138672, {'accuracy': 0.6264, 'data_size': 10000}, 347.76240884599974)
INFO flwr 2024-04-06 03:11:29,824 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:11:29,825 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:11:40,086 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:12:53,117 | server.py:125 | fit progress: (9, 1.7929999828338623, {'accuracy': 0.6668, 'data_size': 10000}, 431.05565196898533)
INFO flwr 2024-04-06 03:12:53,118 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:12:53,118 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:13:03,206 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:14:23,526 | server.py:125 | fit progress: (10, 1.9119702577590942, {'accuracy': 0.5484, 'data_size': 10000}, 521.4646550100006)
INFO flwr 2024-04-06 03:14:23,527 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:14:23,528 | server.py:153 | FL finished in 521.4659357309865
INFO flwr 2024-04-06 03:14:23,528 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:14:23,529 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:14:23,529 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:14:23,529 | app.py:229 | app_fit: losses_centralized [(0, 2.302541494369507), (1, 2.1945250034332275), (2, 1.9422460794448853), (3, 1.7930700778961182), (4, 1.8377074003219604), (5, 1.9045562744140625), (6, 1.8885656595230103), (7, 1.7578473091125488), (8, 1.8336162567138672), (9, 1.7929999828338623), (10, 1.9119702577590942)]
INFO flwr 2024-04-06 03:14:23,529 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0926), (1, 0.3464), (2, 0.5833), (3, 0.6864), (4, 0.6201), (5, 0.5532), (6, 0.5715), (7, 0.7032), (8, 0.6264), (9, 0.6668), (10, 0.5484)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5484
wandb:     loss 1.91197
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_030516-u3f3ixy1
wandb: Find logs at: ./wandb/offline-run-20240406_030516-u3f3ixy1/logs
INFO flwr 2024-04-06 03:14:27,168 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:21:52,319 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 03:21:59,132	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:21:59,407	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:21:59,696	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_efda8ce76a24f73c.zip' (8.43MiB) to Ray cluster...
2024-04-06 03:21:59,728	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_efda8ce76a24f73c.zip'.
INFO flwr 2024-04-06 03:22:10,839 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 146322568192.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66995386368.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 03:22:10,840 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:22:10,840 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:22:10,862 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:22:10,864 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:22:10,864 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:22:10,864 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=763277)[0m 2024-04-06 03:22:16.615961: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=763277)[0m 2024-04-06 03:22:16.717779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=763277)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 03:22:18,074 | server.py:94 | initial parameters (loss, other metrics): 2.3026304244995117, {'accuracy': 0.1183, 'data_size': 10000}
INFO flwr 2024-04-06 03:22:18,075 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:22:18,076 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=763269)[0m 2024-04-06 03:22:18.951793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=763276)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=763276)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=763273)[0m 2024-04-06 03:22:17.508554: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=763273)[0m 2024-04-06 03:22:17.609018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=763273)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=763266)[0m 2024-04-06 03:22:19.743843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=763266)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=763266)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 03:22:35,318 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:22:38,737 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:22:42,729 | server.py:125 | fit progress: (1, 2.2865428924560547, {'accuracy': 0.189, 'data_size': 10000}, 24.653198030981002)
INFO flwr 2024-04-06 03:22:42,729 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:22:42,730 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:22:53,279 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:23:07,635 | server.py:125 | fit progress: (2, 2.209444999694824, {'accuracy': 0.2566, 'data_size': 10000}, 49.55933008599095)
INFO flwr 2024-04-06 03:23:07,636 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:23:07,636 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:23:17,772 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:23:39,265 | server.py:125 | fit progress: (3, 2.09511137008667, {'accuracy': 0.3739, 'data_size': 10000}, 81.18899305997184)
INFO flwr 2024-04-06 03:23:39,265 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:23:39,265 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:23:48,882 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:24:17,902 | server.py:125 | fit progress: (4, 2.1058127880096436, {'accuracy': 0.346, 'data_size': 10000}, 119.82658448297298)
INFO flwr 2024-04-06 03:24:17,903 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:24:17,903 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:24:28,314 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:25:04,722 | server.py:125 | fit progress: (5, 1.9577152729034424, {'accuracy': 0.5121, 'data_size': 10000}, 166.64625289299875)
INFO flwr 2024-04-06 03:25:04,722 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:25:04,722 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:25:14,465 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:25:56,519 | server.py:125 | fit progress: (6, 1.9264512062072754, {'accuracy': 0.5495, 'data_size': 10000}, 218.44358717597788)
INFO flwr 2024-04-06 03:25:56,520 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:25:56,520 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:26:06,908 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:27:02,104 | server.py:125 | fit progress: (7, 1.8821029663085938, {'accuracy': 0.5753, 'data_size': 10000}, 284.02857209497597)
INFO flwr 2024-04-06 03:27:02,105 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:27:02,105 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:27:12,274 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:28:15,790 | server.py:125 | fit progress: (8, 1.9046123027801514, {'accuracy': 0.5519, 'data_size': 10000}, 357.7143692919926)
INFO flwr 2024-04-06 03:28:15,790 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:28:15,791 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:28:26,275 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:29:34,845 | server.py:125 | fit progress: (9, 1.8886982202529907, {'accuracy': 0.5702, 'data_size': 10000}, 436.7696097679727)
INFO flwr 2024-04-06 03:29:34,846 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:29:34,846 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:29:45,067 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:30:55,047 | server.py:125 | fit progress: (10, 1.8284564018249512, {'accuracy': 0.6317, 'data_size': 10000}, 516.9710499619832)
INFO flwr 2024-04-06 03:30:55,047 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:30:55,048 | server.py:153 | FL finished in 516.971783675981
INFO flwr 2024-04-06 03:30:55,048 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:30:55,048 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:30:55,048 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:30:55,048 | app.py:229 | app_fit: losses_centralized [(0, 2.3026304244995117), (1, 2.2865428924560547), (2, 2.209444999694824), (3, 2.09511137008667), (4, 2.1058127880096436), (5, 1.9577152729034424), (6, 1.9264512062072754), (7, 1.8821029663085938), (8, 1.9046123027801514), (9, 1.8886982202529907), (10, 1.8284564018249512)]
INFO flwr 2024-04-06 03:30:55,048 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1183), (1, 0.189), (2, 0.2566), (3, 0.3739), (4, 0.346), (5, 0.5121), (6, 0.5495), (7, 0.5753), (8, 0.5519), (9, 0.5702), (10, 0.6317)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6317
wandb:     loss 1.82846
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_032151-26ormz32
wandb: Find logs at: ./wandb/offline-run-20240406_032151-26ormz32/logs
INFO flwr 2024-04-06 03:30:58,647 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:38:19,220 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=763263)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=763263)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 03:38:25,443	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:38:25,784	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:38:26,213	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ad60775e6fc607f1.zip' (8.45MiB) to Ray cluster...
2024-04-06 03:38:26,233	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ad60775e6fc607f1.zip'.
INFO flwr 2024-04-06 03:38:37,350 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 145134305895.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66486131097.0}
INFO flwr 2024-04-06 03:38:37,350 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:38:37,351 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:38:37,366 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:38:37,367 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:38:37,367 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:38:37,367 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=771745)[0m 2024-04-06 03:38:42.955991: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=771745)[0m 2024-04-06 03:38:43.057709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=771745)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 03:38:45,554 | server.py:94 | initial parameters (loss, other metrics): 2.302995204925537, {'accuracy': 0.0723, 'data_size': 10000}
INFO flwr 2024-04-06 03:38:45,555 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:38:45,555 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=771741)[0m 2024-04-06 03:38:45.473432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=771745)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=771745)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=771738)[0m 2024-04-06 03:38:44.081277: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=771738)[0m 2024-04-06 03:38:44.178627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=771738)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=771738)[0m 2024-04-06 03:38:46.432005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=771736)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=771736)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 03:39:02,277 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:39:05,794 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:39:09,759 | server.py:125 | fit progress: (1, 2.3110265731811523, {'accuracy': 0.098, 'data_size': 10000}, 24.203716848016484)
INFO flwr 2024-04-06 03:39:09,759 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:39:09,759 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:39:20,851 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:39:35,035 | server.py:125 | fit progress: (2, 2.296588659286499, {'accuracy': 0.1016, 'data_size': 10000}, 49.47977356199408)
INFO flwr 2024-04-06 03:39:35,035 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:39:35,035 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:39:44,883 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:40:10,018 | server.py:125 | fit progress: (3, 2.269817590713501, {'accuracy': 0.1695, 'data_size': 10000}, 84.46342864699545)
INFO flwr 2024-04-06 03:40:10,019 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:40:10,019 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:40:19,797 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:40:47,657 | server.py:125 | fit progress: (4, 2.2536027431488037, {'accuracy': 0.2041, 'data_size': 10000}, 122.1016041719995)
INFO flwr 2024-04-06 03:40:47,657 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:40:47,657 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:40:57,380 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:41:34,670 | server.py:125 | fit progress: (5, 2.1142053604125977, {'accuracy': 0.3295, 'data_size': 10000}, 169.11477590099094)
INFO flwr 2024-04-06 03:41:34,670 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:41:34,670 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:41:45,191 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:42:31,074 | server.py:125 | fit progress: (6, 2.1311118602752686, {'accuracy': 0.3214, 'data_size': 10000}, 225.51936833799118)
INFO flwr 2024-04-06 03:42:31,075 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:42:31,075 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:42:40,340 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:43:37,958 | server.py:125 | fit progress: (7, 2.1761436462402344, {'accuracy': 0.2829, 'data_size': 10000}, 292.4034239600005)
INFO flwr 2024-04-06 03:43:37,959 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:43:37,959 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:43:47,668 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 03:44:43,383 | server.py:125 | fit progress: (8, 2.02252197265625, {'accuracy': 0.4377, 'data_size': 10000}, 357.8276530330186)
INFO flwr 2024-04-06 03:44:43,383 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 03:44:43,383 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:44:53,619 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 03:46:07,117 | server.py:125 | fit progress: (9, 2.103261709213257, {'accuracy': 0.3562, 'data_size': 10000}, 441.5621369430155)
INFO flwr 2024-04-06 03:46:07,118 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 03:46:07,118 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:46:16,647 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 03:47:27,524 | server.py:125 | fit progress: (10, 2.130471706390381, {'accuracy': 0.3297, 'data_size': 10000}, 521.9690432990028)
INFO flwr 2024-04-06 03:47:27,525 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 03:47:27,525 | server.py:153 | FL finished in 521.9697519089968
INFO flwr 2024-04-06 03:47:27,525 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 03:47:27,525 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 03:47:27,525 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 03:47:27,525 | app.py:229 | app_fit: losses_centralized [(0, 2.302995204925537), (1, 2.3110265731811523), (2, 2.296588659286499), (3, 2.269817590713501), (4, 2.2536027431488037), (5, 2.1142053604125977), (6, 2.1311118602752686), (7, 2.1761436462402344), (8, 2.02252197265625), (9, 2.103261709213257), (10, 2.130471706390381)]
INFO flwr 2024-04-06 03:47:27,525 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0723), (1, 0.098), (2, 0.1016), (3, 0.1695), (4, 0.2041), (5, 0.3295), (6, 0.3214), (7, 0.2829), (8, 0.4377), (9, 0.3562), (10, 0.3297)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3297
wandb:     loss 2.13047
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_033818-gfs701kp
wandb: Find logs at: ./wandb/offline-run-20240406_033818-gfs701kp/logs
INFO flwr 2024-04-06 03:47:31,188 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 03:54:52,594 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=771737)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=771737)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 03:54:58,486	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 03:54:58,762	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 03:54:59,079	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_13283ee53d52c683.zip' (8.48MiB) to Ray cluster...
2024-04-06 03:54:59,111	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_13283ee53d52c683.zip'.
INFO flwr 2024-04-06 03:55:10,341 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 144882803712.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 66378344448.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 03:55:10,342 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 03:55:10,342 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 03:55:10,356 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 03:55:10,357 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 03:55:10,357 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 03:55:10,358 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=780649)[0m 2024-04-06 03:55:15.797812: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=780649)[0m 2024-04-06 03:55:15.919959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=780649)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 03:55:18,313 | server.py:94 | initial parameters (loss, other metrics): 2.3025903701782227, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-06 03:55:18,313 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 03:55:18,314 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=780651)[0m 2024-04-06 03:55:18.285375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=780651)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=780651)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=780642)[0m 2024-04-06 03:55:17.348580: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=780642)[0m 2024-04-06 03:55:17.447216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=780642)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=780642)[0m 2024-04-06 03:55:19.596038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 03:55:35,899 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 03:55:39,538 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 03:55:43,516 | server.py:125 | fit progress: (1, 2.3011646270751953, {'accuracy': 0.1093, 'data_size': 10000}, 25.201931795018027)
INFO flwr 2024-04-06 03:55:43,516 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 03:55:43,516 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:55:53,431 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 03:56:07,615 | server.py:125 | fit progress: (2, 2.281254529953003, {'accuracy': 0.1152, 'data_size': 10000}, 49.30103341900394)
INFO flwr 2024-04-06 03:56:07,615 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 03:56:07,615 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:56:17,702 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 03:56:39,676 | server.py:125 | fit progress: (3, 2.251028537750244, {'accuracy': 0.2181, 'data_size': 10000}, 81.36275358099374)
INFO flwr 2024-04-06 03:56:39,677 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 03:56:39,677 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:56:48,787 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 03:57:17,011 | server.py:125 | fit progress: (4, 2.2122726440429688, {'accuracy': 0.2376, 'data_size': 10000}, 118.69716851800331)
INFO flwr 2024-04-06 03:57:17,011 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 03:57:17,011 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:57:26,707 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 03:58:03,196 | server.py:125 | fit progress: (5, 2.1298627853393555, {'accuracy': 0.362, 'data_size': 10000}, 164.8824145529943)
INFO flwr 2024-04-06 03:58:03,196 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 03:58:03,196 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:58:13,178 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 03:58:54,811 | server.py:125 | fit progress: (6, 2.0792834758758545, {'accuracy': 0.3901, 'data_size': 10000}, 216.49768639399554)
INFO flwr 2024-04-06 03:58:54,812 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 03:58:54,812 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 03:59:04,527 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 03:59:59,528 | server.py:125 | fit progress: (7, 1.941910743713379, {'accuracy': 0.5593, 'data_size': 10000}, 281.21484954000334)
INFO flwr 2024-04-06 03:59:59,529 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 03:59:59,529 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:00:08,536 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:01:12,849 | server.py:125 | fit progress: (8, 2.052067279815674, {'accuracy': 0.4025, 'data_size': 10000}, 354.5351456509961)
INFO flwr 2024-04-06 04:01:12,849 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:01:12,849 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:01:22,056 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:02:35,890 | server.py:125 | fit progress: (9, 2.071920394897461, {'accuracy': 0.3873, 'data_size': 10000}, 437.5762073440128)
INFO flwr 2024-04-06 04:02:35,890 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:02:35,890 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:02:45,269 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:03:56,476 | server.py:125 | fit progress: (10, 2.231285810470581, {'accuracy': 0.2288, 'data_size': 10000}, 518.1628679230053)
INFO flwr 2024-04-06 04:03:56,477 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:03:56,477 | server.py:153 | FL finished in 518.1633876760025
INFO flwr 2024-04-06 04:03:56,477 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:03:56,477 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:03:56,477 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:03:56,477 | app.py:229 | app_fit: losses_centralized [(0, 2.3025903701782227), (1, 2.3011646270751953), (2, 2.281254529953003), (3, 2.251028537750244), (4, 2.2122726440429688), (5, 2.1298627853393555), (6, 2.0792834758758545), (7, 1.941910743713379), (8, 2.052067279815674), (9, 2.071920394897461), (10, 2.231285810470581)]
INFO flwr 2024-04-06 04:03:56,478 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.1093), (2, 0.1152), (3, 0.2181), (4, 0.2376), (5, 0.362), (6, 0.3901), (7, 0.5593), (8, 0.4025), (9, 0.3873), (10, 0.2288)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2288
wandb:     loss 2.23129
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_035452-nvfchk9u
wandb: Find logs at: ./wandb/offline-run-20240406_035452-nvfchk9u/logs
INFO flwr 2024-04-06 04:04:00,081 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:11:20,555 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=780642)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=780642)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 04:11:25,349	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:11:25,668	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:11:25,947	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6f9de2039151ebed.zip' (8.50MiB) to Ray cluster...
2024-04-06 04:11:25,966	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6f9de2039151ebed.zip'.
INFO flwr 2024-04-06 04:11:37,078 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 146108941722.0, 'object_store_memory': 66903832166.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 04:11:37,079 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:11:37,079 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:11:37,094 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:11:37,095 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:11:37,096 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:11:37,096 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=789042)[0m 2024-04-06 04:11:42.875238: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=789042)[0m 2024-04-06 04:11:43.011391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=789042)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 04:11:44,659 | server.py:94 | initial parameters (loss, other metrics): 2.3024842739105225, {'accuracy': 0.049, 'data_size': 10000}
INFO flwr 2024-04-06 04:11:44,659 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:11:44,660 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=789042)[0m 2024-04-06 04:11:45.289301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=789044)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=789044)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=789046)[0m 2024-04-06 04:11:43.747310: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=789046)[0m 2024-04-06 04:11:43.845324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=789046)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=789046)[0m 2024-04-06 04:11:46.038349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=789032)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=789032)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 04:12:16,352 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:12:19,937 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:12:23,821 | server.py:125 | fit progress: (1, 2.298520803451538, {'accuracy': 0.1992, 'data_size': 10000}, 39.16125339499558)
INFO flwr 2024-04-06 04:12:23,821 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:12:23,821 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:12:46,846 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:13:02,656 | server.py:125 | fit progress: (2, 2.2828707695007324, {'accuracy': 0.0977, 'data_size': 10000}, 77.99633082500077)
INFO flwr 2024-04-06 04:13:02,656 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:13:02,656 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:13:26,455 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:13:46,997 | server.py:125 | fit progress: (3, 2.2000770568847656, {'accuracy': 0.3654, 'data_size': 10000}, 122.3370492120157)
INFO flwr 2024-04-06 04:13:46,997 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:13:46,997 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:14:11,038 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:14:42,411 | server.py:125 | fit progress: (4, 2.0687005519866943, {'accuracy': 0.4036, 'data_size': 10000}, 177.75112952999189)
INFO flwr 2024-04-06 04:14:42,411 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:14:42,411 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:15:05,919 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:15:41,077 | server.py:125 | fit progress: (5, 1.9375272989273071, {'accuracy': 0.5239, 'data_size': 10000}, 236.4172795920167)
INFO flwr 2024-04-06 04:15:41,077 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:15:41,077 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:16:06,366 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:16:49,821 | server.py:125 | fit progress: (6, 1.7345227003097534, {'accuracy': 0.7627, 'data_size': 10000}, 305.16100882101455)
INFO flwr 2024-04-06 04:16:49,821 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:16:49,821 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:17:16,702 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:18:06,246 | server.py:125 | fit progress: (7, 1.7102261781692505, {'accuracy': 0.7633, 'data_size': 10000}, 381.58636889699847)
INFO flwr 2024-04-06 04:18:06,246 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:18:06,246 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:18:30,064 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:19:24,576 | server.py:125 | fit progress: (8, 1.6682322025299072, {'accuracy': 0.8019, 'data_size': 10000}, 459.9163705559913)
INFO flwr 2024-04-06 04:19:24,576 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:19:24,576 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:19:48,057 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:20:58,359 | server.py:125 | fit progress: (9, 1.6425316333770752, {'accuracy': 0.8314, 'data_size': 10000}, 553.6993785020022)
INFO flwr 2024-04-06 04:20:58,359 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:20:58,360 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:21:20,141 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:22:39,750 | server.py:125 | fit progress: (10, 1.6325103044509888, {'accuracy': 0.8399, 'data_size': 10000}, 655.0908625010052)
INFO flwr 2024-04-06 04:22:39,751 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:22:39,751 | server.py:153 | FL finished in 655.0914519510115
INFO flwr 2024-04-06 04:22:39,751 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:22:39,751 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:22:39,751 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:22:39,752 | app.py:229 | app_fit: losses_centralized [(0, 2.3024842739105225), (1, 2.298520803451538), (2, 2.2828707695007324), (3, 2.2000770568847656), (4, 2.0687005519866943), (5, 1.9375272989273071), (6, 1.7345227003097534), (7, 1.7102261781692505), (8, 1.6682322025299072), (9, 1.6425316333770752), (10, 1.6325103044509888)]
INFO flwr 2024-04-06 04:22:39,752 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.049), (1, 0.1992), (2, 0.0977), (3, 0.3654), (4, 0.4036), (5, 0.5239), (6, 0.7627), (7, 0.7633), (8, 0.8019), (9, 0.8314), (10, 0.8399)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8399
wandb:     loss 1.63251
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_041119-urbek46w
wandb: Find logs at: ./wandb/offline-run-20240406_041119-urbek46w/logs
INFO flwr 2024-04-06 04:22:43,342 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:30:05,961 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 04:30:10,668	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:30:11,021	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:30:11,352	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9a97bc7eb1f5a50e.zip' (8.52MiB) to Ray cluster...
2024-04-06 04:30:11,374	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9a97bc7eb1f5a50e.zip'.
INFO flwr 2024-04-06 04:30:22,972 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 145148917146.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 66492393062.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 04:30:22,973 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:30:22,973 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:30:22,989 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:30:22,990 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:30:22,990 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:30:22,990 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=798114)[0m 2024-04-06 04:30:29.085410: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=798114)[0m 2024-04-06 04:30:29.160377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=798114)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 04:30:30,925 | server.py:94 | initial parameters (loss, other metrics): 2.3024861812591553, {'accuracy': 0.1101, 'data_size': 10000}
INFO flwr 2024-04-06 04:30:30,926 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:30:30,926 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=798118)[0m 2024-04-06 04:30:31.248308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=798117)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=798117)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=798116)[0m 2024-04-06 04:30:29.396065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=798116)[0m 2024-04-06 04:30:29.493614: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=798116)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=798111)[0m 2024-04-06 04:30:31.759518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=798111)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=798111)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 04:31:02,090 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:31:05,736 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:31:09,712 | server.py:125 | fit progress: (1, 2.151092767715454, {'accuracy': 0.3817, 'data_size': 10000}, 38.78612439701101)
INFO flwr 2024-04-06 04:31:09,713 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:31:09,713 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:31:33,964 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:31:49,034 | server.py:125 | fit progress: (2, 1.6575318574905396, {'accuracy': 0.8008, 'data_size': 10000}, 78.10812409900245)
INFO flwr 2024-04-06 04:31:49,035 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:31:49,035 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:32:10,146 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:32:31,471 | server.py:125 | fit progress: (3, 1.5178991556167603, {'accuracy': 0.9455, 'data_size': 10000}, 120.54457921898575)
INFO flwr 2024-04-06 04:32:31,471 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:32:31,471 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:32:56,126 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:33:23,756 | server.py:125 | fit progress: (4, 1.5044735670089722, {'accuracy': 0.956, 'data_size': 10000}, 172.82947486100602)
INFO flwr 2024-04-06 04:33:23,756 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:33:23,756 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:33:47,537 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:34:28,230 | server.py:125 | fit progress: (5, 1.4980955123901367, {'accuracy': 0.9633, 'data_size': 10000}, 237.3033594560111)
INFO flwr 2024-04-06 04:34:28,230 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:34:28,230 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:34:50,675 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:35:32,598 | server.py:125 | fit progress: (6, 1.497295618057251, {'accuracy': 0.9643, 'data_size': 10000}, 301.6721937310067)
INFO flwr 2024-04-06 04:35:32,599 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:35:32,599 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:35:57,356 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:36:47,111 | server.py:125 | fit progress: (7, 1.494408369064331, {'accuracy': 0.9664, 'data_size': 10000}, 376.1853113299876)
INFO flwr 2024-04-06 04:36:47,112 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:36:47,112 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:37:11,380 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:38:13,806 | server.py:125 | fit progress: (8, 1.496361255645752, {'accuracy': 0.9648, 'data_size': 10000}, 462.879671837989)
INFO flwr 2024-04-06 04:38:13,806 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:38:13,806 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:38:38,578 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:39:49,058 | server.py:125 | fit progress: (9, 1.512508511543274, {'accuracy': 0.9488, 'data_size': 10000}, 558.131939821993)
INFO flwr 2024-04-06 04:39:49,058 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:39:49,059 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:40:16,172 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 04:41:34,145 | server.py:125 | fit progress: (10, 1.504929542541504, {'accuracy': 0.956, 'data_size': 10000}, 663.218469754007)
INFO flwr 2024-04-06 04:41:34,145 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 04:41:34,145 | server.py:153 | FL finished in 663.2190646860108
INFO flwr 2024-04-06 04:41:34,145 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 04:41:34,146 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 04:41:34,146 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 04:41:34,146 | app.py:229 | app_fit: losses_centralized [(0, 2.3024861812591553), (1, 2.151092767715454), (2, 1.6575318574905396), (3, 1.5178991556167603), (4, 1.5044735670089722), (5, 1.4980955123901367), (6, 1.497295618057251), (7, 1.494408369064331), (8, 1.496361255645752), (9, 1.512508511543274), (10, 1.504929542541504)]
INFO flwr 2024-04-06 04:41:34,146 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1101), (1, 0.3817), (2, 0.8008), (3, 0.9455), (4, 0.956), (5, 0.9633), (6, 0.9643), (7, 0.9664), (8, 0.9648), (9, 0.9488), (10, 0.956)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.956
wandb:     loss 1.50493
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_043005-2mh7gvd6
wandb: Find logs at: ./wandb/offline-run-20240406_043005-2mh7gvd6/logs
INFO flwr 2024-04-06 04:41:37,802 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 04:48:57,976 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=798109)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=798109)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 04:49:04,004	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 04:49:04,304	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 04:49:04,617	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bf7216a5a4bae9a3.zip' (8.55MiB) to Ray cluster...
2024-04-06 04:49:04,636	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bf7216a5a4bae9a3.zip'.
INFO flwr 2024-04-06 04:49:15,565 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 66553114214.0, 'node:10.20.240.18': 1.0, 'memory': 145290599834.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 04:49:15,566 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 04:49:15,566 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 04:49:15,583 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 04:49:15,584 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 04:49:15,584 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 04:49:15,584 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=806419)[0m 2024-04-06 04:49:21.369768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=806419)[0m 2024-04-06 04:49:21.462361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=806419)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=806409)[0m 2024-04-06 04:49:23.522588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 04:49:24,257 | server.py:94 | initial parameters (loss, other metrics): 2.3026444911956787, {'accuracy': 0.0931, 'data_size': 10000}
INFO flwr 2024-04-06 04:49:24,257 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 04:49:24,258 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=806424)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=806424)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=806422)[0m 2024-04-06 04:49:21.823651: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=806422)[0m 2024-04-06 04:49:21.932138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=806422)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=806422)[0m 2024-04-06 04:49:24.185391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=806415)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=806415)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 04:49:55,616 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 04:49:58,795 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 04:50:02,638 | server.py:125 | fit progress: (1, 2.072032928466797, {'accuracy': 0.4236, 'data_size': 10000}, 38.38021200400544)
INFO flwr 2024-04-06 04:50:02,638 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 04:50:02,638 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:50:27,300 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 04:50:41,981 | server.py:125 | fit progress: (2, 1.814885139465332, {'accuracy': 0.6474, 'data_size': 10000}, 77.72325439998531)
INFO flwr 2024-04-06 04:50:41,981 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 04:50:41,981 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:51:07,965 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 04:51:29,099 | server.py:125 | fit progress: (3, 1.7758513689041138, {'accuracy': 0.6843, 'data_size': 10000}, 124.84173871899839)
INFO flwr 2024-04-06 04:51:29,099 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 04:51:29,100 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:51:55,877 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 04:52:23,334 | server.py:125 | fit progress: (4, 1.878516435623169, {'accuracy': 0.582, 'data_size': 10000}, 179.07698782498483)
INFO flwr 2024-04-06 04:52:23,335 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 04:52:23,335 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:52:47,365 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 04:53:28,429 | server.py:125 | fit progress: (5, 1.9106148481369019, {'accuracy': 0.5498, 'data_size': 10000}, 244.17125322899665)
INFO flwr 2024-04-06 04:53:28,429 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 04:53:28,429 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:53:50,571 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 04:54:29,036 | server.py:125 | fit progress: (6, 1.7152777910232544, {'accuracy': 0.7455, 'data_size': 10000}, 304.77848874698975)
INFO flwr 2024-04-06 04:54:29,036 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 04:54:29,036 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:54:53,183 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 04:55:48,918 | server.py:125 | fit progress: (7, 2.041588068008423, {'accuracy': 0.4192, 'data_size': 10000}, 384.6610087050067)
INFO flwr 2024-04-06 04:55:48,919 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 04:55:48,919 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:56:13,150 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 04:57:14,228 | server.py:125 | fit progress: (8, 1.9721994400024414, {'accuracy': 0.4881, 'data_size': 10000}, 469.97100543099805)
INFO flwr 2024-04-06 04:57:14,229 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 04:57:14,229 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:57:37,812 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 04:58:44,791 | server.py:125 | fit progress: (9, 1.903480052947998, {'accuracy': 0.5574, 'data_size': 10000}, 560.5337143740035)
INFO flwr 2024-04-06 04:58:44,791 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 04:58:44,792 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 04:59:13,024 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:00:28,696 | server.py:125 | fit progress: (10, 1.9367244243621826, {'accuracy': 0.5244, 'data_size': 10000}, 664.4386956419912)
INFO flwr 2024-04-06 05:00:28,697 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:00:28,697 | server.py:153 | FL finished in 664.4393503119936
INFO flwr 2024-04-06 05:00:28,697 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:00:28,697 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:00:28,697 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:00:28,697 | app.py:229 | app_fit: losses_centralized [(0, 2.3026444911956787), (1, 2.072032928466797), (2, 1.814885139465332), (3, 1.7758513689041138), (4, 1.878516435623169), (5, 1.9106148481369019), (6, 1.7152777910232544), (7, 2.041588068008423), (8, 1.9721994400024414), (9, 1.903480052947998), (10, 1.9367244243621826)]
INFO flwr 2024-04-06 05:00:28,697 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0931), (1, 0.4236), (2, 0.6474), (3, 0.6843), (4, 0.582), (5, 0.5498), (6, 0.7455), (7, 0.4192), (8, 0.4881), (9, 0.5574), (10, 0.5244)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5244
wandb:     loss 1.93672
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_044857-ffz691gt
wandb: Find logs at: ./wandb/offline-run-20240406_044857-ffz691gt/logs
INFO flwr 2024-04-06 05:00:32,307 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:07:52,386 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=806409)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=806409)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 05:07:57,605	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:07:57,957	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:07:58,350	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d3c5f511c5ccd6be.zip' (8.57MiB) to Ray cluster...
2024-04-06 05:07:58,368	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d3c5f511c5ccd6be.zip'.
INFO flwr 2024-04-06 05:08:09,552 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 145064862311.0, 'object_store_memory': 66456369561.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 05:08:09,552 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:08:09,553 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:08:09,569 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:08:09,570 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:08:09,570 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:08:09,570 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=815523)[0m 2024-04-06 05:08:15.364817: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=815523)[0m 2024-04-06 05:08:15.464679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=815523)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=815531)[0m 2024-04-06 05:08:17.769579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 05:08:18,474 | server.py:94 | initial parameters (loss, other metrics): 2.302600145339966, {'accuracy': 0.0953, 'data_size': 10000}
INFO flwr 2024-04-06 05:08:18,475 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:08:18,475 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=815530)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=815530)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=815530)[0m 2024-04-06 05:08:15.938185: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=815530)[0m 2024-04-06 05:08:16.050522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=815530)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=815525)[0m 2024-04-06 05:08:18.451394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=815520)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=815520)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:08:48,902 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:08:52,485 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:08:56,409 | server.py:125 | fit progress: (1, 2.2258501052856445, {'accuracy': 0.1604, 'data_size': 10000}, 37.934665534994565)
INFO flwr 2024-04-06 05:08:56,410 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:08:56,410 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:09:19,225 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:09:33,009 | server.py:125 | fit progress: (2, 2.1653010845184326, {'accuracy': 0.2791, 'data_size': 10000}, 74.53464392398018)
INFO flwr 2024-04-06 05:09:33,010 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:09:33,010 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:09:57,860 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:10:19,177 | server.py:125 | fit progress: (3, 1.9775910377502441, {'accuracy': 0.4786, 'data_size': 10000}, 120.70251673099119)
INFO flwr 2024-04-06 05:10:19,178 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:10:19,178 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:10:43,830 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:11:13,147 | server.py:125 | fit progress: (4, 2.0642974376678467, {'accuracy': 0.3951, 'data_size': 10000}, 174.67210249599884)
INFO flwr 2024-04-06 05:11:13,148 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:11:13,148 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:11:36,632 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:12:13,296 | server.py:125 | fit progress: (5, 1.9809123277664185, {'accuracy': 0.4772, 'data_size': 10000}, 234.8213479190017)
INFO flwr 2024-04-06 05:12:13,296 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:12:13,297 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:12:34,077 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:13:17,561 | server.py:125 | fit progress: (6, 1.9449058771133423, {'accuracy': 0.5153, 'data_size': 10000}, 299.0860037620005)
INFO flwr 2024-04-06 05:13:17,561 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:13:17,561 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:13:38,830 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:14:25,464 | server.py:125 | fit progress: (7, 2.0420854091644287, {'accuracy': 0.4187, 'data_size': 10000}, 366.98890896097873)
INFO flwr 2024-04-06 05:14:25,464 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:14:25,464 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:14:49,854 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:15:53,327 | server.py:125 | fit progress: (8, 2.2207674980163574, {'accuracy': 0.2404, 'data_size': 10000}, 454.8523965420027)
INFO flwr 2024-04-06 05:15:53,327 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:15:53,328 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:16:15,980 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:17:16,457 | server.py:125 | fit progress: (9, 2.1254374980926514, {'accuracy': 0.3356, 'data_size': 10000}, 537.9817366859934)
INFO flwr 2024-04-06 05:17:16,457 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:17:16,457 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:17:40,906 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:19:03,259 | server.py:125 | fit progress: (10, 2.2326314449310303, {'accuracy': 0.2284, 'data_size': 10000}, 644.7839380989899)
INFO flwr 2024-04-06 05:19:03,259 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:19:03,259 | server.py:153 | FL finished in 644.7844827270019
INFO flwr 2024-04-06 05:19:03,259 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:19:03,259 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:19:03,260 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:19:03,260 | app.py:229 | app_fit: losses_centralized [(0, 2.302600145339966), (1, 2.2258501052856445), (2, 2.1653010845184326), (3, 1.9775910377502441), (4, 2.0642974376678467), (5, 1.9809123277664185), (6, 1.9449058771133423), (7, 2.0420854091644287), (8, 2.2207674980163574), (9, 2.1254374980926514), (10, 2.2326314449310303)]
INFO flwr 2024-04-06 05:19:03,260 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0953), (1, 0.1604), (2, 0.2791), (3, 0.4786), (4, 0.3951), (5, 0.4772), (6, 0.5153), (7, 0.4187), (8, 0.2404), (9, 0.3356), (10, 0.2284)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2284
wandb:     loss 2.23263
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_050752-6jps9qpo
wandb: Find logs at: ./wandb/offline-run-20240406_050752-6jps9qpo/logs
INFO flwr 2024-04-06 05:19:06,813 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:26:27,006 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 05:26:32,908	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:26:33,177	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:26:33,554	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e07262ce56df6289.zip' (8.60MiB) to Ray cluster...
2024-04-06 05:26:33,582	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e07262ce56df6289.zip'.
INFO flwr 2024-04-06 05:26:44,447 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 66334135910.0, 'node:10.20.240.18': 1.0, 'memory': 144779650458.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 05:26:44,447 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:26:44,447 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:26:44,464 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:26:44,465 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:26:44,465 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:26:44,465 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=824298)[0m 2024-04-06 05:26:50.055952: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=824298)[0m 2024-04-06 05:26:50.159737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=824298)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=824287)[0m 2024-04-06 05:26:52.533589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 05:26:52,834 | server.py:94 | initial parameters (loss, other metrics): 2.3026926517486572, {'accuracy': 0.1022, 'data_size': 10000}
INFO flwr 2024-04-06 05:26:52,835 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:26:52,835 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=824297)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=824297)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=824296)[0m 2024-04-06 05:26:50.697788: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=824296)[0m 2024-04-06 05:26:50.799370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=824296)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=824301)[0m 2024-04-06 05:26:53.279430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=824295)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=824295)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 05:27:25,719 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:27:28,571 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:27:32,436 | server.py:125 | fit progress: (1, 2.273118257522583, {'accuracy': 0.1125, 'data_size': 10000}, 39.600771131023066)
INFO flwr 2024-04-06 05:27:32,436 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:27:32,436 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:27:58,919 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:28:13,258 | server.py:125 | fit progress: (2, 2.014465570449829, {'accuracy': 0.4773, 'data_size': 10000}, 80.42304286500439)
INFO flwr 2024-04-06 05:28:13,259 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:28:13,260 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:28:39,373 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:29:00,646 | server.py:125 | fit progress: (3, 2.0773324966430664, {'accuracy': 0.3816, 'data_size': 10000}, 127.81151922402205)
INFO flwr 2024-04-06 05:29:00,647 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:29:00,647 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:29:24,416 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:29:52,418 | server.py:125 | fit progress: (4, 2.132143974304199, {'accuracy': 0.3265, 'data_size': 10000}, 179.58324666702538)
INFO flwr 2024-04-06 05:29:52,418 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:29:52,419 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:30:14,979 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:30:53,564 | server.py:125 | fit progress: (5, 2.0870442390441895, {'accuracy': 0.3743, 'data_size': 10000}, 240.72947618400212)
INFO flwr 2024-04-06 05:30:53,565 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:30:53,565 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:31:20,186 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:32:06,558 | server.py:125 | fit progress: (6, 2.092275857925415, {'accuracy': 0.3681, 'data_size': 10000}, 313.72324395700707)
INFO flwr 2024-04-06 05:32:06,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:32:06,559 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:32:30,418 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:33:23,632 | server.py:125 | fit progress: (7, 2.2777037620544434, {'accuracy': 0.1832, 'data_size': 10000}, 390.79671828402206)
INFO flwr 2024-04-06 05:33:23,632 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:33:23,632 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:33:45,593 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:34:44,056 | server.py:125 | fit progress: (8, 2.2795610427856445, {'accuracy': 0.1815, 'data_size': 10000}, 471.2212487180077)
INFO flwr 2024-04-06 05:34:44,057 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:34:44,057 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:35:06,452 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:36:25,047 | server.py:125 | fit progress: (9, 2.1803858280181885, {'accuracy': 0.2806, 'data_size': 10000}, 572.2118173550116)
INFO flwr 2024-04-06 05:36:25,047 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:36:25,047 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:36:49,189 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:38:17,461 | server.py:125 | fit progress: (10, 2.312833070755005, {'accuracy': 0.1483, 'data_size': 10000}, 684.6263603590196)
INFO flwr 2024-04-06 05:38:17,461 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:38:17,462 | server.py:153 | FL finished in 684.6267426880077
INFO flwr 2024-04-06 05:38:17,462 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:38:17,462 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:38:17,462 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:38:17,462 | app.py:229 | app_fit: losses_centralized [(0, 2.3026926517486572), (1, 2.273118257522583), (2, 2.014465570449829), (3, 2.0773324966430664), (4, 2.132143974304199), (5, 2.0870442390441895), (6, 2.092275857925415), (7, 2.2777037620544434), (8, 2.2795610427856445), (9, 2.1803858280181885), (10, 2.312833070755005)]
INFO flwr 2024-04-06 05:38:17,462 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1022), (1, 0.1125), (2, 0.4773), (3, 0.3816), (4, 0.3265), (5, 0.3743), (6, 0.3681), (7, 0.1832), (8, 0.1815), (9, 0.2806), (10, 0.1483)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1483
wandb:     loss 2.31283
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_052626-8cny5kg8
wandb: Find logs at: ./wandb/offline-run-20240406_052626-8cny5kg8/logs
INFO flwr 2024-04-06 05:38:20,967 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 05:45:42,755 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=824287)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=824287)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 05:45:49,831	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 05:45:50,850	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 05:45:51,234	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_60dc0c1063855258.zip' (8.62MiB) to Ray cluster...
2024-04-06 05:45:51,252	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_60dc0c1063855258.zip'.
INFO flwr 2024-04-06 05:46:02,226 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 65848383897.0, 'node:10.20.240.18': 1.0, 'memory': 143646229095.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 05:46:02,227 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 05:46:02,227 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 05:46:02,244 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 05:46:02,245 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 05:46:02,245 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 05:46:02,245 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=833392)[0m 2024-04-06 05:46:07.944068: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=833392)[0m 2024-04-06 05:46:08.032610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=833392)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=833386)[0m 2024-04-06 05:46:10.374157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 05:46:10,483 | server.py:94 | initial parameters (loss, other metrics): 2.302732229232788, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-06 05:46:10,484 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 05:46:10,484 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=833388)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=833388)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=833388)[0m 2024-04-06 05:46:08.943722: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=833388)[0m 2024-04-06 05:46:09.071992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=833388)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=833388)[0m 2024-04-06 05:46:11.367714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=833384)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=833384)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 05:46:42,589 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 05:46:45,697 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 05:46:49,654 | server.py:125 | fit progress: (1, 2.2719836235046387, {'accuracy': 0.1788, 'data_size': 10000}, 39.16964372099028)
INFO flwr 2024-04-06 05:46:49,654 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 05:46:49,654 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:47:13,939 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 05:47:28,163 | server.py:125 | fit progress: (2, 2.1800856590270996, {'accuracy': 0.2931, 'data_size': 10000}, 77.67854782298673)
INFO flwr 2024-04-06 05:47:28,163 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 05:47:28,164 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:47:52,152 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 05:48:13,044 | server.py:125 | fit progress: (3, 1.9572218656539917, {'accuracy': 0.5389, 'data_size': 10000}, 122.56016244698549)
INFO flwr 2024-04-06 05:48:13,045 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 05:48:13,045 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:48:38,131 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 05:49:04,775 | server.py:125 | fit progress: (4, 2.0891497135162354, {'accuracy': 0.378, 'data_size': 10000}, 174.29068464698503)
INFO flwr 2024-04-06 05:49:04,775 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 05:49:04,775 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:49:27,509 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 05:50:06,920 | server.py:125 | fit progress: (5, 2.130066156387329, {'accuracy': 0.3254, 'data_size': 10000}, 236.4361098149966)
INFO flwr 2024-04-06 05:50:06,921 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 05:50:06,921 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:50:31,999 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 05:51:12,769 | server.py:125 | fit progress: (6, 2.1478018760681152, {'accuracy': 0.3129, 'data_size': 10000}, 302.28476414698525)
INFO flwr 2024-04-06 05:51:12,769 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 05:51:12,769 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:51:35,609 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 05:52:22,994 | server.py:125 | fit progress: (7, 2.362924814224243, {'accuracy': 0.0982, 'data_size': 10000}, 372.5098401920113)
INFO flwr 2024-04-06 05:52:22,994 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 05:52:22,994 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:52:43,386 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 05:53:40,472 | server.py:125 | fit progress: (8, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 449.9878650319879)
INFO flwr 2024-04-06 05:53:40,472 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 05:53:40,473 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:54:04,675 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 05:55:21,592 | server.py:125 | fit progress: (9, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 551.107771534007)
INFO flwr 2024-04-06 05:55:21,592 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 05:55:21,592 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 05:55:46,409 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 05:57:17,154 | server.py:125 | fit progress: (10, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 666.6699322460045)
INFO flwr 2024-04-06 05:57:17,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 05:57:17,155 | server.py:153 | FL finished in 666.6705619469867
INFO flwr 2024-04-06 05:57:17,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 05:57:17,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 05:57:17,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 05:57:17,155 | app.py:229 | app_fit: losses_centralized [(0, 2.302732229232788), (1, 2.2719836235046387), (2, 2.1800856590270996), (3, 1.9572218656539917), (4, 2.0891497135162354), (5, 2.130066156387329), (6, 2.1478018760681152), (7, 2.362924814224243), (8, 2.3629422187805176), (9, 2.3629422187805176), (10, 2.3629422187805176)]
INFO flwr 2024-04-06 05:57:17,155 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.1788), (2, 0.2931), (3, 0.5389), (4, 0.378), (5, 0.3254), (6, 0.3129), (7, 0.0982), (8, 0.0982), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.36294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_054542-qi5sjdgw
wandb: Find logs at: ./wandb/offline-run-20240406_054542-qi5sjdgw/logs
INFO flwr 2024-04-06 05:57:20,739 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:04:41,232 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 06:04:46,129	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:04:46,429	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:04:46,703	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_85bcd97c1777c59c.zip' (8.65MiB) to Ray cluster...
2024-04-06 06:04:46,725	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_85bcd97c1777c59c.zip'.
INFO flwr 2024-04-06 06:04:57,474 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 66136660377.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 144318874215.0}
INFO flwr 2024-04-06 06:04:57,475 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:04:57,475 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:04:57,492 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:04:57,492 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:04:57,493 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:04:57,493 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=841884)[0m 2024-04-06 06:05:03.091530: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=841884)[0m 2024-04-06 06:05:03.194808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=841884)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 06:05:05,290 | server.py:94 | initial parameters (loss, other metrics): 2.3023605346679688, {'accuracy': 0.1717, 'data_size': 10000}
INFO flwr 2024-04-06 06:05:05,290 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:05:05,291 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=841884)[0m 2024-04-06 06:05:05.413802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=841893)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=841893)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=841892)[0m 2024-04-06 06:05:04.103882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841892)[0m 2024-04-06 06:05:04.193963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841892)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=841881)[0m 2024-04-06 06:05:06.515387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=841882)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=841882)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 06:05:35,462 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:05:38,793 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:05:42,673 | server.py:125 | fit progress: (1, 2.2850747108459473, {'accuracy': 0.1795, 'data_size': 10000}, 37.382316981995245)
INFO flwr 2024-04-06 06:05:42,673 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:05:42,674 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:06:02,830 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:06:15,400 | server.py:125 | fit progress: (2, 2.262186288833618, {'accuracy': 0.1883, 'data_size': 10000}, 70.10935644400888)
INFO flwr 2024-04-06 06:06:15,401 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:06:15,401 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:06:38,323 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:06:59,245 | server.py:125 | fit progress: (3, 2.094850778579712, {'accuracy': 0.3766, 'data_size': 10000}, 113.95454560799408)
INFO flwr 2024-04-06 06:06:59,246 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:06:59,246 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:07:21,780 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:07:49,145 | server.py:125 | fit progress: (4, 2.197979688644409, {'accuracy': 0.2568, 'data_size': 10000}, 163.85443165100878)
INFO flwr 2024-04-06 06:07:49,146 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:07:49,146 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:08:12,461 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:08:45,855 | server.py:125 | fit progress: (5, 1.9982084035873413, {'accuracy': 0.4609, 'data_size': 10000}, 220.56447468101396)
INFO flwr 2024-04-06 06:08:45,856 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:08:45,856 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:09:08,544 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:09:57,098 | server.py:125 | fit progress: (6, 2.04016375541687, {'accuracy': 0.4186, 'data_size': 10000}, 291.80692142099724)
INFO flwr 2024-04-06 06:09:57,098 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:09:57,098 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:10:20,024 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:11:09,082 | server.py:125 | fit progress: (7, 2.1488025188446045, {'accuracy': 0.3123, 'data_size': 10000}, 363.7911088090041)
INFO flwr 2024-04-06 06:11:09,082 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:11:09,082 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:11:30,894 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:12:26,920 | server.py:125 | fit progress: (8, 2.061258554458618, {'accuracy': 0.397, 'data_size': 10000}, 441.62935490798554)
INFO flwr 2024-04-06 06:12:26,921 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:12:26,921 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:12:50,618 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:13:53,462 | server.py:125 | fit progress: (9, 2.162973403930664, {'accuracy': 0.2981, 'data_size': 10000}, 528.1709706129914)
INFO flwr 2024-04-06 06:13:53,462 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:13:53,462 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:14:16,973 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:15:22,033 | server.py:125 | fit progress: (10, 2.186066150665283, {'accuracy': 0.2748, 'data_size': 10000}, 616.7420126750076)
INFO flwr 2024-04-06 06:15:22,033 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:15:22,033 | server.py:153 | FL finished in 616.7424597179925
INFO flwr 2024-04-06 06:15:22,033 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:15:22,034 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:15:22,034 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:15:22,034 | app.py:229 | app_fit: losses_centralized [(0, 2.3023605346679688), (1, 2.2850747108459473), (2, 2.262186288833618), (3, 2.094850778579712), (4, 2.197979688644409), (5, 1.9982084035873413), (6, 2.04016375541687), (7, 2.1488025188446045), (8, 2.061258554458618), (9, 2.162973403930664), (10, 2.186066150665283)]
INFO flwr 2024-04-06 06:15:22,034 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1717), (1, 0.1795), (2, 0.1883), (3, 0.3766), (4, 0.2568), (5, 0.4609), (6, 0.4186), (7, 0.3123), (8, 0.397), (9, 0.2981), (10, 0.2748)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2748
wandb:     loss 2.18607
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_060440-ytnno6g9
wandb: Find logs at: ./wandb/offline-run-20240406_060440-ytnno6g9/logs
INFO flwr 2024-04-06 06:15:25,681 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:22:45,585 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=841881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=841881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 06:22:50,482	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:22:50,888	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:22:51,243	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_57d5413bd188e8f8.zip' (8.69MiB) to Ray cluster...
2024-04-06 06:22:51,264	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_57d5413bd188e8f8.zip'.
INFO flwr 2024-04-06 06:23:02,027 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 144025493709.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66010925875.0, 'CPU': 64.0}
INFO flwr 2024-04-06 06:23:02,030 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:23:02,030 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:23:02,045 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:23:02,046 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:23:02,046 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:23:02,046 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=853893)[0m 2024-04-06 06:23:08.053736: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=853893)[0m 2024-04-06 06:23:08.158491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=853893)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 06:23:09,750 | server.py:94 | initial parameters (loss, other metrics): 2.3025832176208496, {'accuracy': 0.099, 'data_size': 10000}
INFO flwr 2024-04-06 06:23:09,750 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:23:09,751 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=853888)[0m 2024-04-06 06:23:10.206346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=853893)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=853893)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=853894)[0m 2024-04-06 06:23:08.268358: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=853894)[0m 2024-04-06 06:23:08.366639: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=853894)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=853894)[0m 2024-04-06 06:23:10.353935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=853881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=853881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 06:23:59,185 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:24:02,516 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:24:06,393 | server.py:125 | fit progress: (1, 2.297924280166626, {'accuracy': 0.1009, 'data_size': 10000}, 56.643063411989715)
INFO flwr 2024-04-06 06:24:06,394 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:24:06,394 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:24:45,956 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:24:58,820 | server.py:125 | fit progress: (2, 2.2569210529327393, {'accuracy': 0.2012, 'data_size': 10000}, 109.06981869999436)
INFO flwr 2024-04-06 06:24:58,820 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:24:58,821 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:25:38,821 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:25:59,687 | server.py:125 | fit progress: (3, 2.161339282989502, {'accuracy': 0.2903, 'data_size': 10000}, 169.93627170598484)
INFO flwr 2024-04-06 06:25:59,687 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:25:59,687 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:26:39,036 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:27:05,304 | server.py:125 | fit progress: (4, 1.9090080261230469, {'accuracy': 0.6151, 'data_size': 10000}, 235.5531680219865)
INFO flwr 2024-04-06 06:27:05,304 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:27:05,304 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:27:44,581 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:28:21,832 | server.py:125 | fit progress: (5, 1.647957444190979, {'accuracy': 0.8565, 'data_size': 10000}, 312.08162729800097)
INFO flwr 2024-04-06 06:28:21,832 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:28:21,833 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:29:03,172 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:29:56,606 | server.py:125 | fit progress: (6, 1.6114860773086548, {'accuracy': 0.8743, 'data_size': 10000}, 406.8555036719772)
INFO flwr 2024-04-06 06:29:56,606 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:29:56,606 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:30:39,552 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:31:29,747 | server.py:125 | fit progress: (7, 1.5807223320007324, {'accuracy': 0.898, 'data_size': 10000}, 499.9964702249854)
INFO flwr 2024-04-06 06:31:29,747 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:31:29,747 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:32:12,937 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:33:08,900 | server.py:125 | fit progress: (8, 1.5915738344192505, {'accuracy': 0.8799, 'data_size': 10000}, 599.1496114929905)
INFO flwr 2024-04-06 06:33:08,901 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:33:08,901 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:33:50,853 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:34:59,032 | server.py:125 | fit progress: (9, 1.570054531097412, {'accuracy': 0.9043, 'data_size': 10000}, 709.2813890189864)
INFO flwr 2024-04-06 06:34:59,033 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:34:59,034 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:35:44,287 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:37:11,463 | server.py:125 | fit progress: (10, 1.5603039264678955, {'accuracy': 0.9113, 'data_size': 10000}, 841.7126076169952)
INFO flwr 2024-04-06 06:37:11,464 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:37:11,464 | server.py:153 | FL finished in 841.7133611049794
INFO flwr 2024-04-06 06:37:11,464 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:37:11,464 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:37:11,464 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:37:11,465 | app.py:229 | app_fit: losses_centralized [(0, 2.3025832176208496), (1, 2.297924280166626), (2, 2.2569210529327393), (3, 2.161339282989502), (4, 1.9090080261230469), (5, 1.647957444190979), (6, 1.6114860773086548), (7, 1.5807223320007324), (8, 1.5915738344192505), (9, 1.570054531097412), (10, 1.5603039264678955)]
INFO flwr 2024-04-06 06:37:11,465 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.099), (1, 0.1009), (2, 0.2012), (3, 0.2903), (4, 0.6151), (5, 0.8565), (6, 0.8743), (7, 0.898), (8, 0.8799), (9, 0.9043), (10, 0.9113)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9113
wandb:     loss 1.5603
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_062245-wap34szg
wandb: Find logs at: ./wandb/offline-run-20240406_062245-wap34szg/logs
INFO flwr 2024-04-06 06:37:15,112 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 06:44:35,205 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 06:44:40,386	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 06:44:40,704	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 06:44:41,026	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_35eb82e72e42980e.zip' (8.72MiB) to Ray cluster...
2024-04-06 06:44:41,048	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_35eb82e72e42980e.zip'.
INFO flwr 2024-04-06 06:44:51,810 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 143948472116.0, 'object_store_memory': 65977916620.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 06:44:51,810 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 06:44:51,810 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 06:44:51,827 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 06:44:51,828 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 06:44:51,828 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 06:44:51,829 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=866019)[0m 2024-04-06 06:44:57.584775: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=866019)[0m 2024-04-06 06:44:57.691103: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=866019)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=866030)[0m 2024-04-06 06:44:59.831264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 06:45:00,176 | server.py:94 | initial parameters (loss, other metrics): 2.302597761154175, {'accuracy': 0.1026, 'data_size': 10000}
INFO flwr 2024-04-06 06:45:00,176 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 06:45:00,177 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=866031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=866031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=866033)[0m 2024-04-06 06:44:58.127919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=866033)[0m 2024-04-06 06:44:58.253779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=866033)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=866033)[0m 2024-04-06 06:45:00.370691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=866019)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=866019)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 06:45:52,107 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 06:45:55,688 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 06:45:59,557 | server.py:125 | fit progress: (1, 1.8139456510543823, {'accuracy': 0.7639, 'data_size': 10000}, 59.38035943399882)
INFO flwr 2024-04-06 06:45:59,557 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 06:45:59,557 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:46:41,506 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 06:46:54,020 | server.py:125 | fit progress: (2, 1.533231258392334, {'accuracy': 0.9288, 'data_size': 10000}, 113.84376897700713)
INFO flwr 2024-04-06 06:46:54,021 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 06:46:54,021 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:47:33,543 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 06:47:51,857 | server.py:125 | fit progress: (3, 1.5174599885940552, {'accuracy': 0.944, 'data_size': 10000}, 171.68081336602336)
INFO flwr 2024-04-06 06:47:51,858 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 06:47:51,858 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:48:36,279 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 06:49:02,155 | server.py:125 | fit progress: (4, 1.5478352308273315, {'accuracy': 0.913, 'data_size': 10000}, 241.97840835200623)
INFO flwr 2024-04-06 06:49:02,155 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 06:49:02,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:49:46,016 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 06:50:23,934 | server.py:125 | fit progress: (5, 1.8913769721984863, {'accuracy': 0.5693, 'data_size': 10000}, 323.7570854410005)
INFO flwr 2024-04-06 06:50:23,934 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 06:50:23,934 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:51:05,166 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 06:51:49,061 | server.py:125 | fit progress: (6, 1.7122910022735596, {'accuracy': 0.7489, 'data_size': 10000}, 408.8841128060012)
INFO flwr 2024-04-06 06:51:49,061 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 06:51:49,061 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:52:29,097 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 06:53:15,072 | server.py:125 | fit progress: (7, 1.5543140172958374, {'accuracy': 0.9069, 'data_size': 10000}, 494.8957963810244)
INFO flwr 2024-04-06 06:53:15,073 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 06:53:15,073 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:53:54,572 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 06:54:53,348 | server.py:125 | fit progress: (8, 1.5801461935043335, {'accuracy': 0.881, 'data_size': 10000}, 593.1715208460228)
INFO flwr 2024-04-06 06:54:53,348 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 06:54:53,349 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:55:33,947 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 06:56:41,444 | server.py:125 | fit progress: (9, 1.6351577043533325, {'accuracy': 0.8258, 'data_size': 10000}, 701.267257598025)
INFO flwr 2024-04-06 06:56:41,444 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 06:56:41,444 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 06:57:23,268 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 06:58:35,696 | server.py:125 | fit progress: (10, 1.697503924369812, {'accuracy': 0.7634, 'data_size': 10000}, 815.5190742830164)
INFO flwr 2024-04-06 06:58:35,696 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 06:58:35,696 | server.py:153 | FL finished in 815.5194687280164
INFO flwr 2024-04-06 06:58:35,696 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 06:58:35,696 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 06:58:35,696 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 06:58:35,697 | app.py:229 | app_fit: losses_centralized [(0, 2.302597761154175), (1, 1.8139456510543823), (2, 1.533231258392334), (3, 1.5174599885940552), (4, 1.5478352308273315), (5, 1.8913769721984863), (6, 1.7122910022735596), (7, 1.5543140172958374), (8, 1.5801461935043335), (9, 1.6351577043533325), (10, 1.697503924369812)]
INFO flwr 2024-04-06 06:58:35,697 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1026), (1, 0.7639), (2, 0.9288), (3, 0.944), (4, 0.913), (5, 0.5693), (6, 0.7489), (7, 0.9069), (8, 0.881), (9, 0.8258), (10, 0.7634)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7634
wandb:     loss 1.6975
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_064434-dqd0gjmf
wandb: Find logs at: ./wandb/offline-run-20240406_064434-dqd0gjmf/logs
INFO flwr 2024-04-06 06:58:39,233 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:06:00,256 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=866017)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=866017)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 07:06:05,979	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:06:07,704	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:06:07,978	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d1fb3723caa38f79.zip' (8.76MiB) to Ray cluster...
2024-04-06 07:06:08,007	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d1fb3723caa38f79.zip'.
INFO flwr 2024-04-06 07:06:18,578 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 143649552180.0, 'object_store_memory': 65849808076.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 07:06:18,579 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:06:18,579 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:06:18,595 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:06:18,596 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:06:18,596 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:06:18,596 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=878790)[0m 2024-04-06 07:06:25.091929: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=878795)[0m 2024-04-06 07:06:25.176489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=878795)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 07:06:25,890 | server.py:94 | initial parameters (loss, other metrics): 2.3024702072143555, {'accuracy': 0.0862, 'data_size': 10000}
INFO flwr 2024-04-06 07:06:25,890 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:06:25,890 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=878797)[0m 2024-04-06 07:06:27.945802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=878797)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=878797)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=878791)[0m 2024-04-06 07:06:25.273792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=878791)[0m 2024-04-06 07:06:25.349393: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=878791)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=878792)[0m 2024-04-06 07:06:27.972044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 07:07:18,066 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:07:21,386 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:07:25,281 | server.py:125 | fit progress: (1, 2.3277547359466553, {'accuracy': 0.1329, 'data_size': 10000}, 59.390935709001496)
INFO flwr 2024-04-06 07:07:25,281 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:07:25,282 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:08:05,650 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:08:19,581 | server.py:125 | fit progress: (2, 1.992639422416687, {'accuracy': 0.4693, 'data_size': 10000}, 113.69129554601386)
INFO flwr 2024-04-06 07:08:19,582 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:08:19,582 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:09:03,391 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:09:26,418 | server.py:125 | fit progress: (3, 1.9364722967147827, {'accuracy': 0.5233, 'data_size': 10000}, 180.5281825440179)
INFO flwr 2024-04-06 07:09:26,419 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:09:26,419 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:10:12,777 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:10:41,912 | server.py:125 | fit progress: (4, 1.948026180267334, {'accuracy': 0.5117, 'data_size': 10000}, 256.02140993101057)
INFO flwr 2024-04-06 07:10:41,912 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:10:41,912 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:11:25,235 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:12:01,907 | server.py:125 | fit progress: (5, 1.9914804697036743, {'accuracy': 0.4691, 'data_size': 10000}, 336.017153844994)
INFO flwr 2024-04-06 07:12:01,908 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:12:01,908 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:12:42,470 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:13:23,476 | server.py:125 | fit progress: (6, 1.9616944789886475, {'accuracy': 0.4988, 'data_size': 10000}, 417.5859546500142)
INFO flwr 2024-04-06 07:13:23,476 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:13:23,477 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:14:07,122 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:15:02,577 | server.py:125 | fit progress: (7, 1.8810276985168457, {'accuracy': 0.5801, 'data_size': 10000}, 516.687180776993)
INFO flwr 2024-04-06 07:15:02,578 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:15:02,578 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:15:40,941 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:16:36,595 | server.py:125 | fit progress: (8, 2.1281161308288574, {'accuracy': 0.3329, 'data_size': 10000}, 610.7046992490068)
INFO flwr 2024-04-06 07:16:36,595 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:16:36,595 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:17:18,202 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:18:17,088 | server.py:125 | fit progress: (9, 1.9345372915267944, {'accuracy': 0.5265, 'data_size': 10000}, 711.1980381740141)
INFO flwr 2024-04-06 07:18:17,088 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:18:17,089 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:18:57,657 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:20:35,384 | server.py:125 | fit progress: (10, 2.009145975112915, {'accuracy': 0.4519, 'data_size': 10000}, 849.4938843510172)
INFO flwr 2024-04-06 07:20:35,384 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:20:35,385 | server.py:153 | FL finished in 849.4945164590026
INFO flwr 2024-04-06 07:20:35,387 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:20:35,388 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:20:35,388 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:20:35,388 | app.py:229 | app_fit: losses_centralized [(0, 2.3024702072143555), (1, 2.3277547359466553), (2, 1.992639422416687), (3, 1.9364722967147827), (4, 1.948026180267334), (5, 1.9914804697036743), (6, 1.9616944789886475), (7, 1.8810276985168457), (8, 2.1281161308288574), (9, 1.9345372915267944), (10, 2.009145975112915)]
INFO flwr 2024-04-06 07:20:35,388 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0862), (1, 0.1329), (2, 0.4693), (3, 0.5233), (4, 0.5117), (5, 0.4691), (6, 0.4988), (7, 0.5801), (8, 0.3329), (9, 0.5265), (10, 0.4519)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4519
wandb:     loss 2.00915
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_070559-mbutm2at
wandb: Find logs at: ./wandb/offline-run-20240406_070559-mbutm2at/logs
INFO flwr 2024-04-06 07:20:40,122 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:28:00,006 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=878789)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=878789)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 07:28:05,593	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:28:05,927	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:28:06,213	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_48ba41b5faa6009d.zip' (8.80MiB) to Ray cluster...
2024-04-06 07:28:06,233	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_48ba41b5faa6009d.zip'.
INFO flwr 2024-04-06 07:28:17,003 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 143521144628.0, 'object_store_memory': 65794776268.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 07:28:17,005 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:28:17,005 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:28:17,025 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:28:17,026 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:28:17,027 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:28:17,027 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=890674)[0m 2024-04-06 07:28:22.944284: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=890675)[0m 2024-04-06 07:28:22.999093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=890675)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 07:28:24,303 | server.py:94 | initial parameters (loss, other metrics): 2.3024840354919434, {'accuracy': 0.0966, 'data_size': 10000}
INFO flwr 2024-04-06 07:28:24,303 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:28:24,304 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=890675)[0m 2024-04-06 07:28:24.973502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=890680)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=890680)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=890670)[0m 2024-04-06 07:28:23.239678: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=890670)[0m 2024-04-06 07:28:23.341054: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=890670)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=890670)[0m 2024-04-06 07:28:25.496101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=890673)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=890673)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=890670)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=890670)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-06 07:29:21,740 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:29:25,067 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:29:29,104 | server.py:125 | fit progress: (1, 2.159999132156372, {'accuracy': 0.2715, 'data_size': 10000}, 64.79995830199914)
INFO flwr 2024-04-06 07:29:29,104 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:29:29,104 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:30:09,580 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:30:22,798 | server.py:125 | fit progress: (2, 2.1488735675811768, {'accuracy': 0.3217, 'data_size': 10000}, 118.49445600100444)
INFO flwr 2024-04-06 07:30:22,798 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:30:22,799 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:31:04,584 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:31:26,049 | server.py:125 | fit progress: (3, 2.074023485183716, {'accuracy': 0.3857, 'data_size': 10000}, 181.74550411198288)
INFO flwr 2024-04-06 07:31:26,050 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:31:26,050 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:32:08,238 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:32:36,886 | server.py:125 | fit progress: (4, 1.93070387840271, {'accuracy': 0.5289, 'data_size': 10000}, 252.58243117301026)
INFO flwr 2024-04-06 07:32:36,887 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:32:36,887 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:33:24,232 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:33:59,973 | server.py:125 | fit progress: (5, 2.1022534370422363, {'accuracy': 0.3588, 'data_size': 10000}, 335.6693175119872)
INFO flwr 2024-04-06 07:33:59,973 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:33:59,973 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:34:44,924 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:35:24,399 | server.py:125 | fit progress: (6, 2.1208136081695557, {'accuracy': 0.3403, 'data_size': 10000}, 420.09556350199273)
INFO flwr 2024-04-06 07:35:24,400 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:35:24,400 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:36:01,634 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:36:53,662 | server.py:125 | fit progress: (7, 2.155191659927368, {'accuracy': 0.3059, 'data_size': 10000}, 509.3585494029976)
INFO flwr 2024-04-06 07:36:53,663 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:36:53,663 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:37:34,492 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 07:38:35,172 | server.py:125 | fit progress: (8, 2.058619499206543, {'accuracy': 0.4025, 'data_size': 10000}, 610.8684213279921)
INFO flwr 2024-04-06 07:38:35,172 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 07:38:35,173 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:39:19,104 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 07:40:24,228 | server.py:125 | fit progress: (9, 2.3011415004730225, {'accuracy': 0.16, 'data_size': 10000}, 719.9241673799988)
INFO flwr 2024-04-06 07:40:24,228 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 07:40:24,228 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:41:07,627 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 07:42:33,777 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 849.4734369379876)
INFO flwr 2024-04-06 07:42:33,778 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 07:42:33,778 | server.py:153 | FL finished in 849.4741302710027
INFO flwr 2024-04-06 07:42:33,778 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 07:42:33,778 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 07:42:33,778 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 07:42:33,778 | app.py:229 | app_fit: losses_centralized [(0, 2.3024840354919434), (1, 2.159999132156372), (2, 2.1488735675811768), (3, 2.074023485183716), (4, 1.93070387840271), (5, 2.1022534370422363), (6, 2.1208136081695557), (7, 2.155191659927368), (8, 2.058619499206543), (9, 2.3011415004730225), (10, 2.347642183303833)]
INFO flwr 2024-04-06 07:42:33,778 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0966), (1, 0.2715), (2, 0.3217), (3, 0.3857), (4, 0.5289), (5, 0.3588), (6, 0.3403), (7, 0.3059), (8, 0.4025), (9, 0.16), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_072759-agyluvhg
wandb: Find logs at: ./wandb/offline-run-20240406_072759-agyluvhg/logs
INFO flwr 2024-04-06 07:42:37,328 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 07:49:57,634 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 07:50:02,730	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 07:50:03,135	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 07:50:03,530	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbf268cf4197bc68.zip' (8.83MiB) to Ray cluster...
2024-04-06 07:50:03,549	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbf268cf4197bc68.zip'.
INFO flwr 2024-04-06 07:50:14,233 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 65643034214.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'memory': 143167079834.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 07:50:14,233 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 07:50:14,233 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 07:50:14,246 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 07:50:14,247 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 07:50:14,247 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 07:50:14,247 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=902825)[0m 2024-04-06 07:50:20.247234: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=902841)[0m 2024-04-06 07:50:20.292428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=902841)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 07:50:21,883 | server.py:94 | initial parameters (loss, other metrics): 2.3025095462799072, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-06 07:50:21,883 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 07:50:21,883 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=902825)[0m 2024-04-06 07:50:22.341334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=902841)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=902841)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=902838)[0m 2024-04-06 07:50:20.651147: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=902838)[0m 2024-04-06 07:50:20.752453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=902838)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=902830)[0m 2024-04-06 07:50:22.773494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=902830)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=902830)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=902823)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=902823)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-06 07:51:14,244 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 07:51:17,629 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 07:51:21,667 | server.py:125 | fit progress: (1, 2.3474316596984863, {'accuracy': 0.1032, 'data_size': 10000}, 59.78345184100908)
INFO flwr 2024-04-06 07:51:21,667 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 07:51:21,667 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:52:06,078 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 07:52:18,872 | server.py:125 | fit progress: (2, 2.237191677093506, {'accuracy': 0.2159, 'data_size': 10000}, 116.98842850900837)
INFO flwr 2024-04-06 07:52:18,872 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 07:52:18,873 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:53:03,588 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 07:53:24,077 | server.py:125 | fit progress: (3, 2.0052433013916016, {'accuracy': 0.4701, 'data_size': 10000}, 182.19351657500374)
INFO flwr 2024-04-06 07:53:24,077 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 07:53:24,077 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:54:06,686 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 07:54:34,010 | server.py:125 | fit progress: (4, 1.9849927425384521, {'accuracy': 0.4703, 'data_size': 10000}, 252.12666559498757)
INFO flwr 2024-04-06 07:54:34,010 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 07:54:34,010 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:55:20,651 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 07:55:54,277 | server.py:125 | fit progress: (5, 2.1610028743743896, {'accuracy': 0.2996, 'data_size': 10000}, 332.39406591298757)
INFO flwr 2024-04-06 07:55:54,278 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 07:55:54,278 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:56:35,934 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 07:57:20,581 | server.py:125 | fit progress: (6, 2.1896955966949463, {'accuracy': 0.2713, 'data_size': 10000}, 418.69752586400136)
INFO flwr 2024-04-06 07:57:20,581 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 07:57:20,581 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:58:01,101 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 07:58:54,792 | server.py:125 | fit progress: (7, 2.2303574085235596, {'accuracy': 0.2307, 'data_size': 10000}, 512.9087260740052)
INFO flwr 2024-04-06 07:58:54,792 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 07:58:54,792 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 07:59:41,824 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:00:41,164 | server.py:125 | fit progress: (8, 2.075268268585205, {'accuracy': 0.3857, 'data_size': 10000}, 619.280719197006)
INFO flwr 2024-04-06 08:00:41,164 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:00:41,164 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:01:23,392 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:02:36,800 | server.py:125 | fit progress: (9, 2.291337013244629, {'accuracy': 0.1696, 'data_size': 10000}, 734.9168111940089)
INFO flwr 2024-04-06 08:02:36,800 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:02:36,801 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:03:20,935 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:04:37,154 | server.py:125 | fit progress: (10, 2.2741780281066895, {'accuracy': 0.187, 'data_size': 10000}, 855.2708007030014)
INFO flwr 2024-04-06 08:04:37,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:04:37,155 | server.py:153 | FL finished in 855.271468846011
INFO flwr 2024-04-06 08:04:37,155 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:04:37,155 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:04:37,155 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:04:37,155 | app.py:229 | app_fit: losses_centralized [(0, 2.3025095462799072), (1, 2.3474316596984863), (2, 2.237191677093506), (3, 2.0052433013916016), (4, 1.9849927425384521), (5, 2.1610028743743896), (6, 2.1896955966949463), (7, 2.2303574085235596), (8, 2.075268268585205), (9, 2.291337013244629), (10, 2.2741780281066895)]
INFO flwr 2024-04-06 08:04:37,155 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.1032), (2, 0.2159), (3, 0.4701), (4, 0.4703), (5, 0.2996), (6, 0.2713), (7, 0.2307), (8, 0.3857), (9, 0.1696), (10, 0.187)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.187
wandb:     loss 2.27418
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_074956-eg5oumyf
wandb: Find logs at: ./wandb/offline-run-20240406_074956-eg5oumyf/logs
INFO flwr 2024-04-06 08:04:40,694 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:12:01,940 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 08:12:06,932	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:12:07,292	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:12:07,774	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7b1de9264a55db66.zip' (8.87MiB) to Ray cluster...
2024-04-06 08:12:07,796	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7b1de9264a55db66.zip'.
INFO flwr 2024-04-06 08:12:18,785 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 148401199309.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 67886228275.0}
INFO flwr 2024-04-06 08:12:18,785 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:12:18,785 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:12:18,801 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:12:18,802 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:12:18,802 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:12:18,802 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=918516)[0m 2024-04-06 08:12:24.938148: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=918516)[0m 2024-04-06 08:12:25.044745: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=918516)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 08:12:26,699 | server.py:94 | initial parameters (loss, other metrics): 2.3026282787323, {'accuracy': 0.095, 'data_size': 10000}
INFO flwr 2024-04-06 08:12:26,700 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:12:26,700 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=918516)[0m 2024-04-06 08:12:27.006637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=918529)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=918529)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=918530)[0m 2024-04-06 08:12:25.223141: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=918530)[0m 2024-04-06 08:12:25.321758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=918530)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=918529)[0m 2024-04-06 08:12:27.382824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=918521)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=918521)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 08:13:14,774 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:13:18,021 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:13:21,875 | server.py:125 | fit progress: (1, 2.219989061355591, {'accuracy': 0.2612, 'data_size': 10000}, 55.17495756599237)
INFO flwr 2024-04-06 08:13:21,875 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:13:21,875 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:13:59,655 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:14:12,379 | server.py:125 | fit progress: (2, 2.1307599544525146, {'accuracy': 0.3729, 'data_size': 10000}, 105.67923830301152)
INFO flwr 2024-04-06 08:14:12,380 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:14:12,380 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:14:52,844 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:15:13,188 | server.py:125 | fit progress: (3, 2.074387311935425, {'accuracy': 0.3785, 'data_size': 10000}, 166.48854699599906)
INFO flwr 2024-04-06 08:15:13,189 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:15:13,189 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:15:52,974 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:16:19,668 | server.py:125 | fit progress: (4, 2.25041127204895, {'accuracy': 0.2103, 'data_size': 10000}, 232.96829487898503)
INFO flwr 2024-04-06 08:16:19,669 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:16:19,669 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:17:05,351 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:17:38,342 | server.py:125 | fit progress: (5, 2.154674530029297, {'accuracy': 0.3062, 'data_size': 10000}, 311.6423101919936)
INFO flwr 2024-04-06 08:17:38,342 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:17:38,343 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:18:22,526 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:19:09,256 | server.py:125 | fit progress: (6, 2.1688454151153564, {'accuracy': 0.2923, 'data_size': 10000}, 402.55647351100924)
INFO flwr 2024-04-06 08:19:09,257 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:19:09,257 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:19:51,999 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:20:45,266 | server.py:125 | fit progress: (7, 2.1919872760772705, {'accuracy': 0.2691, 'data_size': 10000}, 498.56628841199563)
INFO flwr 2024-04-06 08:20:45,266 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:20:45,267 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:21:26,597 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:22:29,594 | server.py:125 | fit progress: (8, 2.2608160972595215, {'accuracy': 0.2003, 'data_size': 10000}, 602.8936843170086)
INFO flwr 2024-04-06 08:22:29,594 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:22:29,594 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:23:15,886 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:24:28,043 | server.py:125 | fit progress: (9, 2.19435977935791, {'accuracy': 0.2667, 'data_size': 10000}, 721.342741350003)
INFO flwr 2024-04-06 08:24:28,043 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:24:28,043 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:25:12,015 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:26:14,836 | server.py:125 | fit progress: (10, 2.178640127182007, {'accuracy': 0.2825, 'data_size': 10000}, 828.1360744060075)
INFO flwr 2024-04-06 08:26:14,836 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:26:14,836 | server.py:153 | FL finished in 828.1364812180109
INFO flwr 2024-04-06 08:26:14,837 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:26:14,837 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:26:14,837 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:26:14,837 | app.py:229 | app_fit: losses_centralized [(0, 2.3026282787323), (1, 2.219989061355591), (2, 2.1307599544525146), (3, 2.074387311935425), (4, 2.25041127204895), (5, 2.154674530029297), (6, 2.1688454151153564), (7, 2.1919872760772705), (8, 2.2608160972595215), (9, 2.19435977935791), (10, 2.178640127182007)]
INFO flwr 2024-04-06 08:26:14,837 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.095), (1, 0.2612), (2, 0.3729), (3, 0.3785), (4, 0.2103), (5, 0.3062), (6, 0.2923), (7, 0.2691), (8, 0.2003), (9, 0.2667), (10, 0.2825)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2825
wandb:     loss 2.17864
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_081201-naptg5ey
wandb: Find logs at: ./wandb/offline-run-20240406_081201-naptg5ey/logs
INFO flwr 2024-04-06 08:26:18,441 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:33:38,906 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=918516)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=918516)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 08:33:44,626	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:33:44,953	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:33:45,313	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fe0dfab0bae03fce.zip' (8.91MiB) to Ray cluster...
2024-04-06 08:33:45,333	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fe0dfab0bae03fce.zip'.
INFO flwr 2024-04-06 08:33:56,296 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 142308003636.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 65274858700.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 08:33:56,296 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:33:56,297 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:33:56,312 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:33:56,312 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:33:56,313 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:33:56,313 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=930788)[0m 2024-04-06 08:34:02.236584: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=930794)[0m 2024-04-06 08:34:02.297290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=930794)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=930790)[0m 2024-04-06 08:34:04.301318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 08:34:04,700 | server.py:94 | initial parameters (loss, other metrics): 2.303145170211792, {'accuracy': 0.0575, 'data_size': 10000}
INFO flwr 2024-04-06 08:34:04,701 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:34:04,701 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=930794)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=930794)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=930789)[0m 2024-04-06 08:34:02.664333: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=930789)[0m 2024-04-06 08:34:02.776386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=930789)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=930795)[0m 2024-04-06 08:34:04.909092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=930788)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=930788)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 08:34:50,101 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:34:53,413 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:34:57,317 | server.py:125 | fit progress: (1, 2.274717092514038, {'accuracy': 0.098, 'data_size': 10000}, 52.61610105400905)
INFO flwr 2024-04-06 08:34:57,317 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:34:57,317 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:35:37,355 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:35:51,179 | server.py:125 | fit progress: (2, 2.2508859634399414, {'accuracy': 0.1747, 'data_size': 10000}, 106.47790641602478)
INFO flwr 2024-04-06 08:35:51,179 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:35:51,179 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:36:37,404 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 08:36:58,146 | server.py:125 | fit progress: (3, 2.2694668769836426, {'accuracy': 0.1814, 'data_size': 10000}, 173.44546059300774)
INFO flwr 2024-04-06 08:36:58,147 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 08:36:58,147 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:37:34,567 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 08:38:02,007 | server.py:125 | fit progress: (4, 2.2396180629730225, {'accuracy': 0.2004, 'data_size': 10000}, 237.30616395402467)
INFO flwr 2024-04-06 08:38:02,008 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 08:38:02,008 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:38:44,616 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 08:39:17,479 | server.py:125 | fit progress: (5, 2.0765233039855957, {'accuracy': 0.3795, 'data_size': 10000}, 312.778022967017)
INFO flwr 2024-04-06 08:39:17,479 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 08:39:17,479 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:39:51,465 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 08:40:37,347 | server.py:125 | fit progress: (6, 2.10971999168396, {'accuracy': 0.35, 'data_size': 10000}, 392.64599817901035)
INFO flwr 2024-04-06 08:40:37,347 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 08:40:37,348 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:41:17,729 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 08:42:13,583 | server.py:125 | fit progress: (7, 2.1843554973602295, {'accuracy': 0.2762, 'data_size': 10000}, 488.88256409802125)
INFO flwr 2024-04-06 08:42:13,584 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 08:42:13,584 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:42:53,527 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 08:44:01,334 | server.py:125 | fit progress: (8, 2.1779356002807617, {'accuracy': 0.283, 'data_size': 10000}, 596.632873044)
INFO flwr 2024-04-06 08:44:01,334 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 08:44:01,334 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:44:45,956 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 08:45:55,392 | server.py:125 | fit progress: (9, 2.2666382789611816, {'accuracy': 0.1945, 'data_size': 10000}, 710.6910581540142)
INFO flwr 2024-04-06 08:45:55,392 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 08:45:55,393 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:46:41,729 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 08:48:00,529 | server.py:125 | fit progress: (10, 2.347642183303833, {'accuracy': 0.1135, 'data_size': 10000}, 835.8280765830132)
INFO flwr 2024-04-06 08:48:00,530 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 08:48:00,530 | server.py:153 | FL finished in 835.8292093790078
INFO flwr 2024-04-06 08:48:00,531 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 08:48:00,531 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 08:48:00,531 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 08:48:00,531 | app.py:229 | app_fit: losses_centralized [(0, 2.303145170211792), (1, 2.274717092514038), (2, 2.2508859634399414), (3, 2.2694668769836426), (4, 2.2396180629730225), (5, 2.0765233039855957), (6, 2.10971999168396), (7, 2.1843554973602295), (8, 2.1779356002807617), (9, 2.2666382789611816), (10, 2.347642183303833)]
INFO flwr 2024-04-06 08:48:00,531 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0575), (1, 0.098), (2, 0.1747), (3, 0.1814), (4, 0.2004), (5, 0.3795), (6, 0.35), (7, 0.2762), (8, 0.283), (9, 0.1945), (10, 0.1135)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1135
wandb:     loss 2.34764
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_083338-n2gsdk30
wandb: Find logs at: ./wandb/offline-run-20240406_083338-n2gsdk30/logs
INFO flwr 2024-04-06 08:48:04,146 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 08:55:29,472 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 08:55:34,652	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 08:55:35,050	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 08:55:35,504	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0917d31e0edb7973.zip' (8.95MiB) to Ray cluster...
2024-04-06 08:55:35,545	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0917d31e0edb7973.zip'.
INFO flwr 2024-04-06 08:55:46,514 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 142571103642.0, 'object_store_memory': 65387615846.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 08:55:46,515 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 08:55:46,515 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 08:55:46,534 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 08:55:46,535 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 08:55:46,535 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 08:55:46,536 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=942760)[0m 2024-04-06 08:55:52.646405: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=942758)[0m 2024-04-06 08:55:52.725289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=942758)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 08:55:53,646 | server.py:94 | initial parameters (loss, other metrics): 2.302432060241699, {'accuracy': 0.0774, 'data_size': 10000}
INFO flwr 2024-04-06 08:55:53,646 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 08:55:53,647 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=942760)[0m 2024-04-06 08:55:54.762186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=942761)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=942761)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=942759)[0m 2024-04-06 08:55:52.854607: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=942759)[0m 2024-04-06 08:55:52.995542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=942759)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=942750)[0m 2024-04-06 08:55:55.042446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=942757)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=942757)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 08:57:30,448 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 08:57:33,935 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 08:57:38,245 | server.py:125 | fit progress: (1, 2.2642390727996826, {'accuracy': 0.1433, 'data_size': 10000}, 104.59851363598136)
INFO flwr 2024-04-06 08:57:38,245 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 08:57:38,246 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 08:58:52,915 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 08:59:06,531 | server.py:125 | fit progress: (2, 1.968027949333191, {'accuracy': 0.5275, 'data_size': 10000}, 192.88405286398483)
INFO flwr 2024-04-06 08:59:06,531 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 08:59:06,532 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:00:33,944 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:00:53,587 | server.py:125 | fit progress: (3, 1.7001312971115112, {'accuracy': 0.7665, 'data_size': 10000}, 299.94064347000676)
INFO flwr 2024-04-06 09:00:53,588 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:00:53,588 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:02:11,770 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:02:41,782 | server.py:125 | fit progress: (4, 1.6043503284454346, {'accuracy': 0.8694, 'data_size': 10000}, 408.13571674600826)
INFO flwr 2024-04-06 09:02:41,783 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:02:41,783 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:03:54,544 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:04:29,266 | server.py:125 | fit progress: (5, 1.5667189359664917, {'accuracy': 0.9053, 'data_size': 10000}, 515.6196134320053)
INFO flwr 2024-04-06 09:04:29,267 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:04:29,267 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:06:03,128 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:06:45,223 | server.py:125 | fit progress: (6, 1.5481958389282227, {'accuracy': 0.9187, 'data_size': 10000}, 651.5759438530076)
INFO flwr 2024-04-06 09:06:45,223 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:06:45,223 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:08:08,631 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:08:57,561 | server.py:125 | fit progress: (7, 1.5416585206985474, {'accuracy': 0.9239, 'data_size': 10000}, 783.9140915499884)
INFO flwr 2024-04-06 09:08:57,561 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:08:57,561 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:10:21,139 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:11:23,614 | server.py:125 | fit progress: (8, 1.5379230976104736, {'accuracy': 0.9287, 'data_size': 10000}, 929.9671503930003)
INFO flwr 2024-04-06 09:11:23,614 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:11:23,614 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:12:46,586 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:13:51,174 | server.py:125 | fit progress: (9, 1.5341904163360596, {'accuracy': 0.9305, 'data_size': 10000}, 1077.527885524003)
INFO flwr 2024-04-06 09:13:51,175 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:13:51,175 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:15:12,009 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:16:24,423 | server.py:125 | fit progress: (10, 1.5367333889007568, {'accuracy': 0.9279, 'data_size': 10000}, 1230.776218198007)
INFO flwr 2024-04-06 09:16:24,423 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:16:24,423 | server.py:153 | FL finished in 1230.776755829982
INFO flwr 2024-04-06 09:16:24,424 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:16:24,424 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:16:24,424 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:16:24,424 | app.py:229 | app_fit: losses_centralized [(0, 2.302432060241699), (1, 2.2642390727996826), (2, 1.968027949333191), (3, 1.7001312971115112), (4, 1.6043503284454346), (5, 1.5667189359664917), (6, 1.5481958389282227), (7, 1.5416585206985474), (8, 1.5379230976104736), (9, 1.5341904163360596), (10, 1.5367333889007568)]
INFO flwr 2024-04-06 09:16:24,424 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0774), (1, 0.1433), (2, 0.5275), (3, 0.7665), (4, 0.8694), (5, 0.9053), (6, 0.9187), (7, 0.9239), (8, 0.9287), (9, 0.9305), (10, 0.9279)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9279
wandb:     loss 1.53673
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_085529-jh3fb4b4
wandb: Find logs at: ./wandb/offline-run-20240406_085529-jh3fb4b4/logs
INFO flwr 2024-04-06 09:16:27,973 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:23:52,929 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=942756)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=942756)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 09:23:58,410	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:23:58,695	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:23:59,088	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f6e2d06798521bdc.zip' (8.99MiB) to Ray cluster...
2024-04-06 09:23:59,109	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f6e2d06798521bdc.zip'.
INFO flwr 2024-04-06 09:24:09,912 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 146230812058.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 66956062310.0}
INFO flwr 2024-04-06 09:24:09,913 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:24:09,913 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:24:09,931 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:24:09,932 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:24:09,933 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:24:09,933 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 09:24:16,872 | server.py:94 | initial parameters (loss, other metrics): 2.30269455909729, {'accuracy': 0.1001, 'data_size': 10000}
INFO flwr 2024-04-06 09:24:16,872 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:24:16,872 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=956370)[0m 2024-04-06 09:24:18.507062: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=956370)[0m 2024-04-06 09:24:18.601306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=956370)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=956370)[0m 2024-04-06 09:24:23.687290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=956363)[0m 2024-04-06 09:24:19.033291: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=956363)[0m 2024-04-06 09:24:19.124193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=956363)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=956364)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=956364)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=956363)[0m 2024-04-06 09:24:23.627534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 09:25:40,807 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:25:44,390 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:25:48,666 | server.py:125 | fit progress: (1, 1.7755169868469238, {'accuracy': 0.7136, 'data_size': 10000}, 91.79337708800449)
INFO flwr 2024-04-06 09:25:48,666 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:25:48,666 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:27:15,898 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:27:29,008 | server.py:125 | fit progress: (2, 1.5352635383605957, {'accuracy': 0.9267, 'data_size': 10000}, 192.13557237599161)
INFO flwr 2024-04-06 09:27:29,008 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:27:29,009 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:28:46,158 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:29:06,427 | server.py:125 | fit progress: (3, 1.5157009363174438, {'accuracy': 0.9454, 'data_size': 10000}, 289.55457016901346)
INFO flwr 2024-04-06 09:29:06,427 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:29:06,427 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:30:23,241 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:30:48,506 | server.py:125 | fit progress: (4, 1.5620965957641602, {'accuracy': 0.8984, 'data_size': 10000}, 391.6342243000108)
INFO flwr 2024-04-06 09:30:48,507 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:30:48,507 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:32:07,455 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 09:32:43,326 | server.py:125 | fit progress: (5, 1.5712124109268188, {'accuracy': 0.8902, 'data_size': 10000}, 506.4537801860133)
INFO flwr 2024-04-06 09:32:43,326 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 09:32:43,327 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:34:03,154 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 09:34:46,184 | server.py:125 | fit progress: (6, 1.5594182014465332, {'accuracy': 0.9018, 'data_size': 10000}, 629.3119883809995)
INFO flwr 2024-04-06 09:34:46,185 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 09:34:46,185 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:36:07,067 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 09:37:02,344 | server.py:125 | fit progress: (7, 1.6561416387557983, {'accuracy': 0.805, 'data_size': 10000}, 765.4714966840111)
INFO flwr 2024-04-06 09:37:02,344 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 09:37:02,344 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:38:19,207 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 09:39:11,809 | server.py:125 | fit progress: (8, 1.626654863357544, {'accuracy': 0.8344, 'data_size': 10000}, 894.9362455490045)
INFO flwr 2024-04-06 09:39:11,809 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 09:39:11,809 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:40:37,686 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 09:41:44,107 | server.py:125 | fit progress: (9, 1.5927908420562744, {'accuracy': 0.8684, 'data_size': 10000}, 1047.235146874009)
INFO flwr 2024-04-06 09:41:44,108 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 09:41:44,108 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:42:53,993 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 09:44:13,501 | server.py:125 | fit progress: (10, 1.5668234825134277, {'accuracy': 0.8943, 'data_size': 10000}, 1196.6286328110145)
INFO flwr 2024-04-06 09:44:13,501 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 09:44:13,501 | server.py:153 | FL finished in 1196.6291029030108
INFO flwr 2024-04-06 09:44:13,509 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 09:44:13,509 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 09:44:13,509 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 09:44:13,509 | app.py:229 | app_fit: losses_centralized [(0, 2.30269455909729), (1, 1.7755169868469238), (2, 1.5352635383605957), (3, 1.5157009363174438), (4, 1.5620965957641602), (5, 1.5712124109268188), (6, 1.5594182014465332), (7, 1.6561416387557983), (8, 1.626654863357544), (9, 1.5927908420562744), (10, 1.5668234825134277)]
INFO flwr 2024-04-06 09:44:13,509 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1001), (1, 0.7136), (2, 0.9267), (3, 0.9454), (4, 0.8984), (5, 0.8902), (6, 0.9018), (7, 0.805), (8, 0.8344), (9, 0.8684), (10, 0.8943)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8943
wandb:     loss 1.56682
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_092352-cpmh1tb8
wandb: Find logs at: ./wandb/offline-run-20240406_092352-cpmh1tb8/logs
INFO flwr 2024-04-06 09:44:17,072 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 09:51:41,968 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=956358)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=956358)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 09:51:47,822	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 09:51:48,222	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 09:51:48,606	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0b03bdd02e68f32b.zip' (9.04MiB) to Ray cluster...
2024-04-06 09:51:48,642	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0b03bdd02e68f32b.zip'.
INFO flwr 2024-04-06 09:51:59,533 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 141219195904.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 64808226816.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 09:51:59,533 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 09:51:59,533 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 09:51:59,550 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 09:51:59,551 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 09:51:59,551 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 09:51:59,551 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=973288)[0m 2024-04-06 09:52:05.366814: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=973288)[0m 2024-04-06 09:52:05.458655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=973288)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 09:52:07,529 | server.py:94 | initial parameters (loss, other metrics): 2.3026514053344727, {'accuracy': 0.1136, 'data_size': 10000}
INFO flwr 2024-04-06 09:52:07,529 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 09:52:07,530 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=973286)[0m 2024-04-06 09:52:07.541829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=973292)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=973292)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=973293)[0m 2024-04-06 09:52:05.736304: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=973293)[0m 2024-04-06 09:52:05.837248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=973293)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=973293)[0m 2024-04-06 09:52:08.066109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=973286)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=973286)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=973284)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=973284)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
DEBUG flwr 2024-04-06 09:53:39,715 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 09:53:43,273 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 09:53:47,480 | server.py:125 | fit progress: (1, 1.9560433626174927, {'accuracy': 0.5241, 'data_size': 10000}, 99.9500238619803)
INFO flwr 2024-04-06 09:53:47,480 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 09:53:47,480 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:55:14,949 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 09:55:28,560 | server.py:125 | fit progress: (2, 2.03201961517334, {'accuracy': 0.4295, 'data_size': 10000}, 201.03067281399854)
INFO flwr 2024-04-06 09:55:28,561 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 09:55:28,561 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:56:59,722 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 09:57:20,526 | server.py:125 | fit progress: (3, 1.819770097732544, {'accuracy': 0.6399, 'data_size': 10000}, 312.9966938459838)
INFO flwr 2024-04-06 09:57:20,527 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 09:57:20,527 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 09:58:38,668 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 09:59:05,475 | server.py:125 | fit progress: (4, 2.0059192180633545, {'accuracy': 0.4547, 'data_size': 10000}, 417.94497248600237)
INFO flwr 2024-04-06 09:59:05,475 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 09:59:05,475 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:00:25,297 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:00:56,908 | server.py:125 | fit progress: (5, 1.989225149154663, {'accuracy': 0.4714, 'data_size': 10000}, 529.3788004369999)
INFO flwr 2024-04-06 10:00:56,909 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:00:56,909 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:02:22,369 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:03:06,703 | server.py:125 | fit progress: (6, 2.030275344848633, {'accuracy': 0.4311, 'data_size': 10000}, 659.1734840039862)
INFO flwr 2024-04-06 10:03:06,703 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:03:06,704 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:04:23,180 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:05:16,849 | server.py:125 | fit progress: (7, 1.8268592357635498, {'accuracy': 0.634, 'data_size': 10000}, 789.3196457419835)
INFO flwr 2024-04-06 10:05:16,850 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:05:16,850 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:06:36,157 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:07:26,113 | server.py:125 | fit progress: (8, 1.9828437566757202, {'accuracy': 0.478, 'data_size': 10000}, 918.5830730339803)
INFO flwr 2024-04-06 10:07:26,113 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:07:26,113 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:08:46,114 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:09:59,147 | server.py:125 | fit progress: (9, 1.915833592414856, {'accuracy': 0.545, 'data_size': 10000}, 1071.6176217570028)
INFO flwr 2024-04-06 10:09:59,147 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:09:59,148 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:11:22,795 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:12:27,877 | server.py:125 | fit progress: (10, 2.072824001312256, {'accuracy': 0.3884, 'data_size': 10000}, 1220.347817116999)
INFO flwr 2024-04-06 10:12:27,878 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:12:27,878 | server.py:153 | FL finished in 1220.3482810480054
INFO flwr 2024-04-06 10:12:27,878 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:12:27,878 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:12:27,878 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:12:27,878 | app.py:229 | app_fit: losses_centralized [(0, 2.3026514053344727), (1, 1.9560433626174927), (2, 2.03201961517334), (3, 1.819770097732544), (4, 2.0059192180633545), (5, 1.989225149154663), (6, 2.030275344848633), (7, 1.8268592357635498), (8, 1.9828437566757202), (9, 1.915833592414856), (10, 2.072824001312256)]
INFO flwr 2024-04-06 10:12:27,879 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1136), (1, 0.5241), (2, 0.4295), (3, 0.6399), (4, 0.4547), (5, 0.4714), (6, 0.4311), (7, 0.634), (8, 0.478), (9, 0.545), (10, 0.3884)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3884
wandb:     loss 2.07282
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_095141-t1g4phdh
wandb: Find logs at: ./wandb/offline-run-20240406_095141-t1g4phdh/logs
INFO flwr 2024-04-06 10:12:31,420 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:19:56,501 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 10:20:02,609	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:20:02,944	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:20:03,260	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e2982710d4bdfc0d.zip' (9.09MiB) to Ray cluster...
2024-04-06 10:20:03,283	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e2982710d4bdfc0d.zip'.
INFO flwr 2024-04-06 10:20:14,320 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140962607309.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64698260275.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 10:20:14,321 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:20:14,321 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:20:14,344 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:20:14,345 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:20:14,345 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:20:14,345 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=989193)[0m 2024-04-06 10:20:19.628235: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=989193)[0m 2024-04-06 10:20:19.735766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=989193)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 10:20:21,889 | server.py:94 | initial parameters (loss, other metrics): 2.3026530742645264, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-06 10:20:21,890 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:20:21,890 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=989195)[0m 2024-04-06 10:20:23.011938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=989195)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=989195)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=989198)[0m 2024-04-06 10:20:21.266464: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=989198)[0m 2024-04-06 10:20:21.355965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=989198)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=989198)[0m 2024-04-06 10:20:24.123896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=989190)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=989190)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 10:21:49,355 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:21:52,815 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:21:57,124 | server.py:125 | fit progress: (1, 2.2193500995635986, {'accuracy': 0.3341, 'data_size': 10000}, 95.23381733099814)
INFO flwr 2024-04-06 10:21:57,124 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:21:57,124 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:23:23,920 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:23:37,286 | server.py:125 | fit progress: (2, 1.9954283237457275, {'accuracy': 0.4543, 'data_size': 10000}, 195.39540163299534)
INFO flwr 2024-04-06 10:23:37,286 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:23:37,286 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:24:57,278 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:25:21,174 | server.py:125 | fit progress: (3, 1.9283888339996338, {'accuracy': 0.5283, 'data_size': 10000}, 299.2838583639823)
INFO flwr 2024-04-06 10:25:21,174 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:25:21,175 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:26:44,244 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:27:10,495 | server.py:125 | fit progress: (4, 2.2737114429473877, {'accuracy': 0.1866, 'data_size': 10000}, 408.6050268189865)
INFO flwr 2024-04-06 10:27:10,496 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:27:10,496 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:28:21,544 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:28:53,599 | server.py:125 | fit progress: (5, 2.2469987869262695, {'accuracy': 0.2141, 'data_size': 10000}, 511.7091725670034)
INFO flwr 2024-04-06 10:28:53,600 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:28:53,601 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:30:13,094 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:31:00,128 | server.py:125 | fit progress: (6, 2.031052827835083, {'accuracy': 0.4297, 'data_size': 10000}, 638.2380128729856)
INFO flwr 2024-04-06 10:31:00,129 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:31:00,129 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:32:24,937 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 10:33:10,995 | server.py:125 | fit progress: (7, 2.194026470184326, {'accuracy': 0.2668, 'data_size': 10000}, 769.1052842229838)
INFO flwr 2024-04-06 10:33:10,996 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 10:33:10,996 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:34:32,165 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 10:35:27,591 | server.py:125 | fit progress: (8, 2.3619368076324463, {'accuracy': 0.0992, 'data_size': 10000}, 905.7004358700069)
INFO flwr 2024-04-06 10:35:27,591 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 10:35:27,592 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:36:41,326 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 10:37:49,026 | server.py:125 | fit progress: (9, 2.155261993408203, {'accuracy': 0.3047, 'data_size': 10000}, 1047.1358990849985)
INFO flwr 2024-04-06 10:37:49,026 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 10:37:49,027 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:39:10,891 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 10:40:32,280 | server.py:125 | fit progress: (10, 2.206573724746704, {'accuracy': 0.2546, 'data_size': 10000}, 1210.389805305982)
INFO flwr 2024-04-06 10:40:32,281 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 10:40:32,281 | server.py:153 | FL finished in 1210.390625756001
INFO flwr 2024-04-06 10:40:32,281 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 10:40:32,281 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 10:40:32,281 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 10:40:32,281 | app.py:229 | app_fit: losses_centralized [(0, 2.3026530742645264), (1, 2.2193500995635986), (2, 1.9954283237457275), (3, 1.9283888339996338), (4, 2.2737114429473877), (5, 2.2469987869262695), (6, 2.031052827835083), (7, 2.194026470184326), (8, 2.3619368076324463), (9, 2.155261993408203), (10, 2.206573724746704)]
INFO flwr 2024-04-06 10:40:32,281 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.3341), (2, 0.4543), (3, 0.5283), (4, 0.1866), (5, 0.2141), (6, 0.4297), (7, 0.2668), (8, 0.0992), (9, 0.3047), (10, 0.2546)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2546
wandb:     loss 2.20657
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_101956-2lqv4adk
wandb: Find logs at: ./wandb/offline-run-20240406_101956-2lqv4adk/logs
INFO flwr 2024-04-06 10:40:35,938 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 10:48:00,846 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=989188)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=989188)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 10:48:06,030	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 10:48:06,450	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 10:48:06,855	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_876ead3a401e89fb.zip' (9.13MiB) to Ray cluster...
2024-04-06 10:48:06,885	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_876ead3a401e89fb.zip'.
INFO flwr 2024-04-06 10:48:17,771 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 64582533120.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 140692577280.0}
INFO flwr 2024-04-06 10:48:17,772 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 10:48:17,772 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 10:48:17,787 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 10:48:17,788 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 10:48:17,789 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 10:48:17,789 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1002716)[0m 2024-04-06 10:48:23.300004: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1002716)[0m 2024-04-06 10:48:23.405806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1002716)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 10:48:25,497 | server.py:94 | initial parameters (loss, other metrics): 2.30277156829834, {'accuracy': 0.0985, 'data_size': 10000}
INFO flwr 2024-04-06 10:48:25,498 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 10:48:25,498 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1002726)[0m 2024-04-06 10:48:25.706531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1002726)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1002726)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1002720)[0m 2024-04-06 10:48:24.473409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1002720)[0m 2024-04-06 10:48:24.569438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1002720)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1002720)[0m 2024-04-06 10:48:26.376036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1002716)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1002716)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 10:49:53,523 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 10:49:56,916 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 10:50:01,229 | server.py:125 | fit progress: (1, 2.3582260608673096, {'accuracy': 0.1028, 'data_size': 10000}, 95.73136169297504)
INFO flwr 2024-04-06 10:50:01,230 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 10:50:01,230 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:51:26,401 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 10:51:40,467 | server.py:125 | fit progress: (2, 2.0796477794647217, {'accuracy': 0.3996, 'data_size': 10000}, 194.96873480497743)
INFO flwr 2024-04-06 10:51:40,467 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 10:51:40,468 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:53:00,256 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 10:53:21,352 | server.py:125 | fit progress: (3, 2.178734064102173, {'accuracy': 0.2826, 'data_size': 10000}, 295.85367005399894)
INFO flwr 2024-04-06 10:53:21,352 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 10:53:21,352 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:54:37,073 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 10:55:06,274 | server.py:125 | fit progress: (4, 2.0357956886291504, {'accuracy': 0.423, 'data_size': 10000}, 400.77587303498876)
INFO flwr 2024-04-06 10:55:06,274 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 10:55:06,275 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:56:24,603 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 10:57:01,273 | server.py:125 | fit progress: (5, 2.072077989578247, {'accuracy': 0.3846, 'data_size': 10000}, 515.7752398379962)
INFO flwr 2024-04-06 10:57:01,274 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 10:57:01,274 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 10:58:21,279 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 10:59:05,491 | server.py:125 | fit progress: (6, 2.2015020847320557, {'accuracy': 0.2595, 'data_size': 10000}, 639.9934949889721)
INFO flwr 2024-04-06 10:59:05,492 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 10:59:05,492 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:00:36,727 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:01:32,330 | server.py:125 | fit progress: (7, 2.148726463317871, {'accuracy': 0.3125, 'data_size': 10000}, 786.8317828879808)
INFO flwr 2024-04-06 11:01:32,330 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:01:32,330 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:02:46,570 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:03:37,018 | server.py:125 | fit progress: (8, 2.2330331802368164, {'accuracy': 0.2281, 'data_size': 10000}, 911.5196602139913)
INFO flwr 2024-04-06 11:03:37,018 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:03:37,018 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:05:00,840 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:06:08,028 | server.py:125 | fit progress: (9, 2.1932215690612793, {'accuracy': 0.2679, 'data_size': 10000}, 1062.5297132419946)
INFO flwr 2024-04-06 11:06:08,028 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:06:08,028 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:07:28,812 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:08:44,673 | server.py:125 | fit progress: (10, 2.3030295372009277, {'accuracy': 0.1581, 'data_size': 10000}, 1219.174979376985)
INFO flwr 2024-04-06 11:08:44,673 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:08:44,673 | server.py:153 | FL finished in 1219.1754205079924
INFO flwr 2024-04-06 11:08:44,673 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:08:44,674 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:08:44,674 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:08:44,674 | app.py:229 | app_fit: losses_centralized [(0, 2.30277156829834), (1, 2.3582260608673096), (2, 2.0796477794647217), (3, 2.178734064102173), (4, 2.0357956886291504), (5, 2.072077989578247), (6, 2.2015020847320557), (7, 2.148726463317871), (8, 2.2330331802368164), (9, 2.1932215690612793), (10, 2.3030295372009277)]
INFO flwr 2024-04-06 11:08:44,674 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0985), (1, 0.1028), (2, 0.3996), (3, 0.2826), (4, 0.423), (5, 0.3846), (6, 0.2595), (7, 0.3125), (8, 0.2281), (9, 0.2679), (10, 0.1581)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1581
wandb:     loss 2.30303
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_104800-hjga5dfh
wandb: Find logs at: ./wandb/offline-run-20240406_104800-hjga5dfh/logs
INFO flwr 2024-04-06 11:08:48,255 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:16:13,376 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1002714)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1002714)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 11:16:18,304	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:16:18,588	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:16:18,939	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b230ee386921e355.zip' (9.17MiB) to Ray cluster...
2024-04-06 11:16:18,967	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b230ee386921e355.zip'.
INFO flwr 2024-04-06 11:16:29,851 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 64445905305.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'memory': 140373779047.0}
INFO flwr 2024-04-06 11:16:29,851 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:16:29,851 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:16:29,867 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:16:29,868 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:16:29,868 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:16:29,868 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1018635)[0m 2024-04-06 11:16:35.241349: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1018635)[0m 2024-04-06 11:16:35.342116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1018635)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 11:16:37,579 | server.py:94 | initial parameters (loss, other metrics): 2.3025643825531006, {'accuracy': 0.0843, 'data_size': 10000}
INFO flwr 2024-04-06 11:16:37,579 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:16:37,580 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1018635)[0m 2024-04-06 11:16:37.701678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1018634)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1018634)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1018627)[0m 2024-04-06 11:16:36.539022: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1018627)[0m 2024-04-06 11:16:36.633320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1018627)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1018627)[0m 2024-04-06 11:16:38.562361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1018627)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1018627)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 11:18:02,505 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:18:06,110 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:18:10,326 | server.py:125 | fit progress: (1, 2.2624902725219727, {'accuracy': 0.3055, 'data_size': 10000}, 92.74619605499902)
INFO flwr 2024-04-06 11:18:10,326 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:18:10,326 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:19:30,405 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:19:43,970 | server.py:125 | fit progress: (2, 2.261094093322754, {'accuracy': 0.1953, 'data_size': 10000}, 186.390111893008)
INFO flwr 2024-04-06 11:19:43,970 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:19:43,970 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:21:02,917 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:21:23,323 | server.py:125 | fit progress: (3, 2.2263529300689697, {'accuracy': 0.23, 'data_size': 10000}, 285.7436513119901)
INFO flwr 2024-04-06 11:21:23,324 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:21:23,324 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:22:44,636 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:23:10,637 | server.py:125 | fit progress: (4, 2.192821979522705, {'accuracy': 0.2677, 'data_size': 10000}, 393.05714588199044)
INFO flwr 2024-04-06 11:23:10,637 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:23:10,638 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:24:23,735 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:25:00,640 | server.py:125 | fit progress: (5, 2.1330435276031494, {'accuracy': 0.3278, 'data_size': 10000}, 503.06034852098674)
INFO flwr 2024-04-06 11:25:00,640 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:25:00,641 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:26:14,430 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:26:59,619 | server.py:125 | fit progress: (6, 2.247056245803833, {'accuracy': 0.2133, 'data_size': 10000}, 622.0396603370027)
INFO flwr 2024-04-06 11:26:59,620 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:26:59,620 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:28:21,180 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:29:14,398 | server.py:125 | fit progress: (7, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 756.8185253569973)
INFO flwr 2024-04-06 11:29:14,398 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:29:14,399 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:30:34,377 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:31:26,376 | server.py:125 | fit progress: (8, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 888.7961168480106)
INFO flwr 2024-04-06 11:31:26,376 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:31:26,377 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:32:45,390 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 11:33:43,039 | server.py:125 | fit progress: (9, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 1025.4593688200112)
INFO flwr 2024-04-06 11:33:43,040 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 11:33:43,040 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:35:07,267 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 11:36:21,332 | server.py:125 | fit progress: (10, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 1183.7528795069957)
INFO flwr 2024-04-06 11:36:21,333 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 11:36:21,333 | server.py:153 | FL finished in 1183.7533630899852
INFO flwr 2024-04-06 11:36:21,333 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 11:36:21,333 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 11:36:21,333 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 11:36:21,334 | app.py:229 | app_fit: losses_centralized [(0, 2.3025643825531006), (1, 2.2624902725219727), (2, 2.261094093322754), (3, 2.2263529300689697), (4, 2.192821979522705), (5, 2.1330435276031494), (6, 2.247056245803833), (7, 2.3629422187805176), (8, 2.3629422187805176), (9, 2.3629422187805176), (10, 2.3629422187805176)]
INFO flwr 2024-04-06 11:36:21,334 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0843), (1, 0.3055), (2, 0.1953), (3, 0.23), (4, 0.2677), (5, 0.3278), (6, 0.2133), (7, 0.0982), (8, 0.0982), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.36294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_111613-0bktqb1e
wandb: Find logs at: ./wandb/offline-run-20240406_111613-0bktqb1e/logs
INFO flwr 2024-04-06 11:36:24,877 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 11:43:49,779 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1018625)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1018625)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 11:43:55,812	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 11:43:56,192	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 11:43:56,517	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bbee16a3edcb5da7.zip' (9.22MiB) to Ray cluster...
2024-04-06 11:43:56,553	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bbee16a3edcb5da7.zip'.
INFO flwr 2024-04-06 11:44:07,228 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 140342337332.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 64432430284.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 11:44:07,228 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 11:44:07,228 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 11:44:07,246 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 11:44:07,247 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 11:44:07,247 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 11:44:07,247 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1035147)[0m 2024-04-06 11:44:13.123513: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1035147)[0m 2024-04-06 11:44:13.215021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1035147)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 11:44:14,485 | server.py:94 | initial parameters (loss, other metrics): 2.302441120147705, {'accuracy': 0.1122, 'data_size': 10000}
INFO flwr 2024-04-06 11:44:14,485 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 11:44:14,486 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1035150)[0m 2024-04-06 11:44:15.294977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1035156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1035156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1035149)[0m 2024-04-06 11:44:13.416676: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035149)[0m 2024-04-06 11:44:13.509588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035149)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1035145)[0m 2024-04-06 11:44:15.514026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1035149)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1035149)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 11:45:33,847 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 11:45:37,382 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 11:45:41,749 | server.py:125 | fit progress: (1, 2.255430221557617, {'accuracy': 0.1232, 'data_size': 10000}, 87.26334744898486)
INFO flwr 2024-04-06 11:45:41,750 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 11:45:41,750 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:47:10,852 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 11:47:24,468 | server.py:125 | fit progress: (2, 2.134267807006836, {'accuracy': 0.3232, 'data_size': 10000}, 189.98293760998058)
INFO flwr 2024-04-06 11:47:24,469 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 11:47:24,469 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:48:37,533 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 11:48:57,383 | server.py:125 | fit progress: (3, 2.1620945930480957, {'accuracy': 0.2929, 'data_size': 10000}, 282.8975359759934)
INFO flwr 2024-04-06 11:48:57,383 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 11:48:57,384 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:50:11,268 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 11:50:38,528 | server.py:125 | fit progress: (4, 2.085620403289795, {'accuracy': 0.3732, 'data_size': 10000}, 384.04239972899086)
INFO flwr 2024-04-06 11:50:38,528 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 11:50:38,528 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:52:06,780 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 11:52:48,237 | server.py:125 | fit progress: (5, 2.0188169479370117, {'accuracy': 0.4402, 'data_size': 10000}, 513.7514467199799)
INFO flwr 2024-04-06 11:52:48,238 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 11:52:48,238 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:54:09,803 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 11:54:49,075 | server.py:125 | fit progress: (6, 2.1415534019470215, {'accuracy': 0.3187, 'data_size': 10000}, 634.5896463659883)
INFO flwr 2024-04-06 11:54:49,076 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 11:54:49,076 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:56:13,525 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 11:57:01,451 | server.py:125 | fit progress: (7, 2.1723034381866455, {'accuracy': 0.2886, 'data_size': 10000}, 766.9660341279814)
INFO flwr 2024-04-06 11:57:01,452 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 11:57:01,452 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 11:58:29,501 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 11:59:22,734 | server.py:125 | fit progress: (8, 2.264310836791992, {'accuracy': 0.1968, 'data_size': 10000}, 908.2486152089841)
INFO flwr 2024-04-06 11:59:22,735 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 11:59:22,735 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:00:56,127 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:02:05,807 | server.py:125 | fit progress: (9, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 1071.3220242300013)
INFO flwr 2024-04-06 12:02:05,808 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:02:05,808 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:03:28,452 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:04:47,802 | server.py:125 | fit progress: (10, 2.3629422187805176, {'accuracy': 0.0982, 'data_size': 10000}, 1233.3160932049796)
INFO flwr 2024-04-06 12:04:47,802 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:04:47,802 | server.py:153 | FL finished in 1233.3169599050016
INFO flwr 2024-04-06 12:04:47,803 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:04:47,803 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:04:47,803 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:04:47,803 | app.py:229 | app_fit: losses_centralized [(0, 2.302441120147705), (1, 2.255430221557617), (2, 2.134267807006836), (3, 2.1620945930480957), (4, 2.085620403289795), (5, 2.0188169479370117), (6, 2.1415534019470215), (7, 2.1723034381866455), (8, 2.264310836791992), (9, 2.3629422187805176), (10, 2.3629422187805176)]
INFO flwr 2024-04-06 12:04:47,803 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1122), (1, 0.1232), (2, 0.3232), (3, 0.2929), (4, 0.3732), (5, 0.4402), (6, 0.3187), (7, 0.2886), (8, 0.1968), (9, 0.0982), (10, 0.0982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0982
wandb:     loss 2.36294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_114349-5rfcr2u2
wandb: Find logs at: ./wandb/offline-run-20240406_114349-5rfcr2u2/logs
INFO flwr 2024-04-06 12:04:51,339 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:12:16,951 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1035145)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1035145)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 12:12:23,580	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:12:24,056	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:12:24,401	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_68180bce7aa321d1.zip' (9.27MiB) to Ray cluster...
2024-04-06 12:12:24,428	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_68180bce7aa321d1.zip'.
INFO flwr 2024-04-06 12:12:35,721 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 139511930266.0, 'CPU': 64.0, 'object_store_memory': 64076541542.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 12:12:35,721 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:12:35,721 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:12:35,740 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:12:35,741 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:12:35,742 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:12:35,742 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1051643)[0m 2024-04-06 12:12:41.785385: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1051643)[0m 2024-04-06 12:12:41.875909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1051643)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 12:12:44,164 | server.py:94 | initial parameters (loss, other metrics): 2.30255389213562, {'accuracy': 0.1328, 'data_size': 10000}
INFO flwr 2024-04-06 12:12:44,164 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:12:44,164 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1051643)[0m 2024-04-06 12:12:44.705350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1051648)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1051648)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1051648)[0m 2024-04-06 12:12:42.325064: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1051648)[0m 2024-04-06 12:12:42.402895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1051648)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1051648)[0m 2024-04-06 12:12:45.244252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1051639)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1051639)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:12:59,527 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:13:03,398 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:13:07,657 | server.py:125 | fit progress: (1, 2.302506923675537, {'accuracy': 0.1332, 'data_size': 10000}, 23.492408814985538)
INFO flwr 2024-04-06 12:13:07,657 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:13:07,657 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:13:16,972 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:13:30,727 | server.py:125 | fit progress: (2, 2.3024420738220215, {'accuracy': 0.1387, 'data_size': 10000}, 46.562718740984565)
INFO flwr 2024-04-06 12:13:30,728 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:13:30,728 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:13:40,652 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:14:01,941 | server.py:125 | fit progress: (3, 2.3023736476898193, {'accuracy': 0.1491, 'data_size': 10000}, 77.7771260899899)
INFO flwr 2024-04-06 12:14:01,942 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:14:01,942 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:14:11,180 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:14:37,290 | server.py:125 | fit progress: (4, 2.3023107051849365, {'accuracy': 0.1435, 'data_size': 10000}, 113.12531595799373)
INFO flwr 2024-04-06 12:14:37,290 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:14:37,290 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:14:46,147 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:15:18,415 | server.py:125 | fit progress: (5, 2.302260637283325, {'accuracy': 0.1476, 'data_size': 10000}, 154.25117300497368)
INFO flwr 2024-04-06 12:15:18,416 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:15:18,416 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:15:27,537 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:16:12,010 | server.py:125 | fit progress: (6, 2.3022029399871826, {'accuracy': 0.1617, 'data_size': 10000}, 207.84530472397455)
INFO flwr 2024-04-06 12:16:12,010 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:16:12,010 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:16:21,415 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:17:14,038 | server.py:125 | fit progress: (7, 2.3021433353424072, {'accuracy': 0.1521, 'data_size': 10000}, 269.87367146398174)
INFO flwr 2024-04-06 12:17:14,038 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:17:14,039 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:17:22,914 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:18:21,953 | server.py:125 | fit progress: (8, 2.3020920753479004, {'accuracy': 0.1667, 'data_size': 10000}, 337.78895868899417)
INFO flwr 2024-04-06 12:18:21,954 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:18:21,954 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:18:31,424 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:19:40,237 | server.py:125 | fit progress: (9, 2.30203914642334, {'accuracy': 0.175, 'data_size': 10000}, 416.07229339698097)
INFO flwr 2024-04-06 12:19:40,237 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:19:40,237 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:19:49,203 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:21:09,106 | server.py:125 | fit progress: (10, 2.3019821643829346, {'accuracy': 0.1678, 'data_size': 10000}, 504.941208720993)
INFO flwr 2024-04-06 12:21:09,106 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:21:09,106 | server.py:153 | FL finished in 504.9421342459973
INFO flwr 2024-04-06 12:21:09,107 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:21:09,107 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:21:09,107 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:21:09,107 | app.py:229 | app_fit: losses_centralized [(0, 2.30255389213562), (1, 2.302506923675537), (2, 2.3024420738220215), (3, 2.3023736476898193), (4, 2.3023107051849365), (5, 2.302260637283325), (6, 2.3022029399871826), (7, 2.3021433353424072), (8, 2.3020920753479004), (9, 2.30203914642334), (10, 2.3019821643829346)]
INFO flwr 2024-04-06 12:21:09,107 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1328), (1, 0.1332), (2, 0.1387), (3, 0.1491), (4, 0.1435), (5, 0.1476), (6, 0.1617), (7, 0.1521), (8, 0.1667), (9, 0.175), (10, 0.1678)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1678
wandb:     loss 2.30198
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_121216-lg014gat
wandb: Find logs at: ./wandb/offline-run-20240406_121216-lg014gat/logs
INFO flwr 2024-04-06 12:21:12,705 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:28:37,934 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 12:28:44,140	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:28:44,529	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:28:44,899	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_56bb91366c0dd012.zip' (9.30MiB) to Ray cluster...
2024-04-06 12:28:44,931	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_56bb91366c0dd012.zip'.
INFO flwr 2024-04-06 12:28:55,854 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 64099011379.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 139564359885.0}
INFO flwr 2024-04-06 12:28:55,855 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:28:55,855 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:28:55,871 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:28:55,872 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:28:55,872 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:28:55,872 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 12:29:02,411 | server.py:94 | initial parameters (loss, other metrics): 2.3025243282318115, {'accuracy': 0.1038, 'data_size': 10000}
INFO flwr 2024-04-06 12:29:02,412 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:29:02,412 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1059870)[0m 2024-04-06 12:29:02.764646: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1059870)[0m 2024-04-06 12:29:02.858289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1059870)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1059870)[0m 2024-04-06 12:29:04.995620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1059876)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1059876)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1059866)[0m 2024-04-06 12:29:03.019767: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1059866)[0m 2024-04-06 12:29:03.111424: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1059866)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1059866)[0m 2024-04-06 12:29:05.158072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1059865)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1059865)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 12:29:18,961 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:29:22,314 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:29:26,538 | server.py:125 | fit progress: (1, 2.2993698120117188, {'accuracy': 0.1241, 'data_size': 10000}, 24.126006862992654)
INFO flwr 2024-04-06 12:29:26,538 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:29:26,538 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:29:36,008 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:29:49,852 | server.py:125 | fit progress: (2, 2.2811827659606934, {'accuracy': 0.2792, 'data_size': 10000}, 47.44056061501033)
INFO flwr 2024-04-06 12:29:49,853 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:29:49,853 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:29:58,685 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:30:19,109 | server.py:125 | fit progress: (3, 2.243169069290161, {'accuracy': 0.1942, 'data_size': 10000}, 76.69746342199505)
INFO flwr 2024-04-06 12:30:19,110 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:30:19,110 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:30:27,855 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:30:54,664 | server.py:125 | fit progress: (4, 2.135096311569214, {'accuracy': 0.3539, 'data_size': 10000}, 112.25258632100304)
INFO flwr 2024-04-06 12:30:54,665 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:30:54,665 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:03,756 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:31:37,041 | server.py:125 | fit progress: (5, 1.9397443532943726, {'accuracy': 0.5771, 'data_size': 10000}, 154.62946754999575)
INFO flwr 2024-04-06 12:31:37,042 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:31:37,042 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:31:45,826 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:32:27,788 | server.py:125 | fit progress: (6, 1.7870408296585083, {'accuracy': 0.7392, 'data_size': 10000}, 205.37644867101335)
INFO flwr 2024-04-06 12:32:27,789 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:32:27,789 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:32:36,350 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:33:30,174 | server.py:125 | fit progress: (7, 1.737636685371399, {'accuracy': 0.7596, 'data_size': 10000}, 267.7624197890109)
INFO flwr 2024-04-06 12:33:30,174 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:33:30,175 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:33:39,029 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:34:36,543 | server.py:125 | fit progress: (8, 1.7000946998596191, {'accuracy': 0.794, 'data_size': 10000}, 334.1307953739888)
INFO flwr 2024-04-06 12:34:36,543 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:34:36,543 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:34:46,367 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:35:54,194 | server.py:125 | fit progress: (9, 1.6546248197555542, {'accuracy': 0.8265, 'data_size': 10000}, 411.7825116479944)
INFO flwr 2024-04-06 12:35:54,195 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:35:54,195 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:36:03,094 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:37:20,191 | server.py:125 | fit progress: (10, 1.5992313623428345, {'accuracy': 0.8831, 'data_size': 10000}, 497.7790429230081)
INFO flwr 2024-04-06 12:37:20,191 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:37:20,191 | server.py:153 | FL finished in 497.77955020801164
INFO flwr 2024-04-06 12:37:20,192 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:37:20,192 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:37:20,192 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:37:20,192 | app.py:229 | app_fit: losses_centralized [(0, 2.3025243282318115), (1, 2.2993698120117188), (2, 2.2811827659606934), (3, 2.243169069290161), (4, 2.135096311569214), (5, 1.9397443532943726), (6, 1.7870408296585083), (7, 1.737636685371399), (8, 1.7000946998596191), (9, 1.6546248197555542), (10, 1.5992313623428345)]
INFO flwr 2024-04-06 12:37:20,192 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1038), (1, 0.1241), (2, 0.2792), (3, 0.1942), (4, 0.3539), (5, 0.5771), (6, 0.7392), (7, 0.7596), (8, 0.794), (9, 0.8265), (10, 0.8831)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8831
wandb:     loss 1.59923
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_122837-7rb9qq64
wandb: Find logs at: ./wandb/offline-run-20240406_122837-7rb9qq64/logs
INFO flwr 2024-04-06 12:37:24,631 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 12:44:49,136 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 12:44:54,918	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 12:44:55,354	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 12:44:55,795	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fa026848f41ba9b5.zip' (9.33MiB) to Ray cluster...
2024-04-06 12:44:55,820	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fa026848f41ba9b5.zip'.
INFO flwr 2024-04-06 12:45:06,531 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 139440149914.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64045778534.0}
INFO flwr 2024-04-06 12:45:06,531 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 12:45:06,531 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 12:45:06,549 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 12:45:06,550 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 12:45:06,551 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 12:45:06,551 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1071303)[0m 2024-04-06 12:45:11.985549: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1071309)[0m 2024-04-06 12:45:12.119050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1071309)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 12:45:13,755 | server.py:94 | initial parameters (loss, other metrics): 2.30232834815979, {'accuracy': 0.0949, 'data_size': 10000}
INFO flwr 2024-04-06 12:45:13,756 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 12:45:13,756 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1071309)[0m 2024-04-06 12:45:14.309787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1071311)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1071311)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1071306)[0m 2024-04-06 12:45:12.933011: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1071306)[0m 2024-04-06 12:45:13.037050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1071306)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1071310)[0m 2024-04-06 12:45:15.093267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1071307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1071307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 12:45:30,794 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 12:45:35,112 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 12:45:39,353 | server.py:125 | fit progress: (1, 2.289423704147339, {'accuracy': 0.1028, 'data_size': 10000}, 25.596871010988252)
INFO flwr 2024-04-06 12:45:39,353 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 12:45:39,353 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:45:48,702 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 12:46:02,923 | server.py:125 | fit progress: (2, 2.219069719314575, {'accuracy': 0.3711, 'data_size': 10000}, 49.16700033500092)
INFO flwr 2024-04-06 12:46:02,923 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 12:46:02,923 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:46:12,320 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 12:46:33,614 | server.py:125 | fit progress: (3, 1.9130650758743286, {'accuracy': 0.6126, 'data_size': 10000}, 79.8580286130018)
INFO flwr 2024-04-06 12:46:33,614 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 12:46:33,614 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:46:43,001 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 12:47:10,528 | server.py:125 | fit progress: (4, 1.7794746160507202, {'accuracy': 0.717, 'data_size': 10000}, 116.77260733899311)
INFO flwr 2024-04-06 12:47:10,529 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 12:47:10,529 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:47:20,289 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 12:47:56,904 | server.py:125 | fit progress: (5, 1.6807482242584229, {'accuracy': 0.8066, 'data_size': 10000}, 163.14822149599786)
INFO flwr 2024-04-06 12:47:56,904 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 12:47:56,905 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:48:06,294 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 12:48:52,212 | server.py:125 | fit progress: (6, 1.7224777936935425, {'accuracy': 0.7427, 'data_size': 10000}, 218.45621112998924)
INFO flwr 2024-04-06 12:48:52,212 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 12:48:52,212 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:49:00,804 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 12:49:51,898 | server.py:125 | fit progress: (7, 1.6009557247161865, {'accuracy': 0.8808, 'data_size': 10000}, 278.1422000390012)
INFO flwr 2024-04-06 12:49:51,898 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 12:49:51,899 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:50:00,115 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 12:50:57,812 | server.py:125 | fit progress: (8, 1.5599666833877563, {'accuracy': 0.9175, 'data_size': 10000}, 344.0559037179919)
INFO flwr 2024-04-06 12:50:57,812 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 12:50:57,812 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:51:06,917 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 12:52:19,269 | server.py:125 | fit progress: (9, 1.5676896572113037, {'accuracy': 0.9072, 'data_size': 10000}, 425.5129322159919)
INFO flwr 2024-04-06 12:52:19,269 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 12:52:19,269 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 12:52:28,421 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 12:53:50,384 | server.py:125 | fit progress: (10, 1.550459384918213, {'accuracy': 0.9224, 'data_size': 10000}, 516.628063643002)
INFO flwr 2024-04-06 12:53:50,384 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 12:53:50,384 | server.py:153 | FL finished in 516.6287911779946
INFO flwr 2024-04-06 12:53:50,385 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 12:53:50,385 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 12:53:50,385 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 12:53:50,385 | app.py:229 | app_fit: losses_centralized [(0, 2.30232834815979), (1, 2.289423704147339), (2, 2.219069719314575), (3, 1.9130650758743286), (4, 1.7794746160507202), (5, 1.6807482242584229), (6, 1.7224777936935425), (7, 1.6009557247161865), (8, 1.5599666833877563), (9, 1.5676896572113037), (10, 1.550459384918213)]
INFO flwr 2024-04-06 12:53:50,385 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0949), (1, 0.1028), (2, 0.3711), (3, 0.6126), (4, 0.717), (5, 0.8066), (6, 0.7427), (7, 0.8808), (8, 0.9175), (9, 0.9072), (10, 0.9224)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9224
wandb:     loss 1.55046
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_124448-q1byvsw3
wandb: Find logs at: ./wandb/offline-run-20240406_124448-q1byvsw3/logs
INFO flwr 2024-04-06 12:53:53,997 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:01:18,622 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1071300)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1071300)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 13:01:23,582	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:01:23,920	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:01:24,410	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d2b719cf5e39cf1b.zip' (9.36MiB) to Ray cluster...
2024-04-06 13:01:24,441	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d2b719cf5e39cf1b.zip'.
INFO flwr 2024-04-06 13:01:35,543 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 144659073229.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 66282459955.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 13:01:35,543 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:01:35,544 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:01:35,561 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:01:35,562 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:01:35,562 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:01:35,562 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1082798)[0m 2024-04-06 13:01:41.336509: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1082798)[0m 2024-04-06 13:01:41.538349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1082798)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 13:01:42,856 | server.py:94 | initial parameters (loss, other metrics): 2.3026506900787354, {'accuracy': 0.0737, 'data_size': 10000}
INFO flwr 2024-04-06 13:01:42,856 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:01:42,857 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1082805)[0m 2024-04-06 13:01:43.582651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1082808)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1082808)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1082808)[0m 2024-04-06 13:01:42.177769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1082808)[0m 2024-04-06 13:01:42.266356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1082808)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1082800)[0m 2024-04-06 13:01:44.411131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1082797)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1082797)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 13:01:58,922 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:02:02,429 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:02:06,633 | server.py:125 | fit progress: (1, 2.2839415073394775, {'accuracy': 0.385, 'data_size': 10000}, 23.77628304497921)
INFO flwr 2024-04-06 13:02:06,634 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:06,634 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:16,233 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:29,992 | server.py:125 | fit progress: (2, 2.1140904426574707, {'accuracy': 0.4395, 'data_size': 10000}, 47.13481934700394)
INFO flwr 2024-04-06 13:02:29,992 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:29,992 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:02:38,870 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:02:59,290 | server.py:125 | fit progress: (3, 1.8341196775436401, {'accuracy': 0.6897, 'data_size': 10000}, 76.43347270900267)
INFO flwr 2024-04-06 13:02:59,291 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:02:59,291 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:03:08,258 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:03:35,078 | server.py:125 | fit progress: (4, 1.663588523864746, {'accuracy': 0.8352, 'data_size': 10000}, 112.22071518900339)
INFO flwr 2024-04-06 13:03:35,078 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:03:35,078 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:03:43,788 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:04:17,009 | server.py:125 | fit progress: (5, 1.6190346479415894, {'accuracy': 0.8603, 'data_size': 10000}, 154.1525608219963)
INFO flwr 2024-04-06 13:04:17,010 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:04:17,010 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:04:25,912 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:05:03,039 | server.py:125 | fit progress: (6, 1.556230902671814, {'accuracy': 0.9187, 'data_size': 10000}, 200.182374127995)
INFO flwr 2024-04-06 13:05:03,039 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:05:03,040 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:05:11,332 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:06:03,325 | server.py:125 | fit progress: (7, 1.568770170211792, {'accuracy': 0.9015, 'data_size': 10000}, 260.4677982889989)
INFO flwr 2024-04-06 13:06:03,325 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:06:03,325 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:06:12,133 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:07:12,811 | server.py:125 | fit progress: (8, 1.5363452434539795, {'accuracy': 0.9333, 'data_size': 10000}, 329.9543913519883)
INFO flwr 2024-04-06 13:07:12,812 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:07:12,812 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:07:21,886 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:08:23,307 | server.py:125 | fit progress: (9, 1.5329399108886719, {'accuracy': 0.9333, 'data_size': 10000}, 400.4503099609865)
INFO flwr 2024-04-06 13:08:23,308 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:08:23,308 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:08:32,672 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:09:51,481 | server.py:125 | fit progress: (10, 1.5275416374206543, {'accuracy': 0.9382, 'data_size': 10000}, 488.62445387398475)
INFO flwr 2024-04-06 13:09:51,482 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:09:51,482 | server.py:153 | FL finished in 488.6250397019903
INFO flwr 2024-04-06 13:09:51,482 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:09:51,482 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:09:51,482 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:09:51,482 | app.py:229 | app_fit: losses_centralized [(0, 2.3026506900787354), (1, 2.2839415073394775), (2, 2.1140904426574707), (3, 1.8341196775436401), (4, 1.663588523864746), (5, 1.6190346479415894), (6, 1.556230902671814), (7, 1.568770170211792), (8, 1.5363452434539795), (9, 1.5329399108886719), (10, 1.5275416374206543)]
INFO flwr 2024-04-06 13:09:51,483 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0737), (1, 0.385), (2, 0.4395), (3, 0.6897), (4, 0.8352), (5, 0.8603), (6, 0.9187), (7, 0.9015), (8, 0.9333), (9, 0.9333), (10, 0.9382)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9382
wandb:     loss 1.52754
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_130118-onwca54y
wandb: Find logs at: ./wandb/offline-run-20240406_130118-onwca54y/logs
INFO flwr 2024-04-06 13:09:55,037 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:17:19,939 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1082795)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1082795)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 13:17:24,883	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:17:25,315	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:17:25,675	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1bfb60880ab3d0c4.zip' (9.40MiB) to Ray cluster...
2024-04-06 13:17:25,704	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1bfb60880ab3d0c4.zip'.
INFO flwr 2024-04-06 13:17:36,545 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 63874821734.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 139041250714.0, 'CPU': 64.0}
INFO flwr 2024-04-06 13:17:36,545 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:17:36,546 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:17:36,565 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:17:36,567 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:17:36,568 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:17:36,568 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1091633)[0m 2024-04-06 13:17:42.093359: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1091633)[0m 2024-04-06 13:17:42.225308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1091633)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1091625)[0m 2024-04-06 13:17:44.485407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 13:17:45,042 | server.py:94 | initial parameters (loss, other metrics): 2.302428722381592, {'accuracy': 0.0934, 'data_size': 10000}
INFO flwr 2024-04-06 13:17:45,043 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:17:45,043 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1091634)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1091634)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1091634)[0m 2024-04-06 13:17:42.915017: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1091634)[0m 2024-04-06 13:17:43.024683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1091634)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1091627)[0m 2024-04-06 13:17:45.313357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1091630)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1091630)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 13:18:00,888 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:18:04,080 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:18:08,432 | server.py:125 | fit progress: (1, 2.2708487510681152, {'accuracy': 0.3164, 'data_size': 10000}, 23.38862676700228)
INFO flwr 2024-04-06 13:18:08,432 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:18:08,433 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:18:18,410 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:18:32,037 | server.py:125 | fit progress: (2, 2.0842719078063965, {'accuracy': 0.4091, 'data_size': 10000}, 46.993880498979706)
INFO flwr 2024-04-06 13:18:32,038 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:18:32,038 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:18:41,092 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:19:01,222 | server.py:125 | fit progress: (3, 1.8482328653335571, {'accuracy': 0.6378, 'data_size': 10000}, 76.17880154200247)
INFO flwr 2024-04-06 13:19:01,222 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:19:01,223 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:19:09,725 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:19:35,985 | server.py:125 | fit progress: (4, 1.717423439025879, {'accuracy': 0.7657, 'data_size': 10000}, 110.94239332899451)
INFO flwr 2024-04-06 13:19:35,986 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:19:35,986 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:19:44,711 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:20:16,958 | server.py:125 | fit progress: (5, 1.66878342628479, {'accuracy': 0.8041, 'data_size': 10000}, 151.91505305399187)
INFO flwr 2024-04-06 13:20:16,958 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:20:16,959 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:20:25,982 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:21:09,637 | server.py:125 | fit progress: (6, 1.5550453662872314, {'accuracy': 0.9183, 'data_size': 10000}, 204.59404563397402)
INFO flwr 2024-04-06 13:21:09,638 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:21:09,638 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:21:19,342 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:22:19,880 | server.py:125 | fit progress: (7, 1.5517593622207642, {'accuracy': 0.9148, 'data_size': 10000}, 274.8368639489927)
INFO flwr 2024-04-06 13:22:19,880 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:22:19,881 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:22:28,755 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:23:24,871 | server.py:125 | fit progress: (8, 1.52730131149292, {'accuracy': 0.9406, 'data_size': 10000}, 339.8276211739867)
INFO flwr 2024-04-06 13:23:24,871 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:23:24,871 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:23:34,111 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:24:35,399 | server.py:125 | fit progress: (9, 1.5249965190887451, {'accuracy': 0.9428, 'data_size': 10000}, 410.35606913498486)
INFO flwr 2024-04-06 13:24:35,400 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:24:35,400 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:24:44,536 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:25:51,636 | server.py:125 | fit progress: (10, 1.5145747661590576, {'accuracy': 0.9495, 'data_size': 10000}, 486.59325437399093)
INFO flwr 2024-04-06 13:25:51,637 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:25:51,637 | server.py:153 | FL finished in 486.5937279849895
INFO flwr 2024-04-06 13:25:51,637 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:25:51,637 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:25:51,637 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:25:51,637 | app.py:229 | app_fit: losses_centralized [(0, 2.302428722381592), (1, 2.2708487510681152), (2, 2.0842719078063965), (3, 1.8482328653335571), (4, 1.717423439025879), (5, 1.66878342628479), (6, 1.5550453662872314), (7, 1.5517593622207642), (8, 1.52730131149292), (9, 1.5249965190887451), (10, 1.5145747661590576)]
INFO flwr 2024-04-06 13:25:51,638 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0934), (1, 0.3164), (2, 0.4091), (3, 0.6378), (4, 0.7657), (5, 0.8041), (6, 0.9183), (7, 0.9148), (8, 0.9406), (9, 0.9428), (10, 0.9495)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9495
wandb:     loss 1.51457
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_131719-fl9oasof
wandb: Find logs at: ./wandb/offline-run-20240406_131719-fl9oasof/logs
INFO flwr 2024-04-06 13:25:55,207 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:33:20,086 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1091623)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1091623)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 13:33:24,910	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:33:25,352	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:33:25,756	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1b9dcbdd092da0f1.zip' (9.43MiB) to Ray cluster...
2024-04-06 13:33:25,798	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1b9dcbdd092da0f1.zip'.
INFO flwr 2024-04-06 13:33:36,819 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63282272256.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 137658635264.0}
INFO flwr 2024-04-06 13:33:36,819 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:33:36,820 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:33:36,837 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:33:36,839 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:33:36,839 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:33:36,839 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1103132)[0m 2024-04-06 13:33:42.421480: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1103132)[0m 2024-04-06 13:33:42.509725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1103132)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 13:33:43,986 | server.py:94 | initial parameters (loss, other metrics): 2.3025875091552734, {'accuracy': 0.1003, 'data_size': 10000}
INFO flwr 2024-04-06 13:33:43,986 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:33:43,987 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1103132)[0m 2024-04-06 13:33:45.050274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1103146)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1103146)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1103139)[0m 2024-04-06 13:33:43.348303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1103139)[0m 2024-04-06 13:33:43.438166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1103139)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1103146)[0m 2024-04-06 13:33:45.965031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1103133)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1103133)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 13:34:00,704 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:34:04,180 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:34:08,464 | server.py:125 | fit progress: (1, 2.2735087871551514, {'accuracy': 0.2051, 'data_size': 10000}, 24.478047532000346)
INFO flwr 2024-04-06 13:34:08,465 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:34:08,465 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:34:17,930 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:34:31,568 | server.py:125 | fit progress: (2, 2.058084487915039, {'accuracy': 0.5066, 'data_size': 10000}, 47.58128483599285)
INFO flwr 2024-04-06 13:34:31,568 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:34:31,568 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:34:40,378 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:35:00,557 | server.py:125 | fit progress: (3, 1.757682204246521, {'accuracy': 0.72, 'data_size': 10000}, 76.5706565429864)
INFO flwr 2024-04-06 13:35:00,558 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:35:00,558 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:35:09,607 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:35:36,230 | server.py:125 | fit progress: (4, 1.6522306203842163, {'accuracy': 0.8256, 'data_size': 10000}, 112.24396803998388)
INFO flwr 2024-04-06 13:35:36,231 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:35:36,231 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:35:45,001 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:36:17,308 | server.py:125 | fit progress: (5, 1.6074401140213013, {'accuracy': 0.8655, 'data_size': 10000}, 153.321112575999)
INFO flwr 2024-04-06 13:36:17,308 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:36:17,308 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:36:26,737 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:37:10,984 | server.py:125 | fit progress: (6, 1.5686814785003662, {'accuracy': 0.9025, 'data_size': 10000}, 206.9980361549824)
INFO flwr 2024-04-06 13:37:10,985 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:37:10,985 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:37:19,960 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:38:12,143 | server.py:125 | fit progress: (7, 1.5413614511489868, {'accuracy': 0.9259, 'data_size': 10000}, 268.1563637259824)
INFO flwr 2024-04-06 13:38:12,143 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:38:12,143 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:38:20,604 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:39:21,231 | server.py:125 | fit progress: (8, 1.5466086864471436, {'accuracy': 0.9185, 'data_size': 10000}, 337.24477663100697)
INFO flwr 2024-04-06 13:39:21,232 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:39:21,232 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:39:29,812 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:40:29,951 | server.py:125 | fit progress: (9, 1.5239183902740479, {'accuracy': 0.9405, 'data_size': 10000}, 405.9649304249906)
INFO flwr 2024-04-06 13:40:29,952 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:40:29,952 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:40:38,868 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:41:42,788 | server.py:125 | fit progress: (10, 1.5229573249816895, {'accuracy': 0.9404, 'data_size': 10000}, 478.80106285598595)
INFO flwr 2024-04-06 13:41:42,788 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:41:42,788 | server.py:153 | FL finished in 478.80187063198537
INFO flwr 2024-04-06 13:41:42,789 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:41:42,789 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:41:42,789 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:41:42,789 | app.py:229 | app_fit: losses_centralized [(0, 2.3025875091552734), (1, 2.2735087871551514), (2, 2.058084487915039), (3, 1.757682204246521), (4, 1.6522306203842163), (5, 1.6074401140213013), (6, 1.5686814785003662), (7, 1.5413614511489868), (8, 1.5466086864471436), (9, 1.5239183902740479), (10, 1.5229573249816895)]
INFO flwr 2024-04-06 13:41:42,789 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1003), (1, 0.2051), (2, 0.5066), (3, 0.72), (4, 0.8256), (5, 0.8655), (6, 0.9025), (7, 0.9259), (8, 0.9185), (9, 0.9405), (10, 0.9404)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9404
wandb:     loss 1.52296
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_133319-k63k6wlq
wandb: Find logs at: ./wandb/offline-run-20240406_133319-k63k6wlq/logs
INFO flwr 2024-04-06 13:41:46,329 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 13:49:11,018 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1103132)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1103132)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 13:49:18,019	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 13:49:18,402	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 13:49:18,734	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3148367464bb46c8.zip' (9.46MiB) to Ray cluster...
2024-04-06 13:49:18,756	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3148367464bb46c8.zip'.
INFO flwr 2024-04-06 13:49:29,521 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 138717489357.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 63736066867.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 13:49:29,522 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 13:49:29,522 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 13:49:29,537 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 13:49:29,537 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 13:49:29,537 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 13:49:29,538 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1111303)[0m 2024-04-06 13:49:34.961546: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1111303)[0m 2024-04-06 13:49:35.066727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1111303)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 13:49:36,664 | server.py:94 | initial parameters (loss, other metrics): 2.302830219268799, {'accuracy': 0.0618, 'data_size': 10000}
INFO flwr 2024-04-06 13:49:36,664 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 13:49:36,665 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1111303)[0m 2024-04-06 13:49:37.334875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1111303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1111303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1111294)[0m 2024-04-06 13:49:36.070190: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1111294)[0m 2024-04-06 13:49:36.160128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1111294)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1111294)[0m 2024-04-06 13:49:38.212224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1111295)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1111295)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 13:49:53,534 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 13:49:57,167 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 13:50:01,398 | server.py:125 | fit progress: (1, 2.285064697265625, {'accuracy': 0.2167, 'data_size': 10000}, 24.733377650991315)
INFO flwr 2024-04-06 13:50:01,399 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 13:50:01,399 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:50:11,540 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 13:50:25,841 | server.py:125 | fit progress: (2, 2.179049253463745, {'accuracy': 0.3024, 'data_size': 10000}, 49.17652063301648)
INFO flwr 2024-04-06 13:50:25,842 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 13:50:25,842 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:50:35,286 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 13:50:55,857 | server.py:125 | fit progress: (3, 1.8434040546417236, {'accuracy': 0.6556, 'data_size': 10000}, 79.1928239230183)
INFO flwr 2024-04-06 13:50:55,858 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 13:50:55,858 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:51:04,877 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 13:51:32,925 | server.py:125 | fit progress: (4, 1.6903620958328247, {'accuracy': 0.7788, 'data_size': 10000}, 116.26078959499137)
INFO flwr 2024-04-06 13:51:32,926 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 13:51:32,926 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:51:42,105 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 13:52:15,632 | server.py:125 | fit progress: (5, 1.5778692960739136, {'accuracy': 0.8935, 'data_size': 10000}, 158.96794001801754)
INFO flwr 2024-04-06 13:52:15,633 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 13:52:15,633 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:52:24,647 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 13:53:10,881 | server.py:125 | fit progress: (6, 1.5608795881271362, {'accuracy': 0.9102, 'data_size': 10000}, 214.2165264080104)
INFO flwr 2024-04-06 13:53:10,882 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 13:53:10,882 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:53:19,829 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 13:54:06,890 | server.py:125 | fit progress: (7, 1.5290943384170532, {'accuracy': 0.9356, 'data_size': 10000}, 270.22515035499237)
INFO flwr 2024-04-06 13:54:06,890 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 13:54:06,890 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:54:16,057 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 13:55:10,499 | server.py:125 | fit progress: (8, 1.5493804216384888, {'accuracy': 0.9127, 'data_size': 10000}, 333.834642730013)
INFO flwr 2024-04-06 13:55:10,500 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 13:55:10,500 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:55:19,870 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 13:56:21,140 | server.py:125 | fit progress: (9, 1.5235508680343628, {'accuracy': 0.9406, 'data_size': 10000}, 404.4756633620127)
INFO flwr 2024-04-06 13:56:21,141 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 13:56:21,141 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 13:56:30,655 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 13:57:34,959 | server.py:125 | fit progress: (10, 1.5171382427215576, {'accuracy': 0.9447, 'data_size': 10000}, 478.2941896500124)
INFO flwr 2024-04-06 13:57:34,959 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 13:57:34,959 | server.py:153 | FL finished in 478.2949404650135
INFO flwr 2024-04-06 13:57:34,960 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 13:57:34,960 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 13:57:34,960 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 13:57:34,960 | app.py:229 | app_fit: losses_centralized [(0, 2.302830219268799), (1, 2.285064697265625), (2, 2.179049253463745), (3, 1.8434040546417236), (4, 1.6903620958328247), (5, 1.5778692960739136), (6, 1.5608795881271362), (7, 1.5290943384170532), (8, 1.5493804216384888), (9, 1.5235508680343628), (10, 1.5171382427215576)]
INFO flwr 2024-04-06 13:57:34,960 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0618), (1, 0.2167), (2, 0.3024), (3, 0.6556), (4, 0.7788), (5, 0.8935), (6, 0.9102), (7, 0.9356), (8, 0.9127), (9, 0.9406), (10, 0.9447)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9447
wandb:     loss 1.51714
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_134910-u5aesr9l
wandb: Find logs at: ./wandb/offline-run-20240406_134910-u5aesr9l/logs
INFO flwr 2024-04-06 13:57:38,566 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:05:03,433 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1111294)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1111294)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 14:05:09,494	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:05:09,805	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:05:10,198	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_626ec1d142b7325c.zip' (9.49MiB) to Ray cluster...
2024-04-06 14:05:10,219	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_626ec1d142b7325c.zip'.
INFO flwr 2024-04-06 14:05:20,903 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 63602758041.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 138406435431.0}
INFO flwr 2024-04-06 14:05:20,904 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:05:20,904 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:05:20,925 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:05:20,926 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:05:20,926 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:05:20,926 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1123320)[0m 2024-04-06 14:05:26.374171: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1123320)[0m 2024-04-06 14:05:26.474718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1123320)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1123320)[0m 2024-04-06 14:05:28.693094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 14:05:29,081 | server.py:94 | initial parameters (loss, other metrics): 2.302703619003296, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-06 14:05:29,082 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:05:29,083 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1123324)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1123324)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1123317)[0m 2024-04-06 14:05:27.590023: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1123317)[0m 2024-04-06 14:05:27.688120: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1123317)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1123317)[0m 2024-04-06 14:05:29.900670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1123320)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1123320)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 14:05:45,938 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:05:49,270 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:05:53,414 | server.py:125 | fit progress: (1, 2.302473545074463, {'accuracy': 0.123, 'data_size': 10000}, 24.331442064023577)
INFO flwr 2024-04-06 14:05:53,414 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:05:53,415 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:06:03,646 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:06:17,436 | server.py:125 | fit progress: (2, 2.3021819591522217, {'accuracy': 0.0975, 'data_size': 10000}, 48.35331194702303)
INFO flwr 2024-04-06 14:06:17,436 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:06:17,436 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:06:26,343 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:06:47,671 | server.py:125 | fit progress: (3, 2.3019583225250244, {'accuracy': 0.0976, 'data_size': 10000}, 78.58842589301639)
INFO flwr 2024-04-06 14:06:47,671 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:06:47,671 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:06:57,280 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:07:23,500 | server.py:125 | fit progress: (4, 2.30171799659729, {'accuracy': 0.1152, 'data_size': 10000}, 114.41798416202073)
INFO flwr 2024-04-06 14:07:23,501 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:07:23,501 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:07:33,117 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:08:05,823 | server.py:125 | fit progress: (5, 2.3014919757843018, {'accuracy': 0.1475, 'data_size': 10000}, 156.7410680410103)
INFO flwr 2024-04-06 14:08:05,824 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:08:05,824 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:08:16,652 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:08:54,544 | server.py:125 | fit progress: (6, 2.3012301921844482, {'accuracy': 0.1908, 'data_size': 10000}, 205.46207145100925)
INFO flwr 2024-04-06 14:08:54,545 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:08:54,545 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:09:05,219 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:10:15,664 | server.py:125 | fit progress: (7, 2.300966501235962, {'accuracy': 0.1797, 'data_size': 10000}, 286.58165733501664)
INFO flwr 2024-04-06 14:10:15,664 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:10:15,665 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:10:26,658 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:11:33,908 | server.py:125 | fit progress: (8, 2.3005857467651367, {'accuracy': 0.0974, 'data_size': 10000}, 364.825303894002)
INFO flwr 2024-04-06 14:11:33,909 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:11:33,909 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:11:44,063 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:12:58,845 | server.py:125 | fit progress: (9, 2.3002943992614746, {'accuracy': 0.0974, 'data_size': 10000}, 449.7625634150172)
INFO flwr 2024-04-06 14:12:58,846 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:12:58,846 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:13:09,090 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:14:31,047 | server.py:125 | fit progress: (10, 2.299955129623413, {'accuracy': 0.0974, 'data_size': 10000}, 541.9649780089967)
INFO flwr 2024-04-06 14:14:31,048 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:14:31,048 | server.py:153 | FL finished in 541.9658297490096
INFO flwr 2024-04-06 14:14:31,052 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:14:31,053 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:14:31,053 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:14:31,053 | app.py:229 | app_fit: losses_centralized [(0, 2.302703619003296), (1, 2.302473545074463), (2, 2.3021819591522217), (3, 2.3019583225250244), (4, 2.30171799659729), (5, 2.3014919757843018), (6, 2.3012301921844482), (7, 2.300966501235962), (8, 2.3005857467651367), (9, 2.3002943992614746), (10, 2.299955129623413)]
INFO flwr 2024-04-06 14:14:31,053 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.123), (2, 0.0975), (3, 0.0976), (4, 0.1152), (5, 0.1475), (6, 0.1908), (7, 0.1797), (8, 0.0974), (9, 0.0974), (10, 0.0974)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0974
wandb:     loss 2.29996
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_140503-ji8nwcxa
wandb: Find logs at: ./wandb/offline-run-20240406_140503-ji8nwcxa/logs
INFO flwr 2024-04-06 14:14:34,646 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:21:59,561 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1123313)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1123313)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 14:22:05,717	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:22:06,158	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:22:06,609	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_67753dda324b5b4c.zip' (9.52MiB) to Ray cluster...
2024-04-06 14:22:06,646	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_67753dda324b5b4c.zip'.
INFO flwr 2024-04-06 14:22:17,406 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 137960416666.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63411607142.0}
INFO flwr 2024-04-06 14:22:17,406 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:22:17,406 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:22:17,423 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:22:17,424 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:22:17,424 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:22:17,424 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1131464)[0m 2024-04-06 14:22:23.387419: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1131464)[0m 2024-04-06 14:22:23.491169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1131464)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 14:22:24,745 | server.py:94 | initial parameters (loss, other metrics): 2.302366256713867, {'accuracy': 0.0977, 'data_size': 10000}
INFO flwr 2024-04-06 14:22:24,745 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:22:24,746 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1131460)[0m 2024-04-06 14:22:25.476763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1131464)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1131464)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1131462)[0m 2024-04-06 14:22:23.711026: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1131462)[0m 2024-04-06 14:22:23.812718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1131462)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1131453)[0m 2024-04-06 14:22:25.835286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 14:22:45,732 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:22:49,067 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:22:53,337 | server.py:125 | fit progress: (1, 2.202712297439575, {'accuracy': 0.2811, 'data_size': 10000}, 28.59102638400509)
INFO flwr 2024-04-06 14:22:53,337 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:22:53,337 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:04,065 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:23:18,031 | server.py:125 | fit progress: (2, 1.8426510095596313, {'accuracy': 0.6426, 'data_size': 10000}, 53.28566527500516)
INFO flwr 2024-04-06 14:23:18,032 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:18,032 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:28,290 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:23:48,542 | server.py:125 | fit progress: (3, 1.6585490703582764, {'accuracy': 0.8193, 'data_size': 10000}, 83.79656174001866)
INFO flwr 2024-04-06 14:23:48,543 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:23:48,543 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:23:58,165 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:24:24,646 | server.py:125 | fit progress: (4, 1.5760835409164429, {'accuracy': 0.9016, 'data_size': 10000}, 119.90095585002564)
INFO flwr 2024-04-06 14:24:24,647 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:24:24,647 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:24:33,788 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:25:07,118 | server.py:125 | fit progress: (5, 1.5465422868728638, {'accuracy': 0.9238, 'data_size': 10000}, 162.37251753700548)
INFO flwr 2024-04-06 14:25:07,118 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:25:07,119 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:25:16,778 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:25:55,613 | server.py:125 | fit progress: (6, 1.535046935081482, {'accuracy': 0.9333, 'data_size': 10000}, 210.86751050301245)
INFO flwr 2024-04-06 14:25:55,613 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:25:55,614 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:26:05,201 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:26:48,188 | server.py:125 | fit progress: (7, 1.527038335800171, {'accuracy': 0.9396, 'data_size': 10000}, 263.4429132330115)
INFO flwr 2024-04-06 14:26:48,189 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:26:48,189 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:26:58,099 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:27:59,444 | server.py:125 | fit progress: (8, 1.5148053169250488, {'accuracy': 0.9504, 'data_size': 10000}, 334.6988774820056)
INFO flwr 2024-04-06 14:27:59,445 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:27:59,445 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:28:09,537 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:29:07,382 | server.py:125 | fit progress: (9, 1.514756679534912, {'accuracy': 0.9514, 'data_size': 10000}, 402.63649771999917)
INFO flwr 2024-04-06 14:29:07,382 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:29:07,383 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:29:17,046 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:30:31,592 | server.py:125 | fit progress: (10, 1.5069016218185425, {'accuracy': 0.9567, 'data_size': 10000}, 486.846589822002)
INFO flwr 2024-04-06 14:30:31,593 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:30:31,593 | server.py:153 | FL finished in 486.8473848570138
INFO flwr 2024-04-06 14:30:31,593 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:30:31,593 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:30:31,593 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:30:31,593 | app.py:229 | app_fit: losses_centralized [(0, 2.302366256713867), (1, 2.202712297439575), (2, 1.8426510095596313), (3, 1.6585490703582764), (4, 1.5760835409164429), (5, 1.5465422868728638), (6, 1.535046935081482), (7, 1.527038335800171), (8, 1.5148053169250488), (9, 1.514756679534912), (10, 1.5069016218185425)]
INFO flwr 2024-04-06 14:30:31,594 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0977), (1, 0.2811), (2, 0.6426), (3, 0.8193), (4, 0.9016), (5, 0.9238), (6, 0.9333), (7, 0.9396), (8, 0.9504), (9, 0.9514), (10, 0.9567)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9567
wandb:     loss 1.5069
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_142159-opspngbr
wandb: Find logs at: ./wandb/offline-run-20240406_142159-opspngbr/logs
INFO flwr 2024-04-06 14:30:35,190 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:37:59,800 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1131453)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1131453)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 14:38:04,693	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:38:05,015	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:38:05,355	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0a6af42d5c6cfcff.zip' (9.56MiB) to Ray cluster...
2024-04-06 14:38:05,381	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0a6af42d5c6cfcff.zip'.
INFO flwr 2024-04-06 14:38:16,117 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 63442426675.0, 'node:__internal_head__': 1.0, 'memory': 138032328909.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 14:38:16,117 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:38:16,117 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:38:16,141 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:38:16,142 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:38:16,143 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:38:16,143 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1143506)[0m 2024-04-06 14:38:22.089214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1143506)[0m 2024-04-06 14:38:22.182680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1143506)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 14:38:24,041 | server.py:94 | initial parameters (loss, other metrics): 2.3027191162109375, {'accuracy': 0.103, 'data_size': 10000}
INFO flwr 2024-04-06 14:38:24,041 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:38:24,042 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1143507)[0m 2024-04-06 14:38:24.245367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1143512)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1143512)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1143512)[0m 2024-04-06 14:38:22.269685: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1143509)[0m 2024-04-06 14:38:22.397176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1143509)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1143512)[0m 2024-04-06 14:38:24.291820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1143506)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1143506)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 14:38:40,027 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:38:43,504 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:38:47,750 | server.py:125 | fit progress: (1, 2.2121710777282715, {'accuracy': 0.3854, 'data_size': 10000}, 23.70875602099113)
INFO flwr 2024-04-06 14:38:47,751 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:38:47,751 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:38:58,558 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:39:12,270 | server.py:125 | fit progress: (2, 1.6977509260177612, {'accuracy': 0.804, 'data_size': 10000}, 48.228366950002965)
INFO flwr 2024-04-06 14:39:12,270 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:39:12,271 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:39:21,935 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:39:42,211 | server.py:125 | fit progress: (3, 1.6162152290344238, {'accuracy': 0.849, 'data_size': 10000}, 78.16946243701386)
INFO flwr 2024-04-06 14:39:42,211 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:39:42,211 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:39:51,759 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:40:18,414 | server.py:125 | fit progress: (4, 1.5142848491668701, {'accuracy': 0.9507, 'data_size': 10000}, 114.37304356100503)
INFO flwr 2024-04-06 14:40:18,415 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:40:18,415 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:40:28,475 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:41:01,133 | server.py:125 | fit progress: (5, 1.5137529373168945, {'accuracy': 0.9509, 'data_size': 10000}, 157.09191685300902)
INFO flwr 2024-04-06 14:41:01,134 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:41:01,134 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:41:10,840 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:41:47,926 | server.py:125 | fit progress: (6, 1.5040266513824463, {'accuracy': 0.9589, 'data_size': 10000}, 203.88419782201527)
INFO flwr 2024-04-06 14:41:47,926 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:41:47,927 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:41:58,084 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:43:01,796 | server.py:125 | fit progress: (7, 1.504099726676941, {'accuracy': 0.9581, 'data_size': 10000}, 277.7548002419935)
INFO flwr 2024-04-06 14:43:01,797 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:43:01,797 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:43:12,236 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 14:44:17,615 | server.py:125 | fit progress: (8, 1.4971699714660645, {'accuracy': 0.9665, 'data_size': 10000}, 353.5734940470138)
INFO flwr 2024-04-06 14:44:17,615 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 14:44:17,616 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:44:27,535 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 14:45:28,332 | server.py:125 | fit progress: (9, 1.4976657629013062, {'accuracy': 0.9659, 'data_size': 10000}, 424.2908145709953)
INFO flwr 2024-04-06 14:45:28,333 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 14:45:28,333 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:45:38,087 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 14:46:46,424 | server.py:125 | fit progress: (10, 1.4932595491409302, {'accuracy': 0.9689, 'data_size': 10000}, 502.3821538889897)
INFO flwr 2024-04-06 14:46:46,424 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 14:46:46,424 | server.py:153 | FL finished in 502.3825886620034
INFO flwr 2024-04-06 14:46:46,424 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 14:46:46,424 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 14:46:46,424 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 14:46:46,425 | app.py:229 | app_fit: losses_centralized [(0, 2.3027191162109375), (1, 2.2121710777282715), (2, 1.6977509260177612), (3, 1.6162152290344238), (4, 1.5142848491668701), (5, 1.5137529373168945), (6, 1.5040266513824463), (7, 1.504099726676941), (8, 1.4971699714660645), (9, 1.4976657629013062), (10, 1.4932595491409302)]
INFO flwr 2024-04-06 14:46:46,425 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.103), (1, 0.3854), (2, 0.804), (3, 0.849), (4, 0.9507), (5, 0.9509), (6, 0.9589), (7, 0.9581), (8, 0.9665), (9, 0.9659), (10, 0.9689)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9689
wandb:     loss 1.49326
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_143759-lluw5fcg
wandb: Find logs at: ./wandb/offline-run-20240406_143759-lluw5fcg/logs
INFO flwr 2024-04-06 14:46:49,915 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 14:54:15,100 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1143504)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1143504)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 14:54:19,761	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 14:54:20,094	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 14:54:20,505	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8c8cd7051ca79869.zip' (9.59MiB) to Ray cluster...
2024-04-06 14:54:20,528	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8c8cd7051ca79869.zip'.
INFO flwr 2024-04-06 14:54:31,494 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 138335363277.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63572298547.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 14:54:31,495 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 14:54:31,495 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 14:54:31,512 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 14:54:31,513 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 14:54:31,513 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 14:54:31,513 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1154677)[0m 2024-04-06 14:54:37.462272: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1154677)[0m 2024-04-06 14:54:37.578967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1154677)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 14:54:38,609 | server.py:94 | initial parameters (loss, other metrics): 2.302485227584839, {'accuracy': 0.0884, 'data_size': 10000}
INFO flwr 2024-04-06 14:54:38,609 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 14:54:38,609 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1154677)[0m 2024-04-06 14:54:39.529132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1154752)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1154752)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1154679)[0m 2024-04-06 14:54:38.002642: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154679)[0m 2024-04-06 14:54:38.094226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154679)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1154679)[0m 2024-04-06 14:54:40.357754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1154677)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1154677)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 14:54:56,460 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 14:54:59,981 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 14:55:04,206 | server.py:125 | fit progress: (1, 2.1240477561950684, {'accuracy': 0.4395, 'data_size': 10000}, 25.596628588013118)
INFO flwr 2024-04-06 14:55:04,206 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:04,206 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:14,458 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:28,337 | server.py:125 | fit progress: (2, 1.6562916040420532, {'accuracy': 0.8146, 'data_size': 10000}, 49.72761809200165)
INFO flwr 2024-04-06 14:55:28,337 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:28,337 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:55:38,057 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 14:55:58,137 | server.py:125 | fit progress: (3, 1.5296857357025146, {'accuracy': 0.9388, 'data_size': 10000}, 79.52814365699305)
INFO flwr 2024-04-06 14:55:58,138 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 14:55:58,138 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:56:07,429 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 14:56:33,589 | server.py:125 | fit progress: (4, 1.5129127502441406, {'accuracy': 0.9515, 'data_size': 10000}, 114.97979969100561)
INFO flwr 2024-04-06 14:56:33,589 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 14:56:33,590 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:56:43,225 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 14:57:15,868 | server.py:125 | fit progress: (5, 1.5053640604019165, {'accuracy': 0.958, 'data_size': 10000}, 157.25878294801805)
INFO flwr 2024-04-06 14:57:15,868 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 14:57:15,869 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:57:25,866 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 14:58:03,220 | server.py:125 | fit progress: (6, 1.5029758214950562, {'accuracy': 0.9596, 'data_size': 10000}, 204.61101488300483)
INFO flwr 2024-04-06 14:58:03,221 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 14:58:03,221 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:58:13,218 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 14:59:06,707 | server.py:125 | fit progress: (7, 1.4976354837417603, {'accuracy': 0.9634, 'data_size': 10000}, 268.09747076101485)
INFO flwr 2024-04-06 14:59:06,707 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 14:59:06,707 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 14:59:16,220 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:00:04,637 | server.py:125 | fit progress: (8, 1.4936631917953491, {'accuracy': 0.9683, 'data_size': 10000}, 326.02820502000395)
INFO flwr 2024-04-06 15:00:04,638 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:00:04,638 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:00:14,131 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:01:19,463 | server.py:125 | fit progress: (9, 1.491150975227356, {'accuracy': 0.9705, 'data_size': 10000}, 400.85412373699364)
INFO flwr 2024-04-06 15:01:19,464 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:01:19,464 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:01:29,388 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:02:48,836 | server.py:125 | fit progress: (10, 1.488067865371704, {'accuracy': 0.9731, 'data_size': 10000}, 490.2265780529997)
INFO flwr 2024-04-06 15:02:48,836 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:02:48,836 | server.py:153 | FL finished in 490.2270959750167
INFO flwr 2024-04-06 15:02:48,837 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:02:48,837 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:02:48,837 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:02:48,837 | app.py:229 | app_fit: losses_centralized [(0, 2.302485227584839), (1, 2.1240477561950684), (2, 1.6562916040420532), (3, 1.5296857357025146), (4, 1.5129127502441406), (5, 1.5053640604019165), (6, 1.5029758214950562), (7, 1.4976354837417603), (8, 1.4936631917953491), (9, 1.491150975227356), (10, 1.488067865371704)]
INFO flwr 2024-04-06 15:02:48,837 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0884), (1, 0.4395), (2, 0.8146), (3, 0.9388), (4, 0.9515), (5, 0.958), (6, 0.9596), (7, 0.9634), (8, 0.9683), (9, 0.9705), (10, 0.9731)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9731
wandb:     loss 1.48807
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_145414-s5whb6e7
wandb: Find logs at: ./wandb/offline-run-20240406_145414-s5whb6e7/logs
INFO flwr 2024-04-06 15:02:52,389 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:10:16,850 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1154679)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1154679)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 15:10:21,784	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:10:22,096	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:10:22,436	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e6b6b734d7162832.zip' (9.62MiB) to Ray cluster...
2024-04-06 15:10:22,460	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e6b6b734d7162832.zip'.
INFO flwr 2024-04-06 15:10:33,301 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 137636228096.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63272669184.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 15:10:33,301 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:10:33,301 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:10:33,316 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:10:33,317 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:10:33,318 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:10:33,318 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1163163)[0m 2024-04-06 15:10:38.594851: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1163163)[0m 2024-04-06 15:10:38.695797: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1163163)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 15:10:40,377 | server.py:94 | initial parameters (loss, other metrics): 2.302417516708374, {'accuracy': 0.0988, 'data_size': 10000}
INFO flwr 2024-04-06 15:10:40,377 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:10:40,378 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1163163)[0m 2024-04-06 15:10:41.162790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1163164)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1163164)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1163159)[0m 2024-04-06 15:10:39.815307: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163159)[0m 2024-04-06 15:10:39.920244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163159)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1163164)[0m 2024-04-06 15:10:41.962584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1163156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1163156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:10:56,830 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:11:00,271 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:11:04,285 | server.py:125 | fit progress: (1, 1.9628936052322388, {'accuracy': 0.6099, 'data_size': 10000}, 23.907095649978146)
INFO flwr 2024-04-06 15:11:04,285 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:11:04,285 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:11:14,801 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:11:27,892 | server.py:125 | fit progress: (2, 1.54449462890625, {'accuracy': 0.9331, 'data_size': 10000}, 47.51378579897573)
INFO flwr 2024-04-06 15:11:27,892 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:11:27,892 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:11:38,071 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:11:57,874 | server.py:125 | fit progress: (3, 1.5355607271194458, {'accuracy': 0.9306, 'data_size': 10000}, 77.49651995499153)
INFO flwr 2024-04-06 15:11:57,876 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:11:57,876 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:12:08,267 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:12:34,501 | server.py:125 | fit progress: (4, 1.5097583532333374, {'accuracy': 0.954, 'data_size': 10000}, 114.12308002598002)
INFO flwr 2024-04-06 15:12:34,502 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:12:34,502 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:12:44,264 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:13:16,687 | server.py:125 | fit progress: (5, 1.4997950792312622, {'accuracy': 0.9622, 'data_size': 10000}, 156.3092502130021)
INFO flwr 2024-04-06 15:13:16,687 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:13:16,688 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:13:27,607 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:14:12,267 | server.py:125 | fit progress: (6, 1.494473934173584, {'accuracy': 0.9668, 'data_size': 10000}, 211.88915765998536)
INFO flwr 2024-04-06 15:14:12,267 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:14:12,267 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:14:22,992 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:15:23,573 | server.py:125 | fit progress: (7, 1.4933481216430664, {'accuracy': 0.9684, 'data_size': 10000}, 283.1954381989781)
INFO flwr 2024-04-06 15:15:23,573 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:15:23,574 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:15:33,644 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:16:29,633 | server.py:125 | fit progress: (8, 1.4895955324172974, {'accuracy': 0.9721, 'data_size': 10000}, 349.2556183190027)
INFO flwr 2024-04-06 15:16:29,634 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:16:29,634 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:16:40,436 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:17:43,414 | server.py:125 | fit progress: (9, 1.4888889789581299, {'accuracy': 0.9726, 'data_size': 10000}, 423.0361100709997)
INFO flwr 2024-04-06 15:17:43,414 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:17:43,414 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:17:54,043 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:19:01,464 | server.py:125 | fit progress: (10, 1.4880890846252441, {'accuracy': 0.9742, 'data_size': 10000}, 501.08636096699047)
INFO flwr 2024-04-06 15:19:01,464 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:19:01,465 | server.py:153 | FL finished in 501.08689871197566
INFO flwr 2024-04-06 15:19:01,468 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:19:01,468 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:19:01,468 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:19:01,468 | app.py:229 | app_fit: losses_centralized [(0, 2.302417516708374), (1, 1.9628936052322388), (2, 1.54449462890625), (3, 1.5355607271194458), (4, 1.5097583532333374), (5, 1.4997950792312622), (6, 1.494473934173584), (7, 1.4933481216430664), (8, 1.4895955324172974), (9, 1.4888889789581299), (10, 1.4880890846252441)]
INFO flwr 2024-04-06 15:19:01,468 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0988), (1, 0.6099), (2, 0.9331), (3, 0.9306), (4, 0.954), (5, 0.9622), (6, 0.9668), (7, 0.9684), (8, 0.9721), (9, 0.9726), (10, 0.9742)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9742
wandb:     loss 1.48809
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_151016-fm540058
wandb: Find logs at: ./wandb/offline-run-20240406_151016-fm540058/logs
INFO flwr 2024-04-06 15:19:05,073 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:26:25,404 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 15:26:30,397	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:26:30,831	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:26:31,260	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_473789a96e42683e.zip' (9.65MiB) to Ray cluster...
2024-04-06 15:26:31,283	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_473789a96e42683e.zip'.
INFO flwr 2024-04-06 15:26:42,147 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 63220788019.0, 'node:10.20.240.18': 1.0, 'memory': 137515172045.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 15:26:42,147 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:26:42,147 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:26:42,163 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:26:42,164 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:26:42,165 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:26:42,165 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1174346)[0m 2024-04-06 15:26:48.133962: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1174347)[0m 2024-04-06 15:26:48.231248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1174347)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 15:26:50,197 | server.py:94 | initial parameters (loss, other metrics): 2.3024260997772217, {'accuracy': 0.1262, 'data_size': 10000}
INFO flwr 2024-04-06 15:26:50,197 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:26:50,198 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1174346)[0m 2024-04-06 15:26:50.273284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1174354)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1174354)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1174345)[0m 2024-04-06 15:26:48.466460: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1174345)[0m 2024-04-06 15:26:48.562666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1174345)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1174343)[0m 2024-04-06 15:26:50.623781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1174346)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1174346)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 15:27:07,750 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:27:11,129 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:27:15,118 | server.py:125 | fit progress: (1, 1.9990507364273071, {'accuracy': 0.5427, 'data_size': 10000}, 24.920761647023028)
INFO flwr 2024-04-06 15:27:15,118 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:27:15,119 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:27:25,230 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:27:39,292 | server.py:125 | fit progress: (2, 1.6574972867965698, {'accuracy': 0.8116, 'data_size': 10000}, 49.09472640702734)
INFO flwr 2024-04-06 15:27:39,292 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:27:39,293 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:27:48,140 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:28:09,699 | server.py:125 | fit progress: (3, 1.5192323923110962, {'accuracy': 0.9469, 'data_size': 10000}, 79.50130586701562)
INFO flwr 2024-04-06 15:28:09,699 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:28:09,699 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:28:18,795 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:28:46,595 | server.py:125 | fit progress: (4, 1.5032857656478882, {'accuracy': 0.9596, 'data_size': 10000}, 116.3977646890271)
INFO flwr 2024-04-06 15:28:46,595 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:28:46,596 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:28:55,488 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:29:29,155 | server.py:125 | fit progress: (5, 1.4960384368896484, {'accuracy': 0.9658, 'data_size': 10000}, 158.95796612100094)
INFO flwr 2024-04-06 15:29:29,156 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:29:29,156 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:29:38,502 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:30:19,374 | server.py:125 | fit progress: (6, 1.4945710897445679, {'accuracy': 0.9669, 'data_size': 10000}, 209.17700435902225)
INFO flwr 2024-04-06 15:30:19,375 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:30:19,375 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:30:28,651 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:31:24,640 | server.py:125 | fit progress: (7, 1.5100704431533813, {'accuracy': 0.9516, 'data_size': 10000}, 274.4419696590048)
INFO flwr 2024-04-06 15:31:24,641 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:31:24,641 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:31:35,534 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:32:28,924 | server.py:125 | fit progress: (8, 1.4889476299285889, {'accuracy': 0.9727, 'data_size': 10000}, 338.7267159190087)
INFO flwr 2024-04-06 15:32:28,925 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:32:28,925 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:32:38,552 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:33:47,723 | server.py:125 | fit progress: (9, 1.487633228302002, {'accuracy': 0.9742, 'data_size': 10000}, 417.52604944401537)
INFO flwr 2024-04-06 15:33:47,724 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:33:47,724 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:33:56,978 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:35:33,802 | server.py:125 | fit progress: (10, 1.4848098754882812, {'accuracy': 0.977, 'data_size': 10000}, 523.6044571310049)
INFO flwr 2024-04-06 15:35:33,803 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:35:33,803 | server.py:153 | FL finished in 523.6056862120167
INFO flwr 2024-04-06 15:35:33,804 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:35:33,804 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:35:33,804 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:35:33,804 | app.py:229 | app_fit: losses_centralized [(0, 2.3024260997772217), (1, 1.9990507364273071), (2, 1.6574972867965698), (3, 1.5192323923110962), (4, 1.5032857656478882), (5, 1.4960384368896484), (6, 1.4945710897445679), (7, 1.5100704431533813), (8, 1.4889476299285889), (9, 1.487633228302002), (10, 1.4848098754882812)]
INFO flwr 2024-04-06 15:35:33,805 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1262), (1, 0.5427), (2, 0.8116), (3, 0.9469), (4, 0.9596), (5, 0.9658), (6, 0.9669), (7, 0.9516), (8, 0.9727), (9, 0.9742), (10, 0.977)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.977
wandb:     loss 1.48481
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_152625-znfknnid
wandb: Find logs at: ./wandb/offline-run-20240406_152625-znfknnid/logs
INFO flwr 2024-04-06 15:35:37,389 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:42:58,488 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1174343)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1174343)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 15:43:04,863	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:43:05,231	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:43:05,603	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7ca8082bd26a82e2.zip' (9.68MiB) to Ray cluster...
2024-04-06 15:43:05,642	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7ca8082bd26a82e2.zip'.
INFO flwr 2024-04-06 15:43:16,407 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 63161140838.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 137375995290.0}
INFO flwr 2024-04-06 15:43:16,408 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:43:16,408 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:43:16,425 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:43:16,426 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:43:16,427 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:43:16,427 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1182778)[0m 2024-04-06 15:43:22.369481: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1182778)[0m 2024-04-06 15:43:22.461117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1182778)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 15:43:23,630 | server.py:94 | initial parameters (loss, other metrics): 2.30242657661438, {'accuracy': 0.1092, 'data_size': 10000}
INFO flwr 2024-04-06 15:43:23,631 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:43:23,631 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1182778)[0m 2024-04-06 15:43:24.536255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1182784)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1182784)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1182773)[0m 2024-04-06 15:43:22.718403: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1182773)[0m 2024-04-06 15:43:22.824800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1182773)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1182773)[0m 2024-04-06 15:43:25.015671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1182773)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1182773)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 15:43:40,483 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:43:44,115 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:43:48,584 | server.py:125 | fit progress: (1, 2.094592571258545, {'accuracy': 0.4557, 'data_size': 10000}, 24.952496546000475)
INFO flwr 2024-04-06 15:43:48,584 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:43:48,584 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:43:58,982 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 15:44:13,143 | server.py:125 | fit progress: (2, 1.7196571826934814, {'accuracy': 0.7434, 'data_size': 10000}, 49.51169144699816)
INFO flwr 2024-04-06 15:44:13,143 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 15:44:13,144 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:44:21,940 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 15:44:42,512 | server.py:125 | fit progress: (3, 1.517259120941162, {'accuracy': 0.9487, 'data_size': 10000}, 78.8804534439987)
INFO flwr 2024-04-06 15:44:42,512 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 15:44:42,512 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:44:52,707 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 15:45:22,930 | server.py:125 | fit progress: (4, 1.503415584564209, {'accuracy': 0.9586, 'data_size': 10000}, 119.29920151599799)
INFO flwr 2024-04-06 15:45:22,931 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 15:45:22,931 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:45:32,221 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 15:46:06,917 | server.py:125 | fit progress: (5, 1.5015461444854736, {'accuracy': 0.9601, 'data_size': 10000}, 163.28573073999723)
INFO flwr 2024-04-06 15:46:06,917 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 15:46:06,918 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:46:16,247 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 15:47:05,106 | server.py:125 | fit progress: (6, 1.49998140335083, {'accuracy': 0.961, 'data_size': 10000}, 221.4744670150103)
INFO flwr 2024-04-06 15:47:05,106 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 15:47:05,106 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:47:14,797 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 15:48:02,909 | server.py:125 | fit progress: (7, 1.493985891342163, {'accuracy': 0.9671, 'data_size': 10000}, 279.27799591299845)
INFO flwr 2024-04-06 15:48:02,910 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 15:48:02,910 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:48:12,507 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 15:49:06,375 | server.py:125 | fit progress: (8, 1.494350552558899, {'accuracy': 0.9667, 'data_size': 10000}, 342.74414577899734)
INFO flwr 2024-04-06 15:49:06,376 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 15:49:06,376 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:49:16,983 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 15:50:12,360 | server.py:125 | fit progress: (9, 1.4908984899520874, {'accuracy': 0.9706, 'data_size': 10000}, 408.72838120299275)
INFO flwr 2024-04-06 15:50:12,360 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 15:50:12,360 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 15:50:21,646 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 15:51:33,659 | server.py:125 | fit progress: (10, 1.4893261194229126, {'accuracy': 0.9717, 'data_size': 10000}, 490.027742892009)
INFO flwr 2024-04-06 15:51:33,660 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 15:51:33,660 | server.py:153 | FL finished in 490.0284247529926
INFO flwr 2024-04-06 15:51:33,660 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 15:51:33,660 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 15:51:33,660 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 15:51:33,660 | app.py:229 | app_fit: losses_centralized [(0, 2.30242657661438), (1, 2.094592571258545), (2, 1.7196571826934814), (3, 1.517259120941162), (4, 1.503415584564209), (5, 1.5015461444854736), (6, 1.49998140335083), (7, 1.493985891342163), (8, 1.494350552558899), (9, 1.4908984899520874), (10, 1.4893261194229126)]
INFO flwr 2024-04-06 15:51:33,660 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1092), (1, 0.4557), (2, 0.7434), (3, 0.9487), (4, 0.9586), (5, 0.9601), (6, 0.961), (7, 0.9671), (8, 0.9667), (9, 0.9706), (10, 0.9717)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9717
wandb:     loss 1.48933
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_154258-dge3hlfy
wandb: Find logs at: ./wandb/offline-run-20240406_154258-dge3hlfy/logs
INFO flwr 2024-04-06 15:51:37,267 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 15:58:58,125 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 15:59:04,399	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 15:59:04,710	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 15:59:05,085	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ff0d7e2550cba983.zip' (9.71MiB) to Ray cluster...
2024-04-06 15:59:05,125	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ff0d7e2550cba983.zip'.
INFO flwr 2024-04-06 15:59:16,024 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 136879327437.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62948283187.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 15:59:16,024 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 15:59:16,025 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 15:59:16,041 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 15:59:16,042 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 15:59:16,043 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 15:59:16,043 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1194849)[0m 2024-04-06 15:59:21.752077: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1194849)[0m 2024-04-06 15:59:21.847858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1194849)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1194856)[0m 2024-04-06 15:59:23.819578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 15:59:25,199 | server.py:94 | initial parameters (loss, other metrics): 2.3024189472198486, {'accuracy': 0.0892, 'data_size': 10000}
INFO flwr 2024-04-06 15:59:25,200 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 15:59:25,201 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1194859)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1194859)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1194857)[0m 2024-04-06 15:59:22.056782: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1194857)[0m 2024-04-06 15:59:22.155932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1194857)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1194854)[0m 2024-04-06 15:59:24.538356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1194853)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1194853)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 15:59:44,874 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 15:59:48,451 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 15:59:52,408 | server.py:125 | fit progress: (1, 2.3019626140594482, {'accuracy': 0.0892, 'data_size': 10000}, 27.20734221598832)
INFO flwr 2024-04-06 15:59:52,408 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 15:59:52,408 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:00:04,782 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:00:19,380 | server.py:125 | fit progress: (2, 2.3012008666992188, {'accuracy': 0.098, 'data_size': 10000}, 54.17956173600396)
INFO flwr 2024-04-06 16:00:19,381 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:00:19,381 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:00:30,841 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:00:51,448 | server.py:125 | fit progress: (3, 2.300572395324707, {'accuracy': 0.1043, 'data_size': 10000}, 86.24786857800791)
INFO flwr 2024-04-06 16:00:51,449 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:00:51,449 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:01:03,029 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:01:29,759 | server.py:125 | fit progress: (4, 2.299847364425659, {'accuracy': 0.0892, 'data_size': 10000}, 124.55849717601086)
INFO flwr 2024-04-06 16:01:29,759 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:01:29,760 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:01:41,432 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:02:12,220 | server.py:125 | fit progress: (5, 2.298030376434326, {'accuracy': 0.1805, 'data_size': 10000}, 167.02012495399686)
INFO flwr 2024-04-06 16:02:12,221 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:02:12,221 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:02:23,904 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:03:08,958 | server.py:125 | fit progress: (6, 2.295781135559082, {'accuracy': 0.098, 'data_size': 10000}, 223.75729386598687)
INFO flwr 2024-04-06 16:03:08,958 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:03:08,958 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:03:20,711 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:04:08,635 | server.py:125 | fit progress: (7, 2.2927844524383545, {'accuracy': 0.1777, 'data_size': 10000}, 283.4341562119953)
INFO flwr 2024-04-06 16:04:08,635 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:04:08,636 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:04:20,238 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:05:20,057 | server.py:125 | fit progress: (8, 2.289112091064453, {'accuracy': 0.2441, 'data_size': 10000}, 354.85641230098554)
INFO flwr 2024-04-06 16:05:20,057 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:05:20,058 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:05:31,817 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:06:50,934 | server.py:125 | fit progress: (9, 2.282233715057373, {'accuracy': 0.1938, 'data_size': 10000}, 445.7338976530009)
INFO flwr 2024-04-06 16:06:50,935 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:06:50,935 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:07:01,990 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:08:12,364 | server.py:125 | fit progress: (10, 2.2679035663604736, {'accuracy': 0.2597, 'data_size': 10000}, 527.1633245690027)
INFO flwr 2024-04-06 16:08:12,364 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:08:12,364 | server.py:153 | FL finished in 527.1638276960002
INFO flwr 2024-04-06 16:08:12,367 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:08:12,368 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:08:12,368 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:08:12,368 | app.py:229 | app_fit: losses_centralized [(0, 2.3024189472198486), (1, 2.3019626140594482), (2, 2.3012008666992188), (3, 2.300572395324707), (4, 2.299847364425659), (5, 2.298030376434326), (6, 2.295781135559082), (7, 2.2927844524383545), (8, 2.289112091064453), (9, 2.282233715057373), (10, 2.2679035663604736)]
INFO flwr 2024-04-06 16:08:12,368 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0892), (1, 0.0892), (2, 0.098), (3, 0.1043), (4, 0.0892), (5, 0.1805), (6, 0.098), (7, 0.1777), (8, 0.2441), (9, 0.1938), (10, 0.2597)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2597
wandb:     loss 2.2679
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_155857-25b0pegf
wandb: Find logs at: ./wandb/offline-run-20240406_155857-25b0pegf/logs
INFO flwr 2024-04-06 16:08:16,032 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:15:40,986 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1194849)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1194849)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 16:15:49,391	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:15:50,024	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:15:50,484	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9447fb9a23473144.zip' (9.74MiB) to Ray cluster...
2024-04-06 16:15:50,513	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9447fb9a23473144.zip'.
INFO flwr 2024-04-06 16:16:01,637 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 65043315916.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 141767737140.0}
INFO flwr 2024-04-06 16:16:01,637 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:16:01,637 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:16:01,655 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:16:01,657 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:16:01,657 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:16:01,657 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1205942)[0m 2024-04-06 16:16:07.388144: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1205942)[0m 2024-04-06 16:16:07.490759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1205942)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 16:16:09,173 | server.py:94 | initial parameters (loss, other metrics): 2.302454710006714, {'accuracy': 0.117, 'data_size': 10000}
INFO flwr 2024-04-06 16:16:09,173 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:16:09,173 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1205942)[0m 2024-04-06 16:16:09.677923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1205957)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1205957)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1205952)[0m 2024-04-06 16:16:08.326243: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1205952)[0m 2024-04-06 16:16:08.416838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1205952)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1205952)[0m 2024-04-06 16:16:10.630948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1205946)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1205946)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 16:16:28,137 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:16:31,544 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:16:35,742 | server.py:125 | fit progress: (1, 2.1675186157226562, {'accuracy': 0.355, 'data_size': 10000}, 26.568478047993267)
INFO flwr 2024-04-06 16:16:35,742 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:16:35,743 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:16:47,767 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:01,571 | server.py:125 | fit progress: (2, 1.7106300592422485, {'accuracy': 0.7447, 'data_size': 10000}, 52.39802069598227)
INFO flwr 2024-04-06 16:17:01,572 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:01,572 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:13,619 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:17:35,091 | server.py:125 | fit progress: (3, 1.5430868864059448, {'accuracy': 0.9295, 'data_size': 10000}, 85.91803178799455)
INFO flwr 2024-04-06 16:17:35,092 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:17:35,092 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:17:45,806 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:18:14,199 | server.py:125 | fit progress: (4, 1.517728567123413, {'accuracy': 0.9501, 'data_size': 10000}, 125.0260526649945)
INFO flwr 2024-04-06 16:18:14,200 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:18:14,200 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:18:25,875 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:19:01,340 | server.py:125 | fit progress: (5, 1.5116260051727295, {'accuracy': 0.9529, 'data_size': 10000}, 172.16650194197427)
INFO flwr 2024-04-06 16:19:01,340 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:19:01,340 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:19:12,351 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:19:53,503 | server.py:125 | fit progress: (6, 1.507654070854187, {'accuracy': 0.9572, 'data_size': 10000}, 224.32950750199961)
INFO flwr 2024-04-06 16:19:53,503 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:19:53,504 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:20:05,874 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:20:59,447 | server.py:125 | fit progress: (7, 1.5046751499176025, {'accuracy': 0.9585, 'data_size': 10000}, 290.27381128197885)
INFO flwr 2024-04-06 16:20:59,447 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:20:59,448 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:21:10,136 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:22:12,562 | server.py:125 | fit progress: (8, 1.5026839971542358, {'accuracy': 0.9606, 'data_size': 10000}, 363.388348405977)
INFO flwr 2024-04-06 16:22:12,562 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:22:12,562 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:22:23,712 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:23:23,526 | server.py:125 | fit progress: (9, 1.4984382390975952, {'accuracy': 0.9648, 'data_size': 10000}, 434.3525124929729)
INFO flwr 2024-04-06 16:23:23,526 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:23:23,526 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:23:34,712 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:24:48,692 | server.py:125 | fit progress: (10, 1.4961087703704834, {'accuracy': 0.968, 'data_size': 10000}, 519.5190258829971)
INFO flwr 2024-04-06 16:24:48,693 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:24:48,693 | server.py:153 | FL finished in 519.519514818996
INFO flwr 2024-04-06 16:24:48,696 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:24:48,696 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:24:48,697 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:24:48,697 | app.py:229 | app_fit: losses_centralized [(0, 2.302454710006714), (1, 2.1675186157226562), (2, 1.7106300592422485), (3, 1.5430868864059448), (4, 1.517728567123413), (5, 1.5116260051727295), (6, 1.507654070854187), (7, 1.5046751499176025), (8, 1.5026839971542358), (9, 1.4984382390975952), (10, 1.4961087703704834)]
INFO flwr 2024-04-06 16:24:48,697 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.117), (1, 0.355), (2, 0.7447), (3, 0.9295), (4, 0.9501), (5, 0.9529), (6, 0.9572), (7, 0.9585), (8, 0.9606), (9, 0.9648), (10, 0.968)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.968
wandb:     loss 1.49611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_161540-vs2cxhlm
wandb: Find logs at: ./wandb/offline-run-20240406_161540-vs2cxhlm/logs
INFO flwr 2024-04-06 16:24:52,287 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:32:11,869 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1205942)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1205942)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 16:32:17,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:32:18,132	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:32:18,499	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4132384d68dfadae.zip' (9.78MiB) to Ray cluster...
2024-04-06 16:32:18,521	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4132384d68dfadae.zip'.
INFO flwr 2024-04-06 16:32:29,229 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 136715034010.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62877871718.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 16:32:29,229 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:32:29,229 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:32:29,247 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:32:29,248 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:32:29,248 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:32:29,248 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1214722)[0m 2024-04-06 16:32:34.783610: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1214722)[0m 2024-04-06 16:32:34.882111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1214722)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1214722)[0m 2024-04-06 16:32:37.205815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 16:32:37,321 | server.py:94 | initial parameters (loss, other metrics): 2.302400588989258, {'accuracy': 0.1824, 'data_size': 10000}
INFO flwr 2024-04-06 16:32:37,321 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:32:37,322 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1214726)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1214726)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1214723)[0m 2024-04-06 16:32:35.587497: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1214723)[0m 2024-04-06 16:32:35.682616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1214723)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1214723)[0m 2024-04-06 16:32:37.831280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1214721)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1214721)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 16:32:54,697 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:32:58,123 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:33:01,995 | server.py:125 | fit progress: (1, 2.047135591506958, {'accuracy': 0.6107, 'data_size': 10000}, 24.673549572995398)
INFO flwr 2024-04-06 16:33:01,995 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:33:01,996 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:33:13,011 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:33:26,811 | server.py:125 | fit progress: (2, 1.5841479301452637, {'accuracy': 0.8912, 'data_size': 10000}, 49.48976671198034)
INFO flwr 2024-04-06 16:33:26,812 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:33:26,812 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:33:37,826 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:33:58,665 | server.py:125 | fit progress: (3, 1.5187853574752808, {'accuracy': 0.9461, 'data_size': 10000}, 81.34360548699624)
INFO flwr 2024-04-06 16:33:58,665 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:33:58,666 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:34:09,568 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:34:37,014 | server.py:125 | fit progress: (4, 1.5172220468521118, {'accuracy': 0.9465, 'data_size': 10000}, 119.6925440839841)
INFO flwr 2024-04-06 16:34:37,014 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:34:37,014 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:34:47,873 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:35:26,692 | server.py:125 | fit progress: (5, 1.5091516971588135, {'accuracy': 0.955, 'data_size': 10000}, 169.37092709497665)
INFO flwr 2024-04-06 16:35:26,693 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:35:26,693 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:35:36,276 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:36:30,642 | server.py:125 | fit progress: (6, 1.4963734149932861, {'accuracy': 0.9659, 'data_size': 10000}, 233.32037702697562)
INFO flwr 2024-04-06 16:36:30,642 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:36:30,643 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:36:41,961 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:37:31,848 | server.py:125 | fit progress: (7, 1.4935357570648193, {'accuracy': 0.9687, 'data_size': 10000}, 294.5262284279743)
INFO flwr 2024-04-06 16:37:31,848 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:37:31,848 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:37:42,953 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:38:41,556 | server.py:125 | fit progress: (8, 1.4947842359542847, {'accuracy': 0.9673, 'data_size': 10000}, 364.2349055800005)
INFO flwr 2024-04-06 16:38:41,557 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:38:41,557 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:38:51,956 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:39:54,957 | server.py:125 | fit progress: (9, 1.4920305013656616, {'accuracy': 0.9699, 'data_size': 10000}, 437.6355635969958)
INFO flwr 2024-04-06 16:39:54,957 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:39:54,957 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:40:05,467 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:41:12,556 | server.py:125 | fit progress: (10, 1.488815188407898, {'accuracy': 0.9732, 'data_size': 10000}, 515.2343403389968)
INFO flwr 2024-04-06 16:41:12,556 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:41:12,556 | server.py:153 | FL finished in 515.2347440839803
INFO flwr 2024-04-06 16:41:12,559 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:41:12,559 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:41:12,559 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:41:12,559 | app.py:229 | app_fit: losses_centralized [(0, 2.302400588989258), (1, 2.047135591506958), (2, 1.5841479301452637), (3, 1.5187853574752808), (4, 1.5172220468521118), (5, 1.5091516971588135), (6, 1.4963734149932861), (7, 1.4935357570648193), (8, 1.4947842359542847), (9, 1.4920305013656616), (10, 1.488815188407898)]
INFO flwr 2024-04-06 16:41:12,559 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1824), (1, 0.6107), (2, 0.8912), (3, 0.9461), (4, 0.9465), (5, 0.955), (6, 0.9659), (7, 0.9687), (8, 0.9673), (9, 0.9699), (10, 0.9732)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9732
wandb:     loss 1.48882
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_163211-vq8ueukw
wandb: Find logs at: ./wandb/offline-run-20240406_163211-vq8ueukw/logs
INFO flwr 2024-04-06 16:41:16,160 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 16:48:36,362 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1214717)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1214717)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 16:48:40,841	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 16:48:41,234	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 16:48:41,615	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b62598ad1f016ddc.zip' (9.81MiB) to Ray cluster...
2024-04-06 16:48:41,638	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b62598ad1f016ddc.zip'.
INFO flwr 2024-04-06 16:48:52,354 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 62818905292.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 136577445684.0}
INFO flwr 2024-04-06 16:48:52,354 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 16:48:52,355 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 16:48:52,377 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 16:48:52,378 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 16:48:52,378 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 16:48:52,379 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1226034)[0m 2024-04-06 16:48:58.192278: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1226034)[0m 2024-04-06 16:48:58.292371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1226034)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1226032)[0m 2024-04-06 16:49:00.341120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 16:49:00,463 | server.py:94 | initial parameters (loss, other metrics): 2.302403450012207, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 16:49:00,464 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 16:49:00,465 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1226038)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1226038)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1226038)[0m 2024-04-06 16:48:58.592659: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1226038)[0m 2024-04-06 16:48:58.691633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1226038)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1226038)[0m 2024-04-06 16:49:00.897415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1226031)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1226031)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 16:49:18,846 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 16:49:22,161 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 16:49:26,128 | server.py:125 | fit progress: (1, 1.8491618633270264, {'accuracy': 0.8844, 'data_size': 10000}, 25.664040711999405)
INFO flwr 2024-04-06 16:49:26,129 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 16:49:26,129 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:49:37,208 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 16:49:52,630 | server.py:125 | fit progress: (2, 1.5330114364624023, {'accuracy': 0.9365, 'data_size': 10000}, 52.165416876989184)
INFO flwr 2024-04-06 16:49:52,630 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 16:49:52,630 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:50:04,076 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 16:50:25,223 | server.py:125 | fit progress: (3, 1.505856990814209, {'accuracy': 0.9584, 'data_size': 10000}, 84.7588195129938)
INFO flwr 2024-04-06 16:50:25,224 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 16:50:25,224 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:50:35,355 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 16:51:01,653 | server.py:125 | fit progress: (4, 1.4987554550170898, {'accuracy': 0.9632, 'data_size': 10000}, 121.18895157799125)
INFO flwr 2024-04-06 16:51:01,654 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 16:51:01,654 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:51:13,181 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 16:51:44,763 | server.py:125 | fit progress: (5, 1.4961336851119995, {'accuracy': 0.9663, 'data_size': 10000}, 164.29882659300347)
INFO flwr 2024-04-06 16:51:44,764 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 16:51:44,764 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:51:57,744 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 16:52:41,693 | server.py:125 | fit progress: (6, 1.4904546737670898, {'accuracy': 0.9718, 'data_size': 10000}, 221.22877290201723)
INFO flwr 2024-04-06 16:52:41,694 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 16:52:41,694 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:52:53,176 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 16:53:46,669 | server.py:125 | fit progress: (7, 1.4898948669433594, {'accuracy': 0.9718, 'data_size': 10000}, 286.2044321229914)
INFO flwr 2024-04-06 16:53:46,669 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 16:53:46,669 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:53:58,737 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 16:54:49,843 | server.py:125 | fit progress: (8, 1.4874248504638672, {'accuracy': 0.9748, 'data_size': 10000}, 349.37873459700495)
INFO flwr 2024-04-06 16:54:49,843 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 16:54:49,844 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:55:02,337 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 16:56:13,091 | server.py:125 | fit progress: (9, 1.4856189489364624, {'accuracy': 0.9764, 'data_size': 10000}, 432.62674623701605)
INFO flwr 2024-04-06 16:56:13,091 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 16:56:13,092 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 16:56:24,208 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 16:57:45,122 | server.py:125 | fit progress: (10, 1.4856829643249512, {'accuracy': 0.976, 'data_size': 10000}, 524.6574190150131)
INFO flwr 2024-04-06 16:57:45,122 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 16:57:45,122 | server.py:153 | FL finished in 524.6581260010134
INFO flwr 2024-04-06 16:57:45,123 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 16:57:45,123 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 16:57:45,123 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 16:57:45,123 | app.py:229 | app_fit: losses_centralized [(0, 2.302403450012207), (1, 1.8491618633270264), (2, 1.5330114364624023), (3, 1.505856990814209), (4, 1.4987554550170898), (5, 1.4961336851119995), (6, 1.4904546737670898), (7, 1.4898948669433594), (8, 1.4874248504638672), (9, 1.4856189489364624), (10, 1.4856829643249512)]
INFO flwr 2024-04-06 16:57:45,123 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.8844), (2, 0.9365), (3, 0.9584), (4, 0.9632), (5, 0.9663), (6, 0.9718), (7, 0.9718), (8, 0.9748), (9, 0.9764), (10, 0.976)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.976
wandb:     loss 1.48568
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_164836-v22ist8v
wandb: Find logs at: ./wandb/offline-run-20240406_164836-v22ist8v/logs
INFO flwr 2024-04-06 16:57:48,709 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:05:13,571 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1226029)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1226029)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 17:05:18,605	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:05:18,940	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:05:19,279	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_628abd7c37422a8e.zip' (9.84MiB) to Ray cluster...
2024-04-06 17:05:19,305	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_628abd7c37422a8e.zip'.
INFO flwr 2024-04-06 17:05:30,115 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 136285134644.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62693629132.0}
INFO flwr 2024-04-06 17:05:30,115 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:05:30,115 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:05:30,130 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:05:30,131 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:05:30,132 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:05:30,132 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1235009)[0m 2024-04-06 17:05:35.478144: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1235009)[0m 2024-04-06 17:05:35.577066: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1235009)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 17:05:37,391 | server.py:94 | initial parameters (loss, other metrics): 2.302872657775879, {'accuracy': 0.0933, 'data_size': 10000}
INFO flwr 2024-04-06 17:05:37,392 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:05:37,392 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1235009)[0m 2024-04-06 17:05:38.043557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1235013)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1235013)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1235016)[0m 2024-04-06 17:05:36.877080: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1235016)[0m 2024-04-06 17:05:36.967290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1235016)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1235016)[0m 2024-04-06 17:05:39.177857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1235011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1235011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-04-06 17:05:57,267 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:06:00,309 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:06:04,508 | server.py:125 | fit progress: (1, 1.9430944919586182, {'accuracy': 0.592, 'data_size': 10000}, 27.115987830999075)
INFO flwr 2024-04-06 17:06:04,508 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:06:04,508 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:06:16,953 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:06:31,180 | server.py:125 | fit progress: (2, 1.5369871854782104, {'accuracy': 0.9308, 'data_size': 10000}, 53.78818589801085)
INFO flwr 2024-04-06 17:06:31,181 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:06:31,181 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:06:42,772 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:07:03,858 | server.py:125 | fit progress: (3, 1.50985848903656, {'accuracy': 0.954, 'data_size': 10000}, 86.46614490600768)
INFO flwr 2024-04-06 17:07:03,858 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:07:03,859 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:07:15,379 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:07:48,921 | server.py:125 | fit progress: (4, 1.5000733137130737, {'accuracy': 0.9616, 'data_size': 10000}, 131.5295239900006)
INFO flwr 2024-04-06 17:07:48,922 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:07:48,922 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:08:01,040 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:08:48,667 | server.py:125 | fit progress: (5, 1.4956672191619873, {'accuracy': 0.9661, 'data_size': 10000}, 191.27475956900162)
INFO flwr 2024-04-06 17:08:48,667 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:08:48,667 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:09:00,521 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:09:56,772 | server.py:125 | fit progress: (6, 1.5087114572525024, {'accuracy': 0.9537, 'data_size': 10000}, 259.38044063400594)
INFO flwr 2024-04-06 17:09:56,773 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:09:56,773 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:10:08,430 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:11:06,896 | server.py:125 | fit progress: (7, 1.4903615713119507, {'accuracy': 0.9709, 'data_size': 10000}, 329.5045586900087)
INFO flwr 2024-04-06 17:11:06,897 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:11:06,897 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:11:18,351 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:12:20,266 | server.py:125 | fit progress: (8, 1.4884847402572632, {'accuracy': 0.9732, 'data_size': 10000}, 402.8745107240102)
INFO flwr 2024-04-06 17:12:20,267 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:12:20,267 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:12:31,729 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:13:47,361 | server.py:125 | fit progress: (9, 1.4857720136642456, {'accuracy': 0.9757, 'data_size': 10000}, 489.968798144022)
INFO flwr 2024-04-06 17:13:47,361 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:13:47,361 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:13:58,534 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:15:04,377 | server.py:125 | fit progress: (10, 1.4853272438049316, {'accuracy': 0.9761, 'data_size': 10000}, 566.9847177640186)
INFO flwr 2024-04-06 17:15:04,377 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:15:04,378 | server.py:153 | FL finished in 566.9856809550256
INFO flwr 2024-04-06 17:15:04,378 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:15:04,378 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:15:04,378 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:15:04,378 | app.py:229 | app_fit: losses_centralized [(0, 2.302872657775879), (1, 1.9430944919586182), (2, 1.5369871854782104), (3, 1.50985848903656), (4, 1.5000733137130737), (5, 1.4956672191619873), (6, 1.5087114572525024), (7, 1.4903615713119507), (8, 1.4884847402572632), (9, 1.4857720136642456), (10, 1.4853272438049316)]
INFO flwr 2024-04-06 17:15:04,378 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0933), (1, 0.592), (2, 0.9308), (3, 0.954), (4, 0.9616), (5, 0.9661), (6, 0.9537), (7, 0.9709), (8, 0.9732), (9, 0.9757), (10, 0.9761)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9761
wandb:     loss 1.48533
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_170513-yz1j0ipn
wandb: Find logs at: ./wandb/offline-run-20240406_170513-yz1j0ipn/logs
INFO flwr 2024-04-06 17:15:07,993 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:22:28,672 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1235006)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1235006)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
2024-04-06 17:22:34,852	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:22:35,216	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:22:35,606	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7a3fd7b599af32d0.zip' (9.87MiB) to Ray cluster...
2024-04-06 17:22:35,631	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7a3fd7b599af32d0.zip'.
INFO flwr 2024-04-06 17:22:46,454 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133251766068.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61393614028.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 17:22:46,454 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:22:46,454 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:22:46,468 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:22:46,469 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:22:46,470 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:22:46,470 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1246518)[0m 2024-04-06 17:22:52.103826: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1246518)[0m 2024-04-06 17:22:52.203843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1246518)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1246517)[0m 2024-04-06 17:22:54.404569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 17:22:54,772 | server.py:94 | initial parameters (loss, other metrics): 2.3028807640075684, {'accuracy': 0.0817, 'data_size': 10000}
INFO flwr 2024-04-06 17:22:54,773 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:22:54,773 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1246527)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1246527)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1246527)[0m 2024-04-06 17:22:52.727265: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1246527)[0m 2024-04-06 17:22:52.819659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1246527)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1246523)[0m 2024-04-06 17:22:54.774978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1246518)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1246518)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 17:23:12,729 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:23:16,026 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:23:19,929 | server.py:125 | fit progress: (1, 1.9218226671218872, {'accuracy': 0.7389, 'data_size': 10000}, 25.15664410602767)
INFO flwr 2024-04-06 17:23:19,930 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:23:19,930 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:23:30,850 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:23:44,021 | server.py:125 | fit progress: (2, 1.5208796262741089, {'accuracy': 0.944, 'data_size': 10000}, 49.24797399601084)
INFO flwr 2024-04-06 17:23:44,021 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:23:44,021 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:23:54,514 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:24:15,753 | server.py:125 | fit progress: (3, 1.503187656402588, {'accuracy': 0.9595, 'data_size': 10000}, 80.97979518401553)
INFO flwr 2024-04-06 17:24:15,753 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:24:15,753 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:24:26,077 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:24:53,263 | server.py:125 | fit progress: (4, 1.495255470275879, {'accuracy': 0.9669, 'data_size': 10000}, 118.49043147201883)
INFO flwr 2024-04-06 17:24:53,264 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:24:53,264 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:25:04,132 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:25:39,247 | server.py:125 | fit progress: (5, 1.4935753345489502, {'accuracy': 0.9676, 'data_size': 10000}, 164.47421563102398)
INFO flwr 2024-04-06 17:25:39,247 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:25:39,248 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:25:49,768 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:26:37,030 | server.py:125 | fit progress: (6, 1.4883636236190796, {'accuracy': 0.9737, 'data_size': 10000}, 222.2572796930035)
INFO flwr 2024-04-06 17:26:37,030 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:26:37,031 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:26:47,350 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:27:51,028 | server.py:125 | fit progress: (7, 1.4883151054382324, {'accuracy': 0.9727, 'data_size': 10000}, 296.25548625402735)
INFO flwr 2024-04-06 17:27:51,029 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:27:51,029 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:28:01,752 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:29:05,433 | server.py:125 | fit progress: (8, 1.483275055885315, {'accuracy': 0.9782, 'data_size': 10000}, 370.6605608210084)
INFO flwr 2024-04-06 17:29:05,434 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:29:05,434 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:29:17,409 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:30:27,688 | server.py:125 | fit progress: (9, 1.4822198152542114, {'accuracy': 0.9793, 'data_size': 10000}, 452.91566505501396)
INFO flwr 2024-04-06 17:30:27,689 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:30:27,689 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:30:37,985 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:31:51,189 | server.py:125 | fit progress: (10, 1.4827466011047363, {'accuracy': 0.9789, 'data_size': 10000}, 536.4161972290021)
INFO flwr 2024-04-06 17:31:51,190 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:31:51,190 | server.py:153 | FL finished in 536.417030096025
INFO flwr 2024-04-06 17:31:51,190 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:31:51,190 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:31:51,191 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:31:51,191 | app.py:229 | app_fit: losses_centralized [(0, 2.3028807640075684), (1, 1.9218226671218872), (2, 1.5208796262741089), (3, 1.503187656402588), (4, 1.495255470275879), (5, 1.4935753345489502), (6, 1.4883636236190796), (7, 1.4883151054382324), (8, 1.483275055885315), (9, 1.4822198152542114), (10, 1.4827466011047363)]
INFO flwr 2024-04-06 17:31:51,191 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0817), (1, 0.7389), (2, 0.944), (3, 0.9595), (4, 0.9669), (5, 0.9676), (6, 0.9737), (7, 0.9727), (8, 0.9782), (9, 0.9793), (10, 0.9789)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9789
wandb:     loss 1.48275
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_172228-osz3s4dz
wandb: Find logs at: ./wandb/offline-run-20240406_172228-osz3s4dz/logs
INFO flwr 2024-04-06 17:31:54,759 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:39:19,882 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1246517)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1246517)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 17:39:26,596	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:39:26,997	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:39:27,381	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_603665cafb6ed296.zip' (9.91MiB) to Ray cluster...
2024-04-06 17:39:27,409	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_603665cafb6ed296.zip'.
INFO flwr 2024-04-06 17:39:39,105 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 132650869760.0, 'object_store_memory': 61136087040.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 17:39:39,105 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:39:39,106 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:39:39,123 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:39:39,124 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:39:39,124 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:39:39,124 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1258026)[0m 2024-04-06 17:39:45.399990: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1258026)[0m 2024-04-06 17:39:45.509537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1258026)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 17:39:46,745 | server.py:94 | initial parameters (loss, other metrics): 2.3025195598602295, {'accuracy': 0.1176, 'data_size': 10000}
INFO flwr 2024-04-06 17:39:46,746 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:39:46,746 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1258026)[0m 2024-04-06 17:39:48.295707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1258029)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1258029)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1258030)[0m 2024-04-06 17:39:45.778220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1258030)[0m 2024-04-06 17:39:45.881503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1258030)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1258030)[0m 2024-04-06 17:39:48.470919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 17:40:06,831 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:40:10,429 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:40:14,819 | server.py:125 | fit progress: (1, 1.85886812210083, {'accuracy': 0.8082, 'data_size': 10000}, 28.072861930006184)
INFO flwr 2024-04-06 17:40:14,819 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:40:14,820 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:40:27,354 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:40:41,291 | server.py:125 | fit progress: (2, 1.5188292264938354, {'accuracy': 0.9442, 'data_size': 10000}, 54.54464137699688)
INFO flwr 2024-04-06 17:40:41,291 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:40:41,291 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:40:53,395 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:41:13,963 | server.py:125 | fit progress: (3, 1.4981492757797241, {'accuracy': 0.9625, 'data_size': 10000}, 87.21671350701945)
INFO flwr 2024-04-06 17:41:13,963 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:41:13,964 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:41:25,581 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:41:52,136 | server.py:125 | fit progress: (4, 1.495381474494934, {'accuracy': 0.9658, 'data_size': 10000}, 125.39007221200154)
INFO flwr 2024-04-06 17:41:52,136 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:41:52,137 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:42:03,779 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:42:37,334 | server.py:125 | fit progress: (5, 1.4945060014724731, {'accuracy': 0.9672, 'data_size': 10000}, 170.5883318260021)
INFO flwr 2024-04-06 17:42:37,335 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:42:37,335 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:42:49,167 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 17:43:34,954 | server.py:125 | fit progress: (6, 1.4893639087677002, {'accuracy': 0.9717, 'data_size': 10000}, 228.2083155559958)
INFO flwr 2024-04-06 17:43:34,955 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 17:43:34,955 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:43:46,230 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 17:44:39,645 | server.py:125 | fit progress: (7, 1.4874440431594849, {'accuracy': 0.9737, 'data_size': 10000}, 292.89876900200034)
INFO flwr 2024-04-06 17:44:39,645 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 17:44:39,645 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:44:50,812 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 17:45:50,531 | server.py:125 | fit progress: (8, 1.484284520149231, {'accuracy': 0.9772, 'data_size': 10000}, 363.78559092999785)
INFO flwr 2024-04-06 17:45:50,532 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 17:45:50,532 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:46:01,950 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 17:47:12,508 | server.py:125 | fit progress: (9, 1.483931541442871, {'accuracy': 0.977, 'data_size': 10000}, 445.76222253902233)
INFO flwr 2024-04-06 17:47:12,508 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 17:47:12,509 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:47:23,661 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 17:48:34,929 | server.py:125 | fit progress: (10, 1.4835162162780762, {'accuracy': 0.9781, 'data_size': 10000}, 528.183328818006)
INFO flwr 2024-04-06 17:48:34,930 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 17:48:34,930 | server.py:153 | FL finished in 528.1841796060035
INFO flwr 2024-04-06 17:48:34,930 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 17:48:34,931 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 17:48:34,931 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 17:48:34,931 | app.py:229 | app_fit: losses_centralized [(0, 2.3025195598602295), (1, 1.85886812210083), (2, 1.5188292264938354), (3, 1.4981492757797241), (4, 1.495381474494934), (5, 1.4945060014724731), (6, 1.4893639087677002), (7, 1.4874440431594849), (8, 1.484284520149231), (9, 1.483931541442871), (10, 1.4835162162780762)]
INFO flwr 2024-04-06 17:48:34,931 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1176), (1, 0.8082), (2, 0.9442), (3, 0.9625), (4, 0.9658), (5, 0.9672), (6, 0.9717), (7, 0.9737), (8, 0.9772), (9, 0.977), (10, 0.9781)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9781
wandb:     loss 1.48352
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_173919-t278pvds
wandb: Find logs at: ./wandb/offline-run-20240406_173919-t278pvds/logs
INFO flwr 2024-04-06 17:48:38,575 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 17:55:59,906 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1258024)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1258024)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 17:56:06,071	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 17:56:06,363	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 17:56:06,704	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7848248764d79888.zip' (9.94MiB) to Ray cluster...
2024-04-06 17:56:06,739	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7848248764d79888.zip'.
INFO flwr 2024-04-06 17:56:17,594 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 133234703360.0, 'object_store_memory': 61386301440.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 17:56:17,594 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 17:56:17,594 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 17:56:17,609 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 17:56:17,610 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 17:56:17,610 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 17:56:17,611 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1267736)[0m 2024-04-06 17:56:23.682190: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1267736)[0m 2024-04-06 17:56:23.779231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1267736)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 17:56:25,185 | server.py:94 | initial parameters (loss, other metrics): 2.3025729656219482, {'accuracy': 0.0766, 'data_size': 10000}
INFO flwr 2024-04-06 17:56:25,186 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 17:56:25,187 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1267736)[0m 2024-04-06 17:56:25.827348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1267741)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1267741)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1267733)[0m 2024-04-06 17:56:23.942641: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1267733)[0m 2024-04-06 17:56:24.060372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1267733)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1267733)[0m 2024-04-06 17:56:26.221166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1267736)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1267736)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 17:56:49,585 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 17:56:52,718 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 17:56:56,643 | server.py:125 | fit progress: (1, 2.3007309436798096, {'accuracy': 0.101, 'data_size': 10000}, 31.456502952991286)
INFO flwr 2024-04-06 17:56:56,643 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 17:56:56,643 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:57:13,272 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 17:57:26,488 | server.py:125 | fit progress: (2, 2.296722173690796, {'accuracy': 0.0974, 'data_size': 10000}, 61.301360301993554)
INFO flwr 2024-04-06 17:57:26,488 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 17:57:26,488 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:57:41,604 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 17:58:01,228 | server.py:125 | fit progress: (3, 2.2897355556488037, {'accuracy': 0.1745, 'data_size': 10000}, 96.0419787550054)
INFO flwr 2024-04-06 17:58:01,229 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:01,229 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:58:16,459 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 17:58:48,638 | server.py:125 | fit progress: (4, 2.2836480140686035, {'accuracy': 0.1061, 'data_size': 10000}, 143.45148954101023)
INFO flwr 2024-04-06 17:58:48,638 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 17:58:48,638 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:03,641 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 17:59:39,410 | server.py:125 | fit progress: (5, 2.2507259845733643, {'accuracy': 0.2315, 'data_size': 10000}, 194.22323050699197)
INFO flwr 2024-04-06 17:59:39,410 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 17:59:39,410 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 17:59:55,151 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:00:46,163 | server.py:125 | fit progress: (6, 2.186225414276123, {'accuracy': 0.2599, 'data_size': 10000}, 260.9768760880106)
INFO flwr 2024-04-06 18:00:46,164 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:00:46,164 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:01:02,244 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:01:50,438 | server.py:125 | fit progress: (7, 2.1020805835723877, {'accuracy': 0.4584, 'data_size': 10000}, 325.25174142300966)
INFO flwr 2024-04-06 18:01:50,440 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:01:50,440 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:02:05,260 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:03:04,407 | server.py:125 | fit progress: (8, 1.9489951133728027, {'accuracy': 0.5714, 'data_size': 10000}, 399.22091488799197)
INFO flwr 2024-04-06 18:03:04,408 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:03:04,408 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:03:20,859 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:04:28,288 | server.py:125 | fit progress: (9, 1.8218629360198975, {'accuracy': 0.6811, 'data_size': 10000}, 483.10180146899074)
INFO flwr 2024-04-06 18:04:28,288 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:04:28,289 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:04:43,053 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:06:01,672 | server.py:125 | fit progress: (10, 1.747488260269165, {'accuracy': 0.7649, 'data_size': 10000}, 576.4858531040081)
INFO flwr 2024-04-06 18:06:01,673 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:06:01,673 | server.py:153 | FL finished in 576.486589801003
INFO flwr 2024-04-06 18:06:01,673 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:06:01,673 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:06:01,673 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:06:01,673 | app.py:229 | app_fit: losses_centralized [(0, 2.3025729656219482), (1, 2.3007309436798096), (2, 2.296722173690796), (3, 2.2897355556488037), (4, 2.2836480140686035), (5, 2.2507259845733643), (6, 2.186225414276123), (7, 2.1020805835723877), (8, 1.9489951133728027), (9, 1.8218629360198975), (10, 1.747488260269165)]
INFO flwr 2024-04-06 18:06:01,674 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0766), (1, 0.101), (2, 0.0974), (3, 0.1745), (4, 0.1061), (5, 0.2315), (6, 0.2599), (7, 0.4584), (8, 0.5714), (9, 0.6811), (10, 0.7649)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7649
wandb:     loss 1.74749
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_175559-dmvoxzx1
wandb: Find logs at: ./wandb/offline-run-20240406_175559-dmvoxzx1/logs
INFO flwr 2024-04-06 18:06:05,262 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:13:30,566 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1267729)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1267729)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 18:13:35,626	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:13:35,996	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:13:36,463	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1b4d8a920ad035b8.zip' (9.97MiB) to Ray cluster...
2024-04-06 18:13:36,494	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1b4d8a920ad035b8.zip'.
INFO flwr 2024-04-06 18:13:47,340 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 133085892813.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61322525491.0}
INFO flwr 2024-04-06 18:13:47,341 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:13:47,341 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:13:47,367 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:13:47,368 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:13:47,369 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:13:47,369 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1279275)[0m 2024-04-06 18:13:52.684031: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1279275)[0m 2024-04-06 18:13:52.778934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1279275)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1279272)[0m 2024-04-06 18:13:54.949000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 18:13:55,653 | server.py:94 | initial parameters (loss, other metrics): 2.302672863006592, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-06 18:13:55,659 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:13:55,660 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1279276)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1279276)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1279267)[0m 2024-04-06 18:13:54.082228: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1279267)[0m 2024-04-06 18:13:54.177725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1279267)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1279270)[0m 2024-04-06 18:13:56.475344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1279270)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1279270)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 18:14:18,970 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:14:22,532 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:14:26,943 | server.py:125 | fit progress: (1, 1.9917722940444946, {'accuracy': 0.5091, 'data_size': 10000}, 31.282983277982567)
INFO flwr 2024-04-06 18:14:26,944 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:14:26,944 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:14:43,444 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:14:57,285 | server.py:125 | fit progress: (2, 1.6144598722457886, {'accuracy': 0.8466, 'data_size': 10000}, 61.62539280898636)
INFO flwr 2024-04-06 18:14:57,286 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:14:57,286 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:15:12,818 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:15:34,346 | server.py:125 | fit progress: (3, 1.5279924869537354, {'accuracy': 0.9415, 'data_size': 10000}, 98.68601402599597)
INFO flwr 2024-04-06 18:15:34,346 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:15:34,347 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:15:49,621 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:16:15,820 | server.py:125 | fit progress: (4, 1.510338306427002, {'accuracy': 0.9533, 'data_size': 10000}, 140.1598651690001)
INFO flwr 2024-04-06 18:16:15,820 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:16:15,820 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:16:31,495 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:17:03,076 | server.py:125 | fit progress: (5, 1.5106894969940186, {'accuracy': 0.9536, 'data_size': 10000}, 187.41666022400022)
INFO flwr 2024-04-06 18:17:03,077 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:17:03,077 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:17:20,472 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:18:05,987 | server.py:125 | fit progress: (6, 1.5003780126571655, {'accuracy': 0.9634, 'data_size': 10000}, 250.3277899409877)
INFO flwr 2024-04-06 18:18:05,988 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:18:05,988 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:18:21,752 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:19:09,017 | server.py:125 | fit progress: (7, 1.4960180521011353, {'accuracy': 0.9659, 'data_size': 10000}, 313.3571950249898)
INFO flwr 2024-04-06 18:19:09,017 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:19:09,017 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:19:23,970 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:20:15,410 | server.py:125 | fit progress: (8, 1.4954289197921753, {'accuracy': 0.9657, 'data_size': 10000}, 379.75026390500716)
INFO flwr 2024-04-06 18:20:15,410 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:20:15,411 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:20:31,066 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:21:35,024 | server.py:125 | fit progress: (9, 1.4958056211471558, {'accuracy': 0.9663, 'data_size': 10000}, 459.3640733610082)
INFO flwr 2024-04-06 18:21:35,024 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:21:35,024 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:21:51,156 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:22:55,317 | server.py:125 | fit progress: (10, 1.4929101467132568, {'accuracy': 0.9683, 'data_size': 10000}, 539.65699721899)
INFO flwr 2024-04-06 18:22:55,317 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:22:55,317 | server.py:153 | FL finished in 539.6575439980079
INFO flwr 2024-04-06 18:22:55,320 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:22:55,320 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:22:55,321 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:22:55,321 | app.py:229 | app_fit: losses_centralized [(0, 2.302672863006592), (1, 1.9917722940444946), (2, 1.6144598722457886), (3, 1.5279924869537354), (4, 1.510338306427002), (5, 1.5106894969940186), (6, 1.5003780126571655), (7, 1.4960180521011353), (8, 1.4954289197921753), (9, 1.4958056211471558), (10, 1.4929101467132568)]
INFO flwr 2024-04-06 18:22:55,321 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.5091), (2, 0.8466), (3, 0.9415), (4, 0.9533), (5, 0.9536), (6, 0.9634), (7, 0.9659), (8, 0.9657), (9, 0.9663), (10, 0.9683)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9683
wandb:     loss 1.49291
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_181330-hb3pkmt9
wandb: Find logs at: ./wandb/offline-run-20240406_181330-hb3pkmt9/logs
INFO flwr 2024-04-06 18:22:58,965 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:30:24,153 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1279261)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1279261)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 18:30:31,370	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:30:31,854	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:30:32,335	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7a1ad73186d1e7dd.zip' (10.01MiB) to Ray cluster...
2024-04-06 18:30:32,375	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7a1ad73186d1e7dd.zip'.
INFO flwr 2024-04-06 18:30:43,934 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 132511727412.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61076454604.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 18:30:43,935 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:30:43,935 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:30:43,954 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:30:43,956 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:30:43,956 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:30:43,956 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1291498)[0m 2024-04-06 18:30:50.536425: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1291494)[0m 2024-04-06 18:30:50.655114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1291494)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 18:30:51,714 | server.py:94 | initial parameters (loss, other metrics): 2.3027830123901367, {'accuracy': 0.0924, 'data_size': 10000}
INFO flwr 2024-04-06 18:30:51,715 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:30:51,716 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1291494)[0m 2024-04-06 18:30:54.130522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1291498)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1291498)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1291491)[0m 2024-04-06 18:30:50.991369: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1291491)[0m 2024-04-06 18:30:51.099629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1291491)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1291491)[0m 2024-04-06 18:30:54.377544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1291491)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1291491)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 18:31:18,145 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:31:22,008 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:31:26,250 | server.py:125 | fit progress: (1, 2.1006863117218018, {'accuracy': 0.3871, 'data_size': 10000}, 34.53447190701263)
INFO flwr 2024-04-06 18:31:26,250 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:31:26,250 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:31:41,335 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:31:56,264 | server.py:125 | fit progress: (2, 1.5570000410079956, {'accuracy': 0.9184, 'data_size': 10000}, 64.54847820400028)
INFO flwr 2024-04-06 18:31:56,265 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:31:56,265 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:32:11,661 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:32:34,278 | server.py:125 | fit progress: (3, 1.5117383003234863, {'accuracy': 0.9517, 'data_size': 10000}, 102.56297975301277)
INFO flwr 2024-04-06 18:32:34,279 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:32:34,279 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:32:49,631 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:33:15,692 | server.py:125 | fit progress: (4, 1.499719262123108, {'accuracy': 0.962, 'data_size': 10000}, 143.97692642599577)
INFO flwr 2024-04-06 18:33:15,693 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:33:15,693 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:33:32,151 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:34:11,588 | server.py:125 | fit progress: (5, 1.500362753868103, {'accuracy': 0.9622, 'data_size': 10000}, 199.87274655199144)
INFO flwr 2024-04-06 18:34:11,589 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:34:11,589 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:34:28,012 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:35:15,558 | server.py:125 | fit progress: (6, 1.4920392036437988, {'accuracy': 0.97, 'data_size': 10000}, 263.8422642929945)
INFO flwr 2024-04-06 18:35:15,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:35:15,559 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:35:31,220 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:36:25,676 | server.py:125 | fit progress: (7, 1.4895859956741333, {'accuracy': 0.9727, 'data_size': 10000}, 333.9609077979985)
INFO flwr 2024-04-06 18:36:25,677 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:36:25,677 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:36:40,313 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:37:40,864 | server.py:125 | fit progress: (8, 1.4902986288070679, {'accuracy': 0.9715, 'data_size': 10000}, 409.1484357070003)
INFO flwr 2024-04-06 18:37:40,864 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:37:40,865 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:37:55,432 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:39:04,722 | server.py:125 | fit progress: (9, 1.486770749092102, {'accuracy': 0.9741, 'data_size': 10000}, 493.0064158249879)
INFO flwr 2024-04-06 18:39:04,722 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:39:04,722 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:39:20,499 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:40:29,858 | server.py:125 | fit progress: (10, 1.4861409664154053, {'accuracy': 0.9761, 'data_size': 10000}, 578.1421924319875)
INFO flwr 2024-04-06 18:40:29,858 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:40:29,858 | server.py:153 | FL finished in 578.1427259719931
INFO flwr 2024-04-06 18:40:29,858 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:40:29,859 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:40:29,859 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:40:29,859 | app.py:229 | app_fit: losses_centralized [(0, 2.3027830123901367), (1, 2.1006863117218018), (2, 1.5570000410079956), (3, 1.5117383003234863), (4, 1.499719262123108), (5, 1.500362753868103), (6, 1.4920392036437988), (7, 1.4895859956741333), (8, 1.4902986288070679), (9, 1.486770749092102), (10, 1.4861409664154053)]
INFO flwr 2024-04-06 18:40:29,859 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0924), (1, 0.3871), (2, 0.9184), (3, 0.9517), (4, 0.962), (5, 0.9622), (6, 0.97), (7, 0.9727), (8, 0.9715), (9, 0.9741), (10, 0.9761)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9761
wandb:     loss 1.48614
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_183023-6y9eddrg
wandb: Find logs at: ./wandb/offline-run-20240406_183023-6y9eddrg/logs
INFO flwr 2024-04-06 18:40:33,629 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 18:47:58,666 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 18:48:03,805	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 18:48:04,186	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 18:48:04,498	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_198da9107954936f.zip' (10.03MiB) to Ray cluster...
2024-04-06 18:48:04,536	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_198da9107954936f.zip'.
INFO flwr 2024-04-06 18:48:15,330 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 132153361818.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60922869350.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 18:48:15,330 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 18:48:15,330 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 18:48:15,348 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 18:48:15,349 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 18:48:15,349 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 18:48:15,349 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1299666)[0m 2024-04-06 18:48:21.011243: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1299666)[0m 2024-04-06 18:48:21.112669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1299666)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 18:48:22,569 | server.py:94 | initial parameters (loss, other metrics): 2.3026251792907715, {'accuracy': 0.0981, 'data_size': 10000}
INFO flwr 2024-04-06 18:48:22,570 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 18:48:22,571 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1299666)[0m 2024-04-06 18:48:23.291952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1299668)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1299668)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1299659)[0m 2024-04-06 18:48:22.048370: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1299659)[0m 2024-04-06 18:48:22.138037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1299659)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1299665)[0m 2024-04-06 18:48:24.361834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1299661)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1299661)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 18:48:46,026 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 18:48:49,225 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 18:48:53,706 | server.py:125 | fit progress: (1, 1.802431583404541, {'accuracy': 0.7153, 'data_size': 10000}, 31.135571408987744)
INFO flwr 2024-04-06 18:48:53,707 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 18:48:53,707 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:49:11,682 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 18:49:25,557 | server.py:125 | fit progress: (2, 1.5085489749908447, {'accuracy': 0.9552, 'data_size': 10000}, 62.98612081698957)
INFO flwr 2024-04-06 18:49:25,558 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 18:49:25,558 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:49:41,387 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:01,947 | server.py:125 | fit progress: (3, 1.5021106004714966, {'accuracy': 0.9603, 'data_size': 10000}, 99.37646092899377)
INFO flwr 2024-04-06 18:50:01,947 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:01,948 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:50:17,621 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 18:50:44,067 | server.py:125 | fit progress: (4, 1.497377872467041, {'accuracy': 0.9644, 'data_size': 10000}, 141.49601752997842)
INFO flwr 2024-04-06 18:50:44,068 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 18:50:44,068 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:51:00,526 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 18:51:34,110 | server.py:125 | fit progress: (5, 1.4908640384674072, {'accuracy': 0.9707, 'data_size': 10000}, 191.5396518559719)
INFO flwr 2024-04-06 18:51:34,111 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 18:51:34,111 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:51:49,422 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 18:52:29,175 | server.py:125 | fit progress: (6, 1.4887292385101318, {'accuracy': 0.9727, 'data_size': 10000}, 246.60462907599867)
INFO flwr 2024-04-06 18:52:29,176 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 18:52:29,176 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:52:45,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 18:53:39,079 | server.py:125 | fit progress: (7, 1.4883487224578857, {'accuracy': 0.973, 'data_size': 10000}, 316.5083499439934)
INFO flwr 2024-04-06 18:53:39,080 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 18:53:39,080 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:53:55,750 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 18:54:59,825 | server.py:125 | fit progress: (8, 1.4903415441513062, {'accuracy': 0.9719, 'data_size': 10000}, 397.2539990969817)
INFO flwr 2024-04-06 18:54:59,825 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 18:54:59,826 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:55:16,405 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 18:56:16,961 | server.py:125 | fit progress: (9, 1.4852862358093262, {'accuracy': 0.9762, 'data_size': 10000}, 474.39074551698286)
INFO flwr 2024-04-06 18:56:16,962 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 18:56:16,962 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 18:56:31,972 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 18:57:55,026 | server.py:125 | fit progress: (10, 1.4822086095809937, {'accuracy': 0.9789, 'data_size': 10000}, 572.4556672279723)
INFO flwr 2024-04-06 18:57:55,027 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 18:57:55,027 | server.py:153 | FL finished in 572.4562812859949
INFO flwr 2024-04-06 18:57:55,027 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 18:57:55,027 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 18:57:55,028 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 18:57:55,028 | app.py:229 | app_fit: losses_centralized [(0, 2.3026251792907715), (1, 1.802431583404541), (2, 1.5085489749908447), (3, 1.5021106004714966), (4, 1.497377872467041), (5, 1.4908640384674072), (6, 1.4887292385101318), (7, 1.4883487224578857), (8, 1.4903415441513062), (9, 1.4852862358093262), (10, 1.4822086095809937)]
INFO flwr 2024-04-06 18:57:55,028 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0981), (1, 0.7153), (2, 0.9552), (3, 0.9603), (4, 0.9644), (5, 0.9707), (6, 0.9727), (7, 0.973), (8, 0.9719), (9, 0.9762), (10, 0.9789)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9789
wandb:     loss 1.48221
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_184758-5tk36q0r
wandb: Find logs at: ./wandb/offline-run-20240406_184758-5tk36q0r/logs
INFO flwr 2024-04-06 18:57:58,639 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:05:29,649 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1299659)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1299659)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 19:05:54,234	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-06 19:05:54,236	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-06 19:05:54,237	ERROR services.py:1276 -- 
The last 20 lines of /tmp/ray/session_2024-04-06_19-05-32_594000_440873/logs/dashboard.log (it contains the error message from the dashboard): 
2024-04-06 19:05:54,207	INFO utils.py:123 -- Module ray.dashboard.modules.actor.actor_head cannot be loaded because we cannot import all dependencies. Install this module using `pip install 'ray[default]'` for the full dashboard functionality. Error: No module named 'opencensus'

2024-04-06 19:05:54,620	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:06:03,590	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:06:03,969	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a42af602158eb346.zip' (10.07MiB) to Ray cluster...
2024-04-06 19:06:03,997	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a42af602158eb346.zip'.
INFO flwr 2024-04-06 19:06:14,818 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 61937854464.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 134521660416.0}
INFO flwr 2024-04-06 19:06:14,818 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:06:14,818 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:06:14,834 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:06:14,835 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:06:14,835 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:06:14,835 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 19:06:21,383 | server.py:94 | initial parameters (loss, other metrics): 2.302367687225342, {'accuracy': 0.1183, 'data_size': 10000}
INFO flwr 2024-04-06 19:06:21,384 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:06:21,384 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1311926)[0m 2024-04-06 19:06:26.778293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1311926)[0m 2024-04-06 19:06:26.933461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1311926)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1311926)[0m 2024-04-06 19:06:45.227863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1311929)[0m 2024-04-06 19:06:26.948466: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1311929)[0m 2024-04-06 19:06:27.013578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1311929)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1311921)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1311921)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1311919)[0m 2024-04-06 19:06:45.222961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 19:08:10,957 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:08:14,625 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:08:18,838 | server.py:125 | fit progress: (1, 1.8072494268417358, {'accuracy': 0.7773, 'data_size': 10000}, 117.45387409601244)
INFO flwr 2024-04-06 19:08:18,839 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:08:18,839 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:08:33,641 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:08:47,538 | server.py:125 | fit progress: (2, 1.5157201290130615, {'accuracy': 0.9478, 'data_size': 10000}, 146.1543869540037)
INFO flwr 2024-04-06 19:08:47,539 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:08:47,539 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:09:02,725 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:09:22,928 | server.py:125 | fit progress: (3, 1.5022066831588745, {'accuracy': 0.9598, 'data_size': 10000}, 181.54376182999113)
INFO flwr 2024-04-06 19:09:22,928 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:09:22,928 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:09:37,415 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:10:04,114 | server.py:125 | fit progress: (4, 1.4976691007614136, {'accuracy': 0.9636, 'data_size': 10000}, 222.7303274500009)
INFO flwr 2024-04-06 19:10:04,115 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:10:04,115 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:10:21,060 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:11:00,176 | server.py:125 | fit progress: (5, 1.4896934032440186, {'accuracy': 0.9719, 'data_size': 10000}, 278.79204409901286)
INFO flwr 2024-04-06 19:11:00,176 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:11:00,177 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:11:15,964 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:11:55,163 | server.py:125 | fit progress: (6, 1.487122654914856, {'accuracy': 0.975, 'data_size': 10000}, 333.7790655500139)
INFO flwr 2024-04-06 19:11:55,164 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:11:55,164 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:12:11,315 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:12:57,394 | server.py:125 | fit progress: (7, 1.4863725900650024, {'accuracy': 0.9753, 'data_size': 10000}, 396.01006619699183)
INFO flwr 2024-04-06 19:12:57,394 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:12:57,395 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:13:12,221 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:14:01,784 | server.py:125 | fit progress: (8, 1.484163522720337, {'accuracy': 0.9776, 'data_size': 10000}, 460.40015767200384)
INFO flwr 2024-04-06 19:14:01,785 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:14:01,785 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:14:16,511 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:15:24,012 | server.py:125 | fit progress: (9, 1.483517050743103, {'accuracy': 0.9782, 'data_size': 10000}, 542.6276615310053)
INFO flwr 2024-04-06 19:15:24,012 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:15:24,012 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:15:38,519 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:16:53,732 | server.py:125 | fit progress: (10, 1.4831626415252686, {'accuracy': 0.9783, 'data_size': 10000}, 632.3478570650041)
INFO flwr 2024-04-06 19:16:53,732 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:16:53,732 | server.py:153 | FL finished in 632.3483723450045
INFO flwr 2024-04-06 19:16:53,733 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:16:53,733 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:16:53,733 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:16:53,733 | app.py:229 | app_fit: losses_centralized [(0, 2.302367687225342), (1, 1.8072494268417358), (2, 1.5157201290130615), (3, 1.5022066831588745), (4, 1.4976691007614136), (5, 1.4896934032440186), (6, 1.487122654914856), (7, 1.4863725900650024), (8, 1.484163522720337), (9, 1.483517050743103), (10, 1.4831626415252686)]
INFO flwr 2024-04-06 19:16:53,733 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1183), (1, 0.7773), (2, 0.9478), (3, 0.9598), (4, 0.9636), (5, 0.9719), (6, 0.975), (7, 0.9753), (8, 0.9776), (9, 0.9782), (10, 0.9783)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9783
wandb:     loss 1.48316
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_190526-hehut0lt
wandb: Find logs at: ./wandb/offline-run-20240406_190526-hehut0lt/logs
INFO flwr 2024-04-06 19:16:57,389 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:24:22,906 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1311929)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1311929)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 19:24:28,073	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:24:28,503	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:24:28,860	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_80b90fe5667e1f96.zip' (10.11MiB) to Ray cluster...
2024-04-06 19:24:28,886	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_80b90fe5667e1f96.zip'.
INFO flwr 2024-04-06 19:24:39,616 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 133653013504.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61565577216.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 19:24:39,616 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:24:39,617 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:24:39,633 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:24:39,634 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:24:39,634 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:24:39,634 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1323324)[0m 2024-04-06 19:24:45.585521: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1323324)[0m 2024-04-06 19:24:45.677574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1323324)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1323324)[0m 2024-04-06 19:24:47.706281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 19:24:47,835 | server.py:94 | initial parameters (loss, other metrics): 2.3027498722076416, {'accuracy': 0.0903, 'data_size': 10000}
INFO flwr 2024-04-06 19:24:47,836 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:24:47,836 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1323326)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1323326)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1323326)[0m 2024-04-06 19:24:46.050445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1323326)[0m 2024-04-06 19:24:46.145965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1323326)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1323325)[0m 2024-04-06 19:24:48.155429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1323321)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1323321)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 19:25:10,708 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:25:14,404 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:25:18,639 | server.py:125 | fit progress: (1, 1.809730887413025, {'accuracy': 0.7858, 'data_size': 10000}, 30.803010064002592)
INFO flwr 2024-04-06 19:25:18,639 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:25:18,640 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:25:34,221 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:25:47,932 | server.py:125 | fit progress: (2, 1.5069526433944702, {'accuracy': 0.9572, 'data_size': 10000}, 60.09564315600437)
INFO flwr 2024-04-06 19:25:47,932 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:25:47,932 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:26:02,657 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:26:22,776 | server.py:125 | fit progress: (3, 1.4997183084487915, {'accuracy': 0.9626, 'data_size': 10000}, 94.940388988005)
INFO flwr 2024-04-06 19:26:22,777 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:26:22,777 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:26:38,434 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:27:05,264 | server.py:125 | fit progress: (4, 1.4915200471878052, {'accuracy': 0.9702, 'data_size': 10000}, 137.42810908000683)
INFO flwr 2024-04-06 19:27:05,265 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:27:05,265 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:27:20,096 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:27:57,393 | server.py:125 | fit progress: (5, 1.4885146617889404, {'accuracy': 0.9733, 'data_size': 10000}, 189.5571611180203)
INFO flwr 2024-04-06 19:27:57,394 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:27:57,394 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:28:11,921 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:28:56,601 | server.py:125 | fit progress: (6, 1.4868789911270142, {'accuracy': 0.9746, 'data_size': 10000}, 248.7649665939971)
INFO flwr 2024-04-06 19:28:56,601 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:28:56,602 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:29:12,226 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:30:06,021 | server.py:125 | fit progress: (7, 1.4856741428375244, {'accuracy': 0.9755, 'data_size': 10000}, 318.1850226969982)
INFO flwr 2024-04-06 19:30:06,021 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:30:06,022 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:30:21,857 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:31:14,502 | server.py:125 | fit progress: (8, 1.4860148429870605, {'accuracy': 0.9754, 'data_size': 10000}, 386.6657415740192)
INFO flwr 2024-04-06 19:31:14,502 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:31:14,503 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:31:30,211 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:32:25,613 | server.py:125 | fit progress: (9, 1.4837796688079834, {'accuracy': 0.9775, 'data_size': 10000}, 457.776982484007)
INFO flwr 2024-04-06 19:32:25,613 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:32:25,614 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:32:41,951 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:33:41,190 | server.py:125 | fit progress: (10, 1.4816762208938599, {'accuracy': 0.9795, 'data_size': 10000}, 533.3542861450114)
INFO flwr 2024-04-06 19:33:41,191 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:33:41,191 | server.py:153 | FL finished in 533.354775935004
INFO flwr 2024-04-06 19:33:41,191 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:33:41,191 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:33:41,191 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:33:41,192 | app.py:229 | app_fit: losses_centralized [(0, 2.3027498722076416), (1, 1.809730887413025), (2, 1.5069526433944702), (3, 1.4997183084487915), (4, 1.4915200471878052), (5, 1.4885146617889404), (6, 1.4868789911270142), (7, 1.4856741428375244), (8, 1.4860148429870605), (9, 1.4837796688079834), (10, 1.4816762208938599)]
INFO flwr 2024-04-06 19:33:41,192 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0903), (1, 0.7858), (2, 0.9572), (3, 0.9626), (4, 0.9702), (5, 0.9733), (6, 0.9746), (7, 0.9755), (8, 0.9754), (9, 0.9775), (10, 0.9795)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9795
wandb:     loss 1.48168
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_192422-7wya131u
wandb: Find logs at: ./wandb/offline-run-20240406_192422-7wya131u/logs
INFO flwr 2024-04-06 19:33:44,805 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:41:09,820 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1323314)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1323314)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 19:41:15,090	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:41:15,429	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:41:15,865	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0b1d7fb5e3814907.zip' (10.14MiB) to Ray cluster...
2024-04-06 19:41:15,895	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0b1d7fb5e3814907.zip'.
INFO flwr 2024-04-06 19:41:27,101 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 61411382476.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 133293225780.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 19:41:27,101 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:41:27,101 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:41:27,121 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:41:27,122 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:41:27,122 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:41:27,123 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1335503)[0m 2024-04-06 19:41:33.216298: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1335503)[0m 2024-04-06 19:41:33.309658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1335503)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 19:41:34,558 | server.py:94 | initial parameters (loss, other metrics): 2.3026795387268066, {'accuracy': 0.0973, 'data_size': 10000}
INFO flwr 2024-04-06 19:41:34,559 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:41:34,559 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1335502)[0m 2024-04-06 19:41:35.399616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1335504)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1335504)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1335504)[0m 2024-04-06 19:41:33.476444: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1335504)[0m 2024-04-06 19:41:33.570062: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1335504)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1335501)[0m 2024-04-06 19:41:35.811801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1335498)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1335498)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 19:42:00,341 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:42:03,911 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:42:08,241 | server.py:125 | fit progress: (1, 1.815947413444519, {'accuracy': 0.7543, 'data_size': 10000}, 33.681937307992484)
INFO flwr 2024-04-06 19:42:08,242 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:42:08,242 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:42:24,197 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:42:38,356 | server.py:125 | fit progress: (2, 1.5007014274597168, {'accuracy': 0.9619, 'data_size': 10000}, 63.79661244098679)
INFO flwr 2024-04-06 19:42:38,356 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:42:38,356 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:42:53,118 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 19:43:13,515 | server.py:125 | fit progress: (3, 1.4924945831298828, {'accuracy': 0.9697, 'data_size': 10000}, 98.95587074701325)
INFO flwr 2024-04-06 19:43:13,515 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 19:43:13,516 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:43:29,646 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 19:43:54,339 | server.py:125 | fit progress: (4, 1.4889658689498901, {'accuracy': 0.9727, 'data_size': 10000}, 139.77990158900502)
INFO flwr 2024-04-06 19:43:54,339 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 19:43:54,340 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:44:08,372 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 19:44:45,096 | server.py:125 | fit progress: (5, 1.4863557815551758, {'accuracy': 0.9747, 'data_size': 10000}, 190.53673287900165)
INFO flwr 2024-04-06 19:44:45,096 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 19:44:45,096 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:45:00,281 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 19:45:44,086 | server.py:125 | fit progress: (6, 1.4839950799942017, {'accuracy': 0.9774, 'data_size': 10000}, 249.52647775798687)
INFO flwr 2024-04-06 19:45:44,086 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 19:45:44,086 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:45:59,684 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 19:46:51,333 | server.py:125 | fit progress: (7, 1.4873310327529907, {'accuracy': 0.9741, 'data_size': 10000}, 316.7736645679979)
INFO flwr 2024-04-06 19:46:51,333 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 19:46:51,333 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:47:06,671 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 19:48:05,507 | server.py:125 | fit progress: (8, 1.4823994636535645, {'accuracy': 0.9789, 'data_size': 10000}, 390.9474110729934)
INFO flwr 2024-04-06 19:48:05,507 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 19:48:05,507 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:48:20,346 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 19:49:28,387 | server.py:125 | fit progress: (9, 1.4812331199645996, {'accuracy': 0.9805, 'data_size': 10000}, 473.8280120140116)
INFO flwr 2024-04-06 19:49:28,388 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 19:49:28,388 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:49:42,850 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 19:50:58,536 | server.py:125 | fit progress: (10, 1.4819585084915161, {'accuracy': 0.9789, 'data_size': 10000}, 563.9767268639989)
INFO flwr 2024-04-06 19:50:58,536 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 19:50:58,536 | server.py:153 | FL finished in 563.9772243720072
INFO flwr 2024-04-06 19:50:58,541 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 19:50:58,541 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 19:50:58,541 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 19:50:58,541 | app.py:229 | app_fit: losses_centralized [(0, 2.3026795387268066), (1, 1.815947413444519), (2, 1.5007014274597168), (3, 1.4924945831298828), (4, 1.4889658689498901), (5, 1.4863557815551758), (6, 1.4839950799942017), (7, 1.4873310327529907), (8, 1.4823994636535645), (9, 1.4812331199645996), (10, 1.4819585084915161)]
INFO flwr 2024-04-06 19:50:58,541 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0973), (1, 0.7543), (2, 0.9619), (3, 0.9697), (4, 0.9727), (5, 0.9747), (6, 0.9774), (7, 0.9741), (8, 0.9789), (9, 0.9805), (10, 0.9789)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9789
wandb:     loss 1.48196
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_194109-09b09gcx
wandb: Find logs at: ./wandb/offline-run-20240406_194109-09b09gcx/logs
INFO flwr 2024-04-06 19:51:02,277 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 19:58:27,220 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1335497)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1335497)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 19:58:32,604	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 19:58:32,938	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 19:58:33,389	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_49cc19dca37c1c39.zip' (10.17MiB) to Ray cluster...
2024-04-06 19:58:33,416	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_49cc19dca37c1c39.zip'.
INFO flwr 2024-04-06 19:58:44,141 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 134005432525.0, 'CPU': 64.0, 'object_store_memory': 61716613939.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 19:58:44,141 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 19:58:44,141 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 19:58:44,157 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 19:58:44,158 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 19:58:44,158 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 19:58:44,158 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1343897)[0m 2024-04-06 19:58:49.612780: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1343897)[0m 2024-04-06 19:58:49.716206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1343897)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 19:58:51,590 | server.py:94 | initial parameters (loss, other metrics): 2.303051471710205, {'accuracy': 0.0965, 'data_size': 10000}
INFO flwr 2024-04-06 19:58:51,591 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 19:58:51,591 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1343897)[0m 2024-04-06 19:58:52.032814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1343904)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1343904)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1343901)[0m 2024-04-06 19:58:50.819140: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1343901)[0m 2024-04-06 19:58:50.922952: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1343901)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1343901)[0m 2024-04-06 19:58:52.899661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1343895)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1343895)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 19:59:07,723 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 19:59:10,831 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 19:59:15,070 | server.py:125 | fit progress: (1, 2.303027391433716, {'accuracy': 0.0993, 'data_size': 10000}, 23.47919410999748)
INFO flwr 2024-04-06 19:59:15,071 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 19:59:15,071 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:59:24,240 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 19:59:38,137 | server.py:125 | fit progress: (2, 2.303011655807495, {'accuracy': 0.099, 'data_size': 10000}, 46.545408158010105)
INFO flwr 2024-04-06 19:59:38,137 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 19:59:38,137 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 19:59:46,477 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:00:06,668 | server.py:125 | fit progress: (3, 2.3029942512512207, {'accuracy': 0.0991, 'data_size': 10000}, 75.07710917200893)
INFO flwr 2024-04-06 20:00:06,669 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:00:06,669 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:00:15,235 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:00:46,938 | server.py:125 | fit progress: (4, 2.3029727935791016, {'accuracy': 0.0993, 'data_size': 10000}, 115.34638366400031)
INFO flwr 2024-04-06 20:00:46,938 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:00:46,938 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:00:55,685 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:01:34,358 | server.py:125 | fit progress: (5, 2.3029520511627197, {'accuracy': 0.0993, 'data_size': 10000}, 162.76647797599435)
INFO flwr 2024-04-06 20:01:34,358 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:01:34,358 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:01:43,086 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:02:22,514 | server.py:125 | fit progress: (6, 2.3029322624206543, {'accuracy': 0.0995, 'data_size': 10000}, 210.92247376200976)
INFO flwr 2024-04-06 20:02:22,514 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:02:22,515 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:02:30,995 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:03:16,805 | server.py:125 | fit progress: (7, 2.3029098510742188, {'accuracy': 0.0998, 'data_size': 10000}, 265.21433688598336)
INFO flwr 2024-04-06 20:03:16,806 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:03:16,806 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:03:25,448 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:04:15,063 | server.py:125 | fit progress: (8, 2.3028900623321533, {'accuracy': 0.0999, 'data_size': 10000}, 323.4722833429987)
INFO flwr 2024-04-06 20:04:15,064 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:04:15,064 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:04:23,596 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:05:32,142 | server.py:125 | fit progress: (9, 2.302870988845825, {'accuracy': 0.1, 'data_size': 10000}, 400.5506179840013)
INFO flwr 2024-04-06 20:05:32,142 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:05:32,142 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:05:40,585 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:06:55,148 | server.py:125 | fit progress: (10, 2.3028554916381836, {'accuracy': 0.1003, 'data_size': 10000}, 483.5566563690081)
INFO flwr 2024-04-06 20:06:55,148 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:06:55,149 | server.py:153 | FL finished in 483.557516683999
INFO flwr 2024-04-06 20:06:55,149 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:06:55,149 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:06:55,149 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:06:55,149 | app.py:229 | app_fit: losses_centralized [(0, 2.303051471710205), (1, 2.303027391433716), (2, 2.303011655807495), (3, 2.3029942512512207), (4, 2.3029727935791016), (5, 2.3029520511627197), (6, 2.3029322624206543), (7, 2.3029098510742188), (8, 2.3028900623321533), (9, 2.302870988845825), (10, 2.3028554916381836)]
INFO flwr 2024-04-06 20:06:55,149 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0965), (1, 0.0993), (2, 0.099), (3, 0.0991), (4, 0.0993), (5, 0.0993), (6, 0.0995), (7, 0.0998), (8, 0.0999), (9, 0.1), (10, 0.1003)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1003
wandb:     loss 2.30286
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_195826-4xc5pnva
wandb: Find logs at: ./wandb/offline-run-20240406_195826-4xc5pnva/logs
INFO flwr 2024-04-06 20:06:58,870 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:14:23,446 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1343893)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1343893)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 20:14:29,043	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:14:29,428	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:14:29,795	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7f135205a27ca583.zip' (10.21MiB) to Ray cluster...
2024-04-06 20:14:29,816	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7f135205a27ca583.zip'.
INFO flwr 2024-04-06 20:14:40,649 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 64770448588.0, 'CPU': 64.0, 'memory': 141131046708.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 20:14:40,649 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:14:40,649 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:14:40,664 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:14:40,665 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:14:40,666 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:14:40,666 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1355609)[0m 2024-04-06 20:14:46.226559: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1355607)[0m 2024-04-06 20:14:46.290627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1355607)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 20:14:47,950 | server.py:94 | initial parameters (loss, other metrics): 2.302663803100586, {'accuracy': 0.143, 'data_size': 10000}
INFO flwr 2024-04-06 20:14:47,950 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:14:47,951 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1355609)[0m 2024-04-06 20:14:48.608204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1355611)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1355611)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1355610)[0m 2024-04-06 20:14:47.269679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1355610)[0m 2024-04-06 20:14:47.346187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1355610)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1355610)[0m 2024-04-06 20:14:49.578334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1355607)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1355607)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-04-06 20:15:16,048 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:15:19,588 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:15:23,844 | server.py:125 | fit progress: (1, 2.3017544746398926, {'accuracy': 0.1688, 'data_size': 10000}, 35.893661926005734)
INFO flwr 2024-04-06 20:15:23,844 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:15:23,845 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:15:33,460 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:15:47,553 | server.py:125 | fit progress: (2, 2.2996089458465576, {'accuracy': 0.1746, 'data_size': 10000}, 59.603096183011075)
INFO flwr 2024-04-06 20:15:47,554 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:15:47,554 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:15:56,884 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:16:17,785 | server.py:125 | fit progress: (3, 2.295935869216919, {'accuracy': 0.1032, 'data_size': 10000}, 89.83506962400861)
INFO flwr 2024-04-06 20:16:17,801 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:16:17,801 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:16:26,283 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:16:53,471 | server.py:125 | fit progress: (4, 2.292527914047241, {'accuracy': 0.1404, 'data_size': 10000}, 125.52058481000131)
INFO flwr 2024-04-06 20:16:53,471 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:16:53,471 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:17:02,522 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:17:36,315 | server.py:125 | fit progress: (5, 2.287160634994507, {'accuracy': 0.1168, 'data_size': 10000}, 168.36490808500093)
INFO flwr 2024-04-06 20:17:36,316 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:17:36,316 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:17:45,350 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:18:25,106 | server.py:125 | fit progress: (6, 2.264052629470825, {'accuracy': 0.2903, 'data_size': 10000}, 217.15523200199823)
INFO flwr 2024-04-06 20:18:25,106 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:18:25,106 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:18:34,020 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:19:20,360 | server.py:125 | fit progress: (7, 2.2185544967651367, {'accuracy': 0.2646, 'data_size': 10000}, 272.4098581940052)
INFO flwr 2024-04-06 20:19:20,361 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:19:20,361 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:19:29,278 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:20:18,858 | server.py:125 | fit progress: (8, 2.141510248184204, {'accuracy': 0.3011, 'data_size': 10000}, 330.907667780004)
INFO flwr 2024-04-06 20:20:18,858 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:20:18,859 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:20:28,016 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:21:36,796 | server.py:125 | fit progress: (9, 2.053966522216797, {'accuracy': 0.4369, 'data_size': 10000}, 408.84612387800007)
INFO flwr 2024-04-06 20:21:36,797 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:21:36,797 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:21:45,627 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:23:00,277 | server.py:125 | fit progress: (10, 1.9332228899002075, {'accuracy': 0.6087, 'data_size': 10000}, 492.32645148201846)
INFO flwr 2024-04-06 20:23:00,277 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:23:00,278 | server.py:153 | FL finished in 492.32734467601404
INFO flwr 2024-04-06 20:23:00,278 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:23:00,278 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:23:00,278 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:23:00,278 | app.py:229 | app_fit: losses_centralized [(0, 2.302663803100586), (1, 2.3017544746398926), (2, 2.2996089458465576), (3, 2.295935869216919), (4, 2.292527914047241), (5, 2.287160634994507), (6, 2.264052629470825), (7, 2.2185544967651367), (8, 2.141510248184204), (9, 2.053966522216797), (10, 1.9332228899002075)]
INFO flwr 2024-04-06 20:23:00,279 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.143), (1, 0.1688), (2, 0.1746), (3, 0.1032), (4, 0.1404), (5, 0.1168), (6, 0.2903), (7, 0.2646), (8, 0.3011), (9, 0.4369), (10, 0.6087)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6087
wandb:     loss 1.93322
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_201423-18k2i4ka
wandb: Find logs at: ./wandb/offline-run-20240406_201423-18k2i4ka/logs
INFO flwr 2024-04-06 20:23:03,819 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:30:30,464 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1355610)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1355610)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
2024-04-06 20:30:35,812	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:30:36,196	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:30:36,556	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_217f4d90031a1e40.zip' (10.23MiB) to Ray cluster...
2024-04-06 20:30:36,586	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_217f4d90031a1e40.zip'.
INFO flwr 2024-04-06 20:30:47,475 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 135442140570.0, 'object_store_memory': 62332345958.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 20:30:47,476 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:30:47,476 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:30:47,500 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:30:47,502 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:30:47,502 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:30:47,503 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1364197)[0m 2024-04-06 20:30:53.447574: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1364197)[0m 2024-04-06 20:30:53.547659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1364197)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 20:30:55,422 | server.py:94 | initial parameters (loss, other metrics): 2.3027632236480713, {'accuracy': 0.1154, 'data_size': 10000}
INFO flwr 2024-04-06 20:30:55,422 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:30:55,423 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1364197)[0m 2024-04-06 20:30:55.598137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1364196)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1364196)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1364160)[0m 2024-04-06 20:30:53.959260: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1364160)[0m 2024-04-06 20:30:54.060282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1364160)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1364160)[0m 2024-04-06 20:30:56.291348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1364138)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1364138)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:31:10,238 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:31:13,653 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:31:17,929 | server.py:125 | fit progress: (1, 2.2994508743286133, {'accuracy': 0.1028, 'data_size': 10000}, 22.50667776798946)
INFO flwr 2024-04-06 20:31:17,930 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:31:17,930 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:31:27,537 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:31:41,470 | server.py:125 | fit progress: (2, 2.290308713912964, {'accuracy': 0.1042, 'data_size': 10000}, 46.047227734990884)
INFO flwr 2024-04-06 20:31:41,470 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:31:41,470 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:31:50,050 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:32:10,730 | server.py:125 | fit progress: (3, 2.2388875484466553, {'accuracy': 0.2439, 'data_size': 10000}, 75.30792424699757)
INFO flwr 2024-04-06 20:32:10,731 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:32:10,731 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:32:19,520 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:32:46,010 | server.py:125 | fit progress: (4, 2.1694626808166504, {'accuracy': 0.4301, 'data_size': 10000}, 110.58721709798556)
INFO flwr 2024-04-06 20:32:46,010 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:32:46,010 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:32:55,009 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:33:32,649 | server.py:125 | fit progress: (5, 2.069242238998413, {'accuracy': 0.4126, 'data_size': 10000}, 157.22655135998502)
INFO flwr 2024-04-06 20:33:32,649 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:33:32,649 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:33:41,355 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:27,988 | server.py:125 | fit progress: (6, 1.9815254211425781, {'accuracy': 0.4656, 'data_size': 10000}, 212.56533485997352)
INFO flwr 2024-04-06 20:34:27,988 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:27,988 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:36,375 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:35:22,754 | server.py:125 | fit progress: (7, 1.7689576148986816, {'accuracy': 0.7545, 'data_size': 10000}, 267.33208675598144)
INFO flwr 2024-04-06 20:35:22,755 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:35:22,755 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:35:31,549 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:36:21,077 | server.py:125 | fit progress: (8, 1.6891945600509644, {'accuracy': 0.829, 'data_size': 10000}, 325.6543961579737)
INFO flwr 2024-04-06 20:36:21,077 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:36:21,077 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:36:29,813 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:37:37,857 | server.py:125 | fit progress: (9, 1.6654117107391357, {'accuracy': 0.8334, 'data_size': 10000}, 402.43504378397483)
INFO flwr 2024-04-06 20:37:37,858 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:37:37,858 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:37:46,843 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:39:00,424 | server.py:125 | fit progress: (10, 1.6639574766159058, {'accuracy': 0.8165, 'data_size': 10000}, 485.001699526998)
INFO flwr 2024-04-06 20:39:00,425 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:39:00,425 | server.py:153 | FL finished in 485.00244937397656
INFO flwr 2024-04-06 20:39:00,425 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:39:00,425 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:39:00,425 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:39:00,425 | app.py:229 | app_fit: losses_centralized [(0, 2.3027632236480713), (1, 2.2994508743286133), (2, 2.290308713912964), (3, 2.2388875484466553), (4, 2.1694626808166504), (5, 2.069242238998413), (6, 1.9815254211425781), (7, 1.7689576148986816), (8, 1.6891945600509644), (9, 1.6654117107391357), (10, 1.6639574766159058)]
INFO flwr 2024-04-06 20:39:00,426 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1154), (1, 0.1028), (2, 0.1042), (3, 0.2439), (4, 0.4301), (5, 0.4126), (6, 0.4656), (7, 0.7545), (8, 0.829), (9, 0.8334), (10, 0.8165)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8165
wandb:     loss 1.66396
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_203030-yjyawmao
wandb: Find logs at: ./wandb/offline-run-20240406_203030-yjyawmao/logs
INFO flwr 2024-04-06 20:39:04,066 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:46:28,728 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 20:46:34,553	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:46:34,895	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:46:35,242	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_aca85d5a66a35794.zip' (10.27MiB) to Ray cluster...
2024-04-06 20:46:35,271	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_aca85d5a66a35794.zip'.
INFO flwr 2024-04-06 20:46:46,236 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 134168349696.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61786435584.0}
INFO flwr 2024-04-06 20:46:46,236 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:46:46,236 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:46:46,253 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:46:46,254 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:46:46,254 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:46:46,254 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1376045)[0m 2024-04-06 20:46:51.438717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1376045)[0m 2024-04-06 20:46:51.573388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1376045)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 20:46:53,578 | server.py:94 | initial parameters (loss, other metrics): 2.302537679672241, {'accuracy': 0.1582, 'data_size': 10000}
INFO flwr 2024-04-06 20:46:53,578 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:46:53,579 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1376045)[0m 2024-04-06 20:46:54.159029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1376041)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1376041)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1376032)[0m 2024-04-06 20:46:53.063836: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1376032)[0m 2024-04-06 20:46:53.159630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1376032)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1376038)[0m 2024-04-06 20:46:55.136321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1376038)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1376038)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 20:47:10,749 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:47:13,938 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:47:17,990 | server.py:125 | fit progress: (1, 2.297138214111328, {'accuracy': 0.1853, 'data_size': 10000}, 24.411531543009914)
INFO flwr 2024-04-06 20:47:17,990 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:47:17,991 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:47:28,104 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:47:41,004 | server.py:125 | fit progress: (2, 2.2601895332336426, {'accuracy': 0.2479, 'data_size': 10000}, 47.424969801999396)
INFO flwr 2024-04-06 20:47:41,005 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:47:41,005 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:47:49,451 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:48:08,781 | server.py:125 | fit progress: (3, 2.1519219875335693, {'accuracy': 0.3139, 'data_size': 10000}, 75.20239170701825)
INFO flwr 2024-04-06 20:48:08,782 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:48:08,782 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:48:16,463 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:48:41,297 | server.py:125 | fit progress: (4, 2.0841023921966553, {'accuracy': 0.355, 'data_size': 10000}, 107.71640222900896)
INFO flwr 2024-04-06 20:48:41,297 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:48:41,298 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:48:49,838 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:49:25,064 | server.py:125 | fit progress: (5, 1.8743345737457275, {'accuracy': 0.6515, 'data_size': 10000}, 151.4849726090033)
INFO flwr 2024-04-06 20:49:25,064 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:49:25,064 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:49:32,780 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:50:15,452 | server.py:125 | fit progress: (6, 1.7580828666687012, {'accuracy': 0.7243, 'data_size': 10000}, 201.8732627540012)
INFO flwr 2024-04-06 20:50:15,452 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:50:15,452 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:50:24,142 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:51:05,334 | server.py:125 | fit progress: (7, 1.6819134950637817, {'accuracy': 0.8047, 'data_size': 10000}, 251.7559110330185)
INFO flwr 2024-04-06 20:51:05,335 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:51:05,335 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:51:13,819 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:52:09,898 | server.py:125 | fit progress: (8, 1.616671085357666, {'accuracy': 0.8769, 'data_size': 10000}, 316.3198869480111)
INFO flwr 2024-04-06 20:52:09,899 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:52:09,899 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:52:18,384 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:53:27,759 | server.py:125 | fit progress: (9, 1.6276510953903198, {'accuracy': 0.8516, 'data_size': 10000}, 394.18015412800014)
INFO flwr 2024-04-06 20:53:27,759 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:53:27,759 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:53:36,437 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:54:47,299 | server.py:125 | fit progress: (10, 1.5821077823638916, {'accuracy': 0.8985, 'data_size': 10000}, 473.7199380840175)
INFO flwr 2024-04-06 20:54:47,299 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:54:47,299 | server.py:153 | FL finished in 473.7206728660094
INFO flwr 2024-04-06 20:54:47,299 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:54:47,300 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:54:47,300 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:54:47,300 | app.py:229 | app_fit: losses_centralized [(0, 2.302537679672241), (1, 2.297138214111328), (2, 2.2601895332336426), (3, 2.1519219875335693), (4, 2.0841023921966553), (5, 1.8743345737457275), (6, 1.7580828666687012), (7, 1.6819134950637817), (8, 1.616671085357666), (9, 1.6276510953903198), (10, 1.5821077823638916)]
INFO flwr 2024-04-06 20:54:47,300 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1582), (1, 0.1853), (2, 0.2479), (3, 0.3139), (4, 0.355), (5, 0.6515), (6, 0.7243), (7, 0.8047), (8, 0.8769), (9, 0.8516), (10, 0.8985)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8985
wandb:     loss 1.58211
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_204628-bshran9v
wandb: Find logs at: ./wandb/offline-run-20240406_204628-bshran9v/logs
INFO flwr 2024-04-06 20:54:50,916 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:02:11,894 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1376035)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1376035)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 21:02:20,298	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:02:20,892	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:02:21,236	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e3938140764ed0c7.zip' (10.29MiB) to Ray cluster...
2024-04-06 21:02:21,258	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e3938140764ed0c7.zip'.
INFO flwr 2024-04-06 21:02:31,995 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 63701226700.0, 'node:10.20.240.18': 1.0, 'memory': 138636195636.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 21:02:31,995 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:02:31,995 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:02:32,009 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:02:32,010 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:02:32,011 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:02:32,011 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1385328)[0m 2024-04-06 21:02:38.215729: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1385319)[0m 2024-04-06 21:02:38.311068: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1385319)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 21:02:39,626 | server.py:94 | initial parameters (loss, other metrics): 2.302671194076538, {'accuracy': 0.0652, 'data_size': 10000}
INFO flwr 2024-04-06 21:02:39,626 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:02:39,626 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1385326)[0m 2024-04-06 21:02:40.890190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1385326)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1385326)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1385325)[0m 2024-04-06 21:02:38.336657: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1385325)[0m 2024-04-06 21:02:38.435300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1385325)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1385321)[0m 2024-04-06 21:02:40.886762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:02:55,251 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:02:58,450 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:03:02,355 | server.py:125 | fit progress: (1, 2.297682523727417, {'accuracy': 0.207, 'data_size': 10000}, 22.72920015899581)
INFO flwr 2024-04-06 21:03:02,356 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:02,356 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:03:11,363 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:03:24,420 | server.py:125 | fit progress: (2, 2.2952730655670166, {'accuracy': 0.0974, 'data_size': 10000}, 44.793307837011525)
INFO flwr 2024-04-06 21:03:24,420 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:24,420 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:03:32,683 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:03:51,859 | server.py:125 | fit progress: (3, 2.2442467212677, {'accuracy': 0.3813, 'data_size': 10000}, 72.23286025499692)
INFO flwr 2024-04-06 21:03:51,859 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:51,860 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:03:59,960 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:24,929 | server.py:125 | fit progress: (4, 2.1177594661712646, {'accuracy': 0.5131, 'data_size': 10000}, 105.30277666600887)
INFO flwr 2024-04-06 21:04:24,930 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:24,930 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:33,011 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:05:04,084 | server.py:125 | fit progress: (5, 1.87252676486969, {'accuracy': 0.6454, 'data_size': 10000}, 144.45764457501355)
INFO flwr 2024-04-06 21:05:04,084 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:05:04,084 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:05:11,851 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:05:46,199 | server.py:125 | fit progress: (6, 1.7640007734298706, {'accuracy': 0.7099, 'data_size': 10000}, 186.57315468799789)
INFO flwr 2024-04-06 21:05:46,200 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:05:46,200 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:05:54,360 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:06:42,080 | server.py:125 | fit progress: (7, 1.6805336475372314, {'accuracy': 0.7984, 'data_size': 10000}, 242.45421505099512)
INFO flwr 2024-04-06 21:06:42,081 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:06:42,081 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:06:49,782 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:07:44,602 | server.py:125 | fit progress: (8, 1.6442019939422607, {'accuracy': 0.8419, 'data_size': 10000}, 304.97539932699874)
INFO flwr 2024-04-06 21:07:44,602 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:07:44,602 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:07:52,398 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:08:54,496 | server.py:125 | fit progress: (9, 1.648621916770935, {'accuracy': 0.8311, 'data_size': 10000}, 374.86995897401357)
INFO flwr 2024-04-06 21:08:54,496 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:08:54,497 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:09:02,636 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:10:10,968 | server.py:125 | fit progress: (10, 1.5594587326049805, {'accuracy': 0.916, 'data_size': 10000}, 451.3415661409963)
INFO flwr 2024-04-06 21:10:10,968 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:10:10,968 | server.py:153 | FL finished in 451.34201195201604
INFO flwr 2024-04-06 21:10:10,968 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:10:10,969 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:10:10,969 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:10:10,969 | app.py:229 | app_fit: losses_centralized [(0, 2.302671194076538), (1, 2.297682523727417), (2, 2.2952730655670166), (3, 2.2442467212677), (4, 2.1177594661712646), (5, 1.87252676486969), (6, 1.7640007734298706), (7, 1.6805336475372314), (8, 1.6442019939422607), (9, 1.648621916770935), (10, 1.5594587326049805)]
INFO flwr 2024-04-06 21:10:10,969 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0652), (1, 0.207), (2, 0.0974), (3, 0.3813), (4, 0.5131), (5, 0.6454), (6, 0.7099), (7, 0.7984), (8, 0.8419), (9, 0.8311), (10, 0.916)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.916
wandb:     loss 1.55946
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_210211-gketabyn
wandb: Find logs at: ./wandb/offline-run-20240406_210211-gketabyn/logs
INFO flwr 2024-04-06 21:10:14,601 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:17:34,396 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1385319)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1385319)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:17:39,064	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:17:39,498	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:17:39,831	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_03ab77399321b942.zip' (10.33MiB) to Ray cluster...
2024-04-06 21:17:39,859	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_03ab77399321b942.zip'.
INFO flwr 2024-04-06 21:17:50,768 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 61721445580.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 134016706356.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 21:17:50,768 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:17:50,768 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:17:50,782 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:17:50,782 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:17:50,783 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:17:50,783 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1396551)[0m 2024-04-06 21:17:56.503720: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1396551)[0m 2024-04-06 21:17:56.602101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1396551)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 21:17:58,589 | server.py:94 | initial parameters (loss, other metrics): 2.3027727603912354, {'accuracy': 0.1071, 'data_size': 10000}
INFO flwr 2024-04-06 21:17:58,589 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:17:58,589 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1396551)[0m 2024-04-06 21:17:58.723786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1396553)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1396553)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1396545)[0m 2024-04-06 21:17:57.175935: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1396545)[0m 2024-04-06 21:17:57.270532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1396545)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1396545)[0m 2024-04-06 21:17:59.417435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1396546)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1396546)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 21:18:14,092 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:18:16,883 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:18:20,758 | server.py:125 | fit progress: (1, 2.3024799823760986, {'accuracy': 0.0892, 'data_size': 10000}, 22.169251493003685)
INFO flwr 2024-04-06 21:18:20,759 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:18:20,759 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:18:30,043 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:18:42,769 | server.py:125 | fit progress: (2, 2.2844860553741455, {'accuracy': 0.117, 'data_size': 10000}, 44.1794960690022)
INFO flwr 2024-04-06 21:18:42,769 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:18:42,769 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:18:50,866 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:19:09,588 | server.py:125 | fit progress: (3, 2.235788583755493, {'accuracy': 0.2086, 'data_size': 10000}, 70.9991207900166)
INFO flwr 2024-04-06 21:19:09,589 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:19:09,589 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:19:18,170 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:19:42,815 | server.py:125 | fit progress: (4, 2.0781872272491455, {'accuracy': 0.4114, 'data_size': 10000}, 104.22632616802002)
INFO flwr 2024-04-06 21:19:42,816 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:19:42,816 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:19:50,917 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:20:22,150 | server.py:125 | fit progress: (5, 1.891313910484314, {'accuracy': 0.6044, 'data_size': 10000}, 143.56042009202065)
INFO flwr 2024-04-06 21:20:22,150 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:20:22,150 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:20:30,158 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:21:09,374 | server.py:125 | fit progress: (6, 1.7333840131759644, {'accuracy': 0.7645, 'data_size': 10000}, 190.7853230580222)
INFO flwr 2024-04-06 21:21:09,375 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:21:09,375 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:21:17,377 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:12,293 | server.py:125 | fit progress: (7, 1.7117760181427002, {'accuracy': 0.7698, 'data_size': 10000}, 253.70373714499874)
INFO flwr 2024-04-06 21:22:12,293 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:12,293 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:20,496 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:23:17,516 | server.py:125 | fit progress: (8, 1.5941827297210693, {'accuracy': 0.891, 'data_size': 10000}, 318.9267452589993)
INFO flwr 2024-04-06 21:23:17,516 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:23:17,517 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:23:25,730 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:24:16,597 | server.py:125 | fit progress: (9, 1.5615836381912231, {'accuracy': 0.9163, 'data_size': 10000}, 378.00772784100263)
INFO flwr 2024-04-06 21:24:16,597 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:24:16,598 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:24:25,213 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:25:34,158 | server.py:125 | fit progress: (10, 1.5492897033691406, {'accuracy': 0.9242, 'data_size': 10000}, 455.5685340500204)
INFO flwr 2024-04-06 21:25:34,158 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:25:34,158 | server.py:153 | FL finished in 455.5689329779998
INFO flwr 2024-04-06 21:25:34,158 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:25:34,158 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:25:34,158 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:25:34,158 | app.py:229 | app_fit: losses_centralized [(0, 2.3027727603912354), (1, 2.3024799823760986), (2, 2.2844860553741455), (3, 2.235788583755493), (4, 2.0781872272491455), (5, 1.891313910484314), (6, 1.7333840131759644), (7, 1.7117760181427002), (8, 1.5941827297210693), (9, 1.5615836381912231), (10, 1.5492897033691406)]
INFO flwr 2024-04-06 21:25:34,159 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1071), (1, 0.0892), (2, 0.117), (3, 0.2086), (4, 0.4114), (5, 0.6044), (6, 0.7645), (7, 0.7698), (8, 0.891), (9, 0.9163), (10, 0.9242)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9242
wandb:     loss 1.54929
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_211734-6h8pmehu
wandb: Find logs at: ./wandb/offline-run-20240406_211734-6h8pmehu/logs
INFO flwr 2024-04-06 21:25:37,793 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:32:58,922 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1396543)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1396543)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 21:33:03,601	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:33:03,976	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:33:04,420	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_21116c210c781f8c.zip' (10.37MiB) to Ray cluster...
2024-04-06 21:33:04,449	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_21116c210c781f8c.zip'.
INFO flwr 2024-04-06 21:33:15,280 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 133562309632.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61526704128.0}
INFO flwr 2024-04-06 21:33:15,280 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:33:15,280 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:33:15,293 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:33:15,294 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:33:15,294 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:33:15,294 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1408186)[0m 2024-04-06 21:33:21.338724: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1408186)[0m 2024-04-06 21:33:21.437996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1408186)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 21:33:22,386 | server.py:94 | initial parameters (loss, other metrics): 2.3026084899902344, {'accuracy': 0.111, 'data_size': 10000}
INFO flwr 2024-04-06 21:33:22,386 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:33:22,387 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1408196)[0m 2024-04-06 21:33:23.530060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1408189)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1408189)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1408185)[0m 2024-04-06 21:33:21.630714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1408185)[0m 2024-04-06 21:33:21.724790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1408185)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1408185)[0m 2024-04-06 21:33:23.833844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1408184)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1408184)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:33:39,495 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:33:42,710 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:33:46,651 | server.py:125 | fit progress: (1, 2.2794289588928223, {'accuracy': 0.1824, 'data_size': 10000}, 24.264255628018873)
INFO flwr 2024-04-06 21:33:46,651 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:33:46,651 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:33:55,679 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:34:09,576 | server.py:125 | fit progress: (2, 2.176586389541626, {'accuracy': 0.3353, 'data_size': 10000}, 47.19010001400602)
INFO flwr 2024-04-06 21:34:09,577 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:34:09,577 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:34:17,736 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:34:36,536 | server.py:125 | fit progress: (3, 2.0360467433929443, {'accuracy': 0.4905, 'data_size': 10000}, 74.14977652201196)
INFO flwr 2024-04-06 21:34:36,537 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:34:36,537 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:34:44,962 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:35:09,534 | server.py:125 | fit progress: (4, 1.9144483804702759, {'accuracy': 0.5443, 'data_size': 10000}, 107.14716406300431)
INFO flwr 2024-04-06 21:35:09,534 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:35:09,534 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:35:17,852 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:35:48,278 | server.py:125 | fit progress: (5, 1.7066853046417236, {'accuracy': 0.7669, 'data_size': 10000}, 145.89183196402155)
INFO flwr 2024-04-06 21:35:48,279 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:35:48,279 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:35:56,851 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:36:31,143 | server.py:125 | fit progress: (6, 1.717071294784546, {'accuracy': 0.7484, 'data_size': 10000}, 188.75685910801985)
INFO flwr 2024-04-06 21:36:31,144 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:36:31,144 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:36:41,000 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:37:33,462 | server.py:125 | fit progress: (7, 1.5720778703689575, {'accuracy': 0.9054, 'data_size': 10000}, 251.0756133950199)
INFO flwr 2024-04-06 21:37:33,463 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:37:33,463 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:37:42,415 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:38:39,149 | server.py:125 | fit progress: (8, 1.5426808595657349, {'accuracy': 0.9291, 'data_size': 10000}, 316.7629694160132)
INFO flwr 2024-04-06 21:38:39,150 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:38:39,150 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:38:47,824 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:39:40,354 | server.py:125 | fit progress: (9, 1.539688229560852, {'accuracy': 0.9314, 'data_size': 10000}, 377.96755152000696)
INFO flwr 2024-04-06 21:39:40,354 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:39:40,354 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:39:49,124 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:03,672 | server.py:125 | fit progress: (10, 1.5864520072937012, {'accuracy': 0.881, 'data_size': 10000}, 461.28542294100043)
INFO flwr 2024-04-06 21:41:03,673 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:41:03,673 | server.py:153 | FL finished in 461.28641153901117
INFO flwr 2024-04-06 21:41:03,673 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:41:03,673 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:41:03,673 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:41:03,673 | app.py:229 | app_fit: losses_centralized [(0, 2.3026084899902344), (1, 2.2794289588928223), (2, 2.176586389541626), (3, 2.0360467433929443), (4, 1.9144483804702759), (5, 1.7066853046417236), (6, 1.717071294784546), (7, 1.5720778703689575), (8, 1.5426808595657349), (9, 1.539688229560852), (10, 1.5864520072937012)]
INFO flwr 2024-04-06 21:41:03,673 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.111), (1, 0.1824), (2, 0.3353), (3, 0.4905), (4, 0.5443), (5, 0.7669), (6, 0.7484), (7, 0.9054), (8, 0.9291), (9, 0.9314), (10, 0.881)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.881
wandb:     loss 1.58645
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_213258-qc8eosm3
wandb: Find logs at: ./wandb/offline-run-20240406_213258-qc8eosm3/logs
INFO flwr 2024-04-06 21:41:07,363 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:48:27,787 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 21:48:33,794	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:48:34,149	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:48:34,472	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b27eb9a2dc747218.zip' (10.39MiB) to Ray cluster...
2024-04-06 21:48:34,499	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b27eb9a2dc747218.zip'.
INFO flwr 2024-04-06 21:48:45,388 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 133378906317.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 61448102707.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 21:48:45,388 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:48:45,388 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:48:45,408 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:48:45,409 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:48:45,409 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:48:45,409 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1416293)[0m 2024-04-06 21:48:51.528965: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1416293)[0m 2024-04-06 21:48:51.626452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1416293)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 21:48:52,655 | server.py:94 | initial parameters (loss, other metrics): 2.3024377822875977, {'accuracy': 0.108, 'data_size': 10000}
INFO flwr 2024-04-06 21:48:52,655 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:48:52,656 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1416303)[0m 2024-04-06 21:48:53.731765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1416303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1416303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1416304)[0m 2024-04-06 21:48:51.819326: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1416304)[0m 2024-04-06 21:48:51.919944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1416304)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1416304)[0m 2024-04-06 21:48:53.811324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1416290)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1416290)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:49:09,037 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:49:12,138 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:49:16,083 | server.py:125 | fit progress: (1, 2.302307367324829, {'accuracy': 0.1083, 'data_size': 10000}, 23.427100109984167)
INFO flwr 2024-04-06 21:49:16,083 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:49:16,083 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:49:25,552 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:49:38,607 | server.py:125 | fit progress: (2, 2.3021743297576904, {'accuracy': 0.1107, 'data_size': 10000}, 45.95153082499746)
INFO flwr 2024-04-06 21:49:38,608 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:49:38,608 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:49:47,347 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:06,342 | server.py:125 | fit progress: (3, 2.302069664001465, {'accuracy': 0.1294, 'data_size': 10000}, 73.68646166700637)
INFO flwr 2024-04-06 21:50:06,342 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:06,342 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:14,617 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:39,294 | server.py:125 | fit progress: (4, 2.301924705505371, {'accuracy': 0.1816, 'data_size': 10000}, 106.63800872699358)
INFO flwr 2024-04-06 21:50:39,294 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:39,294 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:48,347 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:51:18,845 | server.py:125 | fit progress: (5, 2.3017961978912354, {'accuracy': 0.2105, 'data_size': 10000}, 146.1894873160054)
INFO flwr 2024-04-06 21:51:18,845 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:51:18,846 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:51:27,835 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:52:02,162 | server.py:125 | fit progress: (6, 2.3016607761383057, {'accuracy': 0.208, 'data_size': 10000}, 189.5059155570052)
INFO flwr 2024-04-06 21:52:02,162 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:52:02,162 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:52:11,256 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:53:00,138 | server.py:125 | fit progress: (7, 2.301532506942749, {'accuracy': 0.2762, 'data_size': 10000}, 247.4823823290062)
INFO flwr 2024-04-06 21:53:00,138 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:53:00,139 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:53:08,968 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:53:54,694 | server.py:125 | fit progress: (8, 2.3014206886291504, {'accuracy': 0.2744, 'data_size': 10000}, 302.03880530499737)
INFO flwr 2024-04-06 21:53:54,695 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:53:54,695 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:54:03,660 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:55:05,443 | server.py:125 | fit progress: (9, 2.3012876510620117, {'accuracy': 0.261, 'data_size': 10000}, 372.7872305479832)
INFO flwr 2024-04-06 21:55:05,443 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:55:05,443 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:55:14,005 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:56:22,873 | server.py:125 | fit progress: (10, 2.301158905029297, {'accuracy': 0.2675, 'data_size': 10000}, 450.21780434198445)
INFO flwr 2024-04-06 21:56:22,874 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:56:22,874 | server.py:153 | FL finished in 450.21824492298765
INFO flwr 2024-04-06 21:56:22,874 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:56:22,874 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:56:22,874 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:56:22,874 | app.py:229 | app_fit: losses_centralized [(0, 2.3024377822875977), (1, 2.302307367324829), (2, 2.3021743297576904), (3, 2.302069664001465), (4, 2.301924705505371), (5, 2.3017961978912354), (6, 2.3016607761383057), (7, 2.301532506942749), (8, 2.3014206886291504), (9, 2.3012876510620117), (10, 2.301158905029297)]
INFO flwr 2024-04-06 21:56:22,874 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.108), (1, 0.1083), (2, 0.1107), (3, 0.1294), (4, 0.1816), (5, 0.2105), (6, 0.208), (7, 0.2762), (8, 0.2744), (9, 0.261), (10, 0.2675)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2675
wandb:     loss 2.30116
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_214827-53h1f2qi
wandb: Find logs at: ./wandb/offline-run-20240406_214827-53h1f2qi/logs
INFO flwr 2024-04-06 21:56:26,615 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:03:47,596 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 22:03:52,387	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:03:52,800	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:03:53,237	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_be255df71b0ca65f.zip' (10.43MiB) to Ray cluster...
2024-04-06 22:03:53,270	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_be255df71b0ca65f.zip'.
INFO flwr 2024-04-06 22:04:04,061 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 132922617242.0, 'object_store_memory': 61252550246.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 22:04:04,061 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:04:04,061 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:04:04,077 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:04:04,078 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:04:04,078 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:04:04,078 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1427921)[0m 2024-04-06 22:04:09.947403: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1427921)[0m 2024-04-06 22:04:10.043472: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1427921)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1427916)[0m 2024-04-06 22:04:12.090702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 22:04:12,169 | server.py:94 | initial parameters (loss, other metrics): 2.3027498722076416, {'accuracy': 0.0763, 'data_size': 10000}
INFO flwr 2024-04-06 22:04:12,169 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:04:12,170 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1427921)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1427921)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1427919)[0m 2024-04-06 22:04:10.603337: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1427919)[0m 2024-04-06 22:04:10.733329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1427919)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1427919)[0m 2024-04-06 22:04:12.920406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1427910)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1427910)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 22:04:27,367 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:04:30,591 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:04:34,458 | server.py:125 | fit progress: (1, 2.278714179992676, {'accuracy': 0.1771, 'data_size': 10000}, 22.288635714998236)
INFO flwr 2024-04-06 22:04:34,458 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:04:34,459 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:04:43,731 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:04:56,330 | server.py:125 | fit progress: (2, 2.136488199234009, {'accuracy': 0.2981, 'data_size': 10000}, 44.160799572011456)
INFO flwr 2024-04-06 22:04:56,331 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:04:56,331 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:05:04,469 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:05:23,721 | server.py:125 | fit progress: (3, 1.8032277822494507, {'accuracy': 0.6867, 'data_size': 10000}, 71.55133100101375)
INFO flwr 2024-04-06 22:05:23,721 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:05:23,721 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:05:32,221 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:05:56,652 | server.py:125 | fit progress: (4, 1.7145341634750366, {'accuracy': 0.7399, 'data_size': 10000}, 104.4823359180009)
INFO flwr 2024-04-06 22:05:56,652 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:05:56,652 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:06:04,869 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:06:34,878 | server.py:125 | fit progress: (5, 1.6528977155685425, {'accuracy': 0.8111, 'data_size': 10000}, 142.70860769000137)
INFO flwr 2024-04-06 22:06:34,878 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:06:34,879 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:06:43,931 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:07:17,165 | server.py:125 | fit progress: (6, 1.5764822959899902, {'accuracy': 0.9005, 'data_size': 10000}, 184.9959897150111)
INFO flwr 2024-04-06 22:07:17,166 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:07:17,166 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:07:25,597 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:08:14,222 | server.py:125 | fit progress: (7, 1.5496574640274048, {'accuracy': 0.9225, 'data_size': 10000}, 242.05227184301475)
INFO flwr 2024-04-06 22:08:14,222 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:14,222 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:08:22,728 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:21,054 | server.py:125 | fit progress: (8, 1.5383520126342773, {'accuracy': 0.9307, 'data_size': 10000}, 308.8850168380013)
INFO flwr 2024-04-06 22:09:21,055 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:09:21,055 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:30,546 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:10:29,678 | server.py:125 | fit progress: (9, 1.5372604131698608, {'accuracy': 0.9306, 'data_size': 10000}, 377.50885719500366)
INFO flwr 2024-04-06 22:10:29,679 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:10:29,679 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:10:38,526 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:11:43,632 | server.py:125 | fit progress: (10, 1.5305968523025513, {'accuracy': 0.9351, 'data_size': 10000}, 451.4625822629896)
INFO flwr 2024-04-06 22:11:43,632 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:11:43,633 | server.py:153 | FL finished in 451.4630558670033
INFO flwr 2024-04-06 22:11:43,633 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:11:43,633 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:11:43,633 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:11:43,633 | app.py:229 | app_fit: losses_centralized [(0, 2.3027498722076416), (1, 2.278714179992676), (2, 2.136488199234009), (3, 1.8032277822494507), (4, 1.7145341634750366), (5, 1.6528977155685425), (6, 1.5764822959899902), (7, 1.5496574640274048), (8, 1.5383520126342773), (9, 1.5372604131698608), (10, 1.5305968523025513)]
INFO flwr 2024-04-06 22:11:43,633 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0763), (1, 0.1771), (2, 0.2981), (3, 0.6867), (4, 0.7399), (5, 0.8111), (6, 0.9005), (7, 0.9225), (8, 0.9307), (9, 0.9306), (10, 0.9351)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9351
wandb:     loss 1.5306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_220347-nyqhde68
wandb: Find logs at: ./wandb/offline-run-20240406_220347-nyqhde68/logs
INFO flwr 2024-04-06 22:11:47,272 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:19:07,868 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1427908)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1427908)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 22:19:14,745	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:19:15,129	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:19:15,506	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_eb4d5fc19d775e7b.zip' (10.46MiB) to Ray cluster...
2024-04-06 22:19:15,545	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_eb4d5fc19d775e7b.zip'.
INFO flwr 2024-04-06 22:19:26,501 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 60986346700.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 132301475636.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 22:19:26,502 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:19:26,502 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:19:26,517 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:19:26,518 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:19:26,518 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:19:26,518 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1439114)[0m 2024-04-06 22:19:32.362349: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1439114)[0m 2024-04-06 22:19:32.458129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1439114)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 22:19:34,171 | server.py:94 | initial parameters (loss, other metrics): 2.3026323318481445, {'accuracy': 0.104, 'data_size': 10000}
INFO flwr 2024-04-06 22:19:34,171 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:19:34,172 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1439114)[0m 2024-04-06 22:19:34.586592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1439120)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1439120)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1439121)[0m 2024-04-06 22:19:32.991395: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1439121)[0m 2024-04-06 22:19:33.079424: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1439121)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1439121)[0m 2024-04-06 22:19:35.460295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1439104)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1439104)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:19:49,961 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:19:53,146 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:19:57,019 | server.py:125 | fit progress: (1, 2.245948314666748, {'accuracy': 0.343, 'data_size': 10000}, 22.848085025994806)
INFO flwr 2024-04-06 22:19:57,020 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:19:57,020 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:20:05,992 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:20:18,952 | server.py:125 | fit progress: (2, 1.886502742767334, {'accuracy': 0.6095, 'data_size': 10000}, 44.78103737899801)
INFO flwr 2024-04-06 22:20:18,953 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:20:18,953 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:20:27,188 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:20:47,416 | server.py:125 | fit progress: (3, 1.6767935752868652, {'accuracy': 0.8015, 'data_size': 10000}, 73.2440676719998)
INFO flwr 2024-04-06 22:20:47,416 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:20:47,416 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:20:56,032 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:21:24,399 | server.py:125 | fit progress: (4, 1.5462828874588013, {'accuracy': 0.9303, 'data_size': 10000}, 110.22769112198148)
INFO flwr 2024-04-06 22:21:24,399 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:21:24,400 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:21:32,846 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:22:08,072 | server.py:125 | fit progress: (5, 1.5343232154846191, {'accuracy': 0.9337, 'data_size': 10000}, 153.90057734097354)
INFO flwr 2024-04-06 22:22:08,072 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:22:08,072 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:22:17,312 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:22:59,903 | server.py:125 | fit progress: (6, 1.5211068391799927, {'accuracy': 0.9443, 'data_size': 10000}, 205.73204178997548)
INFO flwr 2024-04-06 22:22:59,904 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:22:59,904 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:23:08,804 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:24:01,794 | server.py:125 | fit progress: (7, 1.5398370027542114, {'accuracy': 0.9257, 'data_size': 10000}, 267.6229188449797)
INFO flwr 2024-04-06 22:24:01,795 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:24:01,795 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:24:10,280 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:25:12,607 | server.py:125 | fit progress: (8, 1.5120608806610107, {'accuracy': 0.9549, 'data_size': 10000}, 338.4355949739984)
INFO flwr 2024-04-06 22:25:12,607 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:25:12,608 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:25:20,896 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:26:30,518 | server.py:125 | fit progress: (9, 1.5076206922531128, {'accuracy': 0.9575, 'data_size': 10000}, 416.34627278597327)
INFO flwr 2024-04-06 22:26:30,518 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:26:30,518 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:26:39,468 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:28:03,758 | server.py:125 | fit progress: (10, 1.5028899908065796, {'accuracy': 0.9615, 'data_size': 10000}, 509.5866845189885)
INFO flwr 2024-04-06 22:28:03,758 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:28:03,759 | server.py:153 | FL finished in 509.58714995899936
INFO flwr 2024-04-06 22:28:03,759 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:28:03,759 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:28:03,759 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:28:03,759 | app.py:229 | app_fit: losses_centralized [(0, 2.3026323318481445), (1, 2.245948314666748), (2, 1.886502742767334), (3, 1.6767935752868652), (4, 1.5462828874588013), (5, 1.5343232154846191), (6, 1.5211068391799927), (7, 1.5398370027542114), (8, 1.5120608806610107), (9, 1.5076206922531128), (10, 1.5028899908065796)]
INFO flwr 2024-04-06 22:28:03,759 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.104), (1, 0.343), (2, 0.6095), (3, 0.8015), (4, 0.9303), (5, 0.9337), (6, 0.9443), (7, 0.9257), (8, 0.9549), (9, 0.9575), (10, 0.9615)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9615
wandb:     loss 1.50289
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_221907-fmkg9gdk
wandb: Find logs at: ./wandb/offline-run-20240406_221907-fmkg9gdk/logs
INFO flwr 2024-04-06 22:28:07,365 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:35:27,922 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 22:35:37,590	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:35:39,121	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:35:39,497	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_419d7eb84c8afc0c.zip' (10.49MiB) to Ray cluster...
2024-04-06 22:35:39,526	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_419d7eb84c8afc0c.zip'.
INFO flwr 2024-04-06 22:35:50,853 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62851499212.0, 'CPU': 64.0, 'memory': 136653498164.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 22:35:50,854 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:35:50,854 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:35:50,872 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:35:50,872 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:35:50,873 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:35:50,873 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1450270)[0m 2024-04-06 22:35:57.015612: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1450270)[0m 2024-04-06 22:35:57.117192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1450270)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 22:35:58,726 | server.py:94 | initial parameters (loss, other metrics): 2.3024113178253174, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-06 22:35:58,727 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:35:58,727 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1450263)[0m 2024-04-06 22:35:59.316512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1450266)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1450266)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1450265)[0m 2024-04-06 22:35:57.564113: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1450265)[0m 2024-04-06 22:35:57.663628: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1450265)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1450265)[0m 2024-04-06 22:35:59.880385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1450261)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1450261)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 22:36:14,699 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:36:18,313 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:36:22,183 | server.py:125 | fit progress: (1, 2.118515729904175, {'accuracy': 0.4652, 'data_size': 10000}, 23.45601438402082)
INFO flwr 2024-04-06 22:36:22,183 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:22,184 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:31,502 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:46,274 | server.py:125 | fit progress: (2, 1.7718055248260498, {'accuracy': 0.7209, 'data_size': 10000}, 47.54707587201847)
INFO flwr 2024-04-06 22:36:46,275 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:46,275 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:55,185 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:16,799 | server.py:125 | fit progress: (3, 1.5543334484100342, {'accuracy': 0.9207, 'data_size': 10000}, 78.07145271400805)
INFO flwr 2024-04-06 22:37:16,799 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:37:16,799 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:37:25,454 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:53,881 | server.py:125 | fit progress: (4, 1.5658867359161377, {'accuracy': 0.9002, 'data_size': 10000}, 115.15382632499677)
INFO flwr 2024-04-06 22:37:53,881 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:37:53,882 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:38:02,327 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:38:38,781 | server.py:125 | fit progress: (5, 1.5307036638259888, {'accuracy': 0.9357, 'data_size': 10000}, 160.0543721690192)
INFO flwr 2024-04-06 22:38:38,782 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:38:38,782 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:38:47,143 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:39:29,939 | server.py:125 | fit progress: (6, 1.5128974914550781, {'accuracy': 0.9514, 'data_size': 10000}, 211.2122191490198)
INFO flwr 2024-04-06 22:39:29,940 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:39:29,940 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:39:38,560 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:40:36,131 | server.py:125 | fit progress: (7, 1.5083425045013428, {'accuracy': 0.9557, 'data_size': 10000}, 277.40350038101315)
INFO flwr 2024-04-06 22:40:36,131 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:40:36,131 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:40:44,786 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:41:40,485 | server.py:125 | fit progress: (8, 1.502690315246582, {'accuracy': 0.9603, 'data_size': 10000}, 341.7573839930119)
INFO flwr 2024-04-06 22:41:40,485 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:41:40,485 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:41:48,919 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:43:17,024 | server.py:125 | fit progress: (9, 1.5046069622039795, {'accuracy': 0.9592, 'data_size': 10000}, 438.2964816180174)
INFO flwr 2024-04-06 22:43:17,024 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:43:17,025 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:43:26,032 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:44:36,570 | server.py:125 | fit progress: (10, 1.4949740171432495, {'accuracy': 0.9681, 'data_size': 10000}, 517.8433553040086)
INFO flwr 2024-04-06 22:44:36,571 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:44:36,571 | server.py:153 | FL finished in 517.843934338016
INFO flwr 2024-04-06 22:44:36,571 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:44:36,571 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:44:36,572 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:44:36,572 | app.py:229 | app_fit: losses_centralized [(0, 2.3024113178253174), (1, 2.118515729904175), (2, 1.7718055248260498), (3, 1.5543334484100342), (4, 1.5658867359161377), (5, 1.5307036638259888), (6, 1.5128974914550781), (7, 1.5083425045013428), (8, 1.502690315246582), (9, 1.5046069622039795), (10, 1.4949740171432495)]
INFO flwr 2024-04-06 22:44:36,572 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.4652), (2, 0.7209), (3, 0.9207), (4, 0.9002), (5, 0.9357), (6, 0.9514), (7, 0.9557), (8, 0.9603), (9, 0.9592), (10, 0.9681)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9681
wandb:     loss 1.49497
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_223527-go4xatwe
wandb: Find logs at: ./wandb/offline-run-20240406_223527-go4xatwe/logs
INFO flwr 2024-04-06 22:44:40,189 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:52:05,594 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1450260)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1450260)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 22:52:11,397	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:52:11,781	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:52:12,190	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ba32e2378b86ad93.zip' (10.52MiB) to Ray cluster...
2024-04-06 22:52:12,245	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ba32e2378b86ad93.zip'.
INFO flwr 2024-04-06 22:52:23,272 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 131389009306.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 60595289702.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 22:52:23,272 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:52:23,272 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:52:23,289 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:52:23,290 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:52:23,290 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:52:23,290 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1459350)[0m 2024-04-06 22:52:28.422117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1459350)[0m 2024-04-06 22:52:28.520310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1459350)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1459350)[0m 2024-04-06 22:52:31.235246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 22:52:31,983 | server.py:94 | initial parameters (loss, other metrics): 2.3027963638305664, {'accuracy': 0.1045, 'data_size': 10000}
INFO flwr 2024-04-06 22:52:31,983 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:52:31,983 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1459354)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1459354)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1459349)[0m 2024-04-06 22:52:29.961919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1459349)[0m 2024-04-06 22:52:30.056582: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1459349)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1459351)[0m 2024-04-06 22:52:32.225629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1459346)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1459346)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-06 22:52:49,310 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:52:52,874 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:52:57,178 | server.py:125 | fit progress: (1, 2.231975793838501, {'accuracy': 0.2544, 'data_size': 10000}, 25.195259080006508)
INFO flwr 2024-04-06 22:52:57,179 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:52:57,179 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:53:07,916 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:53:22,787 | server.py:125 | fit progress: (2, 1.7493019104003906, {'accuracy': 0.7742, 'data_size': 10000}, 50.80349466201733)
INFO flwr 2024-04-06 22:53:22,787 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:53:22,788 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:53:32,543 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:53:54,360 | server.py:125 | fit progress: (3, 1.5442919731140137, {'accuracy': 0.9314, 'data_size': 10000}, 82.37648959099897)
INFO flwr 2024-04-06 22:53:54,360 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:53:54,360 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:54:04,002 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:54:32,191 | server.py:125 | fit progress: (4, 1.5233672857284546, {'accuracy': 0.9436, 'data_size': 10000}, 120.20797048899112)
INFO flwr 2024-04-06 22:54:32,192 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:54:32,192 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:54:41,509 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:18,474 | server.py:125 | fit progress: (5, 1.5352336168289185, {'accuracy': 0.9306, 'data_size': 10000}, 166.4910979430133)
INFO flwr 2024-04-06 22:55:18,475 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:18,475 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:28,466 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:56:05,374 | server.py:125 | fit progress: (6, 1.5122379064559937, {'accuracy': 0.9517, 'data_size': 10000}, 213.39097758900607)
INFO flwr 2024-04-06 22:56:05,375 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:56:05,375 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:56:15,939 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:56:54,961 | server.py:125 | fit progress: (7, 1.5138694047927856, {'accuracy': 0.9485, 'data_size': 10000}, 262.9779789370077)
INFO flwr 2024-04-06 22:56:54,962 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:56:54,962 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:57:04,591 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:57:59,161 | server.py:125 | fit progress: (8, 1.5004853010177612, {'accuracy': 0.962, 'data_size': 10000}, 327.17763261901564)
INFO flwr 2024-04-06 22:57:59,161 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:57:59,162 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:58:08,758 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:59:10,716 | server.py:125 | fit progress: (9, 1.4947164058685303, {'accuracy': 0.9677, 'data_size': 10000}, 398.73261085900594)
INFO flwr 2024-04-06 22:59:10,716 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:59:10,717 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:59:20,613 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:00:26,389 | server.py:125 | fit progress: (10, 1.494605302810669, {'accuracy': 0.9679, 'data_size': 10000}, 474.40611766601796)
INFO flwr 2024-04-06 23:00:26,390 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:00:26,390 | server.py:153 | FL finished in 474.4066612650058
INFO flwr 2024-04-06 23:00:26,393 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:00:26,394 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:00:26,394 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:00:26,394 | app.py:229 | app_fit: losses_centralized [(0, 2.3027963638305664), (1, 2.231975793838501), (2, 1.7493019104003906), (3, 1.5442919731140137), (4, 1.5233672857284546), (5, 1.5352336168289185), (6, 1.5122379064559937), (7, 1.5138694047927856), (8, 1.5004853010177612), (9, 1.4947164058685303), (10, 1.494605302810669)]
INFO flwr 2024-04-06 23:00:26,394 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1045), (1, 0.2544), (2, 0.7742), (3, 0.9314), (4, 0.9436), (5, 0.9306), (6, 0.9517), (7, 0.9485), (8, 0.962), (9, 0.9677), (10, 0.9679)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9679
wandb:     loss 1.49461
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_225205-9yyh0lpa
wandb: Find logs at: ./wandb/offline-run-20240406_225205-9yyh0lpa/logs
INFO flwr 2024-04-06 23:00:30,013 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:07:53,012 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1459343)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1459343)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-06 23:07:58,134	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:07:58,659	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:07:59,127	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2170f1ad08ae43c0.zip' (10.56MiB) to Ray cluster...
2024-04-06 23:07:59,154	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2170f1ad08ae43c0.zip'.
INFO flwr 2024-04-06 23:08:10,034 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 61493353267.0, 'memory': 133484490957.0, 'CPU': 64.0}
INFO flwr 2024-04-06 23:08:10,034 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:08:10,035 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:08:10,053 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:08:10,054 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:08:10,054 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:08:10,054 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1471077)[0m 2024-04-06 23:08:16.107360: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1471083)[0m 2024-04-06 23:08:16.161986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1471083)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 23:08:18,205 | server.py:94 | initial parameters (loss, other metrics): 2.3027377128601074, {'accuracy': 0.0842, 'data_size': 10000}
INFO flwr 2024-04-06 23:08:18,205 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:08:18,206 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1471080)[0m 2024-04-06 23:08:18.493776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1471083)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1471083)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1471079)[0m 2024-04-06 23:08:16.171783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1471080)[0m 2024-04-06 23:08:16.356262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1471080)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1471084)[0m 2024-04-06 23:08:18.526765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:08:38,480 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:08:41,151 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:08:45,009 | server.py:125 | fit progress: (1, 1.9948699474334717, {'accuracy': 0.5914, 'data_size': 10000}, 26.803167812991887)
INFO flwr 2024-04-06 23:08:45,009 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:08:45,009 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:08:54,537 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:09:07,819 | server.py:125 | fit progress: (2, 1.6315727233886719, {'accuracy': 0.8466, 'data_size': 10000}, 49.61390378201031)
INFO flwr 2024-04-06 23:09:07,820 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:09:07,820 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:09:16,445 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:09:35,482 | server.py:125 | fit progress: (3, 1.546759843826294, {'accuracy': 0.9224, 'data_size': 10000}, 77.27682614300284)
INFO flwr 2024-04-06 23:09:35,483 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:09:35,483 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:09:44,295 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:10:09,143 | server.py:125 | fit progress: (4, 1.514037013053894, {'accuracy': 0.9497, 'data_size': 10000}, 110.93779348698445)
INFO flwr 2024-04-06 23:10:09,144 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:10:09,144 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:10:17,371 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:10:47,623 | server.py:125 | fit progress: (5, 1.5421619415283203, {'accuracy': 0.92, 'data_size': 10000}, 149.41787534099421)
INFO flwr 2024-04-06 23:10:47,624 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:10:47,624 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:10:56,200 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:11:31,145 | server.py:125 | fit progress: (6, 1.51181161403656, {'accuracy': 0.9511, 'data_size': 10000}, 192.93964104898623)
INFO flwr 2024-04-06 23:11:31,145 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:11:31,146 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:11:40,017 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:12:29,178 | server.py:125 | fit progress: (7, 1.501910924911499, {'accuracy': 0.9606, 'data_size': 10000}, 250.97270884699537)
INFO flwr 2024-04-06 23:12:29,178 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:12:29,179 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:12:37,569 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:13:36,689 | server.py:125 | fit progress: (8, 1.4969645738601685, {'accuracy': 0.9658, 'data_size': 10000}, 318.48369160399307)
INFO flwr 2024-04-06 23:13:36,689 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:13:36,690 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:13:46,156 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:44,170 | server.py:125 | fit progress: (9, 1.5008082389831543, {'accuracy': 0.9626, 'data_size': 10000}, 385.96487237198744)
INFO flwr 2024-04-06 23:14:44,171 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:44,171 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:53,325 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:15:53,771 | server.py:125 | fit progress: (10, 1.4949955940246582, {'accuracy': 0.9674, 'data_size': 10000}, 455.5656646490097)
INFO flwr 2024-04-06 23:15:53,771 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:15:53,772 | server.py:153 | FL finished in 455.56617018199177
INFO flwr 2024-04-06 23:15:53,772 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:15:53,772 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:15:53,772 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:15:53,772 | app.py:229 | app_fit: losses_centralized [(0, 2.3027377128601074), (1, 1.9948699474334717), (2, 1.6315727233886719), (3, 1.546759843826294), (4, 1.514037013053894), (5, 1.5421619415283203), (6, 1.51181161403656), (7, 1.501910924911499), (8, 1.4969645738601685), (9, 1.5008082389831543), (10, 1.4949955940246582)]
INFO flwr 2024-04-06 23:15:53,772 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0842), (1, 0.5914), (2, 0.8466), (3, 0.9224), (4, 0.9497), (5, 0.92), (6, 0.9511), (7, 0.9606), (8, 0.9658), (9, 0.9626), (10, 0.9674)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9674
wandb:     loss 1.495
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_230752-ixegcop9
wandb: Find logs at: ./wandb/offline-run-20240406_230752-ixegcop9/logs
INFO flwr 2024-04-06 23:15:57,353 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:23:18,487 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1471077)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1471077)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:23:23,304	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:23:23,671	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:23:24,133	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_93dc1a40cf1730ef.zip' (10.59MiB) to Ray cluster...
2024-04-06 23:23:24,162	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_93dc1a40cf1730ef.zip'.
INFO flwr 2024-04-06 23:23:35,336 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60871136870.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 132032652698.0}
INFO flwr 2024-04-06 23:23:35,336 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:23:35,336 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:23:35,350 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:23:35,351 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:23:35,351 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:23:35,351 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1482226)[0m 2024-04-06 23:23:41.419142: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1482226)[0m 2024-04-06 23:23:41.517720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1482226)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 23:23:42,906 | server.py:94 | initial parameters (loss, other metrics): 2.302664279937744, {'accuracy': 0.059, 'data_size': 10000}
INFO flwr 2024-04-06 23:23:42,907 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:23:42,907 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1482226)[0m 2024-04-06 23:23:43.565714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1482238)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1482238)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1482227)[0m 2024-04-06 23:23:42.013853: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1482227)[0m 2024-04-06 23:23:42.115958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1482227)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1482237)[0m 2024-04-06 23:23:44.109361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1482231)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1482231)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-06 23:23:58,980 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:24:02,248 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:24:06,162 | server.py:125 | fit progress: (1, 2.122573137283325, {'accuracy': 0.5921, 'data_size': 10000}, 23.255403602001024)
INFO flwr 2024-04-06 23:24:06,163 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:24:06,163 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:24:16,021 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:24:28,932 | server.py:125 | fit progress: (2, 1.6328022480010986, {'accuracy': 0.8507, 'data_size': 10000}, 46.02470302101574)
INFO flwr 2024-04-06 23:24:28,932 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:24:28,932 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:24:37,325 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:24:56,127 | server.py:125 | fit progress: (3, 1.5234543085098267, {'accuracy': 0.9432, 'data_size': 10000}, 73.21974603799754)
INFO flwr 2024-04-06 23:24:56,127 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:24:56,127 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:25:04,126 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:25:28,988 | server.py:125 | fit progress: (4, 1.5077170133590698, {'accuracy': 0.9547, 'data_size': 10000}, 106.08133857999928)
INFO flwr 2024-04-06 23:25:28,989 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:25:28,989 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:25:37,381 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:26:07,463 | server.py:125 | fit progress: (5, 1.5370421409606934, {'accuracy': 0.9263, 'data_size': 10000}, 144.55574443601654)
INFO flwr 2024-04-06 23:26:07,463 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:26:07,463 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:26:16,204 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:26:51,247 | server.py:125 | fit progress: (6, 1.5037428140640259, {'accuracy': 0.9592, 'data_size': 10000}, 188.33966281899484)
INFO flwr 2024-04-06 23:26:51,247 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:26:51,247 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:27:00,120 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:27:50,435 | server.py:125 | fit progress: (7, 1.494700312614441, {'accuracy': 0.9672, 'data_size': 10000}, 247.5281640040048)
INFO flwr 2024-04-06 23:27:50,435 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:27:50,436 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:27:59,793 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:28:46,180 | server.py:125 | fit progress: (8, 1.4954302310943604, {'accuracy': 0.966, 'data_size': 10000}, 303.2731348609959)
INFO flwr 2024-04-06 23:28:46,180 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:28:46,181 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:28:55,164 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:29:57,369 | server.py:125 | fit progress: (9, 1.4896421432495117, {'accuracy': 0.9719, 'data_size': 10000}, 374.4618280470022)
INFO flwr 2024-04-06 23:29:57,369 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:29:57,369 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:30:06,470 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:31:05,799 | server.py:125 | fit progress: (10, 1.4879660606384277, {'accuracy': 0.9743, 'data_size': 10000}, 442.8918829149916)
INFO flwr 2024-04-06 23:31:05,799 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:31:05,799 | server.py:153 | FL finished in 442.8922955300077
INFO flwr 2024-04-06 23:31:05,799 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:31:05,799 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:31:05,811 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:31:05,811 | app.py:229 | app_fit: losses_centralized [(0, 2.302664279937744), (1, 2.122573137283325), (2, 1.6328022480010986), (3, 1.5234543085098267), (4, 1.5077170133590698), (5, 1.5370421409606934), (6, 1.5037428140640259), (7, 1.494700312614441), (8, 1.4954302310943604), (9, 1.4896421432495117), (10, 1.4879660606384277)]
INFO flwr 2024-04-06 23:31:05,811 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.059), (1, 0.5921), (2, 0.8507), (3, 0.9432), (4, 0.9547), (5, 0.9263), (6, 0.9592), (7, 0.9672), (8, 0.966), (9, 0.9719), (10, 0.9743)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9743
wandb:     loss 1.48797
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_232318-di93swbn
wandb: Find logs at: ./wandb/offline-run-20240406_232318-di93swbn/logs
INFO flwr 2024-04-06 23:31:09,489 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:38:30,014 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1482226)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1482226)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-06 23:38:36,160	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:38:36,464	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:38:36,774	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_784ab6e0f10b7b99.zip' (10.62MiB) to Ray cluster...
2024-04-06 23:38:36,800	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_784ab6e0f10b7b99.zip'.
INFO flwr 2024-04-06 23:38:47,773 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 131111240704.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60476246016.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 23:38:47,773 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:38:47,773 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:38:47,790 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:38:47,791 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:38:47,792 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:38:47,792 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1490739)[0m 2024-04-06 23:38:53.787016: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1490739)[0m 2024-04-06 23:38:53.887638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1490739)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 23:38:55,772 | server.py:94 | initial parameters (loss, other metrics): 2.3026692867279053, {'accuracy': 0.0839, 'data_size': 10000}
INFO flwr 2024-04-06 23:38:55,773 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:38:55,773 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1490739)[0m 2024-04-06 23:38:55.911038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1490748)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1490748)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1490749)[0m 2024-04-06 23:38:54.034822: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1490749)[0m 2024-04-06 23:38:54.125776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1490749)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1490749)[0m 2024-04-06 23:38:56.278776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1490739)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1490739)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-06 23:39:11,907 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:39:15,114 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:39:19,007 | server.py:125 | fit progress: (1, 2.302480936050415, {'accuracy': 0.1362, 'data_size': 10000}, 23.233970258006593)
INFO flwr 2024-04-06 23:39:19,007 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:39:19,007 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:39:29,106 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:39:41,826 | server.py:125 | fit progress: (2, 2.302288055419922, {'accuracy': 0.1642, 'data_size': 10000}, 46.05289635900408)
INFO flwr 2024-04-06 23:39:41,826 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:39:41,826 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:39:50,536 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:40:09,334 | server.py:125 | fit progress: (3, 2.3021109104156494, {'accuracy': 0.1863, 'data_size': 10000}, 73.56093303201487)
INFO flwr 2024-04-06 23:40:09,334 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:40:09,334 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:40:18,494 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:40:43,024 | server.py:125 | fit progress: (4, 2.3019537925720215, {'accuracy': 0.1356, 'data_size': 10000}, 107.25133029301651)
INFO flwr 2024-04-06 23:40:43,025 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:40:43,025 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:40:52,784 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:41:23,916 | server.py:125 | fit progress: (5, 2.301774024963379, {'accuracy': 0.1655, 'data_size': 10000}, 148.14293774601538)
INFO flwr 2024-04-06 23:41:23,916 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:23,916 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:41:33,533 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:09,890 | server.py:125 | fit progress: (6, 2.301555871963501, {'accuracy': 0.1168, 'data_size': 10000}, 194.11666902300203)
INFO flwr 2024-04-06 23:42:09,890 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:42:09,890 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:19,328 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:43:08,826 | server.py:125 | fit progress: (7, 2.3013906478881836, {'accuracy': 0.1201, 'data_size': 10000}, 253.05274723400362)
INFO flwr 2024-04-06 23:43:08,826 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:43:08,826 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:43:18,456 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:44:04,486 | server.py:125 | fit progress: (8, 2.3011996746063232, {'accuracy': 0.1119, 'data_size': 10000}, 308.7129174080037)
INFO flwr 2024-04-06 23:44:04,486 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:44:04,486 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:44:13,590 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:45:16,230 | server.py:125 | fit progress: (9, 2.3009753227233887, {'accuracy': 0.1057, 'data_size': 10000}, 380.4568991929991)
INFO flwr 2024-04-06 23:45:16,230 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:45:16,230 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:45:25,246 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:46:35,563 | server.py:125 | fit progress: (10, 2.3007752895355225, {'accuracy': 0.1238, 'data_size': 10000}, 459.7899436519947)
INFO flwr 2024-04-06 23:46:35,563 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:46:35,563 | server.py:153 | FL finished in 459.79041794801014
INFO flwr 2024-04-06 23:46:35,563 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:46:35,564 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:46:35,564 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:46:35,564 | app.py:229 | app_fit: losses_centralized [(0, 2.3026692867279053), (1, 2.302480936050415), (2, 2.302288055419922), (3, 2.3021109104156494), (4, 2.3019537925720215), (5, 2.301774024963379), (6, 2.301555871963501), (7, 2.3013906478881836), (8, 2.3011996746063232), (9, 2.3009753227233887), (10, 2.3007752895355225)]
INFO flwr 2024-04-06 23:46:35,564 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0839), (1, 0.1362), (2, 0.1642), (3, 0.1863), (4, 0.1356), (5, 0.1655), (6, 0.1168), (7, 0.1201), (8, 0.1119), (9, 0.1057), (10, 0.1238)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1238
wandb:     loss 2.30078
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_233829-iy8bogcf
wandb: Find logs at: ./wandb/offline-run-20240406_233829-iy8bogcf/logs
INFO flwr 2024-04-06 23:46:39,257 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:53:59,872 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1490738)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1490738)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-06 23:54:05,875	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:54:06,211	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:54:06,550	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_be53474a8422e53e.zip' (10.66MiB) to Ray cluster...
2024-04-06 23:54:06,579	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_be53474a8422e53e.zip'.
INFO flwr 2024-04-06 23:54:17,433 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 130716937626.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60307258982.0}
INFO flwr 2024-04-06 23:54:17,434 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:54:17,434 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:54:17,449 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:54:17,450 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:54:17,450 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:54:17,450 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1502054)[0m 2024-04-06 23:54:23.205331: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1502054)[0m 2024-04-06 23:54:23.301338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1502054)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-06 23:54:25,223 | server.py:94 | initial parameters (loss, other metrics): 2.302257537841797, {'accuracy': 0.1421, 'data_size': 10000}
INFO flwr 2024-04-06 23:54:25,223 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:54:25,223 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1502054)[0m 2024-04-06 23:54:25.505708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1502063)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1502063)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1502057)[0m 2024-04-06 23:54:24.098048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1502057)[0m 2024-04-06 23:54:24.189511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1502057)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1502057)[0m 2024-04-06 23:54:26.330528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1502053)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1502053)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:54:41,076 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:54:44,259 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:54:48,102 | server.py:125 | fit progress: (1, 2.21883225440979, {'accuracy': 0.2006, 'data_size': 10000}, 22.8789695299929)
INFO flwr 2024-04-06 23:54:48,103 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:54:48,103 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:54:58,008 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:55:10,710 | server.py:125 | fit progress: (2, 1.9586902856826782, {'accuracy': 0.4841, 'data_size': 10000}, 45.48655208398122)
INFO flwr 2024-04-06 23:55:10,710 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:55:10,711 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:55:19,654 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:55:38,456 | server.py:125 | fit progress: (3, 1.5902109146118164, {'accuracy': 0.8969, 'data_size': 10000}, 73.23300896998262)
INFO flwr 2024-04-06 23:55:38,457 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:55:38,457 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:55:47,726 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:56:12,210 | server.py:125 | fit progress: (4, 1.5475157499313354, {'accuracy': 0.9235, 'data_size': 10000}, 106.98719816099037)
INFO flwr 2024-04-06 23:56:12,211 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:56:12,211 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:56:21,117 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:56:51,583 | server.py:125 | fit progress: (5, 1.5311282873153687, {'accuracy': 0.9378, 'data_size': 10000}, 146.35951729299268)
INFO flwr 2024-04-06 23:56:51,583 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:56:51,583 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:57:00,798 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:57:35,113 | server.py:125 | fit progress: (6, 1.5218685865402222, {'accuracy': 0.9443, 'data_size': 10000}, 189.88996057698387)
INFO flwr 2024-04-06 23:57:35,114 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:57:35,114 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:57:45,191 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:58:33,889 | server.py:125 | fit progress: (7, 1.519264817237854, {'accuracy': 0.9461, 'data_size': 10000}, 248.66550351097248)
INFO flwr 2024-04-06 23:58:33,889 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:58:33,889 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:58:42,896 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:59:38,646 | server.py:125 | fit progress: (8, 1.5123854875564575, {'accuracy': 0.9515, 'data_size': 10000}, 313.4229862589855)
INFO flwr 2024-04-06 23:59:38,647 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:59:38,647 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:59:48,236 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:51,332 | server.py:125 | fit progress: (9, 1.5119097232818604, {'accuracy': 0.9515, 'data_size': 10000}, 386.1091684619896)
INFO flwr 2024-04-07 00:00:51,333 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:51,333 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:01:00,591 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:01:55,674 | server.py:125 | fit progress: (10, 1.5060930252075195, {'accuracy': 0.9582, 'data_size': 10000}, 450.4504786489997)
INFO flwr 2024-04-07 00:01:55,674 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:01:55,674 | server.py:153 | FL finished in 450.4509656649898
INFO flwr 2024-04-07 00:01:55,674 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:01:55,674 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:01:55,675 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:01:55,675 | app.py:229 | app_fit: losses_centralized [(0, 2.302257537841797), (1, 2.21883225440979), (2, 1.9586902856826782), (3, 1.5902109146118164), (4, 1.5475157499313354), (5, 1.5311282873153687), (6, 1.5218685865402222), (7, 1.519264817237854), (8, 1.5123854875564575), (9, 1.5119097232818604), (10, 1.5060930252075195)]
INFO flwr 2024-04-07 00:01:55,675 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1421), (1, 0.2006), (2, 0.4841), (3, 0.8969), (4, 0.9235), (5, 0.9378), (6, 0.9443), (7, 0.9461), (8, 0.9515), (9, 0.9515), (10, 0.9582)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9582
wandb:     loss 1.50609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_235359-801fwcgv
wandb: Find logs at: ./wandb/offline-run-20240406_235359-801fwcgv/logs
INFO flwr 2024-04-07 00:01:59,317 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:09:21,915 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 00:09:26,583	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:09:26,842	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:09:27,292	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f54f7582b1134b5b.zip' (10.69MiB) to Ray cluster...
2024-04-07 00:09:27,323	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f54f7582b1134b5b.zip'.
INFO flwr 2024-04-07 00:09:38,445 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 129265555252.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59685237964.0}
INFO flwr 2024-04-07 00:09:38,446 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:09:38,446 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:09:38,463 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:09:38,464 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:09:38,464 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:09:38,464 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1513791)[0m 2024-04-07 00:09:44.181788: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1513791)[0m 2024-04-07 00:09:44.286638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1513791)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 00:09:46,086 | server.py:94 | initial parameters (loss, other metrics): 2.3026819229125977, {'accuracy': 0.0843, 'data_size': 10000}
INFO flwr 2024-04-07 00:09:46,086 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:09:46,087 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1513782)[0m 2024-04-07 00:09:46.422358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1513792)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1513792)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1513787)[0m 2024-04-07 00:09:45.256510: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1513787)[0m 2024-04-07 00:09:45.357109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1513787)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1513787)[0m 2024-04-07 00:09:47.559384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1513782)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1513782)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:10:02,789 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:10:06,157 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:10:10,081 | server.py:125 | fit progress: (1, 2.192481279373169, {'accuracy': 0.4123, 'data_size': 10000}, 23.99411778099602)
INFO flwr 2024-04-07 00:10:10,081 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:10:10,081 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:20,411 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:10:33,295 | server.py:125 | fit progress: (2, 1.6888947486877441, {'accuracy': 0.824, 'data_size': 10000}, 47.208420774986735)
INFO flwr 2024-04-07 00:10:33,295 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:10:33,296 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:42,485 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:11:01,359 | server.py:125 | fit progress: (3, 1.5302069187164307, {'accuracy': 0.9411, 'data_size': 10000}, 75.27217356098117)
INFO flwr 2024-04-07 00:11:01,359 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:11:01,359 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:11:10,258 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:11:34,868 | server.py:125 | fit progress: (4, 1.5156941413879395, {'accuracy': 0.9502, 'data_size': 10000}, 108.78101087998948)
INFO flwr 2024-04-07 00:11:34,868 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:11:34,868 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:11:43,912 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:12:13,958 | server.py:125 | fit progress: (5, 1.589114785194397, {'accuracy': 0.8763, 'data_size': 10000}, 147.8710830549826)
INFO flwr 2024-04-07 00:12:13,958 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:12:13,958 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:12:23,391 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:12:58,638 | server.py:125 | fit progress: (6, 1.5046473741531372, {'accuracy': 0.9598, 'data_size': 10000}, 192.55142006199458)
INFO flwr 2024-04-07 00:12:58,639 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:12:58,639 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:13:08,348 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:13:56,225 | server.py:125 | fit progress: (7, 1.5024895668029785, {'accuracy': 0.9607, 'data_size': 10000}, 250.13801561799482)
INFO flwr 2024-04-07 00:13:56,225 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:13:56,225 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:14:06,009 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:15:01,458 | server.py:125 | fit progress: (8, 1.5008147954940796, {'accuracy': 0.9623, 'data_size': 10000}, 315.37111613797606)
INFO flwr 2024-04-07 00:15:01,458 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:15:01,458 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:15:11,274 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:16:03,315 | server.py:125 | fit progress: (9, 1.5023703575134277, {'accuracy': 0.9608, 'data_size': 10000}, 377.22787245799555)
INFO flwr 2024-04-07 00:16:03,315 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:16:03,315 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:16:12,894 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:17:23,205 | server.py:125 | fit progress: (10, 1.4946728944778442, {'accuracy': 0.9673, 'data_size': 10000}, 457.1187890750007)
INFO flwr 2024-04-07 00:17:23,206 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:17:23,206 | server.py:153 | FL finished in 457.1191932329966
INFO flwr 2024-04-07 00:17:23,206 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:17:23,206 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:17:23,206 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:17:23,206 | app.py:229 | app_fit: losses_centralized [(0, 2.3026819229125977), (1, 2.192481279373169), (2, 1.6888947486877441), (3, 1.5302069187164307), (4, 1.5156941413879395), (5, 1.589114785194397), (6, 1.5046473741531372), (7, 1.5024895668029785), (8, 1.5008147954940796), (9, 1.5023703575134277), (10, 1.4946728944778442)]
INFO flwr 2024-04-07 00:17:23,206 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0843), (1, 0.4123), (2, 0.824), (3, 0.9411), (4, 0.9502), (5, 0.8763), (6, 0.9598), (7, 0.9607), (8, 0.9623), (9, 0.9608), (10, 0.9673)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9673
wandb:     loss 1.49467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_000921-7htjvzpo
wandb: Find logs at: ./wandb/offline-run-20240407_000921-7htjvzpo/logs
INFO flwr 2024-04-07 00:17:26,864 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:24:47,610 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 00:24:54,114	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:24:54,448	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:24:54,757	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ad581eb4012390b6.zip' (10.72MiB) to Ray cluster...
2024-04-07 00:24:54,782	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ad581eb4012390b6.zip'.
INFO flwr 2024-04-07 00:25:05,721 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57990343065.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 125310800487.0}
INFO flwr 2024-04-07 00:25:05,721 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:25:05,721 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:25:05,734 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:25:05,735 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:25:05,735 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:25:05,736 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1521964)[0m 2024-04-07 00:25:11.757423: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1521960)[0m 2024-04-07 00:25:11.859236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1521960)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 00:25:13,738 | server.py:94 | initial parameters (loss, other metrics): 2.3022921085357666, {'accuracy': 0.101, 'data_size': 10000}
INFO flwr 2024-04-07 00:25:13,738 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:25:13,738 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1521962)[0m 2024-04-07 00:25:13.987162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1521967)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1521967)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1521955)[0m 2024-04-07 00:25:11.907361: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1521955)[0m 2024-04-07 00:25:12.009451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1521955)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1521957)[0m 2024-04-07 00:25:14.266803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1521955)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1521955)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:25:29,927 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:25:33,354 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:25:37,323 | server.py:125 | fit progress: (1, 2.154574155807495, {'accuracy': 0.5446, 'data_size': 10000}, 23.58448138498352)
INFO flwr 2024-04-07 00:25:37,323 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:25:37,323 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:25:47,097 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:25:59,771 | server.py:125 | fit progress: (2, 1.62589693069458, {'accuracy': 0.8583, 'data_size': 10000}, 46.032335953001166)
INFO flwr 2024-04-07 00:25:59,771 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:25:59,771 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:26:08,855 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:26:27,717 | server.py:125 | fit progress: (3, 1.520504117012024, {'accuracy': 0.9457, 'data_size': 10000}, 73.97860102501)
INFO flwr 2024-04-07 00:26:27,717 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:26:27,717 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:26:37,423 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:27:02,115 | server.py:125 | fit progress: (4, 1.5126906633377075, {'accuracy': 0.9507, 'data_size': 10000}, 108.37703292499646)
INFO flwr 2024-04-07 00:27:02,116 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:27:02,116 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:27:11,747 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:27:42,724 | server.py:125 | fit progress: (5, 1.5045642852783203, {'accuracy': 0.9598, 'data_size': 10000}, 148.9853353970102)
INFO flwr 2024-04-07 00:27:42,724 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:27:42,724 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:27:52,283 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:40,557 | server.py:125 | fit progress: (6, 1.5002180337905884, {'accuracy': 0.963, 'data_size': 10000}, 206.81893607598613)
INFO flwr 2024-04-07 00:28:40,558 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:40,558 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:50,548 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:29:37,790 | server.py:125 | fit progress: (7, 1.4983134269714355, {'accuracy': 0.9648, 'data_size': 10000}, 264.05133851699065)
INFO flwr 2024-04-07 00:29:37,790 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:29:37,790 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:29:47,695 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:30:49,822 | server.py:125 | fit progress: (8, 1.4961920976638794, {'accuracy': 0.9666, 'data_size': 10000}, 336.0838001779921)
INFO flwr 2024-04-07 00:30:49,822 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:30:49,823 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:30:59,345 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:32:01,591 | server.py:125 | fit progress: (9, 1.4997894763946533, {'accuracy': 0.9639, 'data_size': 10000}, 407.85233007199713)
INFO flwr 2024-04-07 00:32:01,591 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:32:01,591 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:32:11,169 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:33:27,851 | server.py:125 | fit progress: (10, 1.4941425323486328, {'accuracy': 0.9684, 'data_size': 10000}, 494.1126065889839)
INFO flwr 2024-04-07 00:33:27,851 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:33:27,851 | server.py:153 | FL finished in 494.1131436029973
INFO flwr 2024-04-07 00:33:27,852 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:33:27,852 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:33:27,852 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:33:27,852 | app.py:229 | app_fit: losses_centralized [(0, 2.3022921085357666), (1, 2.154574155807495), (2, 1.62589693069458), (3, 1.520504117012024), (4, 1.5126906633377075), (5, 1.5045642852783203), (6, 1.5002180337905884), (7, 1.4983134269714355), (8, 1.4961920976638794), (9, 1.4997894763946533), (10, 1.4941425323486328)]
INFO flwr 2024-04-07 00:33:27,852 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.101), (1, 0.5446), (2, 0.8583), (3, 0.9457), (4, 0.9507), (5, 0.9598), (6, 0.963), (7, 0.9648), (8, 0.9666), (9, 0.9639), (10, 0.9684)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9684
wandb:     loss 1.49414
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_002447-73xa5pkd
wandb: Find logs at: ./wandb/offline-run-20240407_002447-73xa5pkd/logs
INFO flwr 2024-04-07 00:33:31,432 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:40:53,174 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 00:40:58,008	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:40:58,321	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:40:58,748	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9d344a3196ba3901.zip' (10.75MiB) to Ray cluster...
2024-04-07 00:40:58,791	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9d344a3196ba3901.zip'.
INFO flwr 2024-04-07 00:41:09,718 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 124203178189.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57515647795.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 00:41:09,718 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:41:09,719 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:41:09,739 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:41:09,741 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:41:09,741 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:41:09,742 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1533487)[0m 2024-04-07 00:41:15.602975: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1533487)[0m 2024-04-07 00:41:15.696036: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1533487)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1533489)[0m 2024-04-07 00:41:17.883411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 00:41:17,993 | server.py:94 | initial parameters (loss, other metrics): 2.302567958831787, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-07 00:41:17,993 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:41:17,994 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1533489)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1533489)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1533481)[0m 2024-04-07 00:41:16.013448: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1533481)[0m 2024-04-07 00:41:16.109729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1533481)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1533483)[0m 2024-04-07 00:41:18.298501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1533481)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1533481)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:41:35,729 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:41:39,960 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:41:43,844 | server.py:125 | fit progress: (1, 2.0933098793029785, {'accuracy': 0.4373, 'data_size': 10000}, 25.85058585400111)
INFO flwr 2024-04-07 00:41:43,844 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:41:43,845 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:41:54,016 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:42:08,177 | server.py:125 | fit progress: (2, 1.7136173248291016, {'accuracy': 0.7509, 'data_size': 10000}, 50.183732811012305)
INFO flwr 2024-04-07 00:42:08,178 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:42:08,178 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:42:17,434 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:42:38,937 | server.py:125 | fit progress: (3, 1.529126763343811, {'accuracy': 0.936, 'data_size': 10000}, 80.94358868099516)
INFO flwr 2024-04-07 00:42:38,937 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:42:38,938 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:42:48,626 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:43:16,212 | server.py:125 | fit progress: (4, 1.509635329246521, {'accuracy': 0.9534, 'data_size': 10000}, 118.21884017399861)
INFO flwr 2024-04-07 00:43:16,213 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:43:16,213 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:43:26,229 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:44:07,046 | server.py:125 | fit progress: (5, 1.502708911895752, {'accuracy': 0.9603, 'data_size': 10000}, 169.05216390901478)
INFO flwr 2024-04-07 00:44:07,046 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:44:07,046 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:44:17,182 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:45:04,652 | server.py:125 | fit progress: (6, 1.5011626482009888, {'accuracy': 0.9616, 'data_size': 10000}, 226.65834314000676)
INFO flwr 2024-04-07 00:45:04,652 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:45:04,652 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:45:14,141 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:46:01,264 | server.py:125 | fit progress: (7, 1.4939000606536865, {'accuracy': 0.9686, 'data_size': 10000}, 283.2708145130018)
INFO flwr 2024-04-07 00:46:01,265 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:46:01,265 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:46:11,031 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:14,584 | server.py:125 | fit progress: (8, 1.508773922920227, {'accuracy': 0.9529, 'data_size': 10000}, 356.5908960420056)
INFO flwr 2024-04-07 00:47:14,585 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:14,585 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:23,755 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:48:38,827 | server.py:125 | fit progress: (9, 1.4890477657318115, {'accuracy': 0.9727, 'data_size': 10000}, 440.8332597099943)
INFO flwr 2024-04-07 00:48:38,827 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:48:38,827 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:48:48,949 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:50:10,220 | server.py:125 | fit progress: (10, 1.4880119562149048, {'accuracy': 0.9749, 'data_size': 10000}, 532.2269902869884)
INFO flwr 2024-04-07 00:50:10,221 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:50:10,221 | server.py:153 | FL finished in 532.2276303200051
INFO flwr 2024-04-07 00:50:10,221 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:50:10,221 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:50:10,221 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:50:10,222 | app.py:229 | app_fit: losses_centralized [(0, 2.302567958831787), (1, 2.0933098793029785), (2, 1.7136173248291016), (3, 1.529126763343811), (4, 1.509635329246521), (5, 1.502708911895752), (6, 1.5011626482009888), (7, 1.4939000606536865), (8, 1.508773922920227), (9, 1.4890477657318115), (10, 1.4880119562149048)]
INFO flwr 2024-04-07 00:50:10,222 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.4373), (2, 0.7509), (3, 0.936), (4, 0.9534), (5, 0.9603), (6, 0.9616), (7, 0.9686), (8, 0.9529), (9, 0.9727), (10, 0.9749)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9749
wandb:     loss 1.48801
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_004052-schk7322
wandb: Find logs at: ./wandb/offline-run-20240407_004052-schk7322/logs
INFO flwr 2024-04-07 00:50:13,850 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:57:36,554 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 00:57:41,394	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:57:41,800	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:57:42,266	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5b10fb0fd1de54f0.zip' (10.79MiB) to Ray cluster...
2024-04-07 00:57:42,314	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5b10fb0fd1de54f0.zip'.
INFO flwr 2024-04-07 00:57:53,621 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 57215571148.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'memory': 123502999348.0}
INFO flwr 2024-04-07 00:57:53,621 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:57:53,622 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:57:53,636 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:57:53,637 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:57:53,637 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:57:53,638 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1545337)[0m 2024-04-07 00:57:59.555892: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1545338)[0m 2024-04-07 00:57:59.714434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1545338)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 00:58:01,205 | server.py:94 | initial parameters (loss, other metrics): 2.30240797996521, {'accuracy': 0.0931, 'data_size': 10000}
INFO flwr 2024-04-07 00:58:01,206 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:58:01,206 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1545331)[0m 2024-04-07 00:58:01.845967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1545334)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1545334)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1545322)[0m 2024-04-07 00:58:00.126731: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545322)[0m 2024-04-07 00:58:00.232961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545322)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1545322)[0m 2024-04-07 00:58:02.583310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1545324)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1545324)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 00:58:19,916 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:58:23,466 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:58:27,326 | server.py:125 | fit progress: (1, 1.8828712701797485, {'accuracy': 0.6647, 'data_size': 10000}, 26.11990234599216)
INFO flwr 2024-04-07 00:58:27,326 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:58:27,327 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:58:37,098 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:58:51,358 | server.py:125 | fit progress: (2, 1.5355494022369385, {'accuracy': 0.9342, 'data_size': 10000}, 50.151685009012)
INFO flwr 2024-04-07 00:58:51,358 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:58:51,358 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:59:00,796 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:59:22,528 | server.py:125 | fit progress: (3, 1.5136455297470093, {'accuracy': 0.9499, 'data_size': 10000}, 81.32218052999815)
INFO flwr 2024-04-07 00:59:22,529 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:59:22,529 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:59:31,485 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:59:59,786 | server.py:125 | fit progress: (4, 1.5035221576690674, {'accuracy': 0.9594, 'data_size': 10000}, 118.58019496800262)
INFO flwr 2024-04-07 00:59:59,787 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:59:59,787 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:00:08,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:00:41,788 | server.py:125 | fit progress: (5, 1.498965859413147, {'accuracy': 0.9641, 'data_size': 10000}, 160.58215089899022)
INFO flwr 2024-04-07 01:00:41,789 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:00:41,789 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:00:50,107 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:01:39,073 | server.py:125 | fit progress: (6, 1.493096113204956, {'accuracy': 0.9695, 'data_size': 10000}, 217.86649285498424)
INFO flwr 2024-04-07 01:01:39,073 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:01:39,073 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:01:47,606 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:02:35,209 | server.py:125 | fit progress: (7, 1.4914168119430542, {'accuracy': 0.9704, 'data_size': 10000}, 274.002684245992)
INFO flwr 2024-04-07 01:02:35,209 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:02:35,209 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:02:44,508 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:03:51,896 | server.py:125 | fit progress: (8, 1.48787260055542, {'accuracy': 0.9742, 'data_size': 10000}, 350.68984493101016)
INFO flwr 2024-04-07 01:03:51,896 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:03:51,897 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:04:00,756 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:05:10,209 | server.py:125 | fit progress: (9, 1.4867204427719116, {'accuracy': 0.9748, 'data_size': 10000}, 429.00253201200394)
INFO flwr 2024-04-07 01:05:10,209 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:05:10,209 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:05:22,133 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:37,792 | server.py:125 | fit progress: (10, 1.4896836280822754, {'accuracy': 0.9724, 'data_size': 10000}, 516.5855307930033)
INFO flwr 2024-04-07 01:06:37,792 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:06:37,792 | server.py:153 | FL finished in 516.5861525830114
INFO flwr 2024-04-07 01:06:37,792 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:06:37,793 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:06:37,793 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:06:37,793 | app.py:229 | app_fit: losses_centralized [(0, 2.30240797996521), (1, 1.8828712701797485), (2, 1.5355494022369385), (3, 1.5136455297470093), (4, 1.5035221576690674), (5, 1.498965859413147), (6, 1.493096113204956), (7, 1.4914168119430542), (8, 1.48787260055542), (9, 1.4867204427719116), (10, 1.4896836280822754)]
INFO flwr 2024-04-07 01:06:37,793 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0931), (1, 0.6647), (2, 0.9342), (3, 0.9499), (4, 0.9594), (5, 0.9641), (6, 0.9695), (7, 0.9704), (8, 0.9742), (9, 0.9748), (10, 0.9724)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9724
wandb:     loss 1.48968
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_005736-hzwjt9ib
wandb: Find logs at: ./wandb/offline-run-20240407_005736-hzwjt9ib/logs
INFO flwr 2024-04-07 01:06:41,409 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:14:07,377 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1545322)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1545322)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 01:14:14,038	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:14:14,438	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:14:14,777	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f5d2d7b177e45e97.zip' (10.81MiB) to Ray cluster...
2024-04-07 01:14:14,810	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f5d2d7b177e45e97.zip'.
INFO flwr 2024-04-07 01:14:25,849 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 122374985524.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56732136652.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 01:14:25,850 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:14:25,850 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:14:25,870 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:14:25,871 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:14:25,871 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:14:25,871 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:14:32,384 | server.py:94 | initial parameters (loss, other metrics): 2.3027143478393555, {'accuracy': 0.0896, 'data_size': 10000}
INFO flwr 2024-04-07 01:14:32,384 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:14:32,385 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1553866)[0m 2024-04-07 01:14:36.853514: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1553866)[0m 2024-04-07 01:14:36.962073: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1553866)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1553866)[0m 2024-04-07 01:14:39.806281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1553857)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1553857)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1553857)[0m 2024-04-07 01:14:36.896295: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1553857)[0m 2024-04-07 01:14:37.024680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1553857)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1553853)[0m 2024-04-07 01:14:40.383521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:14:54,725 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:15:01,316 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:15:05,337 | server.py:125 | fit progress: (1, 1.859341025352478, {'accuracy': 0.6236, 'data_size': 10000}, 32.95236051600659)
INFO flwr 2024-04-07 01:15:05,337 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:05,338 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:16,535 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:15:30,574 | server.py:125 | fit progress: (2, 1.5560047626495361, {'accuracy': 0.9187, 'data_size': 10000}, 58.18955545499921)
INFO flwr 2024-04-07 01:15:30,575 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:30,575 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:40,261 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:02,033 | server.py:125 | fit progress: (3, 1.5070319175720215, {'accuracy': 0.9578, 'data_size': 10000}, 89.64798988201073)
INFO flwr 2024-04-07 01:16:02,033 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:02,033 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:11,698 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:39,768 | server.py:125 | fit progress: (4, 1.495755910873413, {'accuracy': 0.9661, 'data_size': 10000}, 127.38358627201524)
INFO flwr 2024-04-07 01:16:39,769 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:39,769 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:49,198 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:17:26,590 | server.py:125 | fit progress: (5, 1.4966639280319214, {'accuracy': 0.9663, 'data_size': 10000}, 174.2056683380215)
INFO flwr 2024-04-07 01:17:26,591 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:17:26,591 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:17:36,547 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:18:19,976 | server.py:125 | fit progress: (6, 1.4928478002548218, {'accuracy': 0.9693, 'data_size': 10000}, 227.59147714701248)
INFO flwr 2024-04-07 01:18:19,977 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:18:19,977 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:18:30,763 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:19:29,279 | server.py:125 | fit progress: (7, 1.490090012550354, {'accuracy': 0.9712, 'data_size': 10000}, 296.89470762500423)
INFO flwr 2024-04-07 01:19:29,280 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:19:29,280 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:19:39,937 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:20:32,885 | server.py:125 | fit progress: (8, 1.4885364770889282, {'accuracy': 0.9734, 'data_size': 10000}, 360.49997222801903)
INFO flwr 2024-04-07 01:20:32,886 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:20:32,886 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:20:42,739 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:21:56,946 | server.py:125 | fit progress: (9, 1.4855965375900269, {'accuracy': 0.9754, 'data_size': 10000}, 444.561521569005)
INFO flwr 2024-04-07 01:21:56,947 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:21:56,947 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:22:08,274 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:23:15,666 | server.py:125 | fit progress: (10, 1.4830806255340576, {'accuracy': 0.9785, 'data_size': 10000}, 523.28164753702)
INFO flwr 2024-04-07 01:23:15,667 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:23:15,667 | server.py:153 | FL finished in 523.2825950790138
INFO flwr 2024-04-07 01:23:15,668 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:23:15,668 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:23:15,668 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:23:15,668 | app.py:229 | app_fit: losses_centralized [(0, 2.3027143478393555), (1, 1.859341025352478), (2, 1.5560047626495361), (3, 1.5070319175720215), (4, 1.495755910873413), (5, 1.4966639280319214), (6, 1.4928478002548218), (7, 1.490090012550354), (8, 1.4885364770889282), (9, 1.4855965375900269), (10, 1.4830806255340576)]
INFO flwr 2024-04-07 01:23:15,668 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0896), (1, 0.6236), (2, 0.9187), (3, 0.9578), (4, 0.9661), (5, 0.9663), (6, 0.9693), (7, 0.9712), (8, 0.9734), (9, 0.9754), (10, 0.9785)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9785
wandb:     loss 1.48308
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_011407-ue6zxifo
wandb: Find logs at: ./wandb/offline-run-20240407_011407-ue6zxifo/logs
INFO flwr 2024-04-07 01:23:19,335 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:30:40,484 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1553866)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1553866)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:30:46,467	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:30:46,858	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:30:47,289	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9b9af1b1243bdfcb.zip' (10.85MiB) to Ray cluster...
2024-04-07 01:30:47,324	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9b9af1b1243bdfcb.zip'.
INFO flwr 2024-04-07 01:30:58,374 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 57091882598.0, 'node:10.20.240.18': 1.0, 'memory': 123214392730.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 01:30:58,375 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:30:58,375 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:30:58,390 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:30:58,391 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:30:58,391 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:30:58,391 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1565419)[0m 2024-04-07 01:31:04.275915: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1565419)[0m 2024-04-07 01:31:04.378798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1565419)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1565417)[0m 2024-04-07 01:31:06.518183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 01:31:06,881 | server.py:94 | initial parameters (loss, other metrics): 2.3027589321136475, {'accuracy': 0.1247, 'data_size': 10000}
INFO flwr 2024-04-07 01:31:06,882 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:31:06,882 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1565429)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1565429)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1565428)[0m 2024-04-07 01:31:04.892799: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1565428)[0m 2024-04-07 01:31:05.000591: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1565428)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1565429)[0m 2024-04-07 01:31:06.941892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1565421)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1565421)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 01:31:25,965 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:31:29,594 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:31:33,552 | server.py:125 | fit progress: (1, 2.302306652069092, {'accuracy': 0.1399, 'data_size': 10000}, 26.670505451009376)
INFO flwr 2024-04-07 01:31:33,553 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:31:33,553 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:31:46,078 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:32:00,270 | server.py:125 | fit progress: (2, 2.3015401363372803, {'accuracy': 0.1681, 'data_size': 10000}, 53.38842961101909)
INFO flwr 2024-04-07 01:32:00,271 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:32:00,271 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:32:10,727 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:32:32,296 | server.py:125 | fit progress: (3, 2.3007895946502686, {'accuracy': 0.1657, 'data_size': 10000}, 85.41403977500158)
INFO flwr 2024-04-07 01:32:32,296 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:32:32,297 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:32:43,509 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:33:10,041 | server.py:125 | fit progress: (4, 2.299466371536255, {'accuracy': 0.0988, 'data_size': 10000}, 123.15888984900084)
INFO flwr 2024-04-07 01:33:10,041 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:33:10,042 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:33:21,327 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:34:09,041 | server.py:125 | fit progress: (5, 2.298262357711792, {'accuracy': 0.1013, 'data_size': 10000}, 182.15867986300145)
INFO flwr 2024-04-07 01:34:09,041 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:34:09,041 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:34:20,694 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:18,431 | server.py:125 | fit progress: (6, 2.2960832118988037, {'accuracy': 0.1923, 'data_size': 10000}, 251.5491839510214)
INFO flwr 2024-04-07 01:35:18,432 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:18,432 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:29,600 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:36:27,157 | server.py:125 | fit progress: (7, 2.2905938625335693, {'accuracy': 0.136, 'data_size': 10000}, 320.275497678027)
INFO flwr 2024-04-07 01:36:27,158 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:36:27,158 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:36:39,106 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:37:45,868 | server.py:125 | fit progress: (8, 2.2816715240478516, {'accuracy': 0.1124, 'data_size': 10000}, 398.9855412830075)
INFO flwr 2024-04-07 01:37:45,868 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:37:45,868 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:37:56,491 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:39:08,641 | server.py:125 | fit progress: (9, 2.272474527359009, {'accuracy': 0.1187, 'data_size': 10000}, 481.75883598200744)
INFO flwr 2024-04-07 01:39:08,641 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:39:08,642 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:39:19,267 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:40:53,393 | server.py:125 | fit progress: (10, 2.2523903846740723, {'accuracy': 0.1845, 'data_size': 10000}, 586.5106833820173)
INFO flwr 2024-04-07 01:40:53,393 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:40:53,393 | server.py:153 | FL finished in 586.5112997050164
INFO flwr 2024-04-07 01:40:53,393 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:40:53,394 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:40:53,394 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:40:53,394 | app.py:229 | app_fit: losses_centralized [(0, 2.3027589321136475), (1, 2.302306652069092), (2, 2.3015401363372803), (3, 2.3007895946502686), (4, 2.299466371536255), (5, 2.298262357711792), (6, 2.2960832118988037), (7, 2.2905938625335693), (8, 2.2816715240478516), (9, 2.272474527359009), (10, 2.2523903846740723)]
INFO flwr 2024-04-07 01:40:53,394 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1247), (1, 0.1399), (2, 0.1681), (3, 0.1657), (4, 0.0988), (5, 0.1013), (6, 0.1923), (7, 0.136), (8, 0.1124), (9, 0.1187), (10, 0.1845)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1845
wandb:     loss 2.25239
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_013040-phwlxl4y
wandb: Find logs at: ./wandb/offline-run-20240407_013040-phwlxl4y/logs
INFO flwr 2024-04-07 01:40:56,969 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:48:22,569 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1565417)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1565417)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 01:48:28,149	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:48:28,515	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:48:28,956	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b0565feacc456f0b.zip' (10.88MiB) to Ray cluster...
2024-04-07 01:48:29,000	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b0565feacc456f0b.zip'.
INFO flwr 2024-04-07 01:48:39,984 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 122546498560.0, 'CPU': 64.0, 'object_store_memory': 56805642240.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 01:48:39,984 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:48:39,985 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:48:40,002 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:48:40,003 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:48:40,003 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:48:40,004 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1577269)[0m 2024-04-07 01:48:45.734890: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1577269)[0m 2024-04-07 01:48:45.972756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1577269)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 01:48:48,090 | server.py:94 | initial parameters (loss, other metrics): 2.3025426864624023, {'accuracy': 0.0639, 'data_size': 10000}
INFO flwr 2024-04-07 01:48:48,091 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:48:48,092 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1577270)[0m 2024-04-07 01:48:48.155264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1577270)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1577270)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1577271)[0m 2024-04-07 01:48:46.472079: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1577271)[0m 2024-04-07 01:48:46.566352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1577271)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1577275)[0m 2024-04-07 01:48:49.071088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1577269)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1577269)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 01:49:09,833 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:49:13,341 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:49:17,641 | server.py:125 | fit progress: (1, 2.1746420860290527, {'accuracy': 0.2311, 'data_size': 10000}, 29.550138473976403)
INFO flwr 2024-04-07 01:49:17,642 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:49:17,642 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:49:30,549 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:49:44,845 | server.py:125 | fit progress: (2, 1.7298250198364258, {'accuracy': 0.7335, 'data_size': 10000}, 56.7534289189789)
INFO flwr 2024-04-07 01:49:44,845 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:49:44,846 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:49:56,481 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:50:19,254 | server.py:125 | fit progress: (3, 1.5431342124938965, {'accuracy': 0.9269, 'data_size': 10000}, 91.16263954999158)
INFO flwr 2024-04-07 01:50:19,254 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:50:19,254 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:50:31,249 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:51:00,875 | server.py:125 | fit progress: (4, 1.5258233547210693, {'accuracy': 0.9409, 'data_size': 10000}, 132.78320276597515)
INFO flwr 2024-04-07 01:51:00,875 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:51:00,876 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:51:11,570 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:51:47,561 | server.py:125 | fit progress: (5, 1.5142992734909058, {'accuracy': 0.9501, 'data_size': 10000}, 179.46948073798558)
INFO flwr 2024-04-07 01:51:47,561 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:51:47,561 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:51:58,941 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:52:48,224 | server.py:125 | fit progress: (6, 1.5122120380401611, {'accuracy': 0.9519, 'data_size': 10000}, 240.13303161298973)
INFO flwr 2024-04-07 01:52:48,225 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:52:48,225 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:52:59,844 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:53:47,399 | server.py:125 | fit progress: (7, 1.503568410873413, {'accuracy': 0.9595, 'data_size': 10000}, 299.3075766369875)
INFO flwr 2024-04-07 01:53:47,399 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:53:47,400 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:53:58,891 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:55:17,846 | server.py:125 | fit progress: (8, 1.5109615325927734, {'accuracy': 0.9523, 'data_size': 10000}, 389.7551030549803)
INFO flwr 2024-04-07 01:55:17,847 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:55:17,847 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:55:27,948 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:56:44,888 | server.py:125 | fit progress: (9, 1.5012036561965942, {'accuracy': 0.9616, 'data_size': 10000}, 476.79632508498617)
INFO flwr 2024-04-07 01:56:44,888 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:56:44,888 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:56:56,804 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:58:19,789 | server.py:125 | fit progress: (10, 1.5100431442260742, {'accuracy': 0.9523, 'data_size': 10000}, 571.6971175569925)
INFO flwr 2024-04-07 01:58:19,789 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:58:19,790 | server.py:153 | FL finished in 571.6983134659822
INFO flwr 2024-04-07 01:58:19,790 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:58:19,790 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:58:19,790 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:58:19,790 | app.py:229 | app_fit: losses_centralized [(0, 2.3025426864624023), (1, 2.1746420860290527), (2, 1.7298250198364258), (3, 1.5431342124938965), (4, 1.5258233547210693), (5, 1.5142992734909058), (6, 1.5122120380401611), (7, 1.503568410873413), (8, 1.5109615325927734), (9, 1.5012036561965942), (10, 1.5100431442260742)]
INFO flwr 2024-04-07 01:58:19,791 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0639), (1, 0.2311), (2, 0.7335), (3, 0.9269), (4, 0.9409), (5, 0.9501), (6, 0.9519), (7, 0.9595), (8, 0.9523), (9, 0.9616), (10, 0.9523)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9523
wandb:     loss 1.51004
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_014822-y4qwbefd
wandb: Find logs at: ./wandb/offline-run-20240407_014822-y4qwbefd/logs
INFO flwr 2024-04-07 01:58:23,445 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:05:48,549 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1577266)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1577266)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 02:05:54,441	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:05:54,781	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:05:55,143	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b3602e81680eac6f.zip' (10.92MiB) to Ray cluster...
2024-04-07 02:05:55,173	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b3602e81680eac6f.zip'.
INFO flwr 2024-04-07 02:06:06,332 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 122292544922.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56696804966.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 02:06:06,332 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:06:06,332 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:06:06,346 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:06:06,348 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:06:06,348 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:06:06,348 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1589333)[0m 2024-04-07 02:06:12.506184: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1589333)[0m 2024-04-07 02:06:12.600643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1589333)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 02:06:13,718 | server.py:94 | initial parameters (loss, other metrics): 2.302746057510376, {'accuracy': 0.0576, 'data_size': 10000}
INFO flwr 2024-04-07 02:06:13,718 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:06:13,719 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1589326)[0m 2024-04-07 02:06:15.077359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1589331)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1589331)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1589320)[0m 2024-04-07 02:06:12.783011: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1589320)[0m 2024-04-07 02:06:12.890358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1589320)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1589321)[0m 2024-04-07 02:06:15.221177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1589321)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1589321)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 02:06:34,098 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:06:37,557 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:06:41,825 | server.py:125 | fit progress: (1, 2.1283180713653564, {'accuracy': 0.3843, 'data_size': 10000}, 28.106572812015656)
INFO flwr 2024-04-07 02:06:41,826 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:06:41,826 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:06:54,734 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:07:09,410 | server.py:125 | fit progress: (2, 1.6171168088912964, {'accuracy': 0.8645, 'data_size': 10000}, 55.6919162410195)
INFO flwr 2024-04-07 02:07:09,411 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:07:09,411 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:07:21,763 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:07:42,317 | server.py:125 | fit progress: (3, 1.5375562906265259, {'accuracy': 0.9303, 'data_size': 10000}, 88.5989492990193)
INFO flwr 2024-04-07 02:07:42,318 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:07:42,318 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:07:53,408 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:08:21,424 | server.py:125 | fit progress: (4, 1.5110881328582764, {'accuracy': 0.9529, 'data_size': 10000}, 127.70575954101514)
INFO flwr 2024-04-07 02:08:21,425 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:08:21,425 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:08:33,220 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:09:11,717 | server.py:125 | fit progress: (5, 1.5047690868377686, {'accuracy': 0.9581, 'data_size': 10000}, 177.9984653860156)
INFO flwr 2024-04-07 02:09:11,717 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:09:11,718 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:09:24,041 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:10:03,004 | server.py:125 | fit progress: (6, 1.498854160308838, {'accuracy': 0.9643, 'data_size': 10000}, 229.285061683011)
INFO flwr 2024-04-07 02:10:03,004 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:10:03,005 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:10:14,584 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:11:11,553 | server.py:125 | fit progress: (7, 1.5022671222686768, {'accuracy': 0.9608, 'data_size': 10000}, 297.8344199200219)
INFO flwr 2024-04-07 02:11:11,555 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:11:11,555 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:11:22,401 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:12:18,105 | server.py:125 | fit progress: (8, 1.4959478378295898, {'accuracy': 0.9668, 'data_size': 10000}, 364.38625669199973)
INFO flwr 2024-04-07 02:12:18,105 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:12:18,106 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:12:28,597 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:13:43,530 | server.py:125 | fit progress: (9, 1.4941824674606323, {'accuracy': 0.968, 'data_size': 10000}, 449.811606969015)
INFO flwr 2024-04-07 02:13:43,531 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:13:43,531 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:13:56,856 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:15:32,641 | server.py:125 | fit progress: (10, 1.4946675300598145, {'accuracy': 0.9674, 'data_size': 10000}, 558.9227745300159)
INFO flwr 2024-04-07 02:15:32,642 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:15:32,642 | server.py:153 | FL finished in 558.9234266400163
INFO flwr 2024-04-07 02:15:32,642 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:15:32,642 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:15:32,643 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:15:32,643 | app.py:229 | app_fit: losses_centralized [(0, 2.302746057510376), (1, 2.1283180713653564), (2, 1.6171168088912964), (3, 1.5375562906265259), (4, 1.5110881328582764), (5, 1.5047690868377686), (6, 1.498854160308838), (7, 1.5022671222686768), (8, 1.4959478378295898), (9, 1.4941824674606323), (10, 1.4946675300598145)]
INFO flwr 2024-04-07 02:15:32,643 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0576), (1, 0.3843), (2, 0.8645), (3, 0.9303), (4, 0.9529), (5, 0.9581), (6, 0.9643), (7, 0.9608), (8, 0.9668), (9, 0.968), (10, 0.9674)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9674
wandb:     loss 1.49467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_020548-xjaiubcb
wandb: Find logs at: ./wandb/offline-run-20240407_020548-xjaiubcb/logs
INFO flwr 2024-04-07 02:15:36,288 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:22:59,351 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1589320)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1589320)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 02:23:04,557	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:23:05,065	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:23:05,440	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c4af490f81578e72.zip' (10.95MiB) to Ray cluster...
2024-04-07 02:23:05,467	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c4af490f81578e72.zip'.
INFO flwr 2024-04-07 02:23:16,475 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 121969098957.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56558185267.0, 'CPU': 64.0}
INFO flwr 2024-04-07 02:23:16,476 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:23:16,476 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:23:16,496 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:23:16,497 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:23:16,497 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:23:16,497 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1598054)[0m 2024-04-07 02:23:23.217801: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1598054)[0m 2024-04-07 02:23:23.312473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1598054)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 02:23:24,812 | server.py:94 | initial parameters (loss, other metrics): 2.30269718170166, {'accuracy': 0.1118, 'data_size': 10000}
INFO flwr 2024-04-07 02:23:24,813 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:23:24,813 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1598050)[0m 2024-04-07 02:23:25.392203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1598056)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1598056)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1598056)[0m 2024-04-07 02:23:23.440979: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598056)[0m 2024-04-07 02:23:23.558402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598056)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1598044)[0m 2024-04-07 02:23:25.968978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1598047)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1598047)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 02:23:45,268 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:23:48,896 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:23:53,219 | server.py:125 | fit progress: (1, 1.9448187351226807, {'accuracy': 0.579, 'data_size': 10000}, 28.40615407799487)
INFO flwr 2024-04-07 02:23:53,220 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:23:53,220 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:24:06,032 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:24:20,140 | server.py:125 | fit progress: (2, 1.5653607845306396, {'accuracy': 0.9114, 'data_size': 10000}, 55.32659427798353)
INFO flwr 2024-04-07 02:24:20,140 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:24:20,140 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:24:31,791 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:24:54,316 | server.py:125 | fit progress: (3, 1.511877417564392, {'accuracy': 0.9528, 'data_size': 10000}, 89.50311056300416)
INFO flwr 2024-04-07 02:24:54,317 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:24:54,317 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:06,650 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:25:35,742 | server.py:125 | fit progress: (4, 1.5337624549865723, {'accuracy': 0.9302, 'data_size': 10000}, 130.92864171499969)
INFO flwr 2024-04-07 02:25:35,742 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:25:35,743 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:47,843 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:26:36,325 | server.py:125 | fit progress: (5, 1.4997143745422363, {'accuracy': 0.962, 'data_size': 10000}, 191.51179359399248)
INFO flwr 2024-04-07 02:26:36,326 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:26:36,326 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:26:48,271 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:27:43,713 | server.py:125 | fit progress: (6, 1.4951223134994507, {'accuracy': 0.9681, 'data_size': 10000}, 258.9000721540069)
INFO flwr 2024-04-07 02:27:43,714 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:27:43,714 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:27:55,245 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:28:45,943 | server.py:125 | fit progress: (7, 1.5005971193313599, {'accuracy': 0.9614, 'data_size': 10000}, 321.1294697239937)
INFO flwr 2024-04-07 02:28:45,943 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:28:45,943 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:28:57,560 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:30:02,050 | server.py:125 | fit progress: (8, 1.4924877882003784, {'accuracy': 0.9689, 'data_size': 10000}, 397.23668725800235)
INFO flwr 2024-04-07 02:30:02,050 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:30:02,051 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:30:13,477 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:31:17,260 | server.py:125 | fit progress: (9, 1.4894943237304688, {'accuracy': 0.9731, 'data_size': 10000}, 472.44715836600517)
INFO flwr 2024-04-07 02:31:17,261 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:31:17,261 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:31:28,960 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:32:50,798 | server.py:125 | fit progress: (10, 1.4898122549057007, {'accuracy': 0.9725, 'data_size': 10000}, 565.9845699419966)
INFO flwr 2024-04-07 02:32:50,798 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:32:50,798 | server.py:153 | FL finished in 565.9851299989969
INFO flwr 2024-04-07 02:32:50,798 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:32:50,799 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:32:50,799 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:32:50,799 | app.py:229 | app_fit: losses_centralized [(0, 2.30269718170166), (1, 1.9448187351226807), (2, 1.5653607845306396), (3, 1.511877417564392), (4, 1.5337624549865723), (5, 1.4997143745422363), (6, 1.4951223134994507), (7, 1.5005971193313599), (8, 1.4924877882003784), (9, 1.4894943237304688), (10, 1.4898122549057007)]
INFO flwr 2024-04-07 02:32:50,799 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1118), (1, 0.579), (2, 0.9114), (3, 0.9528), (4, 0.9302), (5, 0.962), (6, 0.9681), (7, 0.9614), (8, 0.9689), (9, 0.9731), (10, 0.9725)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9725
wandb:     loss 1.48981
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_022258-jmys8erz
wandb: Find logs at: ./wandb/offline-run-20240407_022258-jmys8erz/logs
INFO flwr 2024-04-07 02:32:54,416 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:40:19,184 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1598044)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1598044)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 02:40:25,065	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:40:25,414	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:40:25,805	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_24eff9493852c794.zip' (10.98MiB) to Ray cluster...
2024-04-07 02:40:25,833	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_24eff9493852c794.zip'.
INFO flwr 2024-04-07 02:40:36,783 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 121350575104.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 56293103616.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 02:40:36,783 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:40:36,783 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:40:36,802 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:40:36,804 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:40:36,804 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:40:36,805 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1609598)[0m 2024-04-07 02:40:42.096034: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1609598)[0m 2024-04-07 02:40:42.190487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1609598)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 02:40:43,724 | server.py:94 | initial parameters (loss, other metrics): 2.3025524616241455, {'accuracy': 0.1157, 'data_size': 10000}
INFO flwr 2024-04-07 02:40:43,725 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:40:43,725 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1609598)[0m 2024-04-07 02:40:44.955058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1609608)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1609608)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1609610)[0m 2024-04-07 02:40:43.590471: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1609600)[0m 2024-04-07 02:40:43.595645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1609600)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1609610)[0m 2024-04-07 02:40:45.957056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:41:03,843 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:41:07,291 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:41:11,587 | server.py:125 | fit progress: (1, 1.8701492547988892, {'accuracy': 0.6242, 'data_size': 10000}, 27.862051094998606)
INFO flwr 2024-04-07 02:41:11,587 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:41:11,587 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:41:23,792 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:41:37,592 | server.py:125 | fit progress: (2, 1.5725592374801636, {'accuracy': 0.8969, 'data_size': 10000}, 53.866956773999846)
INFO flwr 2024-04-07 02:41:37,592 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:41:37,592 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:41:49,315 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:42:12,632 | server.py:125 | fit progress: (3, 1.5294524431228638, {'accuracy': 0.9342, 'data_size': 10000}, 88.90771437602234)
INFO flwr 2024-04-07 02:42:12,633 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:42:12,633 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:42:24,587 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:42:51,311 | server.py:125 | fit progress: (4, 1.5032020807266235, {'accuracy': 0.958, 'data_size': 10000}, 127.5859763440094)
INFO flwr 2024-04-07 02:42:51,311 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:42:51,312 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:43:02,796 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:43:36,128 | server.py:125 | fit progress: (5, 1.498451590538025, {'accuracy': 0.9629, 'data_size': 10000}, 172.40305820701178)
INFO flwr 2024-04-07 02:43:36,128 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:43:36,128 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:43:48,385 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:44:41,577 | server.py:125 | fit progress: (6, 1.4984277486801147, {'accuracy': 0.9638, 'data_size': 10000}, 237.85228417400504)
INFO flwr 2024-04-07 02:44:41,578 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:44:41,578 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:44:54,384 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:46:11,758 | server.py:125 | fit progress: (7, 1.4912618398666382, {'accuracy': 0.9709, 'data_size': 10000}, 328.0328833530075)
INFO flwr 2024-04-07 02:46:11,758 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:46:11,758 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:46:24,025 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:47:46,329 | server.py:125 | fit progress: (8, 1.4886518716812134, {'accuracy': 0.9736, 'data_size': 10000}, 422.60442703802255)
INFO flwr 2024-04-07 02:47:46,330 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:47:46,330 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:47:57,793 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:49:13,017 | server.py:125 | fit progress: (9, 1.4890252351760864, {'accuracy': 0.9729, 'data_size': 10000}, 509.292338380008)
INFO flwr 2024-04-07 02:49:13,018 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:49:13,018 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:49:24,643 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:50:49,611 | server.py:125 | fit progress: (10, 1.4920156002044678, {'accuracy': 0.9694, 'data_size': 10000}, 605.8865870800219)
INFO flwr 2024-04-07 02:50:49,612 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:50:49,612 | server.py:153 | FL finished in 605.8873164420074
INFO flwr 2024-04-07 02:50:49,612 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:50:49,612 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:50:49,613 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:50:49,613 | app.py:229 | app_fit: losses_centralized [(0, 2.3025524616241455), (1, 1.8701492547988892), (2, 1.5725592374801636), (3, 1.5294524431228638), (4, 1.5032020807266235), (5, 1.498451590538025), (6, 1.4984277486801147), (7, 1.4912618398666382), (8, 1.4886518716812134), (9, 1.4890252351760864), (10, 1.4920156002044678)]
INFO flwr 2024-04-07 02:50:49,613 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1157), (1, 0.6242), (2, 0.8969), (3, 0.9342), (4, 0.958), (5, 0.9629), (6, 0.9638), (7, 0.9709), (8, 0.9736), (9, 0.9729), (10, 0.9694)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9694
wandb:     loss 1.49202
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_024018-tz07a4ns
wandb: Find logs at: ./wandb/offline-run-20240407_024018-tz07a4ns/logs
INFO flwr 2024-04-07 02:50:53,228 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:58:18,092 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1609603)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1609603)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:58:23,391	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:58:23,711	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:58:24,171	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d50430ae215f4457.zip' (11.02MiB) to Ray cluster...
2024-04-07 02:58:24,216	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d50430ae215f4457.zip'.
INFO flwr 2024-04-07 02:58:35,503 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 120973314663.0, 'object_store_memory': 56131420569.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 02:58:35,504 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:58:35,504 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:58:35,524 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:58:35,525 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:58:35,525 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:58:35,525 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1621438)[0m 2024-04-07 02:58:41.687938: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1621442)[0m 2024-04-07 02:58:41.712994: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1621442)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1621436)[0m 2024-04-07 02:58:43.831548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 02:58:44,521 | server.py:94 | initial parameters (loss, other metrics): 2.302760362625122, {'accuracy': 0.0694, 'data_size': 10000}
INFO flwr 2024-04-07 02:58:44,522 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:58:44,522 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1621440)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1621440)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1621437)[0m 2024-04-07 02:58:41.761788: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1621437)[0m 2024-04-07 02:58:41.871751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1621437)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1621437)[0m 2024-04-07 02:58:44.012610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1621437)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1621437)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 02:59:02,666 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:59:06,405 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:59:10,655 | server.py:125 | fit progress: (1, 1.8848955631256104, {'accuracy': 0.6528, 'data_size': 10000}, 26.133580166002503)
INFO flwr 2024-04-07 02:59:10,656 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:59:10,656 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:59:22,204 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:59:36,069 | server.py:125 | fit progress: (2, 1.6182881593704224, {'accuracy': 0.8391, 'data_size': 10000}, 51.54726336800377)
INFO flwr 2024-04-07 02:59:36,070 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:59:36,070 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:59:47,845 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:00:08,075 | server.py:125 | fit progress: (3, 1.5110657215118408, {'accuracy': 0.9526, 'data_size': 10000}, 83.55337967799278)
INFO flwr 2024-04-07 03:00:08,076 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:00:08,076 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:00:19,264 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:00:45,064 | server.py:125 | fit progress: (4, 1.4985532760620117, {'accuracy': 0.9634, 'data_size': 10000}, 120.54258027698961)
INFO flwr 2024-04-07 03:00:45,065 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:00:45,065 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:00:56,597 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:01:29,554 | server.py:125 | fit progress: (5, 1.4926382303237915, {'accuracy': 0.9693, 'data_size': 10000}, 165.03194840598735)
INFO flwr 2024-04-07 03:01:29,554 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:01:29,554 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:01:40,972 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:02:19,413 | server.py:125 | fit progress: (6, 1.4916008710861206, {'accuracy': 0.9706, 'data_size': 10000}, 214.89076799200848)
INFO flwr 2024-04-07 03:02:19,413 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:02:19,413 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:02:31,343 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:03:25,598 | server.py:125 | fit progress: (7, 1.4909902811050415, {'accuracy': 0.9709, 'data_size': 10000}, 281.0758425059903)
INFO flwr 2024-04-07 03:03:25,598 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:03:25,598 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:03:38,072 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:04:33,321 | server.py:125 | fit progress: (8, 1.4936347007751465, {'accuracy': 0.9687, 'data_size': 10000}, 348.7993631080026)
INFO flwr 2024-04-07 03:04:33,322 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:04:33,322 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:04:44,747 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:06:04,349 | server.py:125 | fit progress: (9, 1.4862974882125854, {'accuracy': 0.9758, 'data_size': 10000}, 439.82715986599214)
INFO flwr 2024-04-07 03:06:04,349 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:06:04,350 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:06:15,818 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:07:44,342 | server.py:125 | fit progress: (10, 1.4858388900756836, {'accuracy': 0.9768, 'data_size': 10000}, 539.8204943040037)
INFO flwr 2024-04-07 03:07:44,343 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:07:44,343 | server.py:153 | FL finished in 539.8211306949961
INFO flwr 2024-04-07 03:07:44,343 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:07:44,343 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:07:44,344 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:07:44,344 | app.py:229 | app_fit: losses_centralized [(0, 2.302760362625122), (1, 1.8848955631256104), (2, 1.6182881593704224), (3, 1.5110657215118408), (4, 1.4985532760620117), (5, 1.4926382303237915), (6, 1.4916008710861206), (7, 1.4909902811050415), (8, 1.4936347007751465), (9, 1.4862974882125854), (10, 1.4858388900756836)]
INFO flwr 2024-04-07 03:07:44,344 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0694), (1, 0.6528), (2, 0.8391), (3, 0.9526), (4, 0.9634), (5, 0.9693), (6, 0.9706), (7, 0.9709), (8, 0.9687), (9, 0.9758), (10, 0.9768)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9768
wandb:     loss 1.48584
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_025817-78zszaki
wandb: Find logs at: ./wandb/offline-run-20240407_025817-78zszaki/logs
INFO flwr 2024-04-07 03:07:47,988 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:15:12,974 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1621435)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1621435)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 03:15:18,587	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:15:19,048	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:15:19,437	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_77c1ddf8e1fac00b.zip' (11.04MiB) to Ray cluster...
2024-04-07 03:15:19,489	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_77c1ddf8e1fac00b.zip'.
INFO flwr 2024-04-07 03:15:30,470 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 125157089895.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 57924467097.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 03:15:30,471 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:15:30,471 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:15:30,485 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:15:30,486 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:15:30,486 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:15:30,487 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1631173)[0m 2024-04-07 03:15:36.600726: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1631173)[0m 2024-04-07 03:15:36.696025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1631173)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 03:15:37,594 | server.py:94 | initial parameters (loss, other metrics): 2.302452802658081, {'accuracy': 0.1017, 'data_size': 10000}
INFO flwr 2024-04-07 03:15:37,595 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:15:37,595 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1631173)[0m 2024-04-07 03:15:39.117176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1631179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1631179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1631172)[0m 2024-04-07 03:15:36.813735: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1631177)[0m 2024-04-07 03:15:36.861333: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1631177)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1631184)[0m 2024-04-07 03:15:39.098540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1631173)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1631173)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 03:15:58,455 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:16:01,661 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:16:05,969 | server.py:125 | fit progress: (1, 1.833321452140808, {'accuracy': 0.725, 'data_size': 10000}, 28.374062769988086)
INFO flwr 2024-04-07 03:16:05,969 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:16:05,970 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:16:17,764 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:16:31,786 | server.py:125 | fit progress: (2, 1.5652495622634888, {'accuracy': 0.9037, 'data_size': 10000}, 54.19087281200336)
INFO flwr 2024-04-07 03:16:31,787 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:16:31,787 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:16:43,319 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:17:06,892 | server.py:125 | fit progress: (3, 1.503806710243225, {'accuracy': 0.9605, 'data_size': 10000}, 89.29723816300975)
INFO flwr 2024-04-07 03:17:06,893 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:17:06,893 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:17:18,556 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:17:53,486 | server.py:125 | fit progress: (4, 1.5816859006881714, {'accuracy': 0.8799, 'data_size': 10000}, 135.89055438700598)
INFO flwr 2024-04-07 03:17:53,486 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:17:53,487 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:18:05,501 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:18:45,943 | server.py:125 | fit progress: (5, 1.492856740951538, {'accuracy': 0.9697, 'data_size': 10000}, 188.34758809299092)
INFO flwr 2024-04-07 03:18:45,943 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:18:45,943 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:18:57,961 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:19:38,283 | server.py:125 | fit progress: (6, 1.490234375, {'accuracy': 0.9713, 'data_size': 10000}, 240.68746181498864)
INFO flwr 2024-04-07 03:19:38,283 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:19:38,284 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:19:49,891 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:20:44,576 | server.py:125 | fit progress: (7, 1.4862587451934814, {'accuracy': 0.9758, 'data_size': 10000}, 306.9805253749946)
INFO flwr 2024-04-07 03:20:44,576 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:20:44,576 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:20:55,799 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:21:56,891 | server.py:125 | fit progress: (8, 1.4870471954345703, {'accuracy': 0.9743, 'data_size': 10000}, 379.2955699900049)
INFO flwr 2024-04-07 03:21:56,891 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:21:56,892 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:22:08,736 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:23:28,705 | server.py:125 | fit progress: (9, 1.4847928285598755, {'accuracy': 0.9768, 'data_size': 10000}, 471.11008202200173)
INFO flwr 2024-04-07 03:23:28,706 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:23:28,706 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:23:40,487 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:25:05,242 | server.py:125 | fit progress: (10, 1.4837530851364136, {'accuracy': 0.9774, 'data_size': 10000}, 567.6467079120048)
INFO flwr 2024-04-07 03:25:05,242 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:25:05,242 | server.py:153 | FL finished in 567.6472778460011
INFO flwr 2024-04-07 03:25:05,243 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:25:05,243 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:25:05,243 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:25:05,243 | app.py:229 | app_fit: losses_centralized [(0, 2.302452802658081), (1, 1.833321452140808), (2, 1.5652495622634888), (3, 1.503806710243225), (4, 1.5816859006881714), (5, 1.492856740951538), (6, 1.490234375), (7, 1.4862587451934814), (8, 1.4870471954345703), (9, 1.4847928285598755), (10, 1.4837530851364136)]
INFO flwr 2024-04-07 03:25:05,243 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1017), (1, 0.725), (2, 0.9037), (3, 0.9605), (4, 0.8799), (5, 0.9697), (6, 0.9713), (7, 0.9758), (8, 0.9743), (9, 0.9768), (10, 0.9774)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9774
wandb:     loss 1.48375
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_031512-yog0i7mi
wandb: Find logs at: ./wandb/offline-run-20240407_031512-yog0i7mi/logs
INFO flwr 2024-04-07 03:25:08,881 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:32:31,353 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1631169)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1631169)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 03:32:36,289	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:32:36,640	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:32:36,964	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_34c3c0835649e871.zip' (11.08MiB) to Ray cluster...
2024-04-07 03:32:37,013	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_34c3c0835649e871.zip'.
INFO flwr 2024-04-07 03:32:48,213 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 120225414144.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55810891776.0}
INFO flwr 2024-04-07 03:32:48,214 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:32:48,214 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:32:48,239 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:32:48,240 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:32:48,241 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:32:48,241 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1647132)[0m 2024-04-07 03:32:54.123001: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1647132)[0m 2024-04-07 03:32:54.218532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1647132)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1647131)[0m 2024-04-07 03:32:56.459902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 03:32:56,930 | server.py:94 | initial parameters (loss, other metrics): 2.3027942180633545, {'accuracy': 0.1053, 'data_size': 10000}
INFO flwr 2024-04-07 03:32:56,930 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:32:56,931 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1647132)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1647132)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1647129)[0m 2024-04-07 03:32:54.526574: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647129)[0m 2024-04-07 03:32:54.644607: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647129)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1647128)[0m 2024-04-07 03:32:56.819000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1647128)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1647128)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 03:33:13,180 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:33:16,538 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:33:20,832 | server.py:125 | fit progress: (1, 2.302786350250244, {'accuracy': 0.1076, 'data_size': 10000}, 23.901395200984553)
INFO flwr 2024-04-07 03:33:20,833 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:33:20,833 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:33:31,080 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:33:44,917 | server.py:125 | fit progress: (2, 2.3027777671813965, {'accuracy': 0.1115, 'data_size': 10000}, 47.98639526500483)
INFO flwr 2024-04-07 03:33:44,917 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:33:44,918 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:33:53,981 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:34:14,496 | server.py:125 | fit progress: (3, 2.3027689456939697, {'accuracy': 0.1123, 'data_size': 10000}, 77.56552355299937)
INFO flwr 2024-04-07 03:34:14,497 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:34:14,497 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:34:23,295 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:34:49,757 | server.py:125 | fit progress: (4, 2.3027591705322266, {'accuracy': 0.1135, 'data_size': 10000}, 112.82599136300269)
INFO flwr 2024-04-07 03:34:49,757 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:34:49,757 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:34:58,809 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:35:32,780 | server.py:125 | fit progress: (5, 2.3027517795562744, {'accuracy': 0.1139, 'data_size': 10000}, 155.8495566160127)
INFO flwr 2024-04-07 03:35:32,781 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:35:32,781 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:35:42,393 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:36:27,628 | server.py:125 | fit progress: (6, 2.3027427196502686, {'accuracy': 0.1147, 'data_size': 10000}, 210.69681737400242)
INFO flwr 2024-04-07 03:36:27,628 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:36:27,628 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:36:36,292 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:37:30,838 | server.py:125 | fit progress: (7, 2.302734613418579, {'accuracy': 0.1155, 'data_size': 10000}, 273.90681132799364)
INFO flwr 2024-04-07 03:37:30,838 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:37:30,838 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:37:39,769 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:38:41,657 | server.py:125 | fit progress: (8, 2.3027262687683105, {'accuracy': 0.1164, 'data_size': 10000}, 344.72631951700896)
INFO flwr 2024-04-07 03:38:41,658 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:38:41,658 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:38:50,416 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:40:06,189 | server.py:125 | fit progress: (9, 2.302717685699463, {'accuracy': 0.1177, 'data_size': 10000}, 429.25845918798586)
INFO flwr 2024-04-07 03:40:06,190 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:40:06,190 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:40:15,622 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:41:42,233 | server.py:125 | fit progress: (10, 2.3027100563049316, {'accuracy': 0.1169, 'data_size': 10000}, 525.3022839690093)
INFO flwr 2024-04-07 03:41:42,233 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:41:42,234 | server.py:153 | FL finished in 525.3028507010022
INFO flwr 2024-04-07 03:41:42,234 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:41:42,234 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:41:42,234 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:41:42,234 | app.py:229 | app_fit: losses_centralized [(0, 2.3027942180633545), (1, 2.302786350250244), (2, 2.3027777671813965), (3, 2.3027689456939697), (4, 2.3027591705322266), (5, 2.3027517795562744), (6, 2.3027427196502686), (7, 2.302734613418579), (8, 2.3027262687683105), (9, 2.302717685699463), (10, 2.3027100563049316)]
INFO flwr 2024-04-07 03:41:42,234 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1053), (1, 0.1076), (2, 0.1115), (3, 0.1123), (4, 0.1135), (5, 0.1139), (6, 0.1147), (7, 0.1155), (8, 0.1164), (9, 0.1177), (10, 0.1169)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1169
wandb:     loss 2.30271
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_033231-2dlccglr
wandb: Find logs at: ./wandb/offline-run-20240407_033231-2dlccglr/logs
INFO flwr 2024-04-07 03:41:45,852 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:49:10,768 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1647121)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1647121)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 03:49:16,124	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:49:16,487	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:49:16,948	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3963e91461436bf2.zip' (11.10MiB) to Ray cluster...
2024-04-07 03:49:16,980	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3963e91461436bf2.zip'.
INFO flwr 2024-04-07 03:49:28,140 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57118167859.0, 'memory': 123275725005.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 03:49:28,140 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:49:28,141 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:49:28,168 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:49:28,169 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:49:28,170 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:49:28,170 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1664043)[0m 2024-04-07 03:49:34.181519: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1664043)[0m 2024-04-07 03:49:34.280404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1664043)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1664041)[0m 2024-04-07 03:49:36.319044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 03:49:36,693 | server.py:94 | initial parameters (loss, other metrics): 2.302522659301758, {'accuracy': 0.1157, 'data_size': 10000}
INFO flwr 2024-04-07 03:49:36,693 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:49:36,694 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1664044)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1664044)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1664036)[0m 2024-04-07 03:49:34.441799: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1664036)[0m 2024-04-07 03:49:34.539177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1664036)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1664036)[0m 2024-04-07 03:49:36.784617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1664037)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1664037)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 03:49:51,990 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:49:55,608 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:49:59,865 | server.py:125 | fit progress: (1, 2.3015592098236084, {'accuracy': 0.1341, 'data_size': 10000}, 23.17152829200495)
INFO flwr 2024-04-07 03:49:59,865 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:49:59,866 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:50:09,342 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:50:23,253 | server.py:125 | fit progress: (2, 2.300560235977173, {'accuracy': 0.1131, 'data_size': 10000}, 46.55911969701992)
INFO flwr 2024-04-07 03:50:23,253 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:50:23,253 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:50:31,809 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:50:54,603 | server.py:125 | fit progress: (3, 2.2986998558044434, {'accuracy': 0.1182, 'data_size': 10000}, 77.90906919800909)
INFO flwr 2024-04-07 03:50:54,603 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:50:54,603 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:51:03,361 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:51:33,784 | server.py:125 | fit progress: (4, 2.295492649078369, {'accuracy': 0.1561, 'data_size': 10000}, 117.0908867760154)
INFO flwr 2024-04-07 03:51:33,785 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:51:33,785 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:51:42,328 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:52:19,147 | server.py:125 | fit progress: (5, 2.2945637702941895, {'accuracy': 0.1028, 'data_size': 10000}, 162.45317833899753)
INFO flwr 2024-04-07 03:52:19,147 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:52:19,147 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:52:27,836 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:53:06,622 | server.py:125 | fit progress: (6, 2.2856104373931885, {'accuracy': 0.1967, 'data_size': 10000}, 209.92868238300434)
INFO flwr 2024-04-07 03:53:06,623 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:53:06,623 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:53:15,250 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:54:16,512 | server.py:125 | fit progress: (7, 2.2780861854553223, {'accuracy': 0.1969, 'data_size': 10000}, 279.81872441401356)
INFO flwr 2024-04-07 03:54:16,512 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:54:16,513 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:54:25,375 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:55:36,605 | server.py:125 | fit progress: (8, 2.2679271697998047, {'accuracy': 0.2943, 'data_size': 10000}, 359.91108724899823)
INFO flwr 2024-04-07 03:55:36,605 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:55:36,605 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:55:45,271 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:57:03,218 | server.py:125 | fit progress: (9, 2.250767946243286, {'accuracy': 0.2448, 'data_size': 10000}, 446.5244832820026)
INFO flwr 2024-04-07 03:57:03,218 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:57:03,219 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:57:11,843 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:58:39,655 | server.py:125 | fit progress: (10, 2.230117082595825, {'accuracy': 0.3653, 'data_size': 10000}, 542.961575052992)
INFO flwr 2024-04-07 03:58:39,655 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:58:39,656 | server.py:153 | FL finished in 542.962164461991
INFO flwr 2024-04-07 03:58:39,660 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:58:39,660 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:58:39,660 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:58:39,660 | app.py:229 | app_fit: losses_centralized [(0, 2.302522659301758), (1, 2.3015592098236084), (2, 2.300560235977173), (3, 2.2986998558044434), (4, 2.295492649078369), (5, 2.2945637702941895), (6, 2.2856104373931885), (7, 2.2780861854553223), (8, 2.2679271697998047), (9, 2.250767946243286), (10, 2.230117082595825)]
INFO flwr 2024-04-07 03:58:39,661 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1157), (1, 0.1341), (2, 0.1131), (3, 0.1182), (4, 0.1561), (5, 0.1028), (6, 0.1967), (7, 0.1969), (8, 0.2943), (9, 0.2448), (10, 0.3653)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3653
wandb:     loss 2.23012
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_034910-7bf7nv3j
wandb: Find logs at: ./wandb/offline-run-20240407_034910-7bf7nv3j/logs
INFO flwr 2024-04-07 03:58:43,249 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:06:08,003 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1664034)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1664034)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 04:06:13,070	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:06:13,558	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:06:14,053	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2e796946b7b1e28e.zip' (11.14MiB) to Ray cluster...
2024-04-07 04:06:14,083	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2e796946b7b1e28e.zip'.
INFO flwr 2024-04-07 04:06:25,256 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 123279532647.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57119799705.0}
INFO flwr 2024-04-07 04:06:25,257 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:06:25,257 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:06:25,276 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:06:25,277 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:06:25,277 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:06:25,277 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1675547)[0m 2024-04-07 04:06:30.987849: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1675547)[0m 2024-04-07 04:06:31.097199: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1675547)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 04:06:32,593 | server.py:94 | initial parameters (loss, other metrics): 2.302332639694214, {'accuracy': 0.141, 'data_size': 10000}
INFO flwr 2024-04-07 04:06:32,593 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:06:32,594 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1675547)[0m 2024-04-07 04:06:33.457763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1675558)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1675558)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1675557)[0m 2024-04-07 04:06:31.829701: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1675557)[0m 2024-04-07 04:06:31.918104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1675557)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1675553)[0m 2024-04-07 04:06:34.166945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1675546)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1675546)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 04:06:48,696 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:06:51,717 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:06:55,949 | server.py:125 | fit progress: (1, 2.3003087043762207, {'accuracy': 0.0974, 'data_size': 10000}, 23.354758058005245)
INFO flwr 2024-04-07 04:06:55,949 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:06:55,949 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:07:05,742 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:07:19,645 | server.py:125 | fit progress: (2, 2.296320676803589, {'accuracy': 0.1032, 'data_size': 10000}, 47.05168409598991)
INFO flwr 2024-04-07 04:07:19,646 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:07:19,646 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:07:28,736 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:07:48,925 | server.py:125 | fit progress: (3, 2.2970423698425293, {'accuracy': 0.1423, 'data_size': 10000}, 76.33173118898412)
INFO flwr 2024-04-07 04:07:48,926 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:07:48,926 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:07:57,543 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:08:27,707 | server.py:125 | fit progress: (4, 2.285505771636963, {'accuracy': 0.2092, 'data_size': 10000}, 115.11276274200645)
INFO flwr 2024-04-07 04:08:27,707 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:08:27,707 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:08:36,921 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:09:07,237 | server.py:125 | fit progress: (5, 2.276444911956787, {'accuracy': 0.2071, 'data_size': 10000}, 154.64293358800933)
INFO flwr 2024-04-07 04:09:07,237 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:09:07,237 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:09:16,184 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:10:04,582 | server.py:125 | fit progress: (6, 2.2624430656433105, {'accuracy': 0.1934, 'data_size': 10000}, 211.98834774500574)
INFO flwr 2024-04-07 04:10:04,583 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:10:04,583 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:10:12,939 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:11:14,082 | server.py:125 | fit progress: (7, 2.2348997592926025, {'accuracy': 0.2697, 'data_size': 10000}, 281.48847036098596)
INFO flwr 2024-04-07 04:11:14,083 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:11:14,083 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:11:22,931 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:12:31,557 | server.py:125 | fit progress: (8, 2.204777956008911, {'accuracy': 0.2019, 'data_size': 10000}, 358.96283748798305)
INFO flwr 2024-04-07 04:12:31,557 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:12:31,558 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:12:39,874 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:13:58,791 | server.py:125 | fit progress: (9, 2.1295876502990723, {'accuracy': 0.3611, 'data_size': 10000}, 446.19743841700256)
INFO flwr 2024-04-07 04:13:58,792 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:13:58,792 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:14:07,175 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:15:32,368 | server.py:125 | fit progress: (10, 2.0167055130004883, {'accuracy': 0.5637, 'data_size': 10000}, 539.7747325180098)
INFO flwr 2024-04-07 04:15:32,369 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:15:32,369 | server.py:153 | FL finished in 539.7752360710001
INFO flwr 2024-04-07 04:15:32,377 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:15:32,377 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:15:32,377 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:15:32,377 | app.py:229 | app_fit: losses_centralized [(0, 2.302332639694214), (1, 2.3003087043762207), (2, 2.296320676803589), (3, 2.2970423698425293), (4, 2.285505771636963), (5, 2.276444911956787), (6, 2.2624430656433105), (7, 2.2348997592926025), (8, 2.204777956008911), (9, 2.1295876502990723), (10, 2.0167055130004883)]
INFO flwr 2024-04-07 04:15:32,377 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.141), (1, 0.0974), (2, 0.1032), (3, 0.1423), (4, 0.2092), (5, 0.2071), (6, 0.1934), (7, 0.2697), (8, 0.2019), (9, 0.3611), (10, 0.5637)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5637
wandb:     loss 2.01671
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_040607-fomdequ1
wandb: Find logs at: ./wandb/offline-run-20240407_040607-fomdequ1/logs
INFO flwr 2024-04-07 04:15:36,008 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:23:00,947 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1675542)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1675542)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 04:23:06,838	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:23:07,177	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:23:07,516	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_96f7f640dbc010b1.zip' (11.16MiB) to Ray cluster...
2024-04-07 04:23:07,560	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_96f7f640dbc010b1.zip'.
INFO flwr 2024-04-07 04:23:18,731 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56897483980.0, 'CPU': 64.0, 'memory': 122760795956.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 04:23:18,732 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:23:18,732 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:23:18,748 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:23:18,750 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:23:18,750 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:23:18,751 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1683805)[0m 2024-04-07 04:23:24.752673: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1683805)[0m 2024-04-07 04:23:24.817636: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1683805)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 04:23:26,278 | server.py:94 | initial parameters (loss, other metrics): 2.302732229232788, {'accuracy': 0.0536, 'data_size': 10000}
INFO flwr 2024-04-07 04:23:26,278 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:23:26,279 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1683807)[0m 2024-04-07 04:23:26.335261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1683810)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1683810)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1683804)[0m 2024-04-07 04:23:27.498211: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1683804)[0m 2024-04-07 04:23:27.589215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1683804)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1683802)[0m 2024-04-07 04:23:29.631921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1683802)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1683802)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 04:23:43,432 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:23:47,085 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:23:51,348 | server.py:125 | fit progress: (1, 2.2989962100982666, {'accuracy': 0.1925, 'data_size': 10000}, 25.06948210200062)
INFO flwr 2024-04-07 04:23:51,348 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:23:51,349 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:24:01,627 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:24:14,834 | server.py:125 | fit progress: (2, 2.2954022884368896, {'accuracy': 0.0974, 'data_size': 10000}, 48.555314149009064)
INFO flwr 2024-04-07 04:24:14,834 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:24:14,835 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:24:24,001 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:24:48,008 | server.py:125 | fit progress: (3, 2.288191556930542, {'accuracy': 0.0892, 'data_size': 10000}, 81.729122558987)
INFO flwr 2024-04-07 04:24:48,008 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:24:48,009 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:24:57,459 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:25:32,692 | server.py:125 | fit progress: (4, 2.2617721557617188, {'accuracy': 0.169, 'data_size': 10000}, 126.41315349299111)
INFO flwr 2024-04-07 04:25:32,692 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:25:32,693 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:25:42,118 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:26:27,012 | server.py:125 | fit progress: (5, 2.2168405055999756, {'accuracy': 0.2511, 'data_size': 10000}, 180.73353198898258)
INFO flwr 2024-04-07 04:26:27,012 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:26:27,013 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:26:35,978 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:27:44,830 | server.py:125 | fit progress: (6, 2.0945444107055664, {'accuracy': 0.4294, 'data_size': 10000}, 258.5510683169996)
INFO flwr 2024-04-07 04:27:44,830 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:27:44,831 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:27:53,443 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:28:46,757 | server.py:125 | fit progress: (7, 1.9970287084579468, {'accuracy': 0.4722, 'data_size': 10000}, 320.478993814002)
INFO flwr 2024-04-07 04:28:46,758 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:28:46,758 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:28:56,531 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:30:33,039 | server.py:125 | fit progress: (8, 1.9166978597640991, {'accuracy': 0.6073, 'data_size': 10000}, 426.76089181998395)
INFO flwr 2024-04-07 04:30:33,040 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:30:33,040 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:30:41,601 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:32:27,037 | server.py:125 | fit progress: (9, 1.859775185585022, {'accuracy': 0.6242, 'data_size': 10000}, 540.7588025739824)
INFO flwr 2024-04-07 04:32:27,038 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:32:27,038 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:32:35,414 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:34:31,275 | server.py:125 | fit progress: (10, 1.7585091590881348, {'accuracy': 0.7329, 'data_size': 10000}, 664.996967516985)
INFO flwr 2024-04-07 04:34:31,276 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:34:31,276 | server.py:153 | FL finished in 664.9979083070066
INFO flwr 2024-04-07 04:34:31,277 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:34:31,277 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:34:31,277 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:34:31,277 | app.py:229 | app_fit: losses_centralized [(0, 2.302732229232788), (1, 2.2989962100982666), (2, 2.2954022884368896), (3, 2.288191556930542), (4, 2.2617721557617188), (5, 2.2168405055999756), (6, 2.0945444107055664), (7, 1.9970287084579468), (8, 1.9166978597640991), (9, 1.859775185585022), (10, 1.7585091590881348)]
INFO flwr 2024-04-07 04:34:31,277 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0536), (1, 0.1925), (2, 0.0974), (3, 0.0892), (4, 0.169), (5, 0.2511), (6, 0.4294), (7, 0.4722), (8, 0.6073), (9, 0.6242), (10, 0.7329)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7329
wandb:     loss 1.75851
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_042300-05vwqbll
wandb: Find logs at: ./wandb/offline-run-20240407_042300-05vwqbll/logs
INFO flwr 2024-04-07 04:34:34,860 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:42:00,292 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1683799)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1683799)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 04:42:05,620	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:42:05,889	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:42:06,221	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_796a7ff9879c90f4.zip' (11.19MiB) to Ray cluster...
2024-04-07 04:42:06,265	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_796a7ff9879c90f4.zip'.
INFO flwr 2024-04-07 04:42:17,593 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 124538652058.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57659422310.0}
INFO flwr 2024-04-07 04:42:17,593 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:42:17,593 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:42:17,611 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:42:17,612 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:42:17,612 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:42:17,612 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1695871)[0m 2024-04-07 04:42:23.707388: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1695871)[0m 2024-04-07 04:42:23.811339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1695871)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1695879)[0m 2024-04-07 04:42:25.566653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 04:42:25,929 | server.py:94 | initial parameters (loss, other metrics): 2.3024048805236816, {'accuracy': 0.1129, 'data_size': 10000}
INFO flwr 2024-04-07 04:42:25,930 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:42:25,930 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1695879)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1695879)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1695872)[0m 2024-04-07 04:42:24.109889: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1695872)[0m 2024-04-07 04:42:24.206232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1695872)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1695875)[0m 2024-04-07 04:42:26.482899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1695871)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1695871)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 04:42:40,659 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:42:44,114 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:42:48,566 | server.py:125 | fit progress: (1, 2.300013542175293, {'accuracy': 0.0974, 'data_size': 10000}, 22.636323802988045)
INFO flwr 2024-04-07 04:42:48,567 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:42:48,567 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:42:58,735 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:43:13,297 | server.py:125 | fit progress: (2, 2.2958791255950928, {'accuracy': 0.1009, 'data_size': 10000}, 47.36689716798719)
INFO flwr 2024-04-07 04:43:13,297 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:43:13,298 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:43:21,219 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:43:44,312 | server.py:125 | fit progress: (3, 2.2875661849975586, {'accuracy': 0.1028, 'data_size': 10000}, 78.38167886598967)
INFO flwr 2024-04-07 04:43:44,312 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:43:44,312 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:43:52,651 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:44:21,176 | server.py:125 | fit progress: (4, 2.208970308303833, {'accuracy': 0.2926, 'data_size': 10000}, 115.24640260197339)
INFO flwr 2024-04-07 04:44:21,177 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:44:21,177 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:44:29,419 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:45:04,374 | server.py:125 | fit progress: (5, 2.1272528171539307, {'accuracy': 0.381, 'data_size': 10000}, 158.44386417698115)
INFO flwr 2024-04-07 04:45:04,375 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:45:04,376 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:45:13,525 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:46:01,588 | server.py:125 | fit progress: (6, 2.0536434650421143, {'accuracy': 0.4053, 'data_size': 10000}, 215.65780768697732)
INFO flwr 2024-04-07 04:46:01,588 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:46:01,589 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:46:10,583 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:46:55,767 | server.py:125 | fit progress: (7, 1.9232051372528076, {'accuracy': 0.5826, 'data_size': 10000}, 269.83707231399603)
INFO flwr 2024-04-07 04:46:55,768 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:46:55,768 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:47:05,064 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:48:05,541 | server.py:125 | fit progress: (8, 1.8672125339508057, {'accuracy': 0.6053, 'data_size': 10000}, 339.61111027598963)
INFO flwr 2024-04-07 04:48:05,542 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:48:05,542 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:48:14,913 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:49:24,022 | server.py:125 | fit progress: (9, 1.787559986114502, {'accuracy': 0.6952, 'data_size': 10000}, 418.0923588569858)
INFO flwr 2024-04-07 04:49:24,023 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:49:24,023 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:49:33,028 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:51:25,305 | server.py:125 | fit progress: (10, 1.7500886917114258, {'accuracy': 0.7389, 'data_size': 10000}, 539.3751521999948)
INFO flwr 2024-04-07 04:51:25,306 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:51:25,306 | server.py:153 | FL finished in 539.3763294719975
INFO flwr 2024-04-07 04:51:25,307 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:51:25,307 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:51:25,307 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:51:25,307 | app.py:229 | app_fit: losses_centralized [(0, 2.3024048805236816), (1, 2.300013542175293), (2, 2.2958791255950928), (3, 2.2875661849975586), (4, 2.208970308303833), (5, 2.1272528171539307), (6, 2.0536434650421143), (7, 1.9232051372528076), (8, 1.8672125339508057), (9, 1.787559986114502), (10, 1.7500886917114258)]
INFO flwr 2024-04-07 04:51:25,307 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1129), (1, 0.0974), (2, 0.1009), (3, 0.1028), (4, 0.2926), (5, 0.381), (6, 0.4053), (7, 0.5826), (8, 0.6053), (9, 0.6952), (10, 0.7389)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7389
wandb:     loss 1.75009
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_044159-b4x05imu
wandb: Find logs at: ./wandb/offline-run-20240407_044159-b4x05imu/logs
INFO flwr 2024-04-07 04:51:28,898 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:58:54,674 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1695872)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1695872)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 04:59:00,454	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:59:00,796	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:59:01,179	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9457a3cd39adcb06.zip' (11.22MiB) to Ray cluster...
2024-04-07 04:59:01,223	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9457a3cd39adcb06.zip'.
INFO flwr 2024-04-07 04:59:12,549 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 56669462937.0, 'node:10.20.240.18': 1.0, 'memory': 122228746855.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 04:59:12,549 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:59:12,550 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:59:12,567 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:59:12,568 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:59:12,568 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:59:12,568 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1704915)[0m 2024-04-07 04:59:18.765997: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1704914)[0m 2024-04-07 04:59:18.873084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1704914)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1704914)[0m 2024-04-07 04:59:20.908726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 04:59:21,255 | server.py:94 | initial parameters (loss, other metrics): 2.3025424480438232, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-07 04:59:21,256 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:59:21,256 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1704919)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1704919)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1704918)[0m 2024-04-07 04:59:19.094670: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1704918)[0m 2024-04-07 04:59:19.200563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1704918)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1704918)[0m 2024-04-07 04:59:21.679361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1704912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1704912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:59:36,126 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:59:39,712 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:59:44,072 | server.py:125 | fit progress: (1, 2.29995059967041, {'accuracy': 0.119, 'data_size': 10000}, 22.816091351007344)
INFO flwr 2024-04-07 04:59:44,072 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:59:44,073 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:59:53,999 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:00:08,056 | server.py:125 | fit progress: (2, 2.2848594188690186, {'accuracy': 0.1747, 'data_size': 10000}, 46.80014443999971)
INFO flwr 2024-04-07 05:00:08,056 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:00:08,057 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:00:17,164 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:00:37,878 | server.py:125 | fit progress: (3, 2.262965202331543, {'accuracy': 0.1923, 'data_size': 10000}, 76.6222115290002)
INFO flwr 2024-04-07 05:00:37,879 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:00:37,879 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:00:46,732 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:01:13,721 | server.py:125 | fit progress: (4, 2.2269976139068604, {'accuracy': 0.3322, 'data_size': 10000}, 112.4652339949971)
INFO flwr 2024-04-07 05:01:13,722 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:01:13,722 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:01:22,649 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:01:55,419 | server.py:125 | fit progress: (5, 2.104494571685791, {'accuracy': 0.4683, 'data_size': 10000}, 154.16293701098766)
INFO flwr 2024-04-07 05:01:55,419 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:01:55,419 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:02:04,056 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:03:00,681 | server.py:125 | fit progress: (6, 1.9500236511230469, {'accuracy': 0.5361, 'data_size': 10000}, 219.42452142300317)
INFO flwr 2024-04-07 05:03:00,681 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:03:00,682 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:03:09,943 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:03:59,788 | server.py:125 | fit progress: (7, 1.8386800289154053, {'accuracy': 0.6251, 'data_size': 10000}, 278.53240277798614)
INFO flwr 2024-04-07 05:03:59,789 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:03:59,789 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:04:09,217 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:05:31,137 | server.py:125 | fit progress: (8, 1.7550591230392456, {'accuracy': 0.7286, 'data_size': 10000}, 369.88129258100525)
INFO flwr 2024-04-07 05:05:31,138 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:05:31,138 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:05:40,106 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:06:59,785 | server.py:125 | fit progress: (9, 1.6972377300262451, {'accuracy': 0.7753, 'data_size': 10000}, 458.52907217800384)
INFO flwr 2024-04-07 05:06:59,786 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:06:59,786 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:07:08,870 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:09:03,588 | server.py:125 | fit progress: (10, 1.6387872695922852, {'accuracy': 0.846, 'data_size': 10000}, 582.3322088569985)
INFO flwr 2024-04-07 05:09:03,589 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:09:03,589 | server.py:153 | FL finished in 582.3327663610107
INFO flwr 2024-04-07 05:09:03,589 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:09:03,589 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:09:03,589 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:09:03,589 | app.py:229 | app_fit: losses_centralized [(0, 2.3025424480438232), (1, 2.29995059967041), (2, 2.2848594188690186), (3, 2.262965202331543), (4, 2.2269976139068604), (5, 2.104494571685791), (6, 1.9500236511230469), (7, 1.8386800289154053), (8, 1.7550591230392456), (9, 1.6972377300262451), (10, 1.6387872695922852)]
INFO flwr 2024-04-07 05:09:03,590 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.119), (2, 0.1747), (3, 0.1923), (4, 0.3322), (5, 0.4683), (6, 0.5361), (7, 0.6251), (8, 0.7286), (9, 0.7753), (10, 0.846)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.846
wandb:     loss 1.63879
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_045854-m79acglw
wandb: Find logs at: ./wandb/offline-run-20240407_045854-m79acglw/logs
INFO flwr 2024-04-07 05:09:07,161 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:16:32,858 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 05:16:38,159	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:16:38,578	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:16:38,998	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2370b3103365d1cd.zip' (11.25MiB) to Ray cluster...
2024-04-07 05:16:39,043	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2370b3103365d1cd.zip'.
INFO flwr 2024-04-07 05:16:50,089 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 121505203200.0, 'CPU': 64.0, 'object_store_memory': 56359372800.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 05:16:50,089 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:16:50,089 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:16:50,113 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:16:50,114 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:16:50,114 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:16:50,115 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1713153)[0m 2024-04-07 05:16:56.093842: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1713153)[0m 2024-04-07 05:16:56.190727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1713153)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1713158)[0m 2024-04-07 05:16:58.245486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 05:16:58,296 | server.py:94 | initial parameters (loss, other metrics): 2.302541494369507, {'accuracy': 0.096, 'data_size': 10000}
INFO flwr 2024-04-07 05:16:58,296 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:16:58,297 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1713156)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1713156)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1713155)[0m 2024-04-07 05:16:56.502993: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1713155)[0m 2024-04-07 05:16:56.568402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1713155)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1713154)[0m 2024-04-07 05:16:58.874221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1713152)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1713152)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 05:17:13,565 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:17:17,121 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:17:21,452 | server.py:125 | fit progress: (1, 2.2974448204040527, {'accuracy': 0.0974, 'data_size': 10000}, 23.155322870006785)
INFO flwr 2024-04-07 05:17:21,452 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:17:21,452 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:17:31,371 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:17:45,669 | server.py:125 | fit progress: (2, 2.291783094406128, {'accuracy': 0.1041, 'data_size': 10000}, 47.37282336701173)
INFO flwr 2024-04-07 05:17:45,670 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:17:45,670 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:17:53,963 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:18:29,976 | server.py:125 | fit progress: (3, 2.2678446769714355, {'accuracy': 0.118, 'data_size': 10000}, 91.67918944099802)
INFO flwr 2024-04-07 05:18:29,977 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:18:29,977 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:18:38,761 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:20:10,377 | server.py:125 | fit progress: (4, 2.204141139984131, {'accuracy': 0.2646, 'data_size': 10000}, 192.0803177760099)
INFO flwr 2024-04-07 05:20:10,377 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:20:10,378 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:20:19,354 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:21:25,338 | server.py:125 | fit progress: (5, 2.1825246810913086, {'accuracy': 0.2103, 'data_size': 10000}, 267.04150874199695)
INFO flwr 2024-04-07 05:21:25,339 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:21:25,339 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:21:33,686 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:22:42,589 | server.py:125 | fit progress: (6, 1.9861021041870117, {'accuracy': 0.5471, 'data_size': 10000}, 344.29202871301095)
INFO flwr 2024-04-07 05:22:42,589 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:22:42,589 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:22:51,526 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:24:11,755 | server.py:125 | fit progress: (7, 1.8645273447036743, {'accuracy': 0.595, 'data_size': 10000}, 433.4579719589965)
INFO flwr 2024-04-07 05:24:11,755 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:24:11,755 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:24:20,639 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:25:49,365 | server.py:125 | fit progress: (8, 1.7668144702911377, {'accuracy': 0.7095, 'data_size': 10000}, 531.0684700229904)
INFO flwr 2024-04-07 05:25:49,365 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:25:49,366 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:25:58,067 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:27:24,548 | server.py:125 | fit progress: (9, 1.6825220584869385, {'accuracy': 0.8072, 'data_size': 10000}, 626.2512992160046)
INFO flwr 2024-04-07 05:27:24,548 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:27:24,548 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:27:33,282 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:30:03,728 | server.py:125 | fit progress: (10, 1.6198667287826538, {'accuracy': 0.8714, 'data_size': 10000}, 785.431541494996)
INFO flwr 2024-04-07 05:30:03,728 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:30:03,729 | server.py:153 | FL finished in 785.4320691289904
INFO flwr 2024-04-07 05:30:03,729 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:30:03,729 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:30:03,729 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:30:03,729 | app.py:229 | app_fit: losses_centralized [(0, 2.302541494369507), (1, 2.2974448204040527), (2, 2.291783094406128), (3, 2.2678446769714355), (4, 2.204141139984131), (5, 2.1825246810913086), (6, 1.9861021041870117), (7, 1.8645273447036743), (8, 1.7668144702911377), (9, 1.6825220584869385), (10, 1.6198667287826538)]
INFO flwr 2024-04-07 05:30:03,729 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.0974), (2, 0.1041), (3, 0.118), (4, 0.2646), (5, 0.2103), (6, 0.5471), (7, 0.595), (8, 0.7095), (9, 0.8072), (10, 0.8714)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8714
wandb:     loss 1.61987
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_051632-5jj0k9vu
wandb: Find logs at: ./wandb/offline-run-20240407_051632-5jj0k9vu/logs
INFO flwr 2024-04-07 05:30:07,369 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:37:32,543 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1713149)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1713149)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 05:37:37,899	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:37:38,360	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:37:38,748	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fde79eb4ff610c73.zip' (11.27MiB) to Ray cluster...
2024-04-07 05:37:38,779	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fde79eb4ff610c73.zip'.
INFO flwr 2024-04-07 05:37:49,681 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 126023394304.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 58295740416.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 05:37:49,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:37:49,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:37:49,697 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:37:49,698 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:37:49,698 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:37:49,698 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1722280)[0m 2024-04-07 05:37:55.821181: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1722280)[0m 2024-04-07 05:37:55.926569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1722280)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 05:37:57,049 | server.py:94 | initial parameters (loss, other metrics): 2.302757501602173, {'accuracy': 0.1018, 'data_size': 10000}
INFO flwr 2024-04-07 05:37:57,050 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:37:57,050 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1722280)[0m 2024-04-07 05:37:58.076792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1722284)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1722284)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1722286)[0m 2024-04-07 05:37:56.149118: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1722286)[0m 2024-04-07 05:37:56.243038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1722286)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1722286)[0m 2024-04-07 05:37:58.266338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 05:38:13,346 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:38:16,777 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:38:21,012 | server.py:125 | fit progress: (1, 2.30269718170166, {'accuracy': 0.1026, 'data_size': 10000}, 23.96188018898829)
INFO flwr 2024-04-07 05:38:21,012 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:38:21,012 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:38:30,852 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:38:44,601 | server.py:125 | fit progress: (2, 2.302621603012085, {'accuracy': 0.1032, 'data_size': 10000}, 47.55129262598348)
INFO flwr 2024-04-07 05:38:44,602 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:38:44,602 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:38:53,405 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:39:13,891 | server.py:125 | fit progress: (3, 2.302553415298462, {'accuracy': 0.1034, 'data_size': 10000}, 76.84127457498107)
INFO flwr 2024-04-07 05:39:13,892 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:39:13,892 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:39:22,818 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:39:49,136 | server.py:125 | fit progress: (4, 2.30249285697937, {'accuracy': 0.1042, 'data_size': 10000}, 112.08596200597822)
INFO flwr 2024-04-07 05:39:49,136 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:39:49,137 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:39:57,917 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:40:38,005 | server.py:125 | fit progress: (5, 2.3024303913116455, {'accuracy': 0.103, 'data_size': 10000}, 160.95528527497663)
INFO flwr 2024-04-07 05:40:38,006 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:40:38,006 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:40:47,526 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:41:32,202 | server.py:125 | fit progress: (6, 2.3023579120635986, {'accuracy': 0.1024, 'data_size': 10000}, 215.1524477060011)
INFO flwr 2024-04-07 05:41:32,203 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:41:32,203 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:41:41,412 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:43:50,768 | server.py:125 | fit progress: (7, 2.3022754192352295, {'accuracy': 0.1031, 'data_size': 10000}, 353.7178688249842)
INFO flwr 2024-04-07 05:43:50,768 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:43:50,769 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:43:59,816 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:46:53,343 | server.py:125 | fit progress: (8, 2.302206039428711, {'accuracy': 0.1042, 'data_size': 10000}, 536.2924601699924)
INFO flwr 2024-04-07 05:46:53,343 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:46:53,343 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:47:02,225 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:51:09,466 | server.py:125 | fit progress: (9, 2.3021304607391357, {'accuracy': 0.1109, 'data_size': 10000}, 792.4162295970018)
INFO flwr 2024-04-07 05:51:09,467 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:51:09,467 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:51:18,126 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:54:41,885 | server.py:125 | fit progress: (10, 2.3020622730255127, {'accuracy': 0.1293, 'data_size': 10000}, 1004.8348990119994)
INFO flwr 2024-04-07 05:54:41,886 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:54:41,886 | server.py:153 | FL finished in 1004.8357266319799
INFO flwr 2024-04-07 05:54:41,886 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:54:41,886 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:54:41,886 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:54:41,886 | app.py:229 | app_fit: losses_centralized [(0, 2.302757501602173), (1, 2.30269718170166), (2, 2.302621603012085), (3, 2.302553415298462), (4, 2.30249285697937), (5, 2.3024303913116455), (6, 2.3023579120635986), (7, 2.3022754192352295), (8, 2.302206039428711), (9, 2.3021304607391357), (10, 2.3020622730255127)]
INFO flwr 2024-04-07 05:54:41,887 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1018), (1, 0.1026), (2, 0.1032), (3, 0.1034), (4, 0.1042), (5, 0.103), (6, 0.1024), (7, 0.1031), (8, 0.1042), (9, 0.1109), (10, 0.1293)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1293
wandb:     loss 2.30206
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_053732-q9iu7x07
wandb: Find logs at: ./wandb/offline-run-20240407_053732-q9iu7x07/logs
INFO flwr 2024-04-07 05:54:45,486 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:02:10,726 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1722279)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1722279)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:02:17,554	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:02:17,915	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:02:18,285	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d28580dca38e6c70.zip' (11.31MiB) to Ray cluster...
2024-04-07 06:02:18,321	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d28580dca38e6c70.zip'.
INFO flwr 2024-04-07 06:02:29,575 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 120880480461.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56091634483.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 06:02:29,575 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:02:29,575 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:02:29,593 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:02:29,594 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:02:29,594 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:02:29,595 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1735165)[0m 2024-04-07 06:02:35.794335: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1735165)[0m 2024-04-07 06:02:35.893608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1735165)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 06:02:37,913 | server.py:94 | initial parameters (loss, other metrics): 2.3024704456329346, {'accuracy': 0.1162, 'data_size': 10000}
INFO flwr 2024-04-07 06:02:37,914 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:02:37,915 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1735170)[0m 2024-04-07 06:02:37.951340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1735170)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1735170)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1735159)[0m 2024-04-07 06:02:35.941690: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1735159)[0m 2024-04-07 06:02:36.102327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1735159)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1735158)[0m 2024-04-07 06:02:38.624429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1735159)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1735159)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 06:02:54,059 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:02:57,643 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:03:01,967 | server.py:125 | fit progress: (1, 2.297386407852173, {'accuracy': 0.101, 'data_size': 10000}, 24.05232314299792)
INFO flwr 2024-04-07 06:03:01,967 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:03:01,967 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:03:11,797 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:03:25,703 | server.py:125 | fit progress: (2, 2.2806930541992188, {'accuracy': 0.101, 'data_size': 10000}, 47.78819742400083)
INFO flwr 2024-04-07 06:03:25,703 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:03:25,703 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:03:34,752 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:03:55,260 | server.py:125 | fit progress: (3, 2.182511806488037, {'accuracy': 0.257, 'data_size': 10000}, 77.34525690998998)
INFO flwr 2024-04-07 06:03:55,260 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:03:55,260 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:04:04,908 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:04:33,387 | server.py:125 | fit progress: (4, 1.9784672260284424, {'accuracy': 0.4971, 'data_size': 10000}, 115.47210252401419)
INFO flwr 2024-04-07 06:04:33,387 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:04:33,387 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:04:42,744 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:05:22,177 | server.py:125 | fit progress: (5, 1.8322265148162842, {'accuracy': 0.6384, 'data_size': 10000}, 164.26286586601054)
INFO flwr 2024-04-07 06:05:22,178 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:05:22,178 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:05:31,368 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:06:16,689 | server.py:125 | fit progress: (6, 1.73612642288208, {'accuracy': 0.7386, 'data_size': 10000}, 218.77498801398906)
INFO flwr 2024-04-07 06:06:16,690 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:06:16,690 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:06:25,866 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:07:56,074 | server.py:125 | fit progress: (7, 1.6965984106063843, {'accuracy': 0.7813, 'data_size': 10000}, 318.1597128729918)
INFO flwr 2024-04-07 06:07:56,075 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:07:56,075 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:08:05,519 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:10:25,437 | server.py:125 | fit progress: (8, 1.6401764154434204, {'accuracy': 0.844, 'data_size': 10000}, 467.52224852200015)
INFO flwr 2024-04-07 06:10:25,437 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:10:25,438 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:10:35,220 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:13:17,465 | server.py:125 | fit progress: (9, 1.6522523164749146, {'accuracy': 0.8111, 'data_size': 10000}, 639.5509939200128)
INFO flwr 2024-04-07 06:13:17,466 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:13:17,466 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:13:26,484 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:17:19,141 | server.py:125 | fit progress: (10, 1.5783284902572632, {'accuracy': 0.9006, 'data_size': 10000}, 881.2266403370013)
INFO flwr 2024-04-07 06:17:19,141 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:17:19,142 | server.py:153 | FL finished in 881.2271783539909
INFO flwr 2024-04-07 06:17:19,142 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:17:19,142 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:17:19,142 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:17:19,142 | app.py:229 | app_fit: losses_centralized [(0, 2.3024704456329346), (1, 2.297386407852173), (2, 2.2806930541992188), (3, 2.182511806488037), (4, 1.9784672260284424), (5, 1.8322265148162842), (6, 1.73612642288208), (7, 1.6965984106063843), (8, 1.6401764154434204), (9, 1.6522523164749146), (10, 1.5783284902572632)]
INFO flwr 2024-04-07 06:17:19,142 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1162), (1, 0.101), (2, 0.101), (3, 0.257), (4, 0.4971), (5, 0.6384), (6, 0.7386), (7, 0.7813), (8, 0.844), (9, 0.8111), (10, 0.9006)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9006
wandb:     loss 1.57833
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_060210-umzpltam
wandb: Find logs at: ./wandb/offline-run-20240407_060210-umzpltam/logs
INFO flwr 2024-04-07 06:17:22,664 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:24:48,200 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1735158)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1735158)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 06:24:54,162	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:24:54,507	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:24:54,951	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8d2b64b2f5b559a2.zip' (11.35MiB) to Ray cluster...
2024-04-07 06:24:54,980	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8d2b64b2f5b559a2.zip'.
INFO flwr 2024-04-07 06:25:06,214 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 119428854375.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 55469509017.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 06:25:06,214 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:25:06,214 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:25:06,230 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:25:06,231 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:25:06,232 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:25:06,232 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1747709)[0m 2024-04-07 06:25:12.243241: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1747709)[0m 2024-04-07 06:25:12.384379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1747709)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 06:25:14,426 | server.py:94 | initial parameters (loss, other metrics): 2.30237078666687, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-04-07 06:25:14,426 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:25:14,427 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1747720)[0m 2024-04-07 06:25:14.508386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1747720)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1747720)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1747716)[0m 2024-04-07 06:25:12.443385: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1747716)[0m 2024-04-07 06:25:12.551155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1747716)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1747722)[0m 2024-04-07 06:25:14.847342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1747715)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1747715)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 06:25:30,807 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:25:34,458 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:25:38,674 | server.py:125 | fit progress: (1, 2.2345046997070312, {'accuracy': 0.3798, 'data_size': 10000}, 24.24720060901018)
INFO flwr 2024-04-07 06:25:38,674 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:25:38,675 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:25:48,552 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:26:02,477 | server.py:125 | fit progress: (2, 2.0062336921691895, {'accuracy': 0.5046, 'data_size': 10000}, 48.05039992101956)
INFO flwr 2024-04-07 06:26:02,478 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:26:02,478 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:26:11,701 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:26:32,106 | server.py:125 | fit progress: (3, 1.8010767698287964, {'accuracy': 0.6779, 'data_size': 10000}, 77.6789727639989)
INFO flwr 2024-04-07 06:26:32,106 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:26:32,106 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:26:41,245 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:27:07,958 | server.py:125 | fit progress: (4, 1.6331853866577148, {'accuracy': 0.8588, 'data_size': 10000}, 113.5314496490173)
INFO flwr 2024-04-07 06:27:07,958 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:27:07,959 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:27:16,540 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:27:49,262 | server.py:125 | fit progress: (5, 1.5989643335342407, {'accuracy': 0.8779, 'data_size': 10000}, 154.83516542299185)
INFO flwr 2024-04-07 06:27:49,262 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:27:49,262 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:27:58,529 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:29:36,684 | server.py:125 | fit progress: (6, 1.5873209238052368, {'accuracy': 0.8808, 'data_size': 10000}, 262.25715646799654)
INFO flwr 2024-04-07 06:29:36,684 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:29:36,685 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:29:46,502 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:30:59,669 | server.py:125 | fit progress: (7, 1.5432544946670532, {'accuracy': 0.9275, 'data_size': 10000}, 345.2421834290144)
INFO flwr 2024-04-07 06:30:59,669 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:30:59,670 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:09,278 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:33:56,543 | server.py:125 | fit progress: (8, 1.5329532623291016, {'accuracy': 0.9368, 'data_size': 10000}, 522.1165259230183)
INFO flwr 2024-04-07 06:33:56,544 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:33:56,544 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:34:06,079 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:37:16,348 | server.py:125 | fit progress: (9, 1.5300922393798828, {'accuracy': 0.9394, 'data_size': 10000}, 721.9210078339966)
INFO flwr 2024-04-07 06:37:16,348 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:37:16,349 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:37:25,810 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:40:35,392 | server.py:125 | fit progress: (10, 1.5215198993682861, {'accuracy': 0.9443, 'data_size': 10000}, 920.9648253750056)
INFO flwr 2024-04-07 06:40:35,392 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:40:35,392 | server.py:153 | FL finished in 920.9654193920142
INFO flwr 2024-04-07 06:40:35,392 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:40:35,392 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:40:35,393 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:40:35,393 | app.py:229 | app_fit: losses_centralized [(0, 2.30237078666687), (1, 2.2345046997070312), (2, 2.0062336921691895), (3, 1.8010767698287964), (4, 1.6331853866577148), (5, 1.5989643335342407), (6, 1.5873209238052368), (7, 1.5432544946670532), (8, 1.5329532623291016), (9, 1.5300922393798828), (10, 1.5215198993682861)]
INFO flwr 2024-04-07 06:40:35,393 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.3798), (2, 0.5046), (3, 0.6779), (4, 0.8588), (5, 0.8779), (6, 0.8808), (7, 0.9275), (8, 0.9368), (9, 0.9394), (10, 0.9443)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9443
wandb:     loss 1.52152
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_062447-cqi6u3lt
wandb: Find logs at: ./wandb/offline-run-20240407_062447-cqi6u3lt/logs
INFO flwr 2024-04-07 06:40:38,981 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:48:04,170 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1747711)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1747711)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 06:48:09,856	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:48:10,428	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:48:10,873	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a620a388c166571c.zip' (11.38MiB) to Ray cluster...
2024-04-07 06:48:10,906	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a620a388c166571c.zip'.
INFO flwr 2024-04-07 06:48:21,703 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 119311646106.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55419276902.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 06:48:21,704 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:48:21,704 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:48:21,719 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:48:21,722 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:48:21,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:48:21,723 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1760020)[0m 2024-04-07 06:48:27.802849: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1760022)[0m 2024-04-07 06:48:27.888631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1760022)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 06:48:28,885 | server.py:94 | initial parameters (loss, other metrics): 2.302572011947632, {'accuracy': 0.0905, 'data_size': 10000}
INFO flwr 2024-04-07 06:48:28,885 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:48:28,886 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1760022)[0m 2024-04-07 06:48:30.135544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1760023)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1760023)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1760018)[0m 2024-04-07 06:48:28.008778: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1760018)[0m 2024-04-07 06:48:28.087165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1760018)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1760020)[0m 2024-04-07 06:48:30.155269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:48:44,902 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:48:48,208 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:48:52,486 | server.py:125 | fit progress: (1, 2.2715890407562256, {'accuracy': 0.1985, 'data_size': 10000}, 23.599778823001543)
INFO flwr 2024-04-07 06:48:52,486 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:48:52,487 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:49:02,715 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:49:16,958 | server.py:125 | fit progress: (2, 1.9708194732666016, {'accuracy': 0.527, 'data_size': 10000}, 48.07203249100712)
INFO flwr 2024-04-07 06:49:16,958 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:49:16,958 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:49:26,147 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:49:46,972 | server.py:125 | fit progress: (3, 1.6863408088684082, {'accuracy': 0.8085, 'data_size': 10000}, 78.08570134898764)
INFO flwr 2024-04-07 06:49:46,972 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:49:46,973 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:49:56,980 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:27,546 | server.py:125 | fit progress: (4, 1.6280730962753296, {'accuracy': 0.8479, 'data_size': 10000}, 118.66021955799079)
INFO flwr 2024-04-07 06:50:27,547 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:27,547 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:36,800 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:51:16,193 | server.py:125 | fit progress: (5, 1.5587080717086792, {'accuracy': 0.9221, 'data_size': 10000}, 167.30742463000934)
INFO flwr 2024-04-07 06:51:16,194 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:51:16,194 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:51:25,528 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:52:14,637 | server.py:125 | fit progress: (6, 1.5417193174362183, {'accuracy': 0.9283, 'data_size': 10000}, 225.7515831369965)
INFO flwr 2024-04-07 06:52:14,638 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:52:14,638 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:52:23,885 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:53:51,013 | server.py:125 | fit progress: (7, 1.5330355167388916, {'accuracy': 0.9344, 'data_size': 10000}, 322.1274517420097)
INFO flwr 2024-04-07 06:53:51,014 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:53:51,014 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:54:00,060 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:55:08,490 | server.py:125 | fit progress: (8, 1.5183804035186768, {'accuracy': 0.9472, 'data_size': 10000}, 399.604550774995)
INFO flwr 2024-04-07 06:55:08,491 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:55:08,491 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:55:17,675 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:58:41,602 | server.py:125 | fit progress: (9, 1.513179898262024, {'accuracy': 0.9523, 'data_size': 10000}, 612.7162548780034)
INFO flwr 2024-04-07 06:58:41,603 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:58:41,603 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:58:50,626 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:01:05,249 | server.py:125 | fit progress: (10, 1.5206187963485718, {'accuracy': 0.9455, 'data_size': 10000}, 756.3636075420072)
INFO flwr 2024-04-07 07:01:05,250 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:01:05,250 | server.py:153 | FL finished in 756.3641599889961
INFO flwr 2024-04-07 07:01:05,250 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:01:05,250 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:01:05,250 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:01:05,251 | app.py:229 | app_fit: losses_centralized [(0, 2.302572011947632), (1, 2.2715890407562256), (2, 1.9708194732666016), (3, 1.6863408088684082), (4, 1.6280730962753296), (5, 1.5587080717086792), (6, 1.5417193174362183), (7, 1.5330355167388916), (8, 1.5183804035186768), (9, 1.513179898262024), (10, 1.5206187963485718)]
INFO flwr 2024-04-07 07:01:05,251 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0905), (1, 0.1985), (2, 0.527), (3, 0.8085), (4, 0.8479), (5, 0.9221), (6, 0.9283), (7, 0.9344), (8, 0.9472), (9, 0.9523), (10, 0.9455)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9455
wandb:     loss 1.52062
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_064803-axxlwfm6
wandb: Find logs at: ./wandb/offline-run-20240407_064803-axxlwfm6/logs
INFO flwr 2024-04-07 07:01:09,020 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:08:35,232 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1760013)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1760013)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:08:41,188	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:08:41,477	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:08:41,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7473250dbb49f747.zip' (11.42MiB) to Ray cluster...
2024-04-07 07:08:41,863	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7473250dbb49f747.zip'.
INFO flwr 2024-04-07 07:08:52,861 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 120855575962.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56080961126.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 07:08:52,861 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:08:52,861 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:08:52,877 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:08:52,878 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:08:52,878 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:08:52,878 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1775176)[0m 2024-04-07 07:08:58.045817: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1775176)[0m 2024-04-07 07:08:58.120258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1775176)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1775176)[0m 2024-04-07 07:09:00.124122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 07:09:00,293 | server.py:94 | initial parameters (loss, other metrics): 2.3025922775268555, {'accuracy': 0.0976, 'data_size': 10000}
INFO flwr 2024-04-07 07:09:00,293 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:09:00,294 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1775192)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1775192)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1775181)[0m 2024-04-07 07:08:59.436784: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1775178)[0m 2024-04-07 07:08:59.506585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1775178)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1775178)[0m 2024-04-07 07:09:01.953878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1775181)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1775181)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-07 07:09:18,046 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:09:21,500 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:09:25,897 | server.py:125 | fit progress: (1, 2.21523118019104, {'accuracy': 0.4468, 'data_size': 10000}, 25.603136302001076)
INFO flwr 2024-04-07 07:09:25,897 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:25,897 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:36,691 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:50,734 | server.py:125 | fit progress: (2, 1.831983208656311, {'accuracy': 0.7129, 'data_size': 10000}, 50.44023918299354)
INFO flwr 2024-04-07 07:09:50,734 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:50,734 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:10:01,173 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:10:22,157 | server.py:125 | fit progress: (3, 1.6155937910079956, {'accuracy': 0.8731, 'data_size': 10000}, 81.86356136700488)
INFO flwr 2024-04-07 07:10:22,157 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:10:22,158 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:10:31,175 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:10:57,728 | server.py:125 | fit progress: (4, 1.5729700326919556, {'accuracy': 0.9006, 'data_size': 10000}, 117.43441452001571)
INFO flwr 2024-04-07 07:10:57,728 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:10:57,728 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:11:07,236 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:11:40,095 | server.py:125 | fit progress: (5, 1.5342419147491455, {'accuracy': 0.9332, 'data_size': 10000}, 159.8017429120082)
INFO flwr 2024-04-07 07:11:40,095 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:11:40,096 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:11:49,111 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:12:30,008 | server.py:125 | fit progress: (6, 1.5232443809509277, {'accuracy': 0.9435, 'data_size': 10000}, 209.7142720860138)
INFO flwr 2024-04-07 07:12:30,008 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:12:30,009 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:12:39,056 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:13:42,574 | server.py:125 | fit progress: (7, 1.5195817947387695, {'accuracy': 0.946, 'data_size': 10000}, 282.28059882001253)
INFO flwr 2024-04-07 07:13:42,575 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:13:42,576 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:13:52,314 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:15:22,203 | server.py:125 | fit progress: (8, 1.5111085176467896, {'accuracy': 0.9531, 'data_size': 10000}, 381.909670686)
INFO flwr 2024-04-07 07:15:22,204 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:15:22,204 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:15:32,148 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:17:42,427 | server.py:125 | fit progress: (9, 1.508770227432251, {'accuracy': 0.9547, 'data_size': 10000}, 522.1331050379958)
INFO flwr 2024-04-07 07:17:42,427 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:17:42,427 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:17:51,323 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:54,324 | server.py:125 | fit progress: (10, 1.5072286128997803, {'accuracy': 0.9574, 'data_size': 10000}, 654.0301565380068)
INFO flwr 2024-04-07 07:19:54,325 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:19:54,325 | server.py:153 | FL finished in 654.031390166987
INFO flwr 2024-04-07 07:19:54,325 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:19:54,325 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:19:54,325 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:19:54,326 | app.py:229 | app_fit: losses_centralized [(0, 2.3025922775268555), (1, 2.21523118019104), (2, 1.831983208656311), (3, 1.6155937910079956), (4, 1.5729700326919556), (5, 1.5342419147491455), (6, 1.5232443809509277), (7, 1.5195817947387695), (8, 1.5111085176467896), (9, 1.508770227432251), (10, 1.5072286128997803)]
INFO flwr 2024-04-07 07:19:54,326 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0976), (1, 0.4468), (2, 0.7129), (3, 0.8731), (4, 0.9006), (5, 0.9332), (6, 0.9435), (7, 0.946), (8, 0.9531), (9, 0.9547), (10, 0.9574)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9574
wandb:     loss 1.50723
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_070834-nhyw99es
wandb: Find logs at: ./wandb/offline-run-20240407_070834-nhyw99es/logs
INFO flwr 2024-04-07 07:19:58,210 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:27:22,554 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1775176)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1775176)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-07 07:27:27,998	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:27:28,344	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:27:28,711	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_967d11b1b3ae4991.zip' (11.46MiB) to Ray cluster...
2024-04-07 07:27:28,760	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_967d11b1b3ae4991.zip'.
INFO flwr 2024-04-07 07:27:40,820 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 125523529524.0, 'node:__internal_head__': 1.0, 'object_store_memory': 58081512652.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 07:27:40,821 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:27:40,821 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:27:40,837 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:27:40,838 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:27:40,838 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:27:40,838 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1785489)[0m 2024-04-07 07:27:47.684994: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1785495)[0m 2024-04-07 07:27:47.774478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1785495)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 07:27:49,301 | server.py:94 | initial parameters (loss, other metrics): 2.302604913711548, {'accuracy': 0.0626, 'data_size': 10000}
INFO flwr 2024-04-07 07:27:49,301 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:27:49,302 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1785484)[0m 2024-04-07 07:27:50.173400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1785504)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1785504)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1785498)[0m 2024-04-07 07:27:47.911303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785498)[0m 2024-04-07 07:27:48.012192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785498)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785505)[0m 2024-04-07 07:27:50.180787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:28:07,596 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:28:11,161 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:28:15,521 | server.py:125 | fit progress: (1, 2.207606554031372, {'accuracy': 0.4098, 'data_size': 10000}, 26.219330998021178)
INFO flwr 2024-04-07 07:28:15,521 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:15,521 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:25,079 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:39,032 | server.py:125 | fit progress: (2, 1.7938337326049805, {'accuracy': 0.7071, 'data_size': 10000}, 49.730179566016886)
INFO flwr 2024-04-07 07:28:39,032 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:39,032 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:48,471 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:09,193 | server.py:125 | fit progress: (3, 1.5705474615097046, {'accuracy': 0.9096, 'data_size': 10000}, 79.89163462101715)
INFO flwr 2024-04-07 07:29:09,193 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:29:09,194 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:17,898 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:45,198 | server.py:125 | fit progress: (4, 1.532450556755066, {'accuracy': 0.9385, 'data_size': 10000}, 115.8968057630118)
INFO flwr 2024-04-07 07:29:45,198 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:29:45,199 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:54,156 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:30:26,918 | server.py:125 | fit progress: (5, 1.5321890115737915, {'accuracy': 0.9328, 'data_size': 10000}, 157.6166355460009)
INFO flwr 2024-04-07 07:30:26,918 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:30:26,919 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:30:36,226 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:31:20,856 | server.py:125 | fit progress: (6, 1.5236455202102661, {'accuracy': 0.9426, 'data_size': 10000}, 211.55505496601108)
INFO flwr 2024-04-07 07:31:20,857 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:31:20,857 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:31:29,944 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:32:14,915 | server.py:125 | fit progress: (7, 1.5088460445404053, {'accuracy': 0.9553, 'data_size': 10000}, 265.61318540602224)
INFO flwr 2024-04-07 07:32:14,915 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:32:14,915 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:32:24,057 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:33:40,221 | server.py:125 | fit progress: (8, 1.5050333738327026, {'accuracy': 0.9593, 'data_size': 10000}, 350.9194194530137)
INFO flwr 2024-04-07 07:33:40,221 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:33:40,222 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:33:49,205 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:35:22,437 | server.py:125 | fit progress: (9, 1.501188039779663, {'accuracy': 0.9622, 'data_size': 10000}, 453.1352586020075)
INFO flwr 2024-04-07 07:35:22,437 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:35:22,437 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:35:31,449 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:37:30,877 | server.py:125 | fit progress: (10, 1.5029839277267456, {'accuracy': 0.9599, 'data_size': 10000}, 581.575762944005)
INFO flwr 2024-04-07 07:37:30,878 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:37:30,878 | server.py:153 | FL finished in 581.576400208025
INFO flwr 2024-04-07 07:37:30,878 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:37:30,878 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:37:30,878 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:37:30,878 | app.py:229 | app_fit: losses_centralized [(0, 2.302604913711548), (1, 2.207606554031372), (2, 1.7938337326049805), (3, 1.5705474615097046), (4, 1.532450556755066), (5, 1.5321890115737915), (6, 1.5236455202102661), (7, 1.5088460445404053), (8, 1.5050333738327026), (9, 1.501188039779663), (10, 1.5029839277267456)]
INFO flwr 2024-04-07 07:37:30,879 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0626), (1, 0.4098), (2, 0.7071), (3, 0.9096), (4, 0.9385), (5, 0.9328), (6, 0.9426), (7, 0.9553), (8, 0.9593), (9, 0.9622), (10, 0.9599)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9599
wandb:     loss 1.50298
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_072721-qo9yvb5c
wandb: Find logs at: ./wandb/offline-run-20240407_072721-qo9yvb5c/logs
INFO flwr 2024-04-07 07:37:34,799 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:44:59,861 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1785484)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1785484)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:45:05,093	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:45:05,551	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:45:06,080	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4c931ccb0cca714c.zip' (11.49MiB) to Ray cluster...
2024-04-07 07:45:06,115	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4c931ccb0cca714c.zip'.
INFO flwr 2024-04-07 07:45:17,134 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 118378062848.0, 'object_store_memory': 55019169792.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 07:45:17,135 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:45:17,135 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:45:17,153 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:45:17,154 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:45:17,154 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:45:17,154 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1796178)[0m 2024-04-07 07:45:23.087368: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1796178)[0m 2024-04-07 07:45:23.187428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1796178)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 07:45:24,947 | server.py:94 | initial parameters (loss, other metrics): 2.302391767501831, {'accuracy': 0.1059, 'data_size': 10000}
INFO flwr 2024-04-07 07:45:24,947 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:45:24,948 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1796173)[0m 2024-04-07 07:45:25.184224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1796180)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1796180)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1796179)[0m 2024-04-07 07:45:23.530324: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1796179)[0m 2024-04-07 07:45:23.629495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1796179)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1796176)[0m 2024-04-07 07:45:25.775962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1796172)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1796172)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:45:40,337 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:45:43,376 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:45:47,675 | server.py:125 | fit progress: (1, 2.1945807933807373, {'accuracy': 0.452, 'data_size': 10000}, 22.727398738003103)
INFO flwr 2024-04-07 07:45:47,676 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:45:47,676 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:45:57,505 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:46:12,313 | server.py:125 | fit progress: (2, 1.9314638376235962, {'accuracy': 0.5246, 'data_size': 10000}, 47.36499659399851)
INFO flwr 2024-04-07 07:46:12,313 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:46:12,313 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:46:21,454 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:46:42,069 | server.py:125 | fit progress: (3, 1.6677433252334595, {'accuracy': 0.7978, 'data_size': 10000}, 77.12117110998952)
INFO flwr 2024-04-07 07:46:42,069 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:46:42,070 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:46:51,379 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:47:27,438 | server.py:125 | fit progress: (4, 1.5363707542419434, {'accuracy': 0.9357, 'data_size': 10000}, 122.49058128200704)
INFO flwr 2024-04-07 07:47:27,438 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:47:27,439 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:47:36,613 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:15,158 | server.py:125 | fit progress: (5, 1.5597342252731323, {'accuracy': 0.9035, 'data_size': 10000}, 170.21041669300757)
INFO flwr 2024-04-07 07:48:15,158 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:15,159 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:24,584 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:49:03,569 | server.py:125 | fit progress: (6, 1.5090798139572144, {'accuracy': 0.9563, 'data_size': 10000}, 218.62130685101147)
INFO flwr 2024-04-07 07:49:03,569 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:49:03,569 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:49:12,610 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:50:05,643 | server.py:125 | fit progress: (7, 1.5181082487106323, {'accuracy': 0.9459, 'data_size': 10000}, 280.6951807359874)
INFO flwr 2024-04-07 07:50:05,643 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:50:05,644 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:50:14,613 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:51:55,360 | server.py:125 | fit progress: (8, 1.5036941766738892, {'accuracy': 0.9594, 'data_size': 10000}, 390.412480717001)
INFO flwr 2024-04-07 07:51:55,360 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:51:55,361 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:52:04,233 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:53:51,570 | server.py:125 | fit progress: (9, 1.500148892402649, {'accuracy': 0.9626, 'data_size': 10000}, 506.62235427301493)
INFO flwr 2024-04-07 07:53:51,570 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:53:51,571 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:54:00,643 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:56:17,756 | server.py:125 | fit progress: (10, 1.501924753189087, {'accuracy': 0.9607, 'data_size': 10000}, 652.8088660670037)
INFO flwr 2024-04-07 07:56:17,757 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:56:17,757 | server.py:153 | FL finished in 652.8095570720034
INFO flwr 2024-04-07 07:56:17,761 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:56:17,761 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:56:17,762 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:56:17,762 | app.py:229 | app_fit: losses_centralized [(0, 2.302391767501831), (1, 2.1945807933807373), (2, 1.9314638376235962), (3, 1.6677433252334595), (4, 1.5363707542419434), (5, 1.5597342252731323), (6, 1.5090798139572144), (7, 1.5181082487106323), (8, 1.5036941766738892), (9, 1.500148892402649), (10, 1.501924753189087)]
INFO flwr 2024-04-07 07:56:17,762 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1059), (1, 0.452), (2, 0.5246), (3, 0.7978), (4, 0.9357), (5, 0.9035), (6, 0.9563), (7, 0.9459), (8, 0.9594), (9, 0.9626), (10, 0.9607)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9607
wandb:     loss 1.50192
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_074459-fk0zgduy
wandb: Find logs at: ./wandb/offline-run-20240407_074459-fk0zgduy/logs
INFO flwr 2024-04-07 07:56:21,404 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:03:46,519 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 08:03:52,003	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:03:52,452	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:03:52,916	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8a934e21ff9820e8.zip' (11.53MiB) to Ray cluster...
2024-04-07 08:03:52,950	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8a934e21ff9820e8.zip'.
INFO flwr 2024-04-07 08:04:03,927 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 118179735757.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54934172467.0}
INFO flwr 2024-04-07 08:04:03,927 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:04:03,927 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:04:03,945 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:04:03,946 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:04:03,946 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:04:03,946 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1807728)[0m 2024-04-07 08:04:09.453038: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1807728)[0m 2024-04-07 08:04:09.523127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1807728)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1807728)[0m 2024-04-07 08:04:11.784447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 08:04:12,953 | server.py:94 | initial parameters (loss, other metrics): 2.3025074005126953, {'accuracy': 0.1193, 'data_size': 10000}
INFO flwr 2024-04-07 08:04:12,954 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:04:12,955 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1807728)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1807728)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1807730)[0m 2024-04-07 08:04:10.697281: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1807730)[0m 2024-04-07 08:04:10.789879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1807730)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1807730)[0m 2024-04-07 08:04:13.182784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1807723)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1807723)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:04:28,513 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:04:32,049 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:04:36,524 | server.py:125 | fit progress: (1, 2.302459478378296, {'accuracy': 0.1232, 'data_size': 10000}, 23.569915921019856)
INFO flwr 2024-04-07 08:04:36,525 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:04:36,525 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:04:47,012 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:05:00,934 | server.py:125 | fit progress: (2, 2.302395820617676, {'accuracy': 0.1146, 'data_size': 10000}, 47.97944422400906)
INFO flwr 2024-04-07 08:05:00,934 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:05:00,934 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:05:11,051 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:05:31,781 | server.py:125 | fit progress: (3, 2.3023383617401123, {'accuracy': 0.1239, 'data_size': 10000}, 78.82651229400653)
INFO flwr 2024-04-07 08:05:31,781 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:05:31,781 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:05:41,837 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:06:08,841 | server.py:125 | fit progress: (4, 2.302281618118286, {'accuracy': 0.1332, 'data_size': 10000}, 115.88625237799715)
INFO flwr 2024-04-07 08:06:08,841 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:06:08,841 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:06:19,251 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:06:51,937 | server.py:125 | fit progress: (5, 2.3022255897521973, {'accuracy': 0.1378, 'data_size': 10000}, 158.98289675399428)
INFO flwr 2024-04-07 08:06:51,938 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:06:51,938 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:02,246 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:47,505 | server.py:125 | fit progress: (6, 2.3021700382232666, {'accuracy': 0.1415, 'data_size': 10000}, 214.55103128001792)
INFO flwr 2024-04-07 08:07:47,506 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:47,506 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:56,933 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:08:51,658 | server.py:125 | fit progress: (7, 2.302107572555542, {'accuracy': 0.142, 'data_size': 10000}, 278.7033247380168)
INFO flwr 2024-04-07 08:08:51,658 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:08:51,658 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:09:01,484 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:10:09,999 | server.py:125 | fit progress: (8, 2.3020424842834473, {'accuracy': 0.1176, 'data_size': 10000}, 357.0449144790182)
INFO flwr 2024-04-07 08:10:10,000 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:10:10,000 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:10:19,919 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:11:57,607 | server.py:125 | fit progress: (9, 2.3019790649414062, {'accuracy': 0.1083, 'data_size': 10000}, 464.6530139580136)
INFO flwr 2024-04-07 08:11:57,608 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:11:57,608 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:12:07,134 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:14:44,937 | server.py:125 | fit progress: (10, 2.301919460296631, {'accuracy': 0.1085, 'data_size': 10000}, 631.9826572070015)
INFO flwr 2024-04-07 08:14:44,938 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:14:44,938 | server.py:153 | FL finished in 631.983594958001
INFO flwr 2024-04-07 08:14:44,938 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:14:44,938 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:14:44,938 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:14:44,938 | app.py:229 | app_fit: losses_centralized [(0, 2.3025074005126953), (1, 2.302459478378296), (2, 2.302395820617676), (3, 2.3023383617401123), (4, 2.302281618118286), (5, 2.3022255897521973), (6, 2.3021700382232666), (7, 2.302107572555542), (8, 2.3020424842834473), (9, 2.3019790649414062), (10, 2.301919460296631)]
INFO flwr 2024-04-07 08:14:44,939 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1193), (1, 0.1232), (2, 0.1146), (3, 0.1239), (4, 0.1332), (5, 0.1378), (6, 0.1415), (7, 0.142), (8, 0.1176), (9, 0.1083), (10, 0.1085)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1085
wandb:     loss 2.30192
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_080346-i58a22fc
wandb: Find logs at: ./wandb/offline-run-20240407_080346-i58a22fc/logs
INFO flwr 2024-04-07 08:14:48,470 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:22:10,904 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 08:22:17,048	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:22:17,427	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:22:17,873	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f7d93531a8962e63.zip' (11.56MiB) to Ray cluster...
2024-04-07 08:22:17,918	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f7d93531a8962e63.zip'.
INFO flwr 2024-04-07 08:22:28,703 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 118101713511.0, 'object_store_memory': 54900734361.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 08:22:28,704 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:22:28,704 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:22:28,721 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:22:28,722 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:22:28,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:22:28,722 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1819657)[0m 2024-04-07 08:22:34.686199: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1819657)[0m 2024-04-07 08:22:34.831790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1819657)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 08:22:36,750 | server.py:94 | initial parameters (loss, other metrics): 2.302664279937744, {'accuracy': 0.1371, 'data_size': 10000}
INFO flwr 2024-04-07 08:22:36,751 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:22:36,752 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1819657)[0m 2024-04-07 08:22:36.927163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1819656)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1819656)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1819649)[0m 2024-04-07 08:22:35.239350: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1819649)[0m 2024-04-07 08:22:35.332545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1819649)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1819649)[0m 2024-04-07 08:22:37.510775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1819649)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1819649)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 08:22:52,115 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:22:55,685 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:23:00,004 | server.py:125 | fit progress: (1, 2.2813310623168945, {'accuracy': 0.2392, 'data_size': 10000}, 23.25256944799912)
INFO flwr 2024-04-07 08:23:00,005 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:23:00,005 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:23:10,478 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:23:24,512 | server.py:125 | fit progress: (2, 2.194178581237793, {'accuracy': 0.2138, 'data_size': 10000}, 47.76103113099816)
INFO flwr 2024-04-07 08:23:24,513 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:23:24,513 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:23:33,724 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:23:54,532 | server.py:125 | fit progress: (3, 1.9059687852859497, {'accuracy': 0.5618, 'data_size': 10000}, 77.78055714498623)
INFO flwr 2024-04-07 08:23:54,532 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:23:54,532 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:24:03,789 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:24:30,508 | server.py:125 | fit progress: (4, 1.704952597618103, {'accuracy': 0.7839, 'data_size': 10000}, 113.75632812699769)
INFO flwr 2024-04-07 08:24:30,508 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:24:30,508 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:24:39,586 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:25:12,706 | server.py:125 | fit progress: (5, 1.6765124797821045, {'accuracy': 0.7967, 'data_size': 10000}, 155.95502411900088)
INFO flwr 2024-04-07 08:25:12,707 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:25:12,707 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:25:23,304 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:08,404 | server.py:125 | fit progress: (6, 1.674100637435913, {'accuracy': 0.7975, 'data_size': 10000}, 211.65223053799127)
INFO flwr 2024-04-07 08:26:08,404 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:08,405 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:17,770 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:27:26,273 | server.py:125 | fit progress: (7, 1.6573618650436401, {'accuracy': 0.8115, 'data_size': 10000}, 289.52175472999807)
INFO flwr 2024-04-07 08:27:26,273 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:27:26,274 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:27:35,304 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:28:56,161 | server.py:125 | fit progress: (8, 1.642919898033142, {'accuracy': 0.8249, 'data_size': 10000}, 379.4099864040036)
INFO flwr 2024-04-07 08:28:56,162 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:28:56,162 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:29:05,267 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:31:51,000 | server.py:125 | fit progress: (9, 1.6391791105270386, {'accuracy': 0.8268, 'data_size': 10000}, 554.2483704809856)
INFO flwr 2024-04-07 08:31:51,000 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:31:51,000 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:32:00,468 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:34:01,860 | server.py:125 | fit progress: (10, 1.6287585496902466, {'accuracy': 0.8358, 'data_size': 10000}, 685.1090624609787)
INFO flwr 2024-04-07 08:34:01,861 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:34:01,861 | server.py:153 | FL finished in 685.1098232450022
INFO flwr 2024-04-07 08:34:01,861 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:34:01,862 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:34:01,862 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:34:01,862 | app.py:229 | app_fit: losses_centralized [(0, 2.302664279937744), (1, 2.2813310623168945), (2, 2.194178581237793), (3, 1.9059687852859497), (4, 1.704952597618103), (5, 1.6765124797821045), (6, 1.674100637435913), (7, 1.6573618650436401), (8, 1.642919898033142), (9, 1.6391791105270386), (10, 1.6287585496902466)]
INFO flwr 2024-04-07 08:34:01,862 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1371), (1, 0.2392), (2, 0.2138), (3, 0.5618), (4, 0.7839), (5, 0.7967), (6, 0.7975), (7, 0.8115), (8, 0.8249), (9, 0.8268), (10, 0.8358)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8358
wandb:     loss 1.62876
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_082210-vt5bxp98
wandb: Find logs at: ./wandb/offline-run-20240407_082210-vt5bxp98/logs
INFO flwr 2024-04-07 08:34:05,498 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:41:30,647 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1819647)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1819647)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 08:41:36,076	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:41:36,547	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:41:37,038	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7555c13dc496dee8.zip' (11.60MiB) to Ray cluster...
2024-04-07 08:41:37,080	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7555c13dc496dee8.zip'.
INFO flwr 2024-04-07 08:41:47,993 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 54703665561.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 117641886311.0}
INFO flwr 2024-04-07 08:41:47,993 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:41:47,993 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:41:48,012 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:41:48,014 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:41:48,014 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:41:48,014 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1831823)[0m 2024-04-07 08:41:54.226409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1831823)[0m 2024-04-07 08:41:54.309256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1831823)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 08:41:56,408 | server.py:94 | initial parameters (loss, other metrics): 2.3024847507476807, {'accuracy': 0.1105, 'data_size': 10000}
INFO flwr 2024-04-07 08:41:56,408 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:41:56,409 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1831823)[0m 2024-04-07 08:41:57.268944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1831833)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1831833)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1831826)[0m 2024-04-07 08:41:54.487834: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1831826)[0m 2024-04-07 08:41:54.564614: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1831826)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1831826)[0m 2024-04-07 08:41:57.619660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1831820)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1831820)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:42:13,526 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:42:17,143 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:42:21,553 | server.py:125 | fit progress: (1, 2.238330364227295, {'accuracy': 0.3756, 'data_size': 10000}, 25.144948869972723)
INFO flwr 2024-04-07 08:42:21,554 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:42:21,554 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:42:32,266 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:42:46,208 | server.py:125 | fit progress: (2, 1.9191398620605469, {'accuracy': 0.5447, 'data_size': 10000}, 49.799327015993185)
INFO flwr 2024-04-07 08:42:46,208 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:42:46,208 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:42:55,822 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:43:16,398 | server.py:125 | fit progress: (3, 1.6617486476898193, {'accuracy': 0.8238, 'data_size': 10000}, 79.98951647299691)
INFO flwr 2024-04-07 08:43:16,398 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:43:16,399 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:43:25,758 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:43:52,477 | server.py:125 | fit progress: (4, 1.6253961324691772, {'accuracy': 0.8428, 'data_size': 10000}, 116.0689906149928)
INFO flwr 2024-04-07 08:43:52,478 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:43:52,478 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:44:01,706 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:44:35,506 | server.py:125 | fit progress: (5, 1.6240171194076538, {'accuracy': 0.8423, 'data_size': 10000}, 159.09795415599365)
INFO flwr 2024-04-07 08:44:35,507 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:44:35,507 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:44:45,906 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:30,425 | server.py:125 | fit progress: (6, 1.6279137134552002, {'accuracy': 0.838, 'data_size': 10000}, 214.01707153199823)
INFO flwr 2024-04-07 08:45:30,426 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:30,426 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:39,675 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:46:49,473 | server.py:125 | fit progress: (7, 1.6096895933151245, {'accuracy': 0.8538, 'data_size': 10000}, 293.0644316629914)
INFO flwr 2024-04-07 08:46:49,473 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:46:49,474 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:46:58,414 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:48:20,301 | server.py:125 | fit progress: (8, 1.6075241565704346, {'accuracy': 0.8564, 'data_size': 10000}, 383.89229959697695)
INFO flwr 2024-04-07 08:48:20,301 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:48:20,301 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:48:30,155 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:50:11,753 | server.py:125 | fit progress: (9, 1.6047945022583008, {'accuracy': 0.8579, 'data_size': 10000}, 495.3447492879932)
INFO flwr 2024-04-07 08:50:11,754 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:50:11,754 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:50:21,448 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:52:31,329 | server.py:125 | fit progress: (10, 1.6004067659378052, {'accuracy': 0.8623, 'data_size': 10000}, 634.9201294749801)
INFO flwr 2024-04-07 08:52:31,329 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:52:31,329 | server.py:153 | FL finished in 634.9209465519816
INFO flwr 2024-04-07 08:52:31,330 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:52:31,330 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:52:31,330 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:52:31,330 | app.py:229 | app_fit: losses_centralized [(0, 2.3024847507476807), (1, 2.238330364227295), (2, 1.9191398620605469), (3, 1.6617486476898193), (4, 1.6253961324691772), (5, 1.6240171194076538), (6, 1.6279137134552002), (7, 1.6096895933151245), (8, 1.6075241565704346), (9, 1.6047945022583008), (10, 1.6004067659378052)]
INFO flwr 2024-04-07 08:52:31,330 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1105), (1, 0.3756), (2, 0.5447), (3, 0.8238), (4, 0.8428), (5, 0.8423), (6, 0.838), (7, 0.8538), (8, 0.8564), (9, 0.8579), (10, 0.8623)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8623
wandb:     loss 1.60041
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_084130-q3iitigf
wandb: Find logs at: ./wandb/offline-run-20240407_084130-q3iitigf/logs
INFO flwr 2024-04-07 08:52:34,889 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:59:56,787 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 09:00:04,428	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:00:04,789	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:00:05,173	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_61cd2e0b6128a8f8.zip' (11.63MiB) to Ray cluster...
2024-04-07 09:00:05,208	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_61cd2e0b6128a8f8.zip'.
INFO flwr 2024-04-07 09:00:16,209 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 117415761716.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54606755020.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 09:00:16,209 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:00:16,209 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:00:16,226 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:00:16,228 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:00:16,229 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:00:16,229 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1843356)[0m 2024-04-07 09:00:22.323294: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1843356)[0m 2024-04-07 09:00:22.439804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1843356)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1843358)[0m 2024-04-07 09:00:24.381924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 09:00:24,677 | server.py:94 | initial parameters (loss, other metrics): 2.3024232387542725, {'accuracy': 0.1073, 'data_size': 10000}
INFO flwr 2024-04-07 09:00:24,678 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:00:24,679 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1843363)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1843363)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1843349)[0m 2024-04-07 09:00:22.467868: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1843349)[0m 2024-04-07 09:00:22.558203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1843349)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1843348)[0m 2024-04-07 09:00:24.934776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1843349)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1843349)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 09:00:41,051 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:00:44,342 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:00:48,794 | server.py:125 | fit progress: (1, 2.2023239135742188, {'accuracy': 0.3129, 'data_size': 10000}, 24.11531973798992)
INFO flwr 2024-04-07 09:00:48,794 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:00:48,794 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:00:58,814 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:01:12,950 | server.py:125 | fit progress: (2, 1.8209152221679688, {'accuracy': 0.6621, 'data_size': 10000}, 48.27117275900673)
INFO flwr 2024-04-07 09:01:12,950 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:01:12,951 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:01:22,633 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:01:43,160 | server.py:125 | fit progress: (3, 1.5472743511199951, {'accuracy': 0.9291, 'data_size': 10000}, 78.48127324099187)
INFO flwr 2024-04-07 09:01:43,160 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:01:43,160 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:01:52,771 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:02:18,958 | server.py:125 | fit progress: (4, 1.5239633321762085, {'accuracy': 0.9428, 'data_size': 10000}, 114.27951941100764)
INFO flwr 2024-04-07 09:02:18,958 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:02:18,959 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:02:28,337 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:03:04,471 | server.py:125 | fit progress: (5, 1.6343764066696167, {'accuracy': 0.8262, 'data_size': 10000}, 159.79284234999795)
INFO flwr 2024-04-07 09:03:04,472 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:03:04,472 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:03:13,996 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:04:06,642 | server.py:125 | fit progress: (6, 1.5128175020217896, {'accuracy': 0.9513, 'data_size': 10000}, 221.9635914620012)
INFO flwr 2024-04-07 09:04:06,643 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:04:06,643 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:04:16,776 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:16,471 | server.py:125 | fit progress: (7, 1.5108884572982788, {'accuracy': 0.9538, 'data_size': 10000}, 291.79232730699005)
INFO flwr 2024-04-07 09:05:16,471 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:16,471 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:26,058 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:07:01,438 | server.py:125 | fit progress: (8, 1.5031639337539673, {'accuracy': 0.9598, 'data_size': 10000}, 396.75934055200196)
INFO flwr 2024-04-07 09:07:01,439 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:07:01,439 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:07:10,957 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:10:08,410 | server.py:125 | fit progress: (9, 1.5010703802108765, {'accuracy': 0.963, 'data_size': 10000}, 583.7310868779896)
INFO flwr 2024-04-07 09:10:08,411 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:10:08,411 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:10:18,335 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:13:31,670 | server.py:125 | fit progress: (10, 1.5028715133666992, {'accuracy': 0.9606, 'data_size': 10000}, 786.9911397029937)
INFO flwr 2024-04-07 09:13:31,670 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:13:31,670 | server.py:153 | FL finished in 786.9920329450106
INFO flwr 2024-04-07 09:13:31,671 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:13:31,671 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:13:31,671 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:13:31,671 | app.py:229 | app_fit: losses_centralized [(0, 2.3024232387542725), (1, 2.2023239135742188), (2, 1.8209152221679688), (3, 1.5472743511199951), (4, 1.5239633321762085), (5, 1.6343764066696167), (6, 1.5128175020217896), (7, 1.5108884572982788), (8, 1.5031639337539673), (9, 1.5010703802108765), (10, 1.5028715133666992)]
INFO flwr 2024-04-07 09:13:31,671 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1073), (1, 0.3129), (2, 0.6621), (3, 0.9291), (4, 0.9428), (5, 0.8262), (6, 0.9513), (7, 0.9538), (8, 0.9598), (9, 0.963), (10, 0.9606)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9606
wandb:     loss 1.50287
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_085956-4ci56js9
wandb: Find logs at: ./wandb/offline-run-20240407_085956-4ci56js9/logs
INFO flwr 2024-04-07 09:13:35,236 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:21:00,498 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1843348)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1843348)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 09:21:06,717	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:21:07,135	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:21:07,556	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b56ec57799d71ac0.zip' (11.67MiB) to Ray cluster...
2024-04-07 09:21:07,591	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b56ec57799d71ac0.zip'.
INFO flwr 2024-04-07 09:21:18,614 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 117493511578.0, 'object_store_memory': 54640076390.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 09:21:18,614 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:21:18,615 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:21:18,630 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:21:18,631 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:21:18,631 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:21:18,631 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1855833)[0m 2024-04-07 09:21:24.601669: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1855833)[0m 2024-04-07 09:21:24.695558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1855833)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1855825)[0m 2024-04-07 09:21:26.735362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-07 09:21:27,057 | server.py:94 | initial parameters (loss, other metrics): 2.3026678562164307, {'accuracy': 0.1013, 'data_size': 10000}
INFO flwr 2024-04-07 09:21:27,057 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:21:27,058 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1855833)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1855833)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1855824)[0m 2024-04-07 09:21:24.845334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1855824)[0m 2024-04-07 09:21:24.945061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1855824)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1855824)[0m 2024-04-07 09:21:27.145080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1855824)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1855824)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-04-07 09:21:43,302 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:21:46,834 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:21:51,097 | server.py:125 | fit progress: (1, 2.126729726791382, {'accuracy': 0.4093, 'data_size': 10000}, 24.039276158000575)
INFO flwr 2024-04-07 09:21:51,097 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:21:51,097 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:22:01,715 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:22:15,459 | server.py:125 | fit progress: (2, 1.6805071830749512, {'accuracy': 0.7854, 'data_size': 10000}, 48.40157771599479)
INFO flwr 2024-04-07 09:22:15,460 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:22:15,460 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:22:24,526 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:22:45,193 | server.py:125 | fit progress: (3, 1.5266568660736084, {'accuracy': 0.9393, 'data_size': 10000}, 78.13582004301134)
INFO flwr 2024-04-07 09:22:45,194 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:22:45,194 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:22:54,374 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:23:21,313 | server.py:125 | fit progress: (4, 1.5493652820587158, {'accuracy': 0.9153, 'data_size': 10000}, 114.25565682799788)
INFO flwr 2024-04-07 09:23:21,313 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:23:21,314 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:23:30,772 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:04,441 | server.py:125 | fit progress: (5, 1.5162328481674194, {'accuracy': 0.9477, 'data_size': 10000}, 157.38354600200546)
INFO flwr 2024-04-07 09:24:04,441 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:04,442 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:13,877 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:59,623 | server.py:125 | fit progress: (6, 1.506074070930481, {'accuracy': 0.9583, 'data_size': 10000}, 212.56535572899156)
INFO flwr 2024-04-07 09:24:59,623 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:59,624 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:25:09,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:26:15,802 | server.py:125 | fit progress: (7, 1.5025272369384766, {'accuracy': 0.9609, 'data_size': 10000}, 288.7446955679916)
INFO flwr 2024-04-07 09:26:15,802 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:26:15,803 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:26:26,132 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:28:07,620 | server.py:125 | fit progress: (8, 1.4999165534973145, {'accuracy': 0.9631, 'data_size': 10000}, 400.56273916899227)
INFO flwr 2024-04-07 09:28:07,621 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:28:07,621 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:28:17,551 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:30:30,309 | server.py:125 | fit progress: (9, 1.4984827041625977, {'accuracy': 0.9648, 'data_size': 10000}, 543.2520249249937)
INFO flwr 2024-04-07 09:30:30,310 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:30:30,311 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:30:40,312 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:34:08,916 | server.py:125 | fit progress: (10, 1.4997990131378174, {'accuracy': 0.9636, 'data_size': 10000}, 761.858161340002)
INFO flwr 2024-04-07 09:34:08,916 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:34:08,917 | server.py:153 | FL finished in 761.85921516901
INFO flwr 2024-04-07 09:34:08,917 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:34:08,917 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:34:08,917 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:34:08,917 | app.py:229 | app_fit: losses_centralized [(0, 2.3026678562164307), (1, 2.126729726791382), (2, 1.6805071830749512), (3, 1.5266568660736084), (4, 1.5493652820587158), (5, 1.5162328481674194), (6, 1.506074070930481), (7, 1.5025272369384766), (8, 1.4999165534973145), (9, 1.4984827041625977), (10, 1.4997990131378174)]
INFO flwr 2024-04-07 09:34:08,918 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1013), (1, 0.4093), (2, 0.7854), (3, 0.9393), (4, 0.9153), (5, 0.9477), (6, 0.9583), (7, 0.9609), (8, 0.9631), (9, 0.9648), (10, 0.9636)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9636
wandb:     loss 1.4998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_092100-cq5zna2u
wandb: Find logs at: ./wandb/offline-run-20240407_092100-cq5zna2u/logs
INFO flwr 2024-04-07 09:34:12,556 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:41:40,885 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1855821)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1855821)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-04-07 09:41:50,721	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:41:52,982	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:41:53,879	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bcc8d1d5686dd92c.zip' (11.71MiB) to Ray cluster...
2024-04-07 09:41:53,932	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bcc8d1d5686dd92c.zip'.
INFO flwr 2024-04-07 09:42:06,628 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 117718936576.0, 'object_store_memory': 54736687104.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 09:42:06,628 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:42:06,628 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:42:06,643 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:42:06,644 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:42:06,645 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:42:06,645 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1867983)[0m 2024-04-07 09:42:12.690792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1867983)[0m 2024-04-07 09:42:12.789466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1867983)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 09:42:14,186 | server.py:94 | initial parameters (loss, other metrics): 2.302565336227417, {'accuracy': 0.1171, 'data_size': 10000}
INFO flwr 2024-04-07 09:42:14,187 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:42:14,187 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1867979)[0m 2024-04-07 09:42:15.141955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1867986)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1867986)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1867990)[0m 2024-04-07 09:42:13.231800: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1867990)[0m 2024-04-07 09:42:13.331374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1867990)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1867990)[0m 2024-04-07 09:42:15.834849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:42:31,516 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:42:35,240 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:42:39,497 | server.py:125 | fit progress: (1, 2.0875916481018066, {'accuracy': 0.5287, 'data_size': 10000}, 25.310074699984398)
INFO flwr 2024-04-07 09:42:39,497 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:42:39,498 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:42:49,880 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:43:03,787 | server.py:125 | fit progress: (2, 1.611478567123413, {'accuracy': 0.8584, 'data_size': 10000}, 49.60014251098619)
INFO flwr 2024-04-07 09:43:03,788 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:43:03,788 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:43:13,285 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:43:34,038 | server.py:125 | fit progress: (3, 1.5280770063400269, {'accuracy': 0.9435, 'data_size': 10000}, 79.85140363598475)
INFO flwr 2024-04-07 09:43:34,039 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:43:34,039 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:43:43,188 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:44:10,643 | server.py:125 | fit progress: (4, 1.5158717632293701, {'accuracy': 0.9511, 'data_size': 10000}, 116.4557471209846)
INFO flwr 2024-04-07 09:44:10,643 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:44:10,643 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:44:19,911 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:44:52,042 | server.py:125 | fit progress: (5, 1.507601022720337, {'accuracy': 0.9561, 'data_size': 10000}, 157.85550768699613)
INFO flwr 2024-04-07 09:44:52,043 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:44:52,043 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:01,969 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:45:47,393 | server.py:125 | fit progress: (6, 1.5010724067687988, {'accuracy': 0.9629, 'data_size': 10000}, 213.20648030599114)
INFO flwr 2024-04-07 09:45:47,394 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:45:47,394 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:57,220 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:46:57,148 | server.py:125 | fit progress: (7, 1.5008004903793335, {'accuracy': 0.9627, 'data_size': 10000}, 282.9609275999828)
INFO flwr 2024-04-07 09:46:57,149 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:46:57,149 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:47:06,421 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:48:31,205 | server.py:125 | fit progress: (8, 1.4948798418045044, {'accuracy': 0.9684, 'data_size': 10000}, 377.0178959189798)
INFO flwr 2024-04-07 09:48:31,205 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:48:31,206 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:48:41,264 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:50:25,864 | server.py:125 | fit progress: (9, 1.4923231601715088, {'accuracy': 0.9708, 'data_size': 10000}, 491.6768613900058)
INFO flwr 2024-04-07 09:50:25,864 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:50:25,865 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:50:35,520 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:54:21,155 | server.py:125 | fit progress: (10, 1.515511155128479, {'accuracy': 0.9483, 'data_size': 10000}, 726.9677628859936)
INFO flwr 2024-04-07 09:54:21,155 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:54:21,155 | server.py:153 | FL finished in 726.9685383589822
INFO flwr 2024-04-07 09:54:21,156 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:54:21,156 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:54:21,156 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:54:21,156 | app.py:229 | app_fit: losses_centralized [(0, 2.302565336227417), (1, 2.0875916481018066), (2, 1.611478567123413), (3, 1.5280770063400269), (4, 1.5158717632293701), (5, 1.507601022720337), (6, 1.5010724067687988), (7, 1.5008004903793335), (8, 1.4948798418045044), (9, 1.4923231601715088), (10, 1.515511155128479)]
INFO flwr 2024-04-07 09:54:21,156 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1171), (1, 0.5287), (2, 0.8584), (3, 0.9435), (4, 0.9511), (5, 0.9561), (6, 0.9629), (7, 0.9627), (8, 0.9684), (9, 0.9708), (10, 0.9483)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9483
wandb:     loss 1.51551
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_094138-09r7nrsu
wandb: Find logs at: ./wandb/offline-run-20240407_094138-09r7nrsu/logs
INFO flwr 2024-04-07 09:54:25,801 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:01:52,164 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1867979)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1867979)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:01:58,008	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:01:58,432	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:01:58,865	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_bf9582f38947c94b.zip' (11.74MiB) to Ray cluster...
2024-04-07 10:01:58,896	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_bf9582f38947c94b.zip'.
INFO flwr 2024-04-07 10:02:09,800 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 117332007936.0, 'object_store_memory': 54570860544.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 10:02:09,800 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:02:09,801 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:02:09,820 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:02:09,822 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:02:09,823 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:02:09,823 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1880562)[0m 2024-04-07 10:02:15.949587: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1880562)[0m 2024-04-07 10:02:16.040771: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1880562)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 10:02:17,175 | server.py:94 | initial parameters (loss, other metrics): 2.302788496017456, {'accuracy': 0.036, 'data_size': 10000}
INFO flwr 2024-04-07 10:02:17,176 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:02:17,176 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1880554)[0m 2024-04-07 10:02:18.470598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1880562)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1880562)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1880555)[0m 2024-04-07 10:02:16.076310: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1880554)[0m 2024-04-07 10:02:16.243415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1880554)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1880553)[0m 2024-04-07 10:02:18.488991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:02:36,169 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:02:39,347 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:02:43,406 | server.py:125 | fit progress: (1, 2.1105167865753174, {'accuracy': 0.444, 'data_size': 10000}, 26.230194378003944)
INFO flwr 2024-04-07 10:02:43,407 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:02:43,407 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:02:54,279 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:03:08,079 | server.py:125 | fit progress: (2, 1.6794066429138184, {'accuracy': 0.7875, 'data_size': 10000}, 50.90267715801019)
INFO flwr 2024-04-07 10:03:08,079 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:03:08,080 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:03:17,655 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:03:37,956 | server.py:125 | fit progress: (3, 1.5327284336090088, {'accuracy': 0.9376, 'data_size': 10000}, 80.78032447700389)
INFO flwr 2024-04-07 10:03:37,957 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:03:37,957 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:03:47,571 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:04:13,882 | server.py:125 | fit progress: (4, 1.5169976949691772, {'accuracy': 0.9488, 'data_size': 10000}, 116.70616937999148)
INFO flwr 2024-04-07 10:04:13,882 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:04:13,883 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:04:22,972 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:04:54,725 | server.py:125 | fit progress: (5, 1.504759430885315, {'accuracy': 0.9596, 'data_size': 10000}, 157.54910166701302)
INFO flwr 2024-04-07 10:04:54,725 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:04:54,726 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:05:04,396 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:01,928 | server.py:125 | fit progress: (6, 1.5011320114135742, {'accuracy': 0.962, 'data_size': 10000}, 224.75162338098744)
INFO flwr 2024-04-07 10:06:01,928 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:01,928 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:11,732 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:08:04,289 | server.py:125 | fit progress: (7, 1.502043604850769, {'accuracy': 0.961, 'data_size': 10000}, 347.11266942700604)
INFO flwr 2024-04-07 10:08:04,289 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:08:04,289 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:08:13,851 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:10:45,861 | server.py:125 | fit progress: (8, 1.4950906038284302, {'accuracy': 0.9674, 'data_size': 10000}, 508.6851875979919)
INFO flwr 2024-04-07 10:10:45,862 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:10:45,862 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:10:55,763 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:14:07,020 | server.py:125 | fit progress: (9, 1.492631435394287, {'accuracy': 0.9687, 'data_size': 10000}, 709.8440456940152)
INFO flwr 2024-04-07 10:14:07,021 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:14:07,021 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:14:17,322 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:38,795 | server.py:125 | fit progress: (10, 1.4906110763549805, {'accuracy': 0.9715, 'data_size': 10000}, 921.6189314470103)
INFO flwr 2024-04-07 10:17:38,795 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:17:38,795 | server.py:153 | FL finished in 921.6196047360136
INFO flwr 2024-04-07 10:17:38,796 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:17:38,796 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:17:38,796 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:17:38,796 | app.py:229 | app_fit: losses_centralized [(0, 2.302788496017456), (1, 2.1105167865753174), (2, 1.6794066429138184), (3, 1.5327284336090088), (4, 1.5169976949691772), (5, 1.504759430885315), (6, 1.5011320114135742), (7, 1.502043604850769), (8, 1.4950906038284302), (9, 1.492631435394287), (10, 1.4906110763549805)]
INFO flwr 2024-04-07 10:17:38,796 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.036), (1, 0.444), (2, 0.7875), (3, 0.9376), (4, 0.9488), (5, 0.9596), (6, 0.962), (7, 0.961), (8, 0.9674), (9, 0.9687), (10, 0.9715)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9715
wandb:     loss 1.49061
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_100151-z9uuppit
wandb: Find logs at: ./wandb/offline-run-20240407_100151-z9uuppit/logs
INFO flwr 2024-04-07 10:17:42,447 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:25:08,761 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1880555)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1880555)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:25:13,622	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:25:15,708	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:25:16,085	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7cf7350aa09d18a3.zip' (11.78MiB) to Ray cluster...
2024-04-07 10:25:16,124	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7cf7350aa09d18a3.zip'.
INFO flwr 2024-04-07 10:25:28,216 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 123755163853.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57323641651.0}
INFO flwr 2024-04-07 10:25:28,216 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:25:28,216 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:25:28,234 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:25:28,235 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:25:28,235 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:25:28,235 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1893975)[0m 2024-04-07 10:25:35.470860: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1893980)[0m 2024-04-07 10:25:35.556902: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1893980)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 10:25:35,638 | server.py:94 | initial parameters (loss, other metrics): 2.302438974380493, {'accuracy': 0.1009, 'data_size': 10000}
INFO flwr 2024-04-07 10:25:35,639 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:25:35,639 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1893975)[0m 2024-04-07 10:25:38.407476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1893978)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1893978)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1893977)[0m 2024-04-07 10:25:35.662741: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893977)[0m 2024-04-07 10:25:35.731697: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893977)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893977)[0m 2024-04-07 10:25:38.401342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:25:57,919 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:26:01,408 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:26:05,707 | server.py:125 | fit progress: (1, 2.302215337753296, {'accuracy': 0.1009, 'data_size': 10000}, 30.06800011199084)
INFO flwr 2024-04-07 10:26:05,707 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:05,708 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:16,289 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:26:30,174 | server.py:125 | fit progress: (2, 2.3019442558288574, {'accuracy': 0.1009, 'data_size': 10000}, 54.53473132097861)
INFO flwr 2024-04-07 10:26:30,174 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:30,175 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:39,862 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:00,348 | server.py:125 | fit progress: (3, 2.301647901535034, {'accuracy': 0.0995, 'data_size': 10000}, 84.70926547297859)
INFO flwr 2024-04-07 10:27:00,349 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:00,349 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:10,615 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:37,414 | server.py:125 | fit progress: (4, 2.301434278488159, {'accuracy': 0.0979, 'data_size': 10000}, 121.77483062498504)
INFO flwr 2024-04-07 10:27:37,414 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:37,414 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:47,346 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:28:20,247 | server.py:125 | fit progress: (5, 2.3011415004730225, {'accuracy': 0.1468, 'data_size': 10000}, 164.60769484000048)
INFO flwr 2024-04-07 10:28:20,247 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:28:20,247 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:28:30,654 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:29:10,680 | server.py:125 | fit progress: (6, 2.30077862739563, {'accuracy': 0.0974, 'data_size': 10000}, 215.04108934497344)
INFO flwr 2024-04-07 10:29:10,680 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:29:10,681 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:29:20,328 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:30:48,425 | server.py:125 | fit progress: (7, 2.300503969192505, {'accuracy': 0.0974, 'data_size': 10000}, 312.78632380097406)
INFO flwr 2024-04-07 10:30:48,426 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:30:48,426 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:30:58,276 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:32:11,634 | server.py:125 | fit progress: (8, 2.300156354904175, {'accuracy': 0.0974, 'data_size': 10000}, 395.9946070719743)
INFO flwr 2024-04-07 10:32:11,634 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:32:11,634 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:32:21,546 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:34:11,506 | server.py:125 | fit progress: (9, 2.2999143600463867, {'accuracy': 0.1382, 'data_size': 10000}, 515.866699509992)
INFO flwr 2024-04-07 10:34:11,506 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:34:11,506 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:34:21,938 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:44,324 | server.py:125 | fit progress: (10, 2.2995827198028564, {'accuracy': 0.1389, 'data_size': 10000}, 668.6853566869977)
INFO flwr 2024-04-07 10:36:44,325 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:36:44,325 | server.py:153 | FL finished in 668.6859512809897
INFO flwr 2024-04-07 10:36:44,325 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:36:44,325 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:36:44,326 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:36:44,326 | app.py:229 | app_fit: losses_centralized [(0, 2.302438974380493), (1, 2.302215337753296), (2, 2.3019442558288574), (3, 2.301647901535034), (4, 2.301434278488159), (5, 2.3011415004730225), (6, 2.30077862739563), (7, 2.300503969192505), (8, 2.300156354904175), (9, 2.2999143600463867), (10, 2.2995827198028564)]
INFO flwr 2024-04-07 10:36:44,326 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1009), (1, 0.1009), (2, 0.1009), (3, 0.0995), (4, 0.0979), (5, 0.1468), (6, 0.0974), (7, 0.0974), (8, 0.0974), (9, 0.1382), (10, 0.1389)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1389
wandb:     loss 2.29958
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_102508-xod7b1b2
wandb: Find logs at: ./wandb/offline-run-20240407_102508-xod7b1b2/logs
INFO flwr 2024-04-07 10:36:47,884 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:44:13,388 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1893973)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1893973)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:44:19,185	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:44:19,864	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:44:20,297	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_87122174c30a66ed.zip' (11.81MiB) to Ray cluster...
2024-04-07 10:44:20,334	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_87122174c30a66ed.zip'.
INFO flwr 2024-04-07 10:44:31,278 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 116577911399.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 54247676313.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 10:44:31,278 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:44:31,278 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:44:31,295 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:44:31,296 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:44:31,296 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:44:31,296 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1904614)[0m 2024-04-07 10:44:37.349575: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1904614)[0m 2024-04-07 10:44:37.439842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1904614)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-04-07 10:44:38,917 | server.py:94 | initial parameters (loss, other metrics): 2.302590847015381, {'accuracy': 0.0587, 'data_size': 10000}
INFO flwr 2024-04-07 10:44:38,918 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:44:38,918 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1904607)[0m 2024-04-07 10:44:39.613776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1904612)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1904612)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1904612)[0m 2024-04-07 10:44:37.555117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1904612)[0m 2024-04-07 10:44:37.652484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1904612)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1904614)[0m 2024-04-07 10:44:39.638374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1904606)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1904606)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-04-07 10:44:55,819 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:44:59,198 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:45:03,415 | server.py:125 | fit progress: (1, 2.3098959922790527, {'accuracy': 0.1135, 'data_size': 10000}, 24.496864120010287)
INFO flwr 2024-04-07 10:45:03,415 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:03,416 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:45:15,382 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:45:29,540 | server.py:125 | fit progress: (2, 1.8415511846542358, {'accuracy': 0.6683, 'data_size': 10000}, 50.622193796007195)
INFO flwr 2024-04-07 10:45:29,541 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:29,541 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:45:39,916 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:00,559 | server.py:125 | fit progress: (3, 1.6630327701568604, {'accuracy': 0.8099, 'data_size': 10000}, 81.64089981900179)
INFO flwr 2024-04-07 10:46:00,559 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:00,560 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:09,937 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:36,840 | server.py:125 | fit progress: (4, 1.6163978576660156, {'accuracy': 0.8506, 'data_size': 10000}, 117.92202038998948)
INFO flwr 2024-04-07 10:46:36,841 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:36,864 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:46,841 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:47:19,166 | server.py:125 | fit progress: (5, 1.5757194757461548, {'accuracy': 0.889, 'data_size': 10000}, 160.24741979700048)
INFO flwr 2024-04-07 10:47:19,166 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:47:19,166 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:47:28,968 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:48:13,292 | server.py:125 | fit progress: (6, 1.5231236219406128, {'accuracy': 0.9436, 'data_size': 10000}, 214.37384191399906)
INFO flwr 2024-04-07 10:48:13,292 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:48:13,293 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:48:23,240 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:49:53,792 | server.py:125 | fit progress: (7, 1.5168718099594116, {'accuracy': 0.9483, 'data_size': 10000}, 314.8741427859932)
INFO flwr 2024-04-07 10:49:53,793 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:49:53,793 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:50:04,092 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:52:37,227 | server.py:125 | fit progress: (8, 1.5191702842712402, {'accuracy': 0.9465, 'data_size': 10000}, 478.30864241099334)
INFO flwr 2024-04-07 10:52:37,227 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:52:37,228 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:52:47,818 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:55:18,784 | server.py:125 | fit progress: (9, 1.5107165575027466, {'accuracy': 0.953, 'data_size': 10000}, 639.8657498729881)
INFO flwr 2024-04-07 10:55:18,784 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:55:18,785 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:55:28,839 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:59:11,407 | server.py:125 | fit progress: (10, 1.506178379058838, {'accuracy': 0.9585, 'data_size': 10000}, 872.4885636479885)
INFO flwr 2024-04-07 10:59:11,407 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:59:11,407 | server.py:153 | FL finished in 872.4891541519901
INFO flwr 2024-04-07 10:59:11,407 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:59:11,408 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:59:11,408 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:59:11,408 | app.py:229 | app_fit: losses_centralized [(0, 2.302590847015381), (1, 2.3098959922790527), (2, 1.8415511846542358), (3, 1.6630327701568604), (4, 1.6163978576660156), (5, 1.5757194757461548), (6, 1.5231236219406128), (7, 1.5168718099594116), (8, 1.5191702842712402), (9, 1.5107165575027466), (10, 1.506178379058838)]
INFO flwr 2024-04-07 10:59:11,408 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0587), (1, 0.1135), (2, 0.6683), (3, 0.8099), (4, 0.8506), (5, 0.889), (6, 0.9436), (7, 0.9483), (8, 0.9465), (9, 0.953), (10, 0.9585)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9585
wandb:     loss 1.50618
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_104412-tdmt6jbn
wandb: Find logs at: ./wandb/offline-run-20240407_104412-tdmt6jbn/logs
INFO flwr 2024-04-07 10:59:15,016 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(1, 28, 28) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: CNN
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (1): ReLU()
			    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
			    (4): ReLU()
			    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
			    (6): Flatten(start_dim=1, end_dim=-1)
			    (7): Linear(in_features=3136, out_features=512, bias=True)
			    (8): ReLU()
			    (9): Linear(in_features=512, out_features=10, bias=True)
			    (10): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:06:40,481 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1904605)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1904605)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-04-07 11:06:46,611	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:06:47,032	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:06:47,514	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_06149abc5807e3e1.zip' (11.86MiB) to Ray cluster...
2024-04-07 11:06:47,544	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_06149abc5807e3e1.zip'.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-ctit088: error: *** STEP 279081.0 ON ctit088 CANCELLED AT 2024-04-11T18:19:39 ***
*** SIGTERM received at time=1712852379 on cpu 62 ***
PC: @     0x7f4fa44bd35c  (unknown)  recv
    @     0x7f4fa43e0090  (unknown)  (unknown)
[2024-04-11 18:19:39,796 E 440873 440873] logging.cc:361: *** SIGTERM received at time=1712852379 on cpu 62 ***
[2024-04-11 18:19:39,797 E 440873 440873] logging.cc:361: PC: @     0x7f4fa44bd35c  (unknown)  recv
[2024-04-11 18:19:39,797 E 440873 440873] logging.cc:361:     @     0x7f4fa43e0090  (unknown)  (unknown)
slurmstepd-ctit088: error: *** JOB 279081 ON ctit088 CANCELLED AT 2024-04-11T18:19:39 ***
