ctit088
2024-04-06 20:15:21.467242: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-06 20:15:21.526707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-06 20:15:40.678064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-04-06 20:16:35,439 | batch_run_simulation.py:63 | Loaded 140 configs, running...
INFO flwr 2024-04-06 20:16:35,440 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:23:49,120 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-06 20:23:53,658	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:24:00,734	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:24:01,116	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1790b26157cd02ed.zip' (10.22MiB) to Ray cluster...
2024-04-06 20:24:01,152	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1790b26157cd02ed.zip'.
INFO flwr 2024-04-06 20:24:11,980 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 62177503641.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 135080841831.0}
INFO flwr 2024-04-06 20:24:11,980 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:24:11,980 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:24:11,998 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:24:11,999 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:24:11,999 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:24:11,999 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:24:17,122 | server.py:94 | initial parameters (loss, other metrics): 2.3044753074645996, {'accuracy': 0.0654, 'data_size': 10000}
INFO flwr 2024-04-06 20:24:17,122 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:24:17,122 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1360147)[0m 2024-04-06 20:24:17.742725: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1360156)[0m 2024-04-06 20:24:17.883795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1360156)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1360156)[0m 2024-04-06 20:24:20.105577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1360157)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1360157)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1360145)[0m 2024-04-06 20:24:18.293177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1360145)[0m 2024-04-06 20:24:18.413765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1360145)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1360145)[0m 2024-04-06 20:24:20.628527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:24:32,615 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:24:32,657 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:24:33,688 | server.py:125 | fit progress: (1, 2.3044605255126953, {'accuracy': 0.0655, 'data_size': 10000}, 16.566043770988472)
INFO flwr 2024-04-06 20:24:33,689 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:24:33,689 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:24:41,879 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:24:43,174 | server.py:125 | fit progress: (2, 2.304446220397949, {'accuracy': 0.0657, 'data_size': 10000}, 26.051697268994758)
INFO flwr 2024-04-06 20:24:43,174 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:24:43,174 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:24:50,527 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:24:51,866 | server.py:125 | fit progress: (3, 2.3044331073760986, {'accuracy': 0.0658, 'data_size': 10000}, 34.74367633101065)
INFO flwr 2024-04-06 20:24:51,866 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:24:51,866 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:24:58,914 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:00,241 | server.py:125 | fit progress: (4, 2.304415464401245, {'accuracy': 0.066, 'data_size': 10000}, 43.118377097998746)
INFO flwr 2024-04-06 20:25:00,241 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:00,241 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:07,600 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:08,794 | server.py:125 | fit progress: (5, 2.3043999671936035, {'accuracy': 0.0663, 'data_size': 10000}, 51.67210704900208)
INFO flwr 2024-04-06 20:25:08,795 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:08,795 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:16,520 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:17,707 | server.py:125 | fit progress: (6, 2.3043811321258545, {'accuracy': 0.0666, 'data_size': 10000}, 60.584823868004605)
INFO flwr 2024-04-06 20:25:17,707 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:17,708 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:25,493 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:26,987 | server.py:125 | fit progress: (7, 2.3043673038482666, {'accuracy': 0.0666, 'data_size': 10000}, 69.86473017299431)
INFO flwr 2024-04-06 20:25:26,987 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:26,987 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:34,194 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:35,690 | server.py:125 | fit progress: (8, 2.3043529987335205, {'accuracy': 0.0667, 'data_size': 10000}, 78.56817603899981)
INFO flwr 2024-04-06 20:25:35,691 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:35,691 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:43,180 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:44,698 | server.py:125 | fit progress: (9, 2.3043360710144043, {'accuracy': 0.067, 'data_size': 10000}, 87.57612322398927)
INFO flwr 2024-04-06 20:25:44,699 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:25:44,699 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:25:52,276 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:25:53,604 | server.py:125 | fit progress: (10, 2.304323434829712, {'accuracy': 0.067, 'data_size': 10000}, 96.48208445101045)
INFO flwr 2024-04-06 20:25:53,605 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:25:53,605 | server.py:153 | FL finished in 96.48257069999818
INFO flwr 2024-04-06 20:25:53,605 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:25:53,606 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:25:53,606 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:25:53,606 | app.py:229 | app_fit: losses_centralized [(0, 2.3044753074645996), (1, 2.3044605255126953), (2, 2.304446220397949), (3, 2.3044331073760986), (4, 2.304415464401245), (5, 2.3043999671936035), (6, 2.3043811321258545), (7, 2.3043673038482666), (8, 2.3043529987335205), (9, 2.3043360710144043), (10, 2.304323434829712)]
INFO flwr 2024-04-06 20:25:53,606 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0654), (1, 0.0655), (2, 0.0657), (3, 0.0658), (4, 0.066), (5, 0.0663), (6, 0.0666), (7, 0.0666), (8, 0.0667), (9, 0.067), (10, 0.067)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.067
wandb:     loss 2.30432
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_202346-rrm9v5st
wandb: Find logs at: ./wandb/offline-run-20240406_202346-rrm9v5st/logs
INFO flwr 2024-04-06 20:25:57,073 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:33:03,381 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1360145)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1360145)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 20:33:08,482	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:33:08,962	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:33:09,305	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_851a35fabf372666.zip' (10.24MiB) to Ray cluster...
2024-04-06 20:33:09,339	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_851a35fabf372666.zip'.
INFO flwr 2024-04-06 20:33:20,382 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60132021043.0, 'node:__internal_head__': 1.0, 'memory': 130308049101.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 20:33:20,383 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:33:20,383 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:33:20,399 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:33:20,400 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:33:20,401 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:33:20,401 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:33:23,272 | server.py:94 | initial parameters (loss, other metrics): 2.3022866249084473, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-06 20:33:23,272 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:33:23,273 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1367846)[0m 2024-04-06 20:33:26.795439: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1367846)[0m 2024-04-06 20:33:26.892937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1367846)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1367846)[0m 2024-04-06 20:33:29.342237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1367846)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1367846)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1367839)[0m 2024-04-06 20:33:27.094727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1367839)[0m 2024-04-06 20:33:27.197008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1367839)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1367844)[0m 2024-04-06 20:33:30.303003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:33:41,813 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:33:41,847 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:33:43,377 | server.py:125 | fit progress: (1, 2.3012948036193848, {'accuracy': 0.1098, 'data_size': 10000}, 20.10478935900028)
INFO flwr 2024-04-06 20:33:43,378 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:33:43,378 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:33:52,972 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:33:54,512 | server.py:125 | fit progress: (2, 2.300490140914917, {'accuracy': 0.1188, 'data_size': 10000}, 31.239356740989024)
INFO flwr 2024-04-06 20:33:54,512 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:33:54,512 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:03,336 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:04,540 | server.py:125 | fit progress: (3, 2.2994930744171143, {'accuracy': 0.1275, 'data_size': 10000}, 41.26811756900861)
INFO flwr 2024-04-06 20:34:04,541 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:04,541 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:12,511 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:14,014 | server.py:125 | fit progress: (4, 2.298726797103882, {'accuracy': 0.1318, 'data_size': 10000}, 50.74169970798539)
INFO flwr 2024-04-06 20:34:14,014 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:14,015 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:21,822 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:23,375 | server.py:125 | fit progress: (5, 2.297738552093506, {'accuracy': 0.1369, 'data_size': 10000}, 60.102965275989845)
INFO flwr 2024-04-06 20:34:23,376 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:23,376 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:31,059 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:32,569 | server.py:125 | fit progress: (6, 2.2969157695770264, {'accuracy': 0.14, 'data_size': 10000}, 69.29671440500533)
INFO flwr 2024-04-06 20:34:32,569 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:32,570 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:40,402 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:41,909 | server.py:125 | fit progress: (7, 2.296020984649658, {'accuracy': 0.1437, 'data_size': 10000}, 78.63620131099015)
INFO flwr 2024-04-06 20:34:41,909 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:41,909 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:50,051 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:34:51,324 | server.py:125 | fit progress: (8, 2.294999361038208, {'accuracy': 0.1512, 'data_size': 10000}, 88.0519176159869)
INFO flwr 2024-04-06 20:34:51,325 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:34:51,325 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:34:59,455 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:35:00,741 | server.py:125 | fit progress: (9, 2.2940635681152344, {'accuracy': 0.1593, 'data_size': 10000}, 97.46886503798305)
INFO flwr 2024-04-06 20:35:00,742 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:35:00,742 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:35:08,902 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:35:10,480 | server.py:125 | fit progress: (10, 2.293236494064331, {'accuracy': 0.1588, 'data_size': 10000}, 107.2072116319905)
INFO flwr 2024-04-06 20:35:10,480 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:35:10,480 | server.py:153 | FL finished in 107.20769290800672
INFO flwr 2024-04-06 20:35:10,480 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:35:10,480 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:35:10,480 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:35:10,481 | app.py:229 | app_fit: losses_centralized [(0, 2.3022866249084473), (1, 2.3012948036193848), (2, 2.300490140914917), (3, 2.2994930744171143), (4, 2.298726797103882), (5, 2.297738552093506), (6, 2.2969157695770264), (7, 2.296020984649658), (8, 2.294999361038208), (9, 2.2940635681152344), (10, 2.293236494064331)]
INFO flwr 2024-04-06 20:35:10,481 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.1098), (2, 0.1188), (3, 0.1275), (4, 0.1318), (5, 0.1369), (6, 0.14), (7, 0.1437), (8, 0.1512), (9, 0.1593), (10, 0.1588)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1588
wandb:     loss 2.29324
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_203303-e2w07l5w
wandb: Find logs at: ./wandb/offline-run-20240406_203303-e2w07l5w/logs
INFO flwr 2024-04-06 20:35:13,983 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:42:20,533 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1367834)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1367834)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 20:42:25,686	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:42:26,146	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:42:26,577	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d149c9cc287b6a41.zip' (10.26MiB) to Ray cluster...
2024-04-06 20:42:26,606	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d149c9cc287b6a41.zip'.
INFO flwr 2024-04-06 20:42:37,519 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 138943679898.0, 'object_store_memory': 63833005670.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 20:42:37,519 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:42:37,519 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:42:37,535 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:42:37,536 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:42:37,536 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:42:37,536 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:42:40,354 | server.py:94 | initial parameters (loss, other metrics): 2.302816867828369, {'accuracy': 0.0734, 'data_size': 10000}
INFO flwr 2024-04-06 20:42:40,355 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:42:40,356 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1372336)[0m 2024-04-06 20:42:43.599838: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1372336)[0m 2024-04-06 20:42:43.701652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1372336)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1372347)[0m 2024-04-06 20:42:45.751044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1372349)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1372349)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1372349)[0m 2024-04-06 20:42:43.902001: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1372349)[0m 2024-04-06 20:42:43.989339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1372349)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1372344)[0m 2024-04-06 20:42:45.955314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:42:57,591 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:42:57,620 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:42:58,958 | server.py:125 | fit progress: (1, 2.3010501861572266, {'accuracy': 0.0975, 'data_size': 10000}, 18.603079662978416)
INFO flwr 2024-04-06 20:42:58,959 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:42:58,959 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:07,059 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:08,135 | server.py:125 | fit progress: (2, 2.299262523651123, {'accuracy': 0.1114, 'data_size': 10000}, 27.77993294497719)
INFO flwr 2024-04-06 20:43:08,136 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:08,136 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:15,723 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:17,068 | server.py:125 | fit progress: (3, 2.2973721027374268, {'accuracy': 0.1384, 'data_size': 10000}, 36.71242276800331)
INFO flwr 2024-04-06 20:43:17,068 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:17,068 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:24,461 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:25,809 | server.py:125 | fit progress: (4, 2.2957117557525635, {'accuracy': 0.1615, 'data_size': 10000}, 45.45395019400166)
INFO flwr 2024-04-06 20:43:25,810 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:25,810 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:33,543 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:34,944 | server.py:125 | fit progress: (5, 2.2938363552093506, {'accuracy': 0.1807, 'data_size': 10000}, 54.58856161599397)
INFO flwr 2024-04-06 20:43:34,944 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:34,944 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:42,525 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:43,797 | server.py:125 | fit progress: (6, 2.291780948638916, {'accuracy': 0.1968, 'data_size': 10000}, 63.44187910298933)
INFO flwr 2024-04-06 20:43:43,798 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:43,798 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:43:51,332 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:43:52,941 | server.py:125 | fit progress: (7, 2.289982795715332, {'accuracy': 0.2297, 'data_size': 10000}, 72.58514759197715)
INFO flwr 2024-04-06 20:43:52,941 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:43:52,941 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:44:00,858 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:44:02,438 | server.py:125 | fit progress: (8, 2.2874956130981445, {'accuracy': 0.2705, 'data_size': 10000}, 82.08284366800217)
INFO flwr 2024-04-06 20:44:02,438 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:44:02,439 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:44:10,132 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:44:11,648 | server.py:125 | fit progress: (9, 2.285092353820801, {'accuracy': 0.296, 'data_size': 10000}, 91.29284145199927)
INFO flwr 2024-04-06 20:44:11,648 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:44:11,649 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:44:19,267 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:44:20,582 | server.py:125 | fit progress: (10, 2.282979965209961, {'accuracy': 0.3253, 'data_size': 10000}, 100.22628169797827)
INFO flwr 2024-04-06 20:44:20,582 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:44:20,582 | server.py:153 | FL finished in 100.22666663798736
INFO flwr 2024-04-06 20:44:20,582 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:44:20,582 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:44:20,582 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:44:20,582 | app.py:229 | app_fit: losses_centralized [(0, 2.302816867828369), (1, 2.3010501861572266), (2, 2.299262523651123), (3, 2.2973721027374268), (4, 2.2957117557525635), (5, 2.2938363552093506), (6, 2.291780948638916), (7, 2.289982795715332), (8, 2.2874956130981445), (9, 2.285092353820801), (10, 2.282979965209961)]
INFO flwr 2024-04-06 20:44:20,583 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0734), (1, 0.0975), (2, 0.1114), (3, 0.1384), (4, 0.1615), (5, 0.1807), (6, 0.1968), (7, 0.2297), (8, 0.2705), (9, 0.296), (10, 0.3253)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3253
wandb:     loss 2.28298
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_204220-jyfm2zea
wandb: Find logs at: ./wandb/offline-run-20240406_204220-jyfm2zea/logs
INFO flwr 2024-04-06 20:44:24,108 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 20:51:32,925 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1372336)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1372336)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 20:51:56,741	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-06 20:51:56,743	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-06 20:51:56,743	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/tmp/ray/session_2024-04-06_20-51-35_278015_1357177/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-04-06 20:51:56,744	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-04-06 20:51:56,877	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 20:51:59,615	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 20:51:59,941	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e5b607a17cad4e26.zip' (10.28MiB) to Ray cluster...
2024-04-06 20:51:59,984	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e5b607a17cad4e26.zip'.
INFO flwr 2024-04-06 20:52:11,132 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 127202487296.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58801065984.0}
INFO flwr 2024-04-06 20:52:11,132 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 20:52:11,132 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 20:52:11,147 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 20:52:11,149 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 20:52:11,149 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 20:52:11,149 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 20:52:13,248 | server.py:94 | initial parameters (loss, other metrics): 2.304152250289917, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-06 20:52:13,248 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 20:52:13,249 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1381017)[0m 2024-04-06 20:52:32.169275: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1381017)[0m 2024-04-06 20:52:32.289878: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1381017)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1381017)[0m 2024-04-06 20:52:52.788761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1381010)[0m 2024-04-06 20:52:32.169580: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1381011)[0m 2024-04-06 20:52:32.299622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1381011)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1381017)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1381017)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1381010)[0m 2024-04-06 20:52:52.789133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 20:54:23,039 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 20:54:23,094 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 20:54:24,491 | server.py:125 | fit progress: (1, 2.3018887042999268, {'accuracy': 0.1205, 'data_size': 10000}, 131.24261685699457)
INFO flwr 2024-04-06 20:54:24,491 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 20:54:24,492 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:54:33,757 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 20:54:34,896 | server.py:125 | fit progress: (2, 2.2996435165405273, {'accuracy': 0.1498, 'data_size': 10000}, 141.6474497569725)
INFO flwr 2024-04-06 20:54:34,896 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 20:54:34,897 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:54:43,200 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 20:54:44,590 | server.py:125 | fit progress: (3, 2.297070026397705, {'accuracy': 0.1728, 'data_size': 10000}, 151.34142837297986)
INFO flwr 2024-04-06 20:54:44,590 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 20:54:44,590 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:54:52,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 20:54:53,950 | server.py:125 | fit progress: (4, 2.294233560562134, {'accuracy': 0.2232, 'data_size': 10000}, 160.70172495298903)
INFO flwr 2024-04-06 20:54:53,951 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 20:54:53,951 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:01,716 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:03,124 | server.py:125 | fit progress: (5, 2.2912678718566895, {'accuracy': 0.2367, 'data_size': 10000}, 169.87499511498027)
INFO flwr 2024-04-06 20:55:03,124 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 20:55:03,124 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:10,574 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:11,792 | server.py:125 | fit progress: (6, 2.288362979888916, {'accuracy': 0.2321, 'data_size': 10000}, 178.54374665897922)
INFO flwr 2024-04-06 20:55:11,793 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 20:55:11,793 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:19,607 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:21,150 | server.py:125 | fit progress: (7, 2.285460948944092, {'accuracy': 0.2518, 'data_size': 10000}, 187.90184381397557)
INFO flwr 2024-04-06 20:55:21,151 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 20:55:21,151 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:28,532 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:29,798 | server.py:125 | fit progress: (8, 2.282181739807129, {'accuracy': 0.264, 'data_size': 10000}, 196.5491336119885)
INFO flwr 2024-04-06 20:55:29,798 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 20:55:29,798 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:37,660 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:38,968 | server.py:125 | fit progress: (9, 2.2794315814971924, {'accuracy': 0.2771, 'data_size': 10000}, 205.7197850929806)
INFO flwr 2024-04-06 20:55:38,969 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 20:55:38,969 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 20:55:46,637 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 20:55:48,201 | server.py:125 | fit progress: (10, 2.2762844562530518, {'accuracy': 0.246, 'data_size': 10000}, 214.95217302098172)
INFO flwr 2024-04-06 20:55:48,201 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 20:55:48,201 | server.py:153 | FL finished in 214.9525236079935
INFO flwr 2024-04-06 20:55:48,201 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 20:55:48,201 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 20:55:48,201 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 20:55:48,201 | app.py:229 | app_fit: losses_centralized [(0, 2.304152250289917), (1, 2.3018887042999268), (2, 2.2996435165405273), (3, 2.297070026397705), (4, 2.294233560562134), (5, 2.2912678718566895), (6, 2.288362979888916), (7, 2.285460948944092), (8, 2.282181739807129), (9, 2.2794315814971924), (10, 2.2762844562530518)]
INFO flwr 2024-04-06 20:55:48,202 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.1205), (2, 0.1498), (3, 0.1728), (4, 0.2232), (5, 0.2367), (6, 0.2321), (7, 0.2518), (8, 0.264), (9, 0.2771), (10, 0.246)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.246
wandb:     loss 2.27628
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_205132-isjjxp3y
wandb: Find logs at: ./wandb/offline-run-20240406_205132-isjjxp3y/logs
INFO flwr 2024-04-06 20:55:51,742 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:02:59,329 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1381010)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1381010)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:03:05,698	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:03:06,029	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:03:06,329	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1eb9453a5b2863a6.zip' (10.30MiB) to Ray cluster...
2024-04-06 21:03:06,355	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1eb9453a5b2863a6.zip'.
INFO flwr 2024-04-06 21:03:17,309 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 61926006374.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 134494014874.0}
INFO flwr 2024-04-06 21:03:17,310 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:03:17,310 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:03:17,323 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:03:17,323 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:03:17,324 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:03:17,324 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:03:19,459 | server.py:94 | initial parameters (loss, other metrics): 2.3035645484924316, {'accuracy': 0.0768, 'data_size': 10000}
INFO flwr 2024-04-06 21:03:19,460 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:03:19,460 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1388350)[0m 2024-04-06 21:03:23.335509: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1388354)[0m 2024-04-06 21:03:23.502873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1388354)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1388350)[0m 2024-04-06 21:03:25.429747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1388350)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1388350)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1388349)[0m 2024-04-06 21:03:23.893222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1388349)[0m 2024-04-06 21:03:23.990862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1388349)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1388354)[0m 2024-04-06 21:03:26.030025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:03:37,934 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:03:37,963 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:03:39,019 | server.py:125 | fit progress: (1, 2.2997937202453613, {'accuracy': 0.1332, 'data_size': 10000}, 19.55918723499053)
INFO flwr 2024-04-06 21:03:39,019 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:39,019 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:03:47,764 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:03:48,849 | server.py:125 | fit progress: (2, 2.29608154296875, {'accuracy': 0.172, 'data_size': 10000}, 29.38958542200271)
INFO flwr 2024-04-06 21:03:48,850 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:48,850 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:03:56,629 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:03:57,732 | server.py:125 | fit progress: (3, 2.291919231414795, {'accuracy': 0.2369, 'data_size': 10000}, 38.272385740012396)
INFO flwr 2024-04-06 21:03:57,732 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:03:57,733 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:05,463 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:06,823 | server.py:125 | fit progress: (4, 2.287574529647827, {'accuracy': 0.3157, 'data_size': 10000}, 47.3629449820146)
INFO flwr 2024-04-06 21:04:06,823 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:06,823 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:14,527 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:15,902 | server.py:125 | fit progress: (5, 2.280438184738159, {'accuracy': 0.3623, 'data_size': 10000}, 56.44225369300693)
INFO flwr 2024-04-06 21:04:15,902 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:15,902 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:23,804 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:25,223 | server.py:125 | fit progress: (6, 2.2742021083831787, {'accuracy': 0.3832, 'data_size': 10000}, 65.76358101901133)
INFO flwr 2024-04-06 21:04:25,224 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:25,224 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:33,021 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:34,525 | server.py:125 | fit progress: (7, 2.267077684402466, {'accuracy': 0.3862, 'data_size': 10000}, 75.0649164020142)
INFO flwr 2024-04-06 21:04:34,525 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:34,525 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:42,363 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:43,612 | server.py:125 | fit progress: (8, 2.2586514949798584, {'accuracy': 0.3864, 'data_size': 10000}, 84.15243776701391)
INFO flwr 2024-04-06 21:04:43,612 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:43,613 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:04:51,829 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:04:53,123 | server.py:125 | fit progress: (9, 2.2510879039764404, {'accuracy': 0.4323, 'data_size': 10000}, 93.66324780299328)
INFO flwr 2024-04-06 21:04:53,123 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:04:53,123 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:05:00,491 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:05:02,012 | server.py:125 | fit progress: (10, 2.2393577098846436, {'accuracy': 0.3073, 'data_size': 10000}, 102.5522760009917)
INFO flwr 2024-04-06 21:05:02,012 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:05:02,012 | server.py:153 | FL finished in 102.55263562701293
INFO flwr 2024-04-06 21:05:02,013 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:05:02,013 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:05:02,013 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:05:02,013 | app.py:229 | app_fit: losses_centralized [(0, 2.3035645484924316), (1, 2.2997937202453613), (2, 2.29608154296875), (3, 2.291919231414795), (4, 2.287574529647827), (5, 2.280438184738159), (6, 2.2742021083831787), (7, 2.267077684402466), (8, 2.2586514949798584), (9, 2.2510879039764404), (10, 2.2393577098846436)]
INFO flwr 2024-04-06 21:05:02,013 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0768), (1, 0.1332), (2, 0.172), (3, 0.2369), (4, 0.3157), (5, 0.3623), (6, 0.3832), (7, 0.3862), (8, 0.3864), (9, 0.4323), (10, 0.3073)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3073
wandb:     loss 2.23936
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_210259-5ttva2d8
wandb: Find logs at: ./wandb/offline-run-20240406_210259-5ttva2d8/logs
INFO flwr 2024-04-06 21:05:05,552 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:12:11,091 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1388357)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1388357)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:12:15,986	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:12:16,426	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:12:16,755	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2616190b7ccf7ca2.zip' (10.32MiB) to Ray cluster...
2024-04-06 21:12:16,781	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2616190b7ccf7ca2.zip'.
INFO flwr 2024-04-06 21:12:27,631 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 60965797478.0, 'node:10.20.240.18': 1.0, 'memory': 132253527450.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 21:12:27,631 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:12:27,631 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:12:27,645 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:12:27,646 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:12:27,646 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:12:27,646 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:12:31,359 | server.py:94 | initial parameters (loss, other metrics): 2.304069757461548, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-06 21:12:31,360 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:12:31,361 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1392822)[0m 2024-04-06 21:12:33.239878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1392825)[0m 2024-04-06 21:12:33.924663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1392825)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1392825)[0m 2024-04-06 21:12:35.966688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1392821)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1392821)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1392823)[0m 2024-04-06 21:12:34.183108: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392823)[0m 2024-04-06 21:12:34.283008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392823)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1392818)[0m 2024-04-06 21:12:36.551426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:12:47,792 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:12:47,829 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:12:48,898 | server.py:125 | fit progress: (1, 2.299907922744751, {'accuracy': 0.1616, 'data_size': 10000}, 17.537068857025588)
INFO flwr 2024-04-06 21:12:48,899 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:12:48,899 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:12:56,891 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:12:58,255 | server.py:125 | fit progress: (2, 2.29484224319458, {'accuracy': 0.208, 'data_size': 10000}, 26.89354223900591)
INFO flwr 2024-04-06 21:12:58,255 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:12:58,255 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:05,851 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:06,987 | server.py:125 | fit progress: (3, 2.2899913787841797, {'accuracy': 0.2632, 'data_size': 10000}, 35.625350372021785)
INFO flwr 2024-04-06 21:13:06,987 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:06,987 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:14,485 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:15,648 | server.py:125 | fit progress: (4, 2.284127712249756, {'accuracy': 0.337, 'data_size': 10000}, 44.286312538024504)
INFO flwr 2024-04-06 21:13:15,648 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:15,648 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:23,342 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:24,761 | server.py:125 | fit progress: (5, 2.275960922241211, {'accuracy': 0.3361, 'data_size': 10000}, 53.40006997901946)
INFO flwr 2024-04-06 21:13:24,762 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:24,762 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:31,943 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:33,424 | server.py:125 | fit progress: (6, 2.2688252925872803, {'accuracy': 0.3605, 'data_size': 10000}, 62.06282816402381)
INFO flwr 2024-04-06 21:13:33,424 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:33,425 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:41,089 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:42,580 | server.py:125 | fit progress: (7, 2.257901191711426, {'accuracy': 0.3659, 'data_size': 10000}, 71.21847572602564)
INFO flwr 2024-04-06 21:13:42,580 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:42,580 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:50,433 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:13:51,740 | server.py:125 | fit progress: (8, 2.252236843109131, {'accuracy': 0.3832, 'data_size': 10000}, 80.37921975500649)
INFO flwr 2024-04-06 21:13:51,741 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:13:51,741 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:13:59,617 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:14:01,153 | server.py:125 | fit progress: (9, 2.2425832748413086, {'accuracy': 0.409, 'data_size': 10000}, 89.79204127500998)
INFO flwr 2024-04-06 21:14:01,154 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:14:01,154 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:14:08,998 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:14:10,322 | server.py:125 | fit progress: (10, 2.228844404220581, {'accuracy': 0.4035, 'data_size': 10000}, 98.9607951620128)
INFO flwr 2024-04-06 21:14:10,322 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:14:10,322 | server.py:153 | FL finished in 98.96119701702264
INFO flwr 2024-04-06 21:14:10,323 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:14:10,323 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:14:10,323 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:14:10,323 | app.py:229 | app_fit: losses_centralized [(0, 2.304069757461548), (1, 2.299907922744751), (2, 2.29484224319458), (3, 2.2899913787841797), (4, 2.284127712249756), (5, 2.275960922241211), (6, 2.2688252925872803), (7, 2.257901191711426), (8, 2.252236843109131), (9, 2.2425832748413086), (10, 2.228844404220581)]
INFO flwr 2024-04-06 21:14:10,323 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.1616), (2, 0.208), (3, 0.2632), (4, 0.337), (5, 0.3361), (6, 0.3605), (7, 0.3659), (8, 0.3832), (9, 0.409), (10, 0.4035)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4035
wandb:     loss 2.22884
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_211210-ukjhzjye
wandb: Find logs at: ./wandb/offline-run-20240406_211210-ukjhzjye/logs
INFO flwr 2024-04-06 21:14:13,829 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:21:21,595 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1392818)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1392818)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:21:27,611	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:21:28,178	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:21:28,679	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_47efba9652b5a3ba.zip' (10.34MiB) to Ray cluster...
2024-04-06 21:21:28,717	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_47efba9652b5a3ba.zip'.
INFO flwr 2024-04-06 21:21:40,046 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 130041984410.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60017993318.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 21:21:40,047 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:21:40,047 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:21:40,067 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:21:40,072 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:21:40,072 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:21:40,072 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:21:41,993 | server.py:94 | initial parameters (loss, other metrics): 2.299149513244629, {'accuracy': 0.1618, 'data_size': 10000}
INFO flwr 2024-04-06 21:21:41,993 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:21:41,993 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1400433)[0m 2024-04-06 21:21:46.035810: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1400433)[0m 2024-04-06 21:21:46.133839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1400433)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1400433)[0m 2024-04-06 21:21:48.439594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1400444)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1400444)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1400443)[0m 2024-04-06 21:21:47.146112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1400443)[0m 2024-04-06 21:21:47.256284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1400443)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1400443)[0m 2024-04-06 21:21:49.583082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1400433)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1400433)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-04-06 21:22:03,864 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:22:03,901 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:22:04,934 | server.py:125 | fit progress: (1, 2.2924909591674805, {'accuracy': 0.2373, 'data_size': 10000}, 22.94078878298751)
INFO flwr 2024-04-06 21:22:04,934 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:04,935 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:13,497 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:14,636 | server.py:125 | fit progress: (2, 2.287459373474121, {'accuracy': 0.2437, 'data_size': 10000}, 32.643068239995046)
INFO flwr 2024-04-06 21:22:14,637 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:14,637 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:22,667 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:23,849 | server.py:125 | fit progress: (3, 2.282036781311035, {'accuracy': 0.2705, 'data_size': 10000}, 41.85590501900879)
INFO flwr 2024-04-06 21:22:23,850 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:23,850 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:31,543 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:33,083 | server.py:125 | fit progress: (4, 2.2760438919067383, {'accuracy': 0.3417, 'data_size': 10000}, 51.09010361300898)
INFO flwr 2024-04-06 21:22:33,084 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:33,084 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:41,140 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:42,638 | server.py:125 | fit progress: (5, 2.267240524291992, {'accuracy': 0.3905, 'data_size': 10000}, 60.64449878499727)
INFO flwr 2024-04-06 21:22:42,638 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:42,639 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:22:50,433 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:22:51,873 | server.py:125 | fit progress: (6, 2.2552032470703125, {'accuracy': 0.4114, 'data_size': 10000}, 69.88012950599659)
INFO flwr 2024-04-06 21:22:51,874 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:22:51,874 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:23:00,207 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:23:01,689 | server.py:125 | fit progress: (7, 2.2448201179504395, {'accuracy': 0.3927, 'data_size': 10000}, 79.69584490198758)
INFO flwr 2024-04-06 21:23:01,689 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:23:01,690 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:23:09,127 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:23:10,764 | server.py:125 | fit progress: (8, 2.229755163192749, {'accuracy': 0.37, 'data_size': 10000}, 88.77041004499188)
INFO flwr 2024-04-06 21:23:10,764 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:23:10,764 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:23:18,803 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:23:20,099 | server.py:125 | fit progress: (9, 2.2176077365875244, {'accuracy': 0.4185, 'data_size': 10000}, 98.10609501399449)
INFO flwr 2024-04-06 21:23:20,100 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:23:20,100 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:23:28,249 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:23:29,824 | server.py:125 | fit progress: (10, 2.20367693901062, {'accuracy': 0.465, 'data_size': 10000}, 107.83087124998565)
INFO flwr 2024-04-06 21:23:29,824 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:23:29,825 | server.py:153 | FL finished in 107.83128415298415
INFO flwr 2024-04-06 21:23:29,825 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:23:29,825 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:23:29,825 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:23:29,825 | app.py:229 | app_fit: losses_centralized [(0, 2.299149513244629), (1, 2.2924909591674805), (2, 2.287459373474121), (3, 2.282036781311035), (4, 2.2760438919067383), (5, 2.267240524291992), (6, 2.2552032470703125), (7, 2.2448201179504395), (8, 2.229755163192749), (9, 2.2176077365875244), (10, 2.20367693901062)]
INFO flwr 2024-04-06 21:23:29,825 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1618), (1, 0.2373), (2, 0.2437), (3, 0.2705), (4, 0.3417), (5, 0.3905), (6, 0.4114), (7, 0.3927), (8, 0.37), (9, 0.4185), (10, 0.465)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.465
wandb:     loss 2.20368
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_212121-gxwlg9df
wandb: Find logs at: ./wandb/offline-run-20240406_212121-gxwlg9df/logs
INFO flwr 2024-04-06 21:23:33,468 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:30:40,881 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1400443)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1400443)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
2024-04-06 21:30:47,099	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:30:47,432	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:30:47,755	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_374f8aad65218ce5.zip' (10.35MiB) to Ray cluster...
2024-04-06 21:30:47,788	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_374f8aad65218ce5.zip'.
INFO flwr 2024-04-06 21:30:58,750 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 137639820698.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63274208870.0}
INFO flwr 2024-04-06 21:30:58,751 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:30:58,751 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:30:58,771 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:30:58,772 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:30:58,772 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:30:58,773 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:31:01,724 | server.py:94 | initial parameters (loss, other metrics): 2.3048603534698486, {'accuracy': 0.098, 'data_size': 10000}
INFO flwr 2024-04-06 21:31:01,724 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:31:01,724 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1404493)[0m 2024-04-06 21:31:04.866537: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1404493)[0m 2024-04-06 21:31:04.964324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1404493)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1404495)[0m 2024-04-06 21:31:07.071342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1404493)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1404493)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1404489)[0m 2024-04-06 21:31:05.238981: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1404489)[0m 2024-04-06 21:31:05.338094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1404489)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1404487)[0m 2024-04-06 21:31:07.378169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:31:19,583 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:31:19,622 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:31:20,684 | server.py:125 | fit progress: (1, 2.30478572845459, {'accuracy': 0.0985, 'data_size': 10000}, 18.96014138200553)
INFO flwr 2024-04-06 21:31:20,685 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:31:20,685 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:31:29,165 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:31:30,477 | server.py:125 | fit progress: (2, 2.3047091960906982, {'accuracy': 0.0991, 'data_size': 10000}, 28.75319376398693)
INFO flwr 2024-04-06 21:31:30,478 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:31:30,478 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:31:38,454 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:31:40,118 | server.py:125 | fit progress: (3, 2.3046371936798096, {'accuracy': 0.1012, 'data_size': 10000}, 38.394278199994005)
INFO flwr 2024-04-06 21:31:40,119 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:31:40,119 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:31:48,512 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:31:50,200 | server.py:125 | fit progress: (4, 2.30454158782959, {'accuracy': 0.1036, 'data_size': 10000}, 48.47601121501066)
INFO flwr 2024-04-06 21:31:50,201 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:31:50,201 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:31:58,869 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:00,105 | server.py:125 | fit progress: (5, 2.3044590950012207, {'accuracy': 0.1049, 'data_size': 10000}, 58.38120961599634)
INFO flwr 2024-04-06 21:32:00,106 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:32:00,106 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:32:07,761 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:09,270 | server.py:125 | fit progress: (6, 2.3043768405914307, {'accuracy': 0.1065, 'data_size': 10000}, 67.54554492500029)
INFO flwr 2024-04-06 21:32:09,270 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:32:09,270 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:32:17,142 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:18,591 | server.py:125 | fit progress: (7, 2.304314136505127, {'accuracy': 0.1075, 'data_size': 10000}, 76.86722025700146)
INFO flwr 2024-04-06 21:32:18,592 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:32:18,592 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:32:26,563 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:28,151 | server.py:125 | fit progress: (8, 2.304222345352173, {'accuracy': 0.1084, 'data_size': 10000}, 86.4264431349875)
INFO flwr 2024-04-06 21:32:28,151 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:32:28,151 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:32:36,282 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:37,791 | server.py:125 | fit progress: (9, 2.304133415222168, {'accuracy': 0.1095, 'data_size': 10000}, 96.06716761901043)
INFO flwr 2024-04-06 21:32:37,792 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:32:37,792 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:32:45,621 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:32:47,025 | server.py:125 | fit progress: (10, 2.304063081741333, {'accuracy': 0.1102, 'data_size': 10000}, 105.30039923699223)
INFO flwr 2024-04-06 21:32:47,025 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:32:47,025 | server.py:153 | FL finished in 105.30094718700275
INFO flwr 2024-04-06 21:32:47,025 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:32:47,025 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:32:47,025 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:32:47,026 | app.py:229 | app_fit: losses_centralized [(0, 2.3048603534698486), (1, 2.30478572845459), (2, 2.3047091960906982), (3, 2.3046371936798096), (4, 2.30454158782959), (5, 2.3044590950012207), (6, 2.3043768405914307), (7, 2.304314136505127), (8, 2.304222345352173), (9, 2.304133415222168), (10, 2.304063081741333)]
INFO flwr 2024-04-06 21:32:47,026 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.0985), (2, 0.0991), (3, 0.1012), (4, 0.1036), (5, 0.1049), (6, 0.1065), (7, 0.1075), (8, 0.1084), (9, 0.1095), (10, 0.1102)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1102
wandb:     loss 2.30406
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_213040-3ki0bvac
wandb: Find logs at: ./wandb/offline-run-20240406_213040-3ki0bvac/logs
INFO flwr 2024-04-06 21:32:50,529 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:39:58,668 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1404487)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1404487)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:40:03,884	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:40:04,326	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:40:04,814	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_223f4301d83d8452.zip' (10.38MiB) to Ray cluster...
2024-04-06 21:40:04,847	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_223f4301d83d8452.zip'.
INFO flwr 2024-04-06 21:40:16,099 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 128157915751.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59210535321.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 21:40:16,099 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:40:16,100 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:40:16,117 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:40:16,118 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:40:16,119 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:40:16,119 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:40:18,832 | server.py:94 | initial parameters (loss, other metrics): 2.3016185760498047, {'accuracy': 0.1246, 'data_size': 10000}
INFO flwr 2024-04-06 21:40:18,834 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:40:18,834 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1412005)[0m 2024-04-06 21:40:22.741969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1412005)[0m 2024-04-06 21:40:22.845369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1412005)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1412005)[0m 2024-04-06 21:40:25.092330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1412001)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1412001)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1412003)[0m 2024-04-06 21:40:22.991436: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1412003)[0m 2024-04-06 21:40:23.095684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1412003)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1412003)[0m 2024-04-06 21:40:25.266028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:40:37,403 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:40:37,744 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:40:38,953 | server.py:125 | fit progress: (1, 2.2961618900299072, {'accuracy': 0.1899, 'data_size': 10000}, 20.11925885398523)
INFO flwr 2024-04-06 21:40:38,954 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:40:38,954 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:40:47,955 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:40:49,643 | server.py:125 | fit progress: (2, 2.2908546924591064, {'accuracy': 0.2307, 'data_size': 10000}, 30.809371648996603)
INFO flwr 2024-04-06 21:40:49,644 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:40:49,644 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:40:58,353 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:00,019 | server.py:125 | fit progress: (3, 2.2850558757781982, {'accuracy': 0.2986, 'data_size': 10000}, 41.18461103900336)
INFO flwr 2024-04-06 21:41:00,019 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:00,019 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:09,707 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:11,192 | server.py:125 | fit progress: (4, 2.2794883251190186, {'accuracy': 0.3413, 'data_size': 10000}, 52.35830854199594)
INFO flwr 2024-04-06 21:41:11,193 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:11,193 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:19,408 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:20,629 | server.py:125 | fit progress: (5, 2.2730722427368164, {'accuracy': 0.363, 'data_size': 10000}, 61.79459622100694)
INFO flwr 2024-04-06 21:41:20,629 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:20,629 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:28,537 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:30,060 | server.py:125 | fit progress: (6, 2.266444444656372, {'accuracy': 0.4046, 'data_size': 10000}, 71.22626393899554)
INFO flwr 2024-04-06 21:41:30,060 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:30,061 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:38,397 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:39,610 | server.py:125 | fit progress: (7, 2.2570676803588867, {'accuracy': 0.442, 'data_size': 10000}, 80.77658585898462)
INFO flwr 2024-04-06 21:41:39,611 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:39,611 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:48,040 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:49,323 | server.py:125 | fit progress: (8, 2.249488353729248, {'accuracy': 0.4848, 'data_size': 10000}, 90.48909624299267)
INFO flwr 2024-04-06 21:41:49,323 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:49,323 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:41:57,259 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:41:58,898 | server.py:125 | fit progress: (9, 2.239234209060669, {'accuracy': 0.5215, 'data_size': 10000}, 100.06389920800575)
INFO flwr 2024-04-06 21:41:58,898 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:41:58,898 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:42:06,793 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:42:08,418 | server.py:125 | fit progress: (10, 2.226497173309326, {'accuracy': 0.5181, 'data_size': 10000}, 109.58376081599272)
INFO flwr 2024-04-06 21:42:08,418 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:42:08,418 | server.py:153 | FL finished in 109.58427150300122
INFO flwr 2024-04-06 21:42:08,418 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:42:08,418 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:42:08,419 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:42:08,419 | app.py:229 | app_fit: losses_centralized [(0, 2.3016185760498047), (1, 2.2961618900299072), (2, 2.2908546924591064), (3, 2.2850558757781982), (4, 2.2794883251190186), (5, 2.2730722427368164), (6, 2.266444444656372), (7, 2.2570676803588867), (8, 2.249488353729248), (9, 2.239234209060669), (10, 2.226497173309326)]
INFO flwr 2024-04-06 21:42:08,419 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1246), (1, 0.1899), (2, 0.2307), (3, 0.2986), (4, 0.3413), (5, 0.363), (6, 0.4046), (7, 0.442), (8, 0.4848), (9, 0.5215), (10, 0.5181)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5181
wandb:     loss 2.2265
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_213958-xf8b7yin
wandb: Find logs at: ./wandb/offline-run-20240406_213958-xf8b7yin/logs
INFO flwr 2024-04-06 21:42:11,981 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:49:18,099 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1411996)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1411996)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:49:22,740	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:49:23,125	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:49:23,426	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6d420640ef1fffb4.zip' (10.40MiB) to Ray cluster...
2024-04-06 21:49:23,451	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6d420640ef1fffb4.zip'.
INFO flwr 2024-04-06 21:49:34,611 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 61163903385.0, 'node:__internal_head__': 1.0, 'memory': 132715774567.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 21:49:34,611 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:49:34,612 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:49:34,627 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:49:34,627 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:49:34,628 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:49:34,628 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:49:37,407 | server.py:94 | initial parameters (loss, other metrics): 2.3034725189208984, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-06 21:49:37,408 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:49:37,408 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1419320)[0m 2024-04-06 21:49:40.491409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1419320)[0m 2024-04-06 21:49:40.594325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1419320)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1419323)[0m 2024-04-06 21:49:42.544305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1419324)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1419324)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1419319)[0m 2024-04-06 21:49:41.030594: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1419319)[0m 2024-04-06 21:49:41.121489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1419319)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1419321)[0m 2024-04-06 21:49:43.626837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:49:54,174 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:49:54,226 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:49:55,281 | server.py:125 | fit progress: (1, 2.2916295528411865, {'accuracy': 0.2262, 'data_size': 10000}, 17.87362661998486)
INFO flwr 2024-04-06 21:49:55,282 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:49:55,282 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:03,691 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:05,081 | server.py:125 | fit progress: (2, 2.278276205062866, {'accuracy': 0.353, 'data_size': 10000}, 27.67305375097203)
INFO flwr 2024-04-06 21:50:05,081 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:05,082 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:12,756 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:14,197 | server.py:125 | fit progress: (3, 2.2638022899627686, {'accuracy': 0.2054, 'data_size': 10000}, 36.78960650798399)
INFO flwr 2024-04-06 21:50:14,198 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:14,198 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:21,719 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:23,150 | server.py:125 | fit progress: (4, 2.2454962730407715, {'accuracy': 0.4021, 'data_size': 10000}, 45.7420351979963)
INFO flwr 2024-04-06 21:50:23,150 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:23,150 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:30,929 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:32,129 | server.py:125 | fit progress: (5, 2.2243165969848633, {'accuracy': 0.4356, 'data_size': 10000}, 54.72144111499074)
INFO flwr 2024-04-06 21:50:32,130 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:32,130 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:39,561 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:41,042 | server.py:125 | fit progress: (6, 2.199054002761841, {'accuracy': 0.3885, 'data_size': 10000}, 63.63409685398801)
INFO flwr 2024-04-06 21:50:41,042 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:41,042 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:49,195 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:50,510 | server.py:125 | fit progress: (7, 2.174849510192871, {'accuracy': 0.418, 'data_size': 10000}, 73.10228871699655)
INFO flwr 2024-04-06 21:50:50,510 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:50,511 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:50:58,546 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 21:50:59,833 | server.py:125 | fit progress: (8, 2.143712043762207, {'accuracy': 0.5641, 'data_size': 10000}, 82.42544998697122)
INFO flwr 2024-04-06 21:50:59,834 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 21:50:59,834 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:51:07,202 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 21:51:08,770 | server.py:125 | fit progress: (9, 2.1198222637176514, {'accuracy': 0.56, 'data_size': 10000}, 91.36236968799494)
INFO flwr 2024-04-06 21:51:08,770 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 21:51:08,771 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:51:16,814 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 21:51:18,469 | server.py:125 | fit progress: (10, 2.1032590866088867, {'accuracy': 0.5474, 'data_size': 10000}, 101.06112222297816)
INFO flwr 2024-04-06 21:51:18,469 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 21:51:18,469 | server.py:153 | FL finished in 101.06152742999257
INFO flwr 2024-04-06 21:51:18,470 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 21:51:18,470 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 21:51:18,470 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 21:51:18,470 | app.py:229 | app_fit: losses_centralized [(0, 2.3034725189208984), (1, 2.2916295528411865), (2, 2.278276205062866), (3, 2.2638022899627686), (4, 2.2454962730407715), (5, 2.2243165969848633), (6, 2.199054002761841), (7, 2.174849510192871), (8, 2.143712043762207), (9, 2.1198222637176514), (10, 2.1032590866088867)]
INFO flwr 2024-04-06 21:51:18,470 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.2262), (2, 0.353), (3, 0.2054), (4, 0.4021), (5, 0.4356), (6, 0.3885), (7, 0.418), (8, 0.5641), (9, 0.56), (10, 0.5474)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5474
wandb:     loss 2.10326
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_214917-8lk2z8lu
wandb: Find logs at: ./wandb/offline-run-20240406_214917-8lk2z8lu/logs
INFO flwr 2024-04-06 21:51:22,002 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 21:58:27,239 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1419321)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1419321)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 21:58:33,355	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 21:58:33,651	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 21:58:33,988	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_63a2efadbd8be5af.zip' (10.42MiB) to Ray cluster...
2024-04-06 21:58:34,028	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_63a2efadbd8be5af.zip'.
INFO flwr 2024-04-06 21:58:44,797 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 131895643546.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 60812418662.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 21:58:44,797 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 21:58:44,797 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 21:58:44,810 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 21:58:44,811 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 21:58:44,811 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 21:58:44,811 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 21:58:48,191 | server.py:94 | initial parameters (loss, other metrics): 2.30568265914917, {'accuracy': 0.0528, 'data_size': 10000}
INFO flwr 2024-04-06 21:58:48,191 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 21:58:48,192 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1423924)[0m 2024-04-06 21:58:50.647978: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1423924)[0m 2024-04-06 21:58:50.745033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1423924)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1423923)[0m 2024-04-06 21:58:52.900376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1423931)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1423931)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1423926)[0m 2024-04-06 21:58:51.133972: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1423926)[0m 2024-04-06 21:58:51.231866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1423926)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1423926)[0m 2024-04-06 21:58:53.548664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 21:59:05,175 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 21:59:05,204 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 21:59:06,220 | server.py:125 | fit progress: (1, 2.291869878768921, {'accuracy': 0.1765, 'data_size': 10000}, 18.02845789398998)
INFO flwr 2024-04-06 21:59:06,220 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:06,221 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:14,554 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 21:59:15,885 | server.py:125 | fit progress: (2, 2.2780535221099854, {'accuracy': 0.3227, 'data_size': 10000}, 27.69305289699696)
INFO flwr 2024-04-06 21:59:15,885 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:15,885 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:23,935 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 21:59:25,266 | server.py:125 | fit progress: (3, 2.251068115234375, {'accuracy': 0.4072, 'data_size': 10000}, 37.07464341499144)
INFO flwr 2024-04-06 21:59:25,267 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:25,267 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:32,564 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 21:59:33,942 | server.py:125 | fit progress: (4, 2.210340738296509, {'accuracy': 0.36, 'data_size': 10000}, 45.750152021995746)
INFO flwr 2024-04-06 21:59:33,942 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:33,942 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:41,502 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 21:59:42,677 | server.py:125 | fit progress: (5, 2.1789584159851074, {'accuracy': 0.4701, 'data_size': 10000}, 54.48522464101552)
INFO flwr 2024-04-06 21:59:42,677 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:42,677 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:50,223 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 21:59:51,635 | server.py:125 | fit progress: (6, 2.137817144393921, {'accuracy': 0.5175, 'data_size': 10000}, 63.443321305006975)
INFO flwr 2024-04-06 21:59:51,635 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 21:59:51,635 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 21:59:59,340 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:00:00,582 | server.py:125 | fit progress: (7, 2.103739023208618, {'accuracy': 0.5655, 'data_size': 10000}, 72.38996312601375)
INFO flwr 2024-04-06 22:00:00,582 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:00:00,582 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:00:08,504 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:00:09,766 | server.py:125 | fit progress: (8, 2.073953151702881, {'accuracy': 0.5841, 'data_size': 10000}, 81.57400366899674)
INFO flwr 2024-04-06 22:00:09,766 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:00:09,766 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:00:16,976 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:00:18,492 | server.py:125 | fit progress: (9, 2.0459368228912354, {'accuracy': 0.6185, 'data_size': 10000}, 90.30019091800204)
INFO flwr 2024-04-06 22:00:18,492 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:00:18,492 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:00:26,254 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:00:27,785 | server.py:125 | fit progress: (10, 2.0105395317077637, {'accuracy': 0.6446, 'data_size': 10000}, 99.59357480201288)
INFO flwr 2024-04-06 22:00:27,785 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:00:27,786 | server.py:153 | FL finished in 99.59393521700986
INFO flwr 2024-04-06 22:00:27,786 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:00:27,786 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:00:27,786 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:00:27,786 | app.py:229 | app_fit: losses_centralized [(0, 2.30568265914917), (1, 2.291869878768921), (2, 2.2780535221099854), (3, 2.251068115234375), (4, 2.210340738296509), (5, 2.1789584159851074), (6, 2.137817144393921), (7, 2.103739023208618), (8, 2.073953151702881), (9, 2.0459368228912354), (10, 2.0105395317077637)]
INFO flwr 2024-04-06 22:00:27,786 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0528), (1, 0.1765), (2, 0.3227), (3, 0.4072), (4, 0.36), (5, 0.4701), (6, 0.5175), (7, 0.5655), (8, 0.5841), (9, 0.6185), (10, 0.6446)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6446
wandb:     loss 2.01054
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_215826-ybmuquf6
wandb: Find logs at: ./wandb/offline-run-20240406_215826-ybmuquf6/logs
INFO flwr 2024-04-06 22:00:31,392 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:07:38,932 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1423918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1423918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:07:44,046	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:07:44,569	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:07:45,012	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_91afa229363d5511.zip' (10.44MiB) to Ray cluster...
2024-04-06 22:07:45,050	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_91afa229363d5511.zip'.
INFO flwr 2024-04-06 22:07:56,149 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 126094059316.0, 'object_store_memory': 58326025420.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 22:07:56,150 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:07:56,150 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:07:56,167 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:07:56,168 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:07:56,168 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:07:56,169 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:07:58,884 | server.py:94 | initial parameters (loss, other metrics): 2.3044581413269043, {'accuracy': 0.0625, 'data_size': 10000}
INFO flwr 2024-04-06 22:07:58,885 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:07:58,885 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1431635)[0m 2024-04-06 22:08:03.629654: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1431635)[0m 2024-04-06 22:08:03.694193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1431635)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1431635)[0m 2024-04-06 22:08:05.769187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1431635)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1431635)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1431645)[0m 2024-04-06 22:08:03.902546: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1431645)[0m 2024-04-06 22:08:03.996691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1431645)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1431645)[0m 2024-04-06 22:08:06.460699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:08:18,533 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:08:18,572 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:08:19,983 | server.py:125 | fit progress: (1, 2.276522159576416, {'accuracy': 0.217, 'data_size': 10000}, 21.09782404699945)
INFO flwr 2024-04-06 22:08:19,983 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:19,984 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:08:29,069 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:08:30,450 | server.py:125 | fit progress: (2, 2.236759662628174, {'accuracy': 0.3176, 'data_size': 10000}, 31.565119318984216)
INFO flwr 2024-04-06 22:08:30,451 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:30,451 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:08:38,660 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:08:40,110 | server.py:125 | fit progress: (3, 2.1978328227996826, {'accuracy': 0.5114, 'data_size': 10000}, 41.22545021001133)
INFO flwr 2024-04-06 22:08:40,111 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:40,111 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:08:48,115 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:08:49,615 | server.py:125 | fit progress: (4, 2.155566453933716, {'accuracy': 0.3588, 'data_size': 10000}, 50.72969323999132)
INFO flwr 2024-04-06 22:08:49,615 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:49,615 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:08:57,369 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:08:58,820 | server.py:125 | fit progress: (5, 2.1175711154937744, {'accuracy': 0.4542, 'data_size': 10000}, 59.93543854699237)
INFO flwr 2024-04-06 22:08:58,821 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:08:58,821 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:06,789 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:08,024 | server.py:125 | fit progress: (6, 2.0744454860687256, {'accuracy': 0.5432, 'data_size': 10000}, 69.1385779909906)
INFO flwr 2024-04-06 22:09:08,024 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:09:08,024 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:16,514 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:17,805 | server.py:125 | fit progress: (7, 2.039492607116699, {'accuracy': 0.6423, 'data_size': 10000}, 78.91950051000458)
INFO flwr 2024-04-06 22:09:17,805 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:09:17,805 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:25,249 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:26,552 | server.py:125 | fit progress: (8, 1.9998879432678223, {'accuracy': 0.6933, 'data_size': 10000}, 87.66735093499301)
INFO flwr 2024-04-06 22:09:26,553 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:09:26,553 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:34,856 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:36,531 | server.py:125 | fit progress: (9, 1.966369867324829, {'accuracy': 0.7055, 'data_size': 10000}, 97.64590418399894)
INFO flwr 2024-04-06 22:09:36,531 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:09:36,531 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:09:44,205 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:09:45,960 | server.py:125 | fit progress: (10, 1.9398019313812256, {'accuracy': 0.7319, 'data_size': 10000}, 107.07533708499977)
INFO flwr 2024-04-06 22:09:45,961 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:09:45,961 | server.py:153 | FL finished in 107.07588292800938
INFO flwr 2024-04-06 22:09:45,961 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:09:45,961 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:09:45,961 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:09:45,961 | app.py:229 | app_fit: losses_centralized [(0, 2.3044581413269043), (1, 2.276522159576416), (2, 2.236759662628174), (3, 2.1978328227996826), (4, 2.155566453933716), (5, 2.1175711154937744), (6, 2.0744454860687256), (7, 2.039492607116699), (8, 1.9998879432678223), (9, 1.966369867324829), (10, 1.9398019313812256)]
INFO flwr 2024-04-06 22:09:45,961 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0625), (1, 0.217), (2, 0.3176), (3, 0.5114), (4, 0.3588), (5, 0.4542), (6, 0.5432), (7, 0.6423), (8, 0.6933), (9, 0.7055), (10, 0.7319)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7319
wandb:     loss 1.9398
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_220738-9s3umuos
wandb: Find logs at: ./wandb/offline-run-20240406_220738-9s3umuos/logs
INFO flwr 2024-04-06 22:09:49,560 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:16:55,311 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1431645)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1431645)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:17:00,667	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:17:01,144	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:17:01,553	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6a41f8b79206a182.zip' (10.45MiB) to Ray cluster...
2024-04-06 22:17:01,585	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6a41f8b79206a182.zip'.
INFO flwr 2024-04-06 22:17:12,632 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 136037503181.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62587501363.0}
INFO flwr 2024-04-06 22:17:12,632 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:17:12,633 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:17:12,651 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:17:12,652 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:17:12,653 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:17:12,653 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:17:15,097 | server.py:94 | initial parameters (loss, other metrics): 2.3048858642578125, {'accuracy': 0.0696, 'data_size': 10000}
INFO flwr 2024-04-06 22:17:15,098 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:17:15,099 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1435999)[0m 2024-04-06 22:17:18.358523: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1435999)[0m 2024-04-06 22:17:18.456508: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1435999)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1435999)[0m 2024-04-06 22:17:20.697853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1435999)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1435999)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1435990)[0m 2024-04-06 22:17:18.912946: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1435990)[0m 2024-04-06 22:17:19.013607: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1435990)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1435997)[0m 2024-04-06 22:17:21.219874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:17:32,723 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:17:32,762 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:17:33,805 | server.py:125 | fit progress: (1, 2.280066728591919, {'accuracy': 0.135, 'data_size': 10000}, 18.70659536999301)
INFO flwr 2024-04-06 22:17:33,805 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:17:33,806 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:17:41,732 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:17:43,039 | server.py:125 | fit progress: (2, 2.2415144443511963, {'accuracy': 0.2408, 'data_size': 10000}, 27.940556807996472)
INFO flwr 2024-04-06 22:17:43,039 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:17:43,040 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:17:50,320 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:17:51,651 | server.py:125 | fit progress: (3, 2.2004411220550537, {'accuracy': 0.5071, 'data_size': 10000}, 36.55198777798796)
INFO flwr 2024-04-06 22:17:51,651 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:17:51,651 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:17:58,927 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:00,289 | server.py:125 | fit progress: (4, 2.1533586978912354, {'accuracy': 0.4748, 'data_size': 10000}, 45.190239082992775)
INFO flwr 2024-04-06 22:18:00,289 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:00,289 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:08,031 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:09,190 | server.py:125 | fit progress: (5, 2.1101677417755127, {'accuracy': 0.5713, 'data_size': 10000}, 54.091257049003616)
INFO flwr 2024-04-06 22:18:09,190 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:09,190 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:16,341 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:17,778 | server.py:125 | fit progress: (6, 2.0635013580322266, {'accuracy': 0.5619, 'data_size': 10000}, 62.679287724982714)
INFO flwr 2024-04-06 22:18:17,778 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:17,778 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:25,112 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:26,529 | server.py:125 | fit progress: (7, 2.02567195892334, {'accuracy': 0.6248, 'data_size': 10000}, 71.43079693199252)
INFO flwr 2024-04-06 22:18:26,530 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:26,530 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:34,264 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:35,781 | server.py:125 | fit progress: (8, 1.992811918258667, {'accuracy': 0.6415, 'data_size': 10000}, 80.68229482500465)
INFO flwr 2024-04-06 22:18:35,781 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:35,781 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:43,159 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:44,445 | server.py:125 | fit progress: (9, 1.959420084953308, {'accuracy': 0.6844, 'data_size': 10000}, 89.3463594649802)
INFO flwr 2024-04-06 22:18:44,445 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:18:44,445 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:18:52,123 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:18:53,655 | server.py:125 | fit progress: (10, 1.9271280765533447, {'accuracy': 0.7189, 'data_size': 10000}, 98.5567563509976)
INFO flwr 2024-04-06 22:18:53,656 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:18:53,656 | server.py:153 | FL finished in 98.55711813498056
INFO flwr 2024-04-06 22:18:53,656 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:18:53,656 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:18:53,656 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:18:53,656 | app.py:229 | app_fit: losses_centralized [(0, 2.3048858642578125), (1, 2.280066728591919), (2, 2.2415144443511963), (3, 2.2004411220550537), (4, 2.1533586978912354), (5, 2.1101677417755127), (6, 2.0635013580322266), (7, 2.02567195892334), (8, 1.992811918258667), (9, 1.959420084953308), (10, 1.9271280765533447)]
INFO flwr 2024-04-06 22:18:53,656 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0696), (1, 0.135), (2, 0.2408), (3, 0.5071), (4, 0.4748), (5, 0.5713), (6, 0.5619), (7, 0.6248), (8, 0.6415), (9, 0.6844), (10, 0.7189)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7189
wandb:     loss 1.92713
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_221654-aixpxdb7
wandb: Find logs at: ./wandb/offline-run-20240406_221654-aixpxdb7/logs
INFO flwr 2024-04-06 22:18:57,105 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:26:03,231 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1435990)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1435990)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:26:10,047	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:26:10,371	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:26:10,836	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c9d05b2f8ab2d596.zip' (10.47MiB) to Ray cluster...
2024-04-06 22:26:10,868	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c9d05b2f8ab2d596.zip'.
INFO flwr 2024-04-06 22:26:22,170 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 123052352922.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57022436966.0}
INFO flwr 2024-04-06 22:26:22,170 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:26:22,170 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:26:22,189 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:26:22,190 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:26:22,190 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:26:22,190 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:26:24,705 | server.py:94 | initial parameters (loss, other metrics): 2.3002538681030273, {'accuracy': 0.1356, 'data_size': 10000}
INFO flwr 2024-04-06 22:26:24,705 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:26:24,706 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1442915)[0m 2024-04-06 22:26:28.670455: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1442915)[0m 2024-04-06 22:26:28.788009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1442915)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1442915)[0m 2024-04-06 22:26:30.849730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1442915)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1442915)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1442909)[0m 2024-04-06 22:26:28.769998: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1442909)[0m 2024-04-06 22:26:28.861200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1442909)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1442917)[0m 2024-04-06 22:26:31.052743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:26:48,167 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:26:48,201 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:26:49,648 | server.py:125 | fit progress: (1, 2.259894609451294, {'accuracy': 0.3601, 'data_size': 10000}, 24.941877664008643)
INFO flwr 2024-04-06 22:26:49,648 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:26:49,648 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:26:58,150 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:26:59,321 | server.py:125 | fit progress: (2, 2.1993088722229004, {'accuracy': 0.3746, 'data_size': 10000}, 34.614925444009714)
INFO flwr 2024-04-06 22:26:59,321 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:26:59,321 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:07,591 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:08,909 | server.py:125 | fit progress: (3, 2.139906883239746, {'accuracy': 0.4352, 'data_size': 10000}, 44.203394201002084)
INFO flwr 2024-04-06 22:27:08,909 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:08,910 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:16,853 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:18,056 | server.py:125 | fit progress: (4, 2.0736684799194336, {'accuracy': 0.6272, 'data_size': 10000}, 53.350422254996374)
INFO flwr 2024-04-06 22:27:18,056 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:18,057 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:25,732 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:27,198 | server.py:125 | fit progress: (5, 2.0269789695739746, {'accuracy': 0.6543, 'data_size': 10000}, 62.492506103997584)
INFO flwr 2024-04-06 22:27:27,199 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:27,199 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:34,737 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:36,184 | server.py:125 | fit progress: (6, 1.9813740253448486, {'accuracy': 0.7417, 'data_size': 10000}, 71.4779240170028)
INFO flwr 2024-04-06 22:27:36,184 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:36,184 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:43,693 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:45,165 | server.py:125 | fit progress: (7, 1.9453791379928589, {'accuracy': 0.6864, 'data_size': 10000}, 80.45930437900824)
INFO flwr 2024-04-06 22:27:45,165 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:45,165 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:27:52,907 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:27:54,513 | server.py:125 | fit progress: (8, 1.9158117771148682, {'accuracy': 0.7315, 'data_size': 10000}, 89.80699112301227)
INFO flwr 2024-04-06 22:27:54,513 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:27:54,513 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:28:02,603 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:28:03,878 | server.py:125 | fit progress: (9, 1.8848949670791626, {'accuracy': 0.7553, 'data_size': 10000}, 99.17263067301246)
INFO flwr 2024-04-06 22:28:03,879 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:28:03,879 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:28:11,754 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:28:13,085 | server.py:125 | fit progress: (10, 1.8651282787322998, {'accuracy': 0.7573, 'data_size': 10000}, 108.37897660699673)
INFO flwr 2024-04-06 22:28:13,085 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:28:13,085 | server.py:153 | FL finished in 108.37938054199913
INFO flwr 2024-04-06 22:28:13,085 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:28:13,085 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:28:13,085 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:28:13,086 | app.py:229 | app_fit: losses_centralized [(0, 2.3002538681030273), (1, 2.259894609451294), (2, 2.1993088722229004), (3, 2.139906883239746), (4, 2.0736684799194336), (5, 2.0269789695739746), (6, 1.9813740253448486), (7, 1.9453791379928589), (8, 1.9158117771148682), (9, 1.8848949670791626), (10, 1.8651282787322998)]
INFO flwr 2024-04-06 22:28:13,086 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1356), (1, 0.3601), (2, 0.3746), (3, 0.4352), (4, 0.6272), (5, 0.6543), (6, 0.7417), (7, 0.6864), (8, 0.7315), (9, 0.7553), (10, 0.7573)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7573
wandb:     loss 1.86513
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_222602-71lluzvk
wandb: Find logs at: ./wandb/offline-run-20240406_222602-71lluzvk/logs
INFO flwr 2024-04-06 22:28:16,580 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:35:22,152 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1442914)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1442914)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:35:27,335	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:35:27,757	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:35:28,101	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e2e41e884c768f61.zip' (10.49MiB) to Ray cluster...
2024-04-06 22:35:28,126	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e2e41e884c768f61.zip'.
INFO flwr 2024-04-06 22:35:39,226 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62426291404.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 135661346612.0}
INFO flwr 2024-04-06 22:35:39,227 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:35:39,227 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:35:39,243 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:35:39,244 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:35:39,244 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:35:39,244 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:35:41,631 | server.py:94 | initial parameters (loss, other metrics): 2.3023762702941895, {'accuracy': 0.0925, 'data_size': 10000}
INFO flwr 2024-04-06 22:35:41,632 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:35:41,632 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1447728)[0m 2024-04-06 22:35:50.822059: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1447728)[0m 2024-04-06 22:35:50.920688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1447728)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1447729)[0m 2024-04-06 22:35:52.935365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1447727)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1447727)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1447724)[0m 2024-04-06 22:35:50.822977: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1447724)[0m 2024-04-06 22:35:50.931337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1447724)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1447723)[0m 2024-04-06 22:35:52.990275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:36:06,045 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:36:06,081 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:36:07,358 | server.py:125 | fit progress: (1, 2.3022241592407227, {'accuracy': 0.0934, 'data_size': 10000}, 25.726704036991578)
INFO flwr 2024-04-06 22:36:07,359 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:07,359 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:16,051 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:17,444 | server.py:125 | fit progress: (2, 2.3020823001861572, {'accuracy': 0.0946, 'data_size': 10000}, 35.811813237000024)
INFO flwr 2024-04-06 22:36:17,444 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:17,444 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:25,771 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:26,934 | server.py:125 | fit progress: (3, 2.301896810531616, {'accuracy': 0.0966, 'data_size': 10000}, 45.30254688198329)
INFO flwr 2024-04-06 22:36:26,935 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:26,935 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:34,919 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:36,093 | server.py:125 | fit progress: (4, 2.3017451763153076, {'accuracy': 0.098, 'data_size': 10000}, 54.46179426499293)
INFO flwr 2024-04-06 22:36:36,094 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:36,094 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:44,040 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:45,419 | server.py:125 | fit progress: (5, 2.301576852798462, {'accuracy': 0.0991, 'data_size': 10000}, 63.787618981994456)
INFO flwr 2024-04-06 22:36:45,420 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:45,420 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:36:53,570 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:36:55,182 | server.py:125 | fit progress: (6, 2.301414728164673, {'accuracy': 0.0999, 'data_size': 10000}, 73.5506651949836)
INFO flwr 2024-04-06 22:36:55,183 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:36:55,183 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:37:03,130 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:04,634 | server.py:125 | fit progress: (7, 2.3012495040893555, {'accuracy': 0.1016, 'data_size': 10000}, 83.0025020139874)
INFO flwr 2024-04-06 22:37:04,634 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:37:04,635 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:37:12,182 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:13,758 | server.py:125 | fit progress: (8, 2.3010778427124023, {'accuracy': 0.1034, 'data_size': 10000}, 92.12615175399696)
INFO flwr 2024-04-06 22:37:13,758 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:37:13,758 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:37:22,295 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:23,635 | server.py:125 | fit progress: (9, 2.3009488582611084, {'accuracy': 0.1046, 'data_size': 10000}, 102.00345540599665)
INFO flwr 2024-04-06 22:37:23,635 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:37:23,636 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:37:31,882 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:37:33,273 | server.py:125 | fit progress: (10, 2.3007383346557617, {'accuracy': 0.1089, 'data_size': 10000}, 111.64126278099138)
INFO flwr 2024-04-06 22:37:33,273 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:37:33,273 | server.py:153 | FL finished in 111.64165138098178
INFO flwr 2024-04-06 22:37:33,273 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:37:33,274 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:37:33,274 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:37:33,274 | app.py:229 | app_fit: losses_centralized [(0, 2.3023762702941895), (1, 2.3022241592407227), (2, 2.3020823001861572), (3, 2.301896810531616), (4, 2.3017451763153076), (5, 2.301576852798462), (6, 2.301414728164673), (7, 2.3012495040893555), (8, 2.3010778427124023), (9, 2.3009488582611084), (10, 2.3007383346557617)]
INFO flwr 2024-04-06 22:37:33,274 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0925), (1, 0.0934), (2, 0.0946), (3, 0.0966), (4, 0.098), (5, 0.0991), (6, 0.0999), (7, 0.1016), (8, 0.1034), (9, 0.1046), (10, 0.1089)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1089
wandb:     loss 2.30074
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_223521-uazv1ghm
wandb: Find logs at: ./wandb/offline-run-20240406_223521-uazv1ghm/logs
INFO flwr 2024-04-06 22:37:36,800 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:44:43,220 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1447724)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1447724)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:44:48,044	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:44:48,436	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:44:48,825	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_acdd955719032452.zip' (10.51MiB) to Ray cluster...
2024-04-06 22:44:48,851	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_acdd955719032452.zip'.
INFO flwr 2024-04-06 22:44:59,721 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 130419018343.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60179579289.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 22:44:59,722 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:44:59,722 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:44:59,741 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:44:59,743 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:44:59,743 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:44:59,743 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:45:02,075 | server.py:94 | initial parameters (loss, other metrics): 2.3067264556884766, {'accuracy': 0.0637, 'data_size': 10000}
INFO flwr 2024-04-06 22:45:02,076 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:45:02,076 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1455064)[0m 2024-04-06 22:45:05.270909: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1455063)[0m 2024-04-06 22:45:05.273654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1455063)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1455064)[0m 2024-04-06 22:45:07.632991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1455067)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1455067)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1455067)[0m 2024-04-06 22:45:06.457494: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1455067)[0m 2024-04-06 22:45:06.548888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1455067)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1455067)[0m 2024-04-06 22:45:08.550540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:45:19,538 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:45:19,577 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:45:20,899 | server.py:125 | fit progress: (1, 2.294459581375122, {'accuracy': 0.1786, 'data_size': 10000}, 18.823375042993575)
INFO flwr 2024-04-06 22:45:20,900 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:45:20,900 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:45:29,036 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:45:30,419 | server.py:125 | fit progress: (2, 2.282195806503296, {'accuracy': 0.2018, 'data_size': 10000}, 28.34319857499213)
INFO flwr 2024-04-06 22:45:30,419 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:45:30,420 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:45:37,960 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:45:39,102 | server.py:125 | fit progress: (3, 2.2625067234039307, {'accuracy': 0.2511, 'data_size': 10000}, 37.02605729701463)
INFO flwr 2024-04-06 22:45:39,102 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:45:39,102 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:45:46,755 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:45:48,156 | server.py:125 | fit progress: (4, 2.2486817836761475, {'accuracy': 0.2454, 'data_size': 10000}, 46.08049459499307)
INFO flwr 2024-04-06 22:45:48,157 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:45:48,157 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:45:55,835 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:45:57,249 | server.py:125 | fit progress: (5, 2.2263059616088867, {'accuracy': 0.4221, 'data_size': 10000}, 55.17330202300218)
INFO flwr 2024-04-06 22:45:57,250 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:45:57,250 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:46:04,403 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:46:05,861 | server.py:125 | fit progress: (6, 2.2034640312194824, {'accuracy': 0.4751, 'data_size': 10000}, 63.78505093199783)
INFO flwr 2024-04-06 22:46:05,861 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:46:05,862 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:46:13,678 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:46:14,970 | server.py:125 | fit progress: (7, 2.180846691131592, {'accuracy': 0.5343, 'data_size': 10000}, 72.89445237998734)
INFO flwr 2024-04-06 22:46:14,971 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:46:14,971 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:46:22,722 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:46:24,249 | server.py:125 | fit progress: (8, 2.1592905521392822, {'accuracy': 0.5913, 'data_size': 10000}, 82.17348063300597)
INFO flwr 2024-04-06 22:46:24,250 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:46:24,250 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:46:31,968 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:46:33,308 | server.py:125 | fit progress: (9, 2.133223295211792, {'accuracy': 0.5704, 'data_size': 10000}, 91.23187040499761)
INFO flwr 2024-04-06 22:46:33,308 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:46:33,308 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:46:41,230 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:46:42,559 | server.py:125 | fit progress: (10, 2.1052186489105225, {'accuracy': 0.5578, 'data_size': 10000}, 100.48317453599884)
INFO flwr 2024-04-06 22:46:42,559 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:46:42,559 | server.py:153 | FL finished in 100.48352340201382
INFO flwr 2024-04-06 22:46:42,560 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:46:42,560 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:46:42,560 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:46:42,560 | app.py:229 | app_fit: losses_centralized [(0, 2.3067264556884766), (1, 2.294459581375122), (2, 2.282195806503296), (3, 2.2625067234039307), (4, 2.2486817836761475), (5, 2.2263059616088867), (6, 2.2034640312194824), (7, 2.180846691131592), (8, 2.1592905521392822), (9, 2.133223295211792), (10, 2.1052186489105225)]
INFO flwr 2024-04-06 22:46:42,560 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0637), (1, 0.1786), (2, 0.2018), (3, 0.2511), (4, 0.2454), (5, 0.4221), (6, 0.4751), (7, 0.5343), (8, 0.5913), (9, 0.5704), (10, 0.5578)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5578
wandb:     loss 2.10522
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_224442-f17vqtl5
wandb: Find logs at: ./wandb/offline-run-20240406_224442-f17vqtl5/logs
INFO flwr 2024-04-06 22:46:46,086 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 22:53:52,988 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1455063)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1455063)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 22:54:07,222	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 22:54:08,037	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 22:54:08,398	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_df717d87de2090ab.zip' (10.53MiB) to Ray cluster...
2024-04-06 22:54:08,424	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_df717d87de2090ab.zip'.
INFO flwr 2024-04-06 22:54:19,579 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 60487879065.0, 'node:10.20.240.18': 1.0, 'memory': 131138384487.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 22:54:19,579 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 22:54:19,579 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 22:54:19,595 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 22:54:19,596 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 22:54:19,597 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 22:54:19,597 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 22:54:22,845 | server.py:94 | initial parameters (loss, other metrics): 2.3047380447387695, {'accuracy': 0.0849, 'data_size': 10000}
INFO flwr 2024-04-06 22:54:22,845 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 22:54:22,845 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1462406)[0m 2024-04-06 22:54:28.676876: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1462405)[0m 2024-04-06 22:54:28.813417: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1462405)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1462407)[0m 2024-04-06 22:54:35.770683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1462400)[0m 2024-04-06 22:54:28.729224: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1462402)[0m 2024-04-06 22:54:28.852779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1462402)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1462407)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1462407)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1462400)[0m 2024-04-06 22:54:35.770681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 22:55:00,769 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 22:55:00,804 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 22:55:02,099 | server.py:125 | fit progress: (1, 2.281757354736328, {'accuracy': 0.279, 'data_size': 10000}, 39.25387570401654)
INFO flwr 2024-04-06 22:55:02,099 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:02,099 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:10,596 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:12,006 | server.py:125 | fit progress: (2, 2.248666524887085, {'accuracy': 0.3862, 'data_size': 10000}, 49.16118787700543)
INFO flwr 2024-04-06 22:55:12,007 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:12,007 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:20,567 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:21,915 | server.py:125 | fit progress: (3, 2.200674295425415, {'accuracy': 0.3252, 'data_size': 10000}, 59.07018931899802)
INFO flwr 2024-04-06 22:55:21,916 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:21,916 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:29,814 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:31,261 | server.py:125 | fit progress: (4, 2.160187005996704, {'accuracy': 0.3822, 'data_size': 10000}, 68.41606381401652)
INFO flwr 2024-04-06 22:55:31,261 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:31,262 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:39,133 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:40,310 | server.py:125 | fit progress: (5, 2.134366750717163, {'accuracy': 0.5101, 'data_size': 10000}, 77.4650782700046)
INFO flwr 2024-04-06 22:55:40,310 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:40,311 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:47,781 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:49,263 | server.py:125 | fit progress: (6, 2.079735040664673, {'accuracy': 0.6152, 'data_size': 10000}, 86.41816504500457)
INFO flwr 2024-04-06 22:55:49,263 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:49,264 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:55:56,578 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 22:55:57,903 | server.py:125 | fit progress: (7, 2.0391995906829834, {'accuracy': 0.7251, 'data_size': 10000}, 95.05792652702075)
INFO flwr 2024-04-06 22:55:57,903 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 22:55:57,904 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:56:06,366 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 22:56:07,653 | server.py:125 | fit progress: (8, 1.9966273307800293, {'accuracy': 0.7237, 'data_size': 10000}, 104.80824504399789)
INFO flwr 2024-04-06 22:56:07,654 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 22:56:07,654 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:56:15,297 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 22:56:16,816 | server.py:125 | fit progress: (9, 1.9696803092956543, {'accuracy': 0.7103, 'data_size': 10000}, 113.97044304001611)
INFO flwr 2024-04-06 22:56:16,816 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 22:56:16,816 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 22:56:24,847 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 22:56:26,441 | server.py:125 | fit progress: (10, 1.9411468505859375, {'accuracy': 0.7566, 'data_size': 10000}, 123.59628526499728)
INFO flwr 2024-04-06 22:56:26,442 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 22:56:26,442 | server.py:153 | FL finished in 123.59668871900067
INFO flwr 2024-04-06 22:56:26,442 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 22:56:26,442 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 22:56:26,442 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 22:56:26,442 | app.py:229 | app_fit: losses_centralized [(0, 2.3047380447387695), (1, 2.281757354736328), (2, 2.248666524887085), (3, 2.200674295425415), (4, 2.160187005996704), (5, 2.134366750717163), (6, 2.079735040664673), (7, 2.0391995906829834), (8, 1.9966273307800293), (9, 1.9696803092956543), (10, 1.9411468505859375)]
INFO flwr 2024-04-06 22:56:26,442 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0849), (1, 0.279), (2, 0.3862), (3, 0.3252), (4, 0.3822), (5, 0.5101), (6, 0.6152), (7, 0.7251), (8, 0.7237), (9, 0.7103), (10, 0.7566)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7566
wandb:     loss 1.94115
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_225352-5u21lmfh
wandb: Find logs at: ./wandb/offline-run-20240406_225352-5u21lmfh/logs
INFO flwr 2024-04-06 22:56:29,964 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:03:36,406 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1462400)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1462400)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:03:45,661	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:03:46,237	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:03:46,600	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7d0a2cb58961edce.zip' (10.55MiB) to Ray cluster...
2024-04-06 23:03:46,623	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7d0a2cb58961edce.zip'.
INFO flwr 2024-04-06 23:03:57,525 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 137831521690.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 63356366438.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-06 23:03:57,526 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:03:57,526 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:03:57,543 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:03:57,545 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:03:57,545 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:03:57,545 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:04:00,009 | server.py:94 | initial parameters (loss, other metrics): 2.3052616119384766, {'accuracy': 0.0876, 'data_size': 10000}
INFO flwr 2024-04-06 23:04:00,009 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:04:00,010 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1467334)[0m 2024-04-06 23:04:03.893847: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1467334)[0m 2024-04-06 23:04:04.012540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1467334)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1467336)[0m 2024-04-06 23:04:06.588216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1467337)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1467337)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1467330)[0m 2024-04-06 23:04:03.992152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1467330)[0m 2024-04-06 23:04:04.093561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1467330)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1467337)[0m 2024-04-06 23:04:06.799395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:04:21,388 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:04:21,420 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:04:22,484 | server.py:125 | fit progress: (1, 2.2630298137664795, {'accuracy': 0.4294, 'data_size': 10000}, 22.47393718798412)
INFO flwr 2024-04-06 23:04:22,484 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:04:22,484 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:04:30,762 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:04:32,053 | server.py:125 | fit progress: (2, 2.1960809230804443, {'accuracy': 0.5101, 'data_size': 10000}, 32.04359529199428)
INFO flwr 2024-04-06 23:04:32,054 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:04:32,054 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:04:39,874 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:04:41,208 | server.py:125 | fit progress: (3, 2.1335463523864746, {'accuracy': 0.5783, 'data_size': 10000}, 41.198075344989775)
INFO flwr 2024-04-06 23:04:41,208 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:04:41,208 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:04:48,674 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:04:50,066 | server.py:125 | fit progress: (4, 2.0720574855804443, {'accuracy': 0.5911, 'data_size': 10000}, 50.0564091319975)
INFO flwr 2024-04-06 23:04:50,066 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:04:50,067 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:04:57,516 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:04:58,707 | server.py:125 | fit progress: (5, 2.01924204826355, {'accuracy': 0.6461, 'data_size': 10000}, 58.69743597798515)
INFO flwr 2024-04-06 23:04:58,707 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:04:58,708 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:05:05,755 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:05:07,200 | server.py:125 | fit progress: (6, 1.9763743877410889, {'accuracy': 0.6798, 'data_size': 10000}, 67.1898505249992)
INFO flwr 2024-04-06 23:05:07,200 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:05:07,200 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:05:15,110 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:05:16,363 | server.py:125 | fit progress: (7, 1.942802906036377, {'accuracy': 0.7284, 'data_size': 10000}, 76.3535619849863)
INFO flwr 2024-04-06 23:05:16,364 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:05:16,364 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:05:24,142 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:05:25,376 | server.py:125 | fit progress: (8, 1.913782000541687, {'accuracy': 0.7193, 'data_size': 10000}, 85.36646292899968)
INFO flwr 2024-04-06 23:05:25,376 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:05:25,377 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:05:32,980 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:05:34,491 | server.py:125 | fit progress: (9, 1.8867406845092773, {'accuracy': 0.7907, 'data_size': 10000}, 94.48116377097904)
INFO flwr 2024-04-06 23:05:34,491 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:05:34,491 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:05:41,851 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:05:43,396 | server.py:125 | fit progress: (10, 1.8677023649215698, {'accuracy': 0.7613, 'data_size': 10000}, 103.38605202100007)
INFO flwr 2024-04-06 23:05:43,396 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:05:43,396 | server.py:153 | FL finished in 103.38646355399396
INFO flwr 2024-04-06 23:05:43,396 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:05:43,396 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:05:43,397 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:05:43,397 | app.py:229 | app_fit: losses_centralized [(0, 2.3052616119384766), (1, 2.2630298137664795), (2, 2.1960809230804443), (3, 2.1335463523864746), (4, 2.0720574855804443), (5, 2.01924204826355), (6, 1.9763743877410889), (7, 1.942802906036377), (8, 1.913782000541687), (9, 1.8867406845092773), (10, 1.8677023649215698)]
INFO flwr 2024-04-06 23:05:43,397 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0876), (1, 0.4294), (2, 0.5101), (3, 0.5783), (4, 0.5911), (5, 0.6461), (6, 0.6798), (7, 0.7284), (8, 0.7193), (9, 0.7907), (10, 0.7613)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7613
wandb:     loss 1.8677
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_230335-hxpl3z2s
wandb: Find logs at: ./wandb/offline-run-20240406_230335-hxpl3z2s/logs
INFO flwr 2024-04-06 23:05:46,836 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:12:54,807 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1467330)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1467330)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:13:01,088	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:13:01,420	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:13:01,753	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4b86532d79877120.zip' (10.57MiB) to Ray cluster...
2024-04-06 23:13:01,787	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4b86532d79877120.zip'.
INFO flwr 2024-04-06 23:13:12,861 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 125323837645.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57995930419.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 23:13:12,862 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:13:12,862 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:13:12,875 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:13:12,876 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:13:12,877 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:13:12,877 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:13:15,623 | server.py:94 | initial parameters (loss, other metrics): 2.3024184703826904, {'accuracy': 0.0784, 'data_size': 10000}
INFO flwr 2024-04-06 23:13:15,624 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:13:15,624 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1474806)[0m 2024-04-06 23:13:18.803831: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1474806)[0m 2024-04-06 23:13:18.899942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1474806)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1474806)[0m 2024-04-06 23:13:21.129241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1474896)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1474896)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1474805)[0m 2024-04-06 23:13:19.370523: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1474805)[0m 2024-04-06 23:13:19.464904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1474805)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1474805)[0m 2024-04-06 23:13:21.816310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:13:36,488 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:13:36,803 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:13:37,864 | server.py:125 | fit progress: (1, 2.2516207695007324, {'accuracy': 0.3375, 'data_size': 10000}, 22.240012680995278)
INFO flwr 2024-04-06 23:13:37,865 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:13:37,865 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:13:46,156 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:13:47,523 | server.py:125 | fit progress: (2, 2.1674110889434814, {'accuracy': 0.4613, 'data_size': 10000}, 31.89867704900098)
INFO flwr 2024-04-06 23:13:47,523 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:13:47,523 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:13:55,546 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:13:56,909 | server.py:125 | fit progress: (3, 2.099890947341919, {'accuracy': 0.6115, 'data_size': 10000}, 41.28516708800453)
INFO flwr 2024-04-06 23:13:56,909 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:13:56,910 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:04,242 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:05,646 | server.py:125 | fit progress: (4, 2.022348642349243, {'accuracy': 0.644, 'data_size': 10000}, 50.02153711699066)
INFO flwr 2024-04-06 23:14:05,646 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:05,646 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:13,603 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:14,823 | server.py:125 | fit progress: (5, 1.9723527431488037, {'accuracy': 0.6784, 'data_size': 10000}, 59.199003828980494)
INFO flwr 2024-04-06 23:14:14,823 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:14,823 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:22,629 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:24,089 | server.py:125 | fit progress: (6, 1.9386961460113525, {'accuracy': 0.6771, 'data_size': 10000}, 68.46487557599903)
INFO flwr 2024-04-06 23:14:24,089 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:24,089 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:32,426 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:33,878 | server.py:125 | fit progress: (7, 1.910913109779358, {'accuracy': 0.7188, 'data_size': 10000}, 78.25418432999868)
INFO flwr 2024-04-06 23:14:33,878 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:33,879 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:42,820 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:44,173 | server.py:125 | fit progress: (8, 1.8746557235717773, {'accuracy': 0.7686, 'data_size': 10000}, 88.54888536297949)
INFO flwr 2024-04-06 23:14:44,173 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:44,173 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:14:52,641 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:14:54,206 | server.py:125 | fit progress: (9, 1.8486865758895874, {'accuracy': 0.8088, 'data_size': 10000}, 98.58157841299544)
INFO flwr 2024-04-06 23:14:54,206 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:14:54,206 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:15:02,329 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:15:03,903 | server.py:125 | fit progress: (10, 1.8222191333770752, {'accuracy': 0.8046, 'data_size': 10000}, 108.278788392985)
INFO flwr 2024-04-06 23:15:03,903 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:15:03,903 | server.py:153 | FL finished in 108.27917451699614
INFO flwr 2024-04-06 23:15:03,903 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:15:03,903 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:15:03,903 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:15:03,904 | app.py:229 | app_fit: losses_centralized [(0, 2.3024184703826904), (1, 2.2516207695007324), (2, 2.1674110889434814), (3, 2.099890947341919), (4, 2.022348642349243), (5, 1.9723527431488037), (6, 1.9386961460113525), (7, 1.910913109779358), (8, 1.8746557235717773), (9, 1.8486865758895874), (10, 1.8222191333770752)]
INFO flwr 2024-04-06 23:15:03,904 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0784), (1, 0.3375), (2, 0.4613), (3, 0.6115), (4, 0.644), (5, 0.6784), (6, 0.6771), (7, 0.7188), (8, 0.7686), (9, 0.8088), (10, 0.8046)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8046
wandb:     loss 1.82222
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_231254-4rfio1ap
wandb: Find logs at: ./wandb/offline-run-20240406_231254-4rfio1ap/logs
INFO flwr 2024-04-06 23:15:07,440 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:22:13,931 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1474805)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1474805)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:22:19,873	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:22:20,241	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:22:20,681	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9e8456a8b4d0bb8b.zip' (10.58MiB) to Ray cluster...
2024-04-06 23:22:20,718	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9e8456a8b4d0bb8b.zip'.
INFO flwr 2024-04-06 23:22:32,006 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 136071445095.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62602047897.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-06 23:22:32,006 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:22:32,006 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:22:32,028 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:22:32,029 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:22:32,029 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:22:32,029 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:22:34,966 | server.py:94 | initial parameters (loss, other metrics): 2.3030407428741455, {'accuracy': 0.1123, 'data_size': 10000}
INFO flwr 2024-04-06 23:22:34,966 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:22:34,966 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1479176)[0m 2024-04-06 23:22:37.976760: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1479176)[0m 2024-04-06 23:22:38.076553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1479176)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1479176)[0m 2024-04-06 23:22:40.249758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1479179)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1479179)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1479181)[0m 2024-04-06 23:22:38.543765: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1479181)[0m 2024-04-06 23:22:38.644023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1479181)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1479181)[0m 2024-04-06 23:22:40.462209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:22:51,786 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:22:51,816 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:22:53,135 | server.py:125 | fit progress: (1, 2.2484774589538574, {'accuracy': 0.4926, 'data_size': 10000}, 18.168878916010726)
INFO flwr 2024-04-06 23:22:53,136 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:22:53,136 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:01,496 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:02,851 | server.py:125 | fit progress: (2, 2.1615254878997803, {'accuracy': 0.4732, 'data_size': 10000}, 27.88531216100091)
INFO flwr 2024-04-06 23:23:02,852 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:02,852 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:10,179 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:11,593 | server.py:125 | fit progress: (3, 2.0603246688842773, {'accuracy': 0.5595, 'data_size': 10000}, 36.62642446701648)
INFO flwr 2024-04-06 23:23:11,593 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:11,593 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:18,845 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:20,271 | server.py:125 | fit progress: (4, 2.0055603981018066, {'accuracy': 0.5693, 'data_size': 10000}, 45.30512433001422)
INFO flwr 2024-04-06 23:23:20,272 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:20,272 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:28,246 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:29,762 | server.py:125 | fit progress: (5, 1.9511975049972534, {'accuracy': 0.6382, 'data_size': 10000}, 54.79561270301929)
INFO flwr 2024-04-06 23:23:29,762 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:29,762 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:37,985 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:39,284 | server.py:125 | fit progress: (6, 1.9109207391738892, {'accuracy': 0.6777, 'data_size': 10000}, 64.3179620130104)
INFO flwr 2024-04-06 23:23:39,284 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:39,285 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:47,579 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:48,883 | server.py:125 | fit progress: (7, 1.8754996061325073, {'accuracy': 0.739, 'data_size': 10000}, 73.91707635100465)
INFO flwr 2024-04-06 23:23:48,884 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:48,884 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:23:57,112 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:23:58,431 | server.py:125 | fit progress: (8, 1.8487882614135742, {'accuracy': 0.7684, 'data_size': 10000}, 83.46461645900854)
INFO flwr 2024-04-06 23:23:58,431 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:23:58,431 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:24:06,356 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:24:07,937 | server.py:125 | fit progress: (9, 1.8237847089767456, {'accuracy': 0.7659, 'data_size': 10000}, 92.97076786600519)
INFO flwr 2024-04-06 23:24:07,937 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:24:07,937 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:24:15,587 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:24:17,391 | server.py:125 | fit progress: (10, 1.7996336221694946, {'accuracy': 0.809, 'data_size': 10000}, 102.42500703901169)
INFO flwr 2024-04-06 23:24:17,391 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:24:17,392 | server.py:153 | FL finished in 102.42545376700582
INFO flwr 2024-04-06 23:24:17,392 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:24:17,392 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:24:17,392 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:24:17,392 | app.py:229 | app_fit: losses_centralized [(0, 2.3030407428741455), (1, 2.2484774589538574), (2, 2.1615254878997803), (3, 2.0603246688842773), (4, 2.0055603981018066), (5, 1.9511975049972534), (6, 1.9109207391738892), (7, 1.8754996061325073), (8, 1.8487882614135742), (9, 1.8237847089767456), (10, 1.7996336221694946)]
INFO flwr 2024-04-06 23:24:17,392 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1123), (1, 0.4926), (2, 0.4732), (3, 0.5595), (4, 0.5693), (5, 0.6382), (6, 0.6777), (7, 0.739), (8, 0.7684), (9, 0.7659), (10, 0.809)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.809
wandb:     loss 1.79963
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_232213-bc0l4dcz
wandb: Find logs at: ./wandb/offline-run-20240406_232213-bc0l4dcz/logs
INFO flwr 2024-04-06 23:24:20,908 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:31:26,279 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1479178)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1479178)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:31:32,049	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:31:32,395	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:31:32,759	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0f4758f8c29f6f80.zip' (10.61MiB) to Ray cluster...
2024-04-06 23:31:32,784	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0f4758f8c29f6f80.zip'.
INFO flwr 2024-04-06 23:31:43,774 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 130450606285.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 60193116979.0}
INFO flwr 2024-04-06 23:31:43,774 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:31:43,775 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:31:43,792 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:31:43,793 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:31:43,793 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:31:43,793 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:31:46,020 | server.py:94 | initial parameters (loss, other metrics): 2.305694818496704, {'accuracy': 0.1165, 'data_size': 10000}
INFO flwr 2024-04-06 23:31:46,020 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:31:46,021 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1486463)[0m 2024-04-06 23:31:49.880241: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1486463)[0m 2024-04-06 23:31:49.977955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1486463)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1486463)[0m 2024-04-06 23:31:51.958578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1486463)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1486463)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1486466)[0m 2024-04-06 23:31:50.183774: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1486466)[0m 2024-04-06 23:31:50.281437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1486466)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1486466)[0m 2024-04-06 23:31:52.452440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:32:04,162 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:32:04,199 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:32:05,259 | server.py:125 | fit progress: (1, 2.2349374294281006, {'accuracy': 0.4799, 'data_size': 10000}, 19.238669034995837)
INFO flwr 2024-04-06 23:32:05,260 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:05,260 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:13,529 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:32:14,872 | server.py:125 | fit progress: (2, 2.1422572135925293, {'accuracy': 0.6709, 'data_size': 10000}, 28.851804071018705)
INFO flwr 2024-04-06 23:32:14,873 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:14,873 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:22,733 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:32:24,113 | server.py:125 | fit progress: (3, 2.0409903526306152, {'accuracy': 0.6712, 'data_size': 10000}, 38.09244423499331)
INFO flwr 2024-04-06 23:32:24,113 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:24,113 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:31,506 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:32:32,970 | server.py:125 | fit progress: (4, 1.955100178718567, {'accuracy': 0.6549, 'data_size': 10000}, 46.94950928201433)
INFO flwr 2024-04-06 23:32:32,970 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:32,971 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:41,097 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:32:42,294 | server.py:125 | fit progress: (5, 1.8942354917526245, {'accuracy': 0.7196, 'data_size': 10000}, 56.27347737600212)
INFO flwr 2024-04-06 23:32:42,294 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:42,295 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:49,702 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:32:51,133 | server.py:125 | fit progress: (6, 1.860688328742981, {'accuracy': 0.7527, 'data_size': 10000}, 65.11201362000429)
INFO flwr 2024-04-06 23:32:51,133 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:32:51,133 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:32:59,211 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:33:00,681 | server.py:125 | fit progress: (7, 1.8140336275100708, {'accuracy': 0.8164, 'data_size': 10000}, 74.66063801400014)
INFO flwr 2024-04-06 23:33:00,681 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:33:00,682 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:33:08,809 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:33:10,351 | server.py:125 | fit progress: (8, 1.7871111631393433, {'accuracy': 0.815, 'data_size': 10000}, 84.32991899500485)
INFO flwr 2024-04-06 23:33:10,351 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:33:10,351 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:33:18,172 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:33:19,466 | server.py:125 | fit progress: (9, 1.7730076313018799, {'accuracy': 0.8324, 'data_size': 10000}, 93.44499649800127)
INFO flwr 2024-04-06 23:33:19,466 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:33:19,466 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:33:27,358 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:33:28,955 | server.py:125 | fit progress: (10, 1.7576831579208374, {'accuracy': 0.8271, 'data_size': 10000}, 102.93419593400904)
INFO flwr 2024-04-06 23:33:28,955 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:33:28,955 | server.py:153 | FL finished in 102.93455969399656
INFO flwr 2024-04-06 23:33:28,955 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:33:28,955 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:33:28,956 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:33:28,956 | app.py:229 | app_fit: losses_centralized [(0, 2.305694818496704), (1, 2.2349374294281006), (2, 2.1422572135925293), (3, 2.0409903526306152), (4, 1.955100178718567), (5, 1.8942354917526245), (6, 1.860688328742981), (7, 1.8140336275100708), (8, 1.7871111631393433), (9, 1.7730076313018799), (10, 1.7576831579208374)]
INFO flwr 2024-04-06 23:33:28,956 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1165), (1, 0.4799), (2, 0.6709), (3, 0.6712), (4, 0.6549), (5, 0.7196), (6, 0.7527), (7, 0.8164), (8, 0.815), (9, 0.8324), (10, 0.8271)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8271
wandb:     loss 1.75768
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_233125-nqnn4dtb
wandb: Find logs at: ./wandb/offline-run-20240406_233125-nqnn4dtb/logs
INFO flwr 2024-04-06 23:33:32,544 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:40:38,880 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1486456)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1486456)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:40:43,834	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:40:44,274	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:40:44,700	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_516d070798808484.zip' (10.63MiB) to Ray cluster...
2024-04-06 23:40:44,726	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_516d070798808484.zip'.
INFO flwr 2024-04-06 23:40:55,818 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 129939278439.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 59973976473.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-06 23:40:55,819 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:40:55,819 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:40:55,838 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:40:55,839 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:40:55,839 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:40:55,840 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:40:58,464 | server.py:94 | initial parameters (loss, other metrics): 2.2981507778167725, {'accuracy': 0.1221, 'data_size': 10000}
INFO flwr 2024-04-06 23:40:58,465 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:40:58,465 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1493819)[0m 2024-04-06 23:41:01.945808: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1493819)[0m 2024-04-06 23:41:02.043088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1493819)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1493826)[0m 2024-04-06 23:41:04.120855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1493826)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1493826)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1493825)[0m 2024-04-06 23:41:02.388360: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1493825)[0m 2024-04-06 23:41:02.478369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1493825)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1493825)[0m 2024-04-06 23:41:04.771191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:41:16,240 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:41:16,278 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:41:17,684 | server.py:125 | fit progress: (1, 2.297757148742676, {'accuracy': 0.1267, 'data_size': 10000}, 19.218699702003505)
INFO flwr 2024-04-06 23:41:17,684 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:17,684 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:41:26,873 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:41:28,008 | server.py:125 | fit progress: (2, 2.2974607944488525, {'accuracy': 0.1294, 'data_size': 10000}, 29.543285340012517)
INFO flwr 2024-04-06 23:41:28,008 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:28,009 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:41:36,168 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:41:37,341 | server.py:125 | fit progress: (3, 2.2970685958862305, {'accuracy': 0.1307, 'data_size': 10000}, 38.87574010298704)
INFO flwr 2024-04-06 23:41:37,341 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:37,341 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:41:45,445 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:41:46,598 | server.py:125 | fit progress: (4, 2.2966184616088867, {'accuracy': 0.1316, 'data_size': 10000}, 48.13345902500441)
INFO flwr 2024-04-06 23:41:46,599 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:46,599 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:41:54,445 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:41:55,876 | server.py:125 | fit progress: (5, 2.296219825744629, {'accuracy': 0.1336, 'data_size': 10000}, 57.41146754700458)
INFO flwr 2024-04-06 23:41:55,877 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:41:55,877 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:03,599 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:05,175 | server.py:125 | fit progress: (6, 2.2957966327667236, {'accuracy': 0.1318, 'data_size': 10000}, 66.70991840198985)
INFO flwr 2024-04-06 23:42:05,175 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:42:05,175 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:13,534 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:15,057 | server.py:125 | fit progress: (7, 2.2954506874084473, {'accuracy': 0.1333, 'data_size': 10000}, 76.5920126879937)
INFO flwr 2024-04-06 23:42:15,057 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:42:15,057 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:22,838 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:24,566 | server.py:125 | fit progress: (8, 2.2950470447540283, {'accuracy': 0.1269, 'data_size': 10000}, 86.10103578399867)
INFO flwr 2024-04-06 23:42:24,566 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:42:24,566 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:32,828 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:34,216 | server.py:125 | fit progress: (9, 2.29465913772583, {'accuracy': 0.1271, 'data_size': 10000}, 95.75144694501068)
INFO flwr 2024-04-06 23:42:34,217 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:42:34,217 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:42:42,611 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:42:43,997 | server.py:125 | fit progress: (10, 2.2943215370178223, {'accuracy': 0.1294, 'data_size': 10000}, 105.53208823601017)
INFO flwr 2024-04-06 23:42:43,997 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:42:43,997 | server.py:153 | FL finished in 105.53251327498583
INFO flwr 2024-04-06 23:42:43,997 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:42:43,998 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:42:43,998 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:42:43,998 | app.py:229 | app_fit: losses_centralized [(0, 2.2981507778167725), (1, 2.297757148742676), (2, 2.2974607944488525), (3, 2.2970685958862305), (4, 2.2966184616088867), (5, 2.296219825744629), (6, 2.2957966327667236), (7, 2.2954506874084473), (8, 2.2950470447540283), (9, 2.29465913772583), (10, 2.2943215370178223)]
INFO flwr 2024-04-06 23:42:43,998 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1221), (1, 0.1267), (2, 0.1294), (3, 0.1307), (4, 0.1316), (5, 0.1336), (6, 0.1318), (7, 0.1333), (8, 0.1269), (9, 0.1271), (10, 0.1294)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1294
wandb:     loss 2.29432
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_234038-hwf7ncau
wandb: Find logs at: ./wandb/offline-run-20240406_234038-hwf7ncau/logs
INFO flwr 2024-04-06 23:42:47,616 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:49:52,997 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1493815)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1493815)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:49:57,716	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:49:58,143	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:49:58,579	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7cbb4f638b24342d.zip' (10.64MiB) to Ray cluster...
2024-04-06 23:49:58,604	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7cbb4f638b24342d.zip'.
INFO flwr 2024-04-06 23:50:09,447 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 134841697280.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62075013120.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-06 23:50:09,448 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:50:09,448 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:50:09,468 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:50:09,469 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:50:09,470 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:50:09,470 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:50:12,430 | server.py:94 | initial parameters (loss, other metrics): 2.3058156967163086, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-06 23:50:12,433 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:50:12,439 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1498363)[0m 2024-04-06 23:50:15.500813: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1498366)[0m 2024-04-06 23:50:15.670727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1498366)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1498366)[0m 2024-04-06 23:50:17.646924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1498363)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1498363)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1498365)[0m 2024-04-06 23:50:15.883653: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1498365)[0m 2024-04-06 23:50:15.976373: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1498365)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1498365)[0m 2024-04-06 23:50:18.230639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:50:29,943 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:50:29,974 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:50:31,215 | server.py:125 | fit progress: (1, 2.284912109375, {'accuracy': 0.2318, 'data_size': 10000}, 18.777548371028388)
INFO flwr 2024-04-06 23:50:31,215 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:50:31,216 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:50:39,363 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:50:40,695 | server.py:125 | fit progress: (2, 2.251913547515869, {'accuracy': 0.2791, 'data_size': 10000}, 28.257391998020466)
INFO flwr 2024-04-06 23:50:40,695 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:50:40,695 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:50:48,330 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-06 23:50:49,425 | server.py:125 | fit progress: (3, 2.224602222442627, {'accuracy': 0.3358, 'data_size': 10000}, 36.987330439005746)
INFO flwr 2024-04-06 23:50:49,425 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-06 23:50:49,425 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:50:57,398 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-06 23:50:58,522 | server.py:125 | fit progress: (4, 2.1805920600891113, {'accuracy': 0.3974, 'data_size': 10000}, 46.0846787400078)
INFO flwr 2024-04-06 23:50:58,522 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-06 23:50:58,523 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:06,240 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:07,688 | server.py:125 | fit progress: (5, 2.1418983936309814, {'accuracy': 0.4532, 'data_size': 10000}, 55.25022706302116)
INFO flwr 2024-04-06 23:51:07,688 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-06 23:51:07,688 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:15,010 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:16,427 | server.py:125 | fit progress: (6, 2.095099449157715, {'accuracy': 0.5263, 'data_size': 10000}, 63.989907948009204)
INFO flwr 2024-04-06 23:51:16,428 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-06 23:51:16,428 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:24,077 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:25,528 | server.py:125 | fit progress: (7, 2.0616233348846436, {'accuracy': 0.5601, 'data_size': 10000}, 73.09038499902817)
INFO flwr 2024-04-06 23:51:25,528 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-06 23:51:25,528 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:33,397 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:34,875 | server.py:125 | fit progress: (8, 2.024460792541504, {'accuracy': 0.5726, 'data_size': 10000}, 82.43751425101073)
INFO flwr 2024-04-06 23:51:34,875 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-06 23:51:34,876 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:42,566 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:43,874 | server.py:125 | fit progress: (9, 2.005359172821045, {'accuracy': 0.5696, 'data_size': 10000}, 91.43606660200749)
INFO flwr 2024-04-06 23:51:43,874 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-06 23:51:43,874 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:51:51,622 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-06 23:51:52,933 | server.py:125 | fit progress: (10, 1.9851140975952148, {'accuracy': 0.6214, 'data_size': 10000}, 100.49534182201023)
INFO flwr 2024-04-06 23:51:52,933 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-06 23:51:52,933 | server.py:153 | FL finished in 100.49573580102879
INFO flwr 2024-04-06 23:51:52,933 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-06 23:51:52,933 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-06 23:51:52,934 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-06 23:51:52,934 | app.py:229 | app_fit: losses_centralized [(0, 2.3058156967163086), (1, 2.284912109375), (2, 2.251913547515869), (3, 2.224602222442627), (4, 2.1805920600891113), (5, 2.1418983936309814), (6, 2.095099449157715), (7, 2.0616233348846436), (8, 2.024460792541504), (9, 2.005359172821045), (10, 1.9851140975952148)]
INFO flwr 2024-04-06 23:51:52,934 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.2318), (2, 0.2791), (3, 0.3358), (4, 0.3974), (5, 0.4532), (6, 0.5263), (7, 0.5601), (8, 0.5726), (9, 0.5696), (10, 0.6214)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6214
wandb:     loss 1.98511
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_234952-a5lawj8o
wandb: Find logs at: ./wandb/offline-run-20240406_234952-a5lawj8o/logs
INFO flwr 2024-04-06 23:51:56,420 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-06 23:59:04,436 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1498358)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1498358)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-06 23:59:09,607	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-06 23:59:09,990	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-06 23:59:10,468	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e3b9f10f8118e396.zip' (10.67MiB) to Ray cluster...
2024-04-06 23:59:10,497	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e3b9f10f8118e396.zip'.
INFO flwr 2024-04-06 23:59:21,603 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 116978272871.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54419259801.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-06 23:59:21,603 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-06 23:59:21,604 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-06 23:59:21,642 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-06 23:59:21,643 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-06 23:59:21,643 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-06 23:59:21,643 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-06 23:59:24,455 | server.py:94 | initial parameters (loss, other metrics): 2.304542303085327, {'accuracy': 0.0715, 'data_size': 10000}
INFO flwr 2024-04-06 23:59:24,456 | server.py:104 | FL starting
DEBUG flwr 2024-04-06 23:59:24,456 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1506073)[0m 2024-04-06 23:59:27.907183: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1506073)[0m 2024-04-06 23:59:27.997389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1506073)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1506075)[0m 2024-04-06 23:59:29.987101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1506073)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1506073)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1506072)[0m 2024-04-06 23:59:28.300152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1506072)[0m 2024-04-06 23:59:28.397173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1506072)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1506062)[0m 2024-04-06 23:59:30.569170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-06 23:59:44,582 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-06 23:59:44,613 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-06 23:59:46,030 | server.py:125 | fit progress: (1, 2.263385772705078, {'accuracy': 0.3069, 'data_size': 10000}, 21.573643985990202)
INFO flwr 2024-04-06 23:59:46,030 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-06 23:59:46,030 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-06 23:59:55,579 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-06 23:59:57,063 | server.py:125 | fit progress: (2, 2.1832308769226074, {'accuracy': 0.5865, 'data_size': 10000}, 32.60711479501333)
INFO flwr 2024-04-06 23:59:57,063 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-06 23:59:57,064 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:05,977 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:07,145 | server.py:125 | fit progress: (3, 2.1053519248962402, {'accuracy': 0.6416, 'data_size': 10000}, 42.689078889001394)
INFO flwr 2024-04-07 00:00:07,145 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:07,146 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:15,904 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:17,424 | server.py:125 | fit progress: (4, 2.0364253520965576, {'accuracy': 0.6052, 'data_size': 10000}, 52.96783596900059)
INFO flwr 2024-04-07 00:00:17,424 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:17,424 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:26,163 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:27,679 | server.py:125 | fit progress: (5, 1.9809848070144653, {'accuracy': 0.6621, 'data_size': 10000}, 63.22333648399217)
INFO flwr 2024-04-07 00:00:27,680 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:27,680 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:36,246 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:37,858 | server.py:125 | fit progress: (6, 1.933693528175354, {'accuracy': 0.757, 'data_size': 10000}, 73.40177281800425)
INFO flwr 2024-04-07 00:00:37,858 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:37,858 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:46,770 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:48,023 | server.py:125 | fit progress: (7, 1.8984225988388062, {'accuracy': 0.7226, 'data_size': 10000}, 83.56715751899173)
INFO flwr 2024-04-07 00:00:48,023 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:48,024 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:00:56,403 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:00:58,088 | server.py:125 | fit progress: (8, 1.8800923824310303, {'accuracy': 0.7034, 'data_size': 10000}, 93.63187092699809)
INFO flwr 2024-04-07 00:00:58,088 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:00:58,088 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:01:06,569 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:01:08,012 | server.py:125 | fit progress: (9, 1.8504066467285156, {'accuracy': 0.7337, 'data_size': 10000}, 103.55558566801483)
INFO flwr 2024-04-07 00:01:08,012 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:01:08,012 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:01:16,985 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:01:18,345 | server.py:125 | fit progress: (10, 1.8321703672409058, {'accuracy': 0.75, 'data_size': 10000}, 113.88874357301393)
INFO flwr 2024-04-07 00:01:18,345 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:01:18,345 | server.py:153 | FL finished in 113.88919699599501
INFO flwr 2024-04-07 00:01:18,345 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:01:18,345 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:01:18,346 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:01:18,346 | app.py:229 | app_fit: losses_centralized [(0, 2.304542303085327), (1, 2.263385772705078), (2, 2.1832308769226074), (3, 2.1053519248962402), (4, 2.0364253520965576), (5, 1.9809848070144653), (6, 1.933693528175354), (7, 1.8984225988388062), (8, 1.8800923824310303), (9, 1.8504066467285156), (10, 1.8321703672409058)]
INFO flwr 2024-04-07 00:01:18,346 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0715), (1, 0.3069), (2, 0.5865), (3, 0.6416), (4, 0.6052), (5, 0.6621), (6, 0.757), (7, 0.7226), (8, 0.7034), (9, 0.7337), (10, 0.75)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.75
wandb:     loss 1.83217
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240406_235903-ke2uq9c6
wandb: Find logs at: ./wandb/offline-run-20240406_235903-ke2uq9c6/logs
INFO flwr 2024-04-07 00:01:21,893 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:08:28,164 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1506074)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1506074)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 00:08:33,062	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:08:33,512	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:08:33,963	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_880715d6d0ca00d2.zip' (10.68MiB) to Ray cluster...
2024-04-07 00:08:34,001	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_880715d6d0ca00d2.zip'.
INFO flwr 2024-04-07 00:08:44,901 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 133211206656.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 61376231424.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 00:08:44,901 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:08:44,901 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:08:44,918 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:08:44,919 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:08:44,920 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:08:44,920 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:08:48,092 | server.py:94 | initial parameters (loss, other metrics): 2.3042690753936768, {'accuracy': 0.0929, 'data_size': 10000}
INFO flwr 2024-04-07 00:08:48,093 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:08:48,093 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1510733)[0m 2024-04-07 00:08:50.841812: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1510733)[0m 2024-04-07 00:08:50.938040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1510733)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1510733)[0m 2024-04-07 00:08:53.116264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1510735)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1510735)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1510744)[0m 2024-04-07 00:08:51.248677: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1510744)[0m 2024-04-07 00:08:51.350422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1510744)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1510741)[0m 2024-04-07 00:08:53.579955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:09:05,414 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:09:05,452 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:09:06,692 | server.py:125 | fit progress: (1, 2.2143616676330566, {'accuracy': 0.4635, 'data_size': 10000}, 18.59915893699508)
INFO flwr 2024-04-07 00:09:06,693 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:06,693 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:09:15,190 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:09:16,313 | server.py:125 | fit progress: (2, 2.087167501449585, {'accuracy': 0.6097, 'data_size': 10000}, 28.219681572983973)
INFO flwr 2024-04-07 00:09:16,313 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:16,313 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:09:24,466 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:09:25,584 | server.py:125 | fit progress: (3, 2.0083227157592773, {'accuracy': 0.6642, 'data_size': 10000}, 37.49104111199267)
INFO flwr 2024-04-07 00:09:25,584 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:25,585 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:09:33,702 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:09:34,931 | server.py:125 | fit progress: (4, 1.9346058368682861, {'accuracy': 0.6548, 'data_size': 10000}, 46.83821805898333)
INFO flwr 2024-04-07 00:09:34,932 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:34,932 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:09:42,999 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:09:44,510 | server.py:125 | fit progress: (5, 1.8933463096618652, {'accuracy': 0.6991, 'data_size': 10000}, 56.41631377997692)
INFO flwr 2024-04-07 00:09:44,510 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:44,510 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:09:52,221 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:09:53,720 | server.py:125 | fit progress: (6, 1.8576571941375732, {'accuracy': 0.7426, 'data_size': 10000}, 65.62667270898237)
INFO flwr 2024-04-07 00:09:53,720 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:09:53,720 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:01,340 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:10:02,803 | server.py:125 | fit progress: (7, 1.8371459245681763, {'accuracy': 0.7253, 'data_size': 10000}, 74.70968924698536)
INFO flwr 2024-04-07 00:10:02,803 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:10:02,803 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:10,511 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:10:12,000 | server.py:125 | fit progress: (8, 1.8054841756820679, {'accuracy': 0.78, 'data_size': 10000}, 83.90665555599844)
INFO flwr 2024-04-07 00:10:12,000 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:10:12,000 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:19,695 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:10:20,977 | server.py:125 | fit progress: (9, 1.7847888469696045, {'accuracy': 0.7988, 'data_size': 10000}, 92.88414360399474)
INFO flwr 2024-04-07 00:10:20,978 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:10:20,978 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:10:29,000 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:10:30,421 | server.py:125 | fit progress: (10, 1.7601239681243896, {'accuracy': 0.8336, 'data_size': 10000}, 102.32735826697899)
INFO flwr 2024-04-07 00:10:30,421 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:10:30,421 | server.py:153 | FL finished in 102.32778337498894
INFO flwr 2024-04-07 00:10:30,421 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:10:30,421 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:10:30,421 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:10:30,421 | app.py:229 | app_fit: losses_centralized [(0, 2.3042690753936768), (1, 2.2143616676330566), (2, 2.087167501449585), (3, 2.0083227157592773), (4, 1.9346058368682861), (5, 1.8933463096618652), (6, 1.8576571941375732), (7, 1.8371459245681763), (8, 1.8054841756820679), (9, 1.7847888469696045), (10, 1.7601239681243896)]
INFO flwr 2024-04-07 00:10:30,421 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0929), (1, 0.4635), (2, 0.6097), (3, 0.6642), (4, 0.6548), (5, 0.6991), (6, 0.7426), (7, 0.7253), (8, 0.78), (9, 0.7988), (10, 0.8336)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8336
wandb:     loss 1.76012
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_000827-ek7p37mj
wandb: Find logs at: ./wandb/offline-run-20240407_000827-ek7p37mj/logs
INFO flwr 2024-04-07 00:10:33,804 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:17:40,520 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1510733)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1510733)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 00:17:46,658	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:17:47,062	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:17:47,465	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c08621b910c642e2.zip' (10.70MiB) to Ray cluster...
2024-04-07 00:17:47,502	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c08621b910c642e2.zip'.
INFO flwr 2024-04-07 00:17:58,646 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 57540684595.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'memory': 124261597389.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 00:17:58,646 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:17:58,646 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:17:58,667 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:17:58,668 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:17:58,668 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:17:58,668 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:18:01,737 | server.py:94 | initial parameters (loss, other metrics): 2.3048839569091797, {'accuracy': 0.0672, 'data_size': 10000}
INFO flwr 2024-04-07 00:18:01,738 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:18:01,738 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1518240)[0m 2024-04-07 00:18:04.162940: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1518244)[0m 2024-04-07 00:18:04.311123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1518244)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1518240)[0m 2024-04-07 00:18:06.375433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1518244)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1518244)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1518246)[0m 2024-04-07 00:18:05.340663: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1518246)[0m 2024-04-07 00:18:05.438671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1518246)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1518243)[0m 2024-04-07 00:18:07.587503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:18:19,221 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:18:19,252 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:18:20,581 | server.py:125 | fit progress: (1, 2.1684980392456055, {'accuracy': 0.3806, 'data_size': 10000}, 18.84339334498509)
INFO flwr 2024-04-07 00:18:20,582 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:18:20,582 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:18:29,465 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:18:30,873 | server.py:125 | fit progress: (2, 2.062321424484253, {'accuracy': 0.5032, 'data_size': 10000}, 29.13529998899321)
INFO flwr 2024-04-07 00:18:30,874 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:18:30,874 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:18:38,625 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:18:39,765 | server.py:125 | fit progress: (3, 1.973328948020935, {'accuracy': 0.6241, 'data_size': 10000}, 38.02736565298983)
INFO flwr 2024-04-07 00:18:39,766 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:18:39,766 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:18:47,952 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:18:49,118 | server.py:125 | fit progress: (4, 1.9129290580749512, {'accuracy': 0.7459, 'data_size': 10000}, 47.38020708799013)
INFO flwr 2024-04-07 00:18:49,118 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:18:49,119 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:18:56,886 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:18:58,294 | server.py:125 | fit progress: (5, 1.8560454845428467, {'accuracy': 0.7836, 'data_size': 10000}, 56.556241054000566)
INFO flwr 2024-04-07 00:18:58,295 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:18:58,295 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:19:06,175 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:19:07,612 | server.py:125 | fit progress: (6, 1.8120447397232056, {'accuracy': 0.7917, 'data_size': 10000}, 65.87392355597694)
INFO flwr 2024-04-07 00:19:07,612 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:19:07,612 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:19:15,290 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:19:16,783 | server.py:125 | fit progress: (7, 1.7880339622497559, {'accuracy': 0.8244, 'data_size': 10000}, 75.0450883449812)
INFO flwr 2024-04-07 00:19:16,783 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:19:16,784 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:19:24,380 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:19:25,885 | server.py:125 | fit progress: (8, 1.7732465267181396, {'accuracy': 0.8357, 'data_size': 10000}, 84.14724012798979)
INFO flwr 2024-04-07 00:19:25,886 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:19:25,886 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:19:33,352 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:19:34,687 | server.py:125 | fit progress: (9, 1.7467695474624634, {'accuracy': 0.8422, 'data_size': 10000}, 92.94941324199317)
INFO flwr 2024-04-07 00:19:34,688 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:19:34,688 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:19:42,501 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:19:43,868 | server.py:125 | fit progress: (10, 1.7281005382537842, {'accuracy': 0.8549, 'data_size': 10000}, 102.1295466759766)
INFO flwr 2024-04-07 00:19:43,868 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:19:43,868 | server.py:153 | FL finished in 102.1299385179882
INFO flwr 2024-04-07 00:19:43,868 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:19:43,868 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:19:43,868 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:19:43,868 | app.py:229 | app_fit: losses_centralized [(0, 2.3048839569091797), (1, 2.1684980392456055), (2, 2.062321424484253), (3, 1.973328948020935), (4, 1.9129290580749512), (5, 1.8560454845428467), (6, 1.8120447397232056), (7, 1.7880339622497559), (8, 1.7732465267181396), (9, 1.7467695474624634), (10, 1.7281005382537842)]
INFO flwr 2024-04-07 00:19:43,868 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0672), (1, 0.3806), (2, 0.5032), (3, 0.6241), (4, 0.7459), (5, 0.7836), (6, 0.7917), (7, 0.8244), (8, 0.8357), (9, 0.8422), (10, 0.8549)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8549
wandb:     loss 1.7281
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_001740-mxqgs62b
wandb: Find logs at: ./wandb/offline-run-20240407_001740-mxqgs62b/logs
INFO flwr 2024-04-07 00:19:47,392 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:26:54,642 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1518235)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1518235)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 00:27:00,791	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:27:01,176	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:27:01,527	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c447a1d461549c9a.zip' (10.72MiB) to Ray cluster...
2024-04-07 00:27:01,567	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c447a1d461549c9a.zip'.
INFO flwr 2024-04-07 00:27:12,770 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 124016738509.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57435745075.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 00:27:12,770 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:27:12,770 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:27:12,788 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:27:12,789 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:27:12,790 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:27:12,790 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:27:16,358 | server.py:94 | initial parameters (loss, other metrics): 2.3012635707855225, {'accuracy': 0.1222, 'data_size': 10000}
INFO flwr 2024-04-07 00:27:16,359 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:27:16,365 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1525033)[0m 2024-04-07 00:27:18.937970: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1525033)[0m 2024-04-07 00:27:19.036042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1525033)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1525033)[0m 2024-04-07 00:27:21.221771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1525033)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1525033)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1525036)[0m 2024-04-07 00:27:19.083943: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1525036)[0m 2024-04-07 00:27:19.188859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1525036)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1525029)[0m 2024-04-07 00:27:21.675048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:27:33,893 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:27:33,921 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:27:35,210 | server.py:125 | fit progress: (1, 2.1542649269104004, {'accuracy': 0.5408, 'data_size': 10000}, 18.845404755003983)
INFO flwr 2024-04-07 00:27:35,210 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:27:35,210 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:27:44,259 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:27:45,605 | server.py:125 | fit progress: (2, 2.0123631954193115, {'accuracy': 0.6353, 'data_size': 10000}, 29.240686599019682)
INFO flwr 2024-04-07 00:27:45,606 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:27:45,606 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:27:54,433 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:27:55,785 | server.py:125 | fit progress: (3, 1.9351685047149658, {'accuracy': 0.6731, 'data_size': 10000}, 39.420452974998625)
INFO flwr 2024-04-07 00:27:55,785 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:27:55,785 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:04,471 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:05,949 | server.py:125 | fit progress: (4, 1.8691954612731934, {'accuracy': 0.7585, 'data_size': 10000}, 49.58422023602179)
INFO flwr 2024-04-07 00:28:05,949 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:05,949 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:14,056 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:15,559 | server.py:125 | fit progress: (5, 1.8389700651168823, {'accuracy': 0.7491, 'data_size': 10000}, 59.1940471889975)
INFO flwr 2024-04-07 00:28:15,559 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:15,559 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:23,317 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:24,785 | server.py:125 | fit progress: (6, 1.801967978477478, {'accuracy': 0.7798, 'data_size': 10000}, 68.42068620800273)
INFO flwr 2024-04-07 00:28:24,785 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:24,786 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:33,023 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:34,370 | server.py:125 | fit progress: (7, 1.7826772928237915, {'accuracy': 0.787, 'data_size': 10000}, 78.00501027001883)
INFO flwr 2024-04-07 00:28:34,370 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:34,370 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:42,997 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:44,587 | server.py:125 | fit progress: (8, 1.7638322114944458, {'accuracy': 0.7841, 'data_size': 10000}, 88.22279185202206)
INFO flwr 2024-04-07 00:28:44,588 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:44,588 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:28:52,840 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:28:54,179 | server.py:125 | fit progress: (9, 1.7499808073043823, {'accuracy': 0.7974, 'data_size': 10000}, 97.81477308200556)
INFO flwr 2024-04-07 00:28:54,180 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:28:54,180 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:29:03,348 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:29:04,767 | server.py:125 | fit progress: (10, 1.7449312210083008, {'accuracy': 0.7955, 'data_size': 10000}, 108.40290822699899)
INFO flwr 2024-04-07 00:29:04,768 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:29:04,768 | server.py:153 | FL finished in 108.40332478101482
INFO flwr 2024-04-07 00:29:04,768 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:29:04,768 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:29:04,768 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:29:04,768 | app.py:229 | app_fit: losses_centralized [(0, 2.3012635707855225), (1, 2.1542649269104004), (2, 2.0123631954193115), (3, 1.9351685047149658), (4, 1.8691954612731934), (5, 1.8389700651168823), (6, 1.801967978477478), (7, 1.7826772928237915), (8, 1.7638322114944458), (9, 1.7499808073043823), (10, 1.7449312210083008)]
INFO flwr 2024-04-07 00:29:04,768 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1222), (1, 0.5408), (2, 0.6353), (3, 0.6731), (4, 0.7585), (5, 0.7491), (6, 0.7798), (7, 0.787), (8, 0.7841), (9, 0.7974), (10, 0.7955)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7955
wandb:     loss 1.74493
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_002654-0cxbogih
wandb: Find logs at: ./wandb/offline-run-20240407_002654-0cxbogih/logs
INFO flwr 2024-04-07 00:29:08,286 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: -1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:36:15,563 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1525025)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1525025)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 00:36:21,598	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:36:21,971	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:36:22,407	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_316e9b100aa65a75.zip' (10.74MiB) to Ray cluster...
2024-04-07 00:36:22,435	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_316e9b100aa65a75.zip'.
INFO flwr 2024-04-07 00:36:33,735 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 128520774247.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59366046105.0}
INFO flwr 2024-04-07 00:36:33,736 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:36:33,736 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:36:33,750 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:36:33,751 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:36:33,751 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:36:33,751 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:36:36,774 | server.py:94 | initial parameters (loss, other metrics): 2.3025295734405518, {'accuracy': 0.081, 'data_size': 10000}
INFO flwr 2024-04-07 00:36:36,774 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:36:36,774 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1529737)[0m 2024-04-07 00:36:39.746618: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1529737)[0m 2024-04-07 00:36:39.845987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1529737)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1529737)[0m 2024-04-07 00:36:42.074088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1529744)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1529744)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1529748)[0m 2024-04-07 00:36:40.114092: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1529748)[0m 2024-04-07 00:36:40.220170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1529748)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1529745)[0m 2024-04-07 00:36:42.349428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1529749)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1529749)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-04-07 00:36:56,710 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:36:56,751 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:36:58,038 | server.py:125 | fit progress: (1, 2.1603822708129883, {'accuracy': 0.4434, 'data_size': 10000}, 21.263948674022686)
INFO flwr 2024-04-07 00:36:58,039 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:36:58,039 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:06,657 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:08,057 | server.py:125 | fit progress: (2, 2.050949811935425, {'accuracy': 0.49, 'data_size': 10000}, 31.28243966001901)
INFO flwr 2024-04-07 00:37:08,057 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:08,057 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:15,774 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:17,135 | server.py:125 | fit progress: (3, 1.9286447763442993, {'accuracy': 0.6962, 'data_size': 10000}, 40.36032562801847)
INFO flwr 2024-04-07 00:37:17,135 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:17,135 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:24,565 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:26,023 | server.py:125 | fit progress: (4, 1.8370789289474487, {'accuracy': 0.7795, 'data_size': 10000}, 49.24901238101302)
INFO flwr 2024-04-07 00:37:26,024 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:26,024 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:33,905 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:35,127 | server.py:125 | fit progress: (5, 1.785310983657837, {'accuracy': 0.8081, 'data_size': 10000}, 58.35268780202023)
INFO flwr 2024-04-07 00:37:35,127 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:35,128 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:43,158 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:44,622 | server.py:125 | fit progress: (6, 1.7568511962890625, {'accuracy': 0.8347, 'data_size': 10000}, 67.8476927470183)
INFO flwr 2024-04-07 00:37:44,622 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:44,622 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:37:53,343 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:37:54,671 | server.py:125 | fit progress: (7, 1.726379156112671, {'accuracy': 0.8566, 'data_size': 10000}, 77.89631970302435)
INFO flwr 2024-04-07 00:37:54,671 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:37:54,671 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:38:03,060 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:38:04,363 | server.py:125 | fit progress: (8, 1.7123297452926636, {'accuracy': 0.857, 'data_size': 10000}, 87.58903170202393)
INFO flwr 2024-04-07 00:38:04,364 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:38:04,364 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:38:12,864 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:38:14,475 | server.py:125 | fit progress: (9, 1.6998142004013062, {'accuracy': 0.8599, 'data_size': 10000}, 97.70095322999987)
INFO flwr 2024-04-07 00:38:14,476 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:38:14,476 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:38:22,654 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:38:24,384 | server.py:125 | fit progress: (10, 1.694137692451477, {'accuracy': 0.8605, 'data_size': 10000}, 107.60970743300277)
INFO flwr 2024-04-07 00:38:24,384 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:38:24,384 | server.py:153 | FL finished in 107.61008928102092
INFO flwr 2024-04-07 00:38:24,385 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:38:24,385 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:38:24,385 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:38:24,385 | app.py:229 | app_fit: losses_centralized [(0, 2.3025295734405518), (1, 2.1603822708129883), (2, 2.050949811935425), (3, 1.9286447763442993), (4, 1.8370789289474487), (5, 1.785310983657837), (6, 1.7568511962890625), (7, 1.726379156112671), (8, 1.7123297452926636), (9, 1.6998142004013062), (10, 1.694137692451477)]
INFO flwr 2024-04-07 00:38:24,385 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.081), (1, 0.4434), (2, 0.49), (3, 0.6962), (4, 0.7795), (5, 0.8081), (6, 0.8347), (7, 0.8566), (8, 0.857), (9, 0.8599), (10, 0.8605)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8605
wandb:     loss 1.69414
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_003615-442lc02m
wandb: Find logs at: ./wandb/offline-run-20240407_003615-442lc02m/logs
INFO flwr 2024-04-07 00:38:27,927 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:45:34,978 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1529745)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1529745)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
2024-04-07 00:45:40,267	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:45:40,747	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:45:41,222	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5e80a06b3a98f50c.zip' (10.76MiB) to Ray cluster...
2024-04-07 00:45:41,264	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5e80a06b3a98f50c.zip'.
INFO flwr 2024-04-07 00:45:52,465 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 116840432231.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54360185241.0}
INFO flwr 2024-04-07 00:45:52,466 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:45:52,466 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:45:52,483 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:45:52,484 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:45:52,484 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:45:52,485 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:45:55,052 | server.py:94 | initial parameters (loss, other metrics): 2.3045308589935303, {'accuracy': 0.0968, 'data_size': 10000}
INFO flwr 2024-04-07 00:45:55,053 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:45:55,054 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1537263)[0m 2024-04-07 00:45:58.937334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1537263)[0m 2024-04-07 00:45:59.048572: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1537263)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1537262)[0m 2024-04-07 00:46:01.166931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1537262)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1537262)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1537258)[0m 2024-04-07 00:45:59.243315: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537258)[0m 2024-04-07 00:45:59.363366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537258)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1537258)[0m 2024-04-07 00:46:01.520228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:46:16,060 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:46:16,124 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:46:17,723 | server.py:125 | fit progress: (1, 2.295071601867676, {'accuracy': 0.1982, 'data_size': 10000}, 22.66964585799724)
INFO flwr 2024-04-07 00:46:17,724 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:46:17,724 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:46:28,221 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:46:29,826 | server.py:125 | fit progress: (2, 2.286520481109619, {'accuracy': 0.3193, 'data_size': 10000}, 34.77193923498271)
INFO flwr 2024-04-07 00:46:29,826 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:46:29,826 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:46:39,453 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:46:40,981 | server.py:125 | fit progress: (3, 2.2741143703460693, {'accuracy': 0.3434, 'data_size': 10000}, 45.927163176995236)
INFO flwr 2024-04-07 00:46:40,981 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:46:40,981 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:46:50,245 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:46:51,750 | server.py:125 | fit progress: (4, 2.256225824356079, {'accuracy': 0.3894, 'data_size': 10000}, 56.69659150397638)
INFO flwr 2024-04-07 00:46:51,750 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:46:51,751 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:01,379 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:03,003 | server.py:125 | fit progress: (5, 2.2343671321868896, {'accuracy': 0.3741, 'data_size': 10000}, 67.94947028599563)
INFO flwr 2024-04-07 00:47:03,003 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:03,004 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:13,174 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:14,435 | server.py:125 | fit progress: (6, 2.2135531902313232, {'accuracy': 0.5048, 'data_size': 10000}, 79.38192347399308)
INFO flwr 2024-04-07 00:47:14,436 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:14,436 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:24,244 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:25,599 | server.py:125 | fit progress: (7, 2.189495086669922, {'accuracy': 0.4376, 'data_size': 10000}, 90.54576570398058)
INFO flwr 2024-04-07 00:47:25,600 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:25,600 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:35,956 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:37,584 | server.py:125 | fit progress: (8, 2.168210983276367, {'accuracy': 0.572, 'data_size': 10000}, 102.52998161199503)
INFO flwr 2024-04-07 00:47:37,584 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:37,584 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:47,317 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:47:49,300 | server.py:125 | fit progress: (9, 2.1458041667938232, {'accuracy': 0.5682, 'data_size': 10000}, 114.24609380299808)
INFO flwr 2024-04-07 00:47:49,300 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:47:49,300 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:47:59,646 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:48:01,409 | server.py:125 | fit progress: (10, 2.126446008682251, {'accuracy': 0.5705, 'data_size': 10000}, 126.35560881299898)
INFO flwr 2024-04-07 00:48:01,409 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:48:01,410 | server.py:153 | FL finished in 126.35614089597948
INFO flwr 2024-04-07 00:48:01,410 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:48:01,410 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:48:01,410 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:48:01,410 | app.py:229 | app_fit: losses_centralized [(0, 2.3045308589935303), (1, 2.295071601867676), (2, 2.286520481109619), (3, 2.2741143703460693), (4, 2.256225824356079), (5, 2.2343671321868896), (6, 2.2135531902313232), (7, 2.189495086669922), (8, 2.168210983276367), (9, 2.1458041667938232), (10, 2.126446008682251)]
INFO flwr 2024-04-07 00:48:01,410 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0968), (1, 0.1982), (2, 0.3193), (3, 0.3434), (4, 0.3894), (5, 0.3741), (6, 0.5048), (7, 0.4376), (8, 0.572), (9, 0.5682), (10, 0.5705)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5705
wandb:     loss 2.12645
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_004534-42lpgsns
wandb: Find logs at: ./wandb/offline-run-20240407_004534-42lpgsns/logs
INFO flwr 2024-04-07 00:48:05,043 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 00:55:14,238 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1537263)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1537263)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 00:55:19,272	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 00:55:19,721	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 00:55:20,113	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e8a3d79de686cd44.zip' (10.78MiB) to Ray cluster...
2024-04-07 00:55:20,150	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e8a3d79de686cd44.zip'.
INFO flwr 2024-04-07 00:55:31,185 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 127617442816.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58978904064.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 00:55:31,185 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 00:55:31,185 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 00:55:31,203 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 00:55:31,205 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 00:55:31,205 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 00:55:31,206 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 00:55:33,281 | server.py:94 | initial parameters (loss, other metrics): 2.3030409812927246, {'accuracy': 0.1002, 'data_size': 10000}
INFO flwr 2024-04-07 00:55:33,282 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 00:55:33,282 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1541633)[0m 2024-04-07 00:55:37.250781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1541633)[0m 2024-04-07 00:55:37.348643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1541633)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1541623)[0m 2024-04-07 00:55:39.337948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1541631)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1541631)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1541631)[0m 2024-04-07 00:55:37.658387: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541631)[0m 2024-04-07 00:55:37.754148: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541631)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1541629)[0m 2024-04-07 00:55:40.345972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 00:55:52,269 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 00:55:52,309 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 00:55:53,439 | server.py:125 | fit progress: (1, 2.0356807708740234, {'accuracy': 0.643, 'data_size': 10000}, 20.156922983995173)
INFO flwr 2024-04-07 00:55:53,439 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 00:55:53,439 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:03,625 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 00:56:05,091 | server.py:125 | fit progress: (2, 1.8465449810028076, {'accuracy': 0.7057, 'data_size': 10000}, 31.809072087984532)
INFO flwr 2024-04-07 00:56:05,091 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 00:56:05,092 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:14,452 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 00:56:15,968 | server.py:125 | fit progress: (3, 1.7300496101379395, {'accuracy': 0.8201, 'data_size': 10000}, 42.68628102299408)
INFO flwr 2024-04-07 00:56:15,968 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 00:56:15,969 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:25,426 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 00:56:26,978 | server.py:125 | fit progress: (4, 1.6929175853729248, {'accuracy': 0.8519, 'data_size': 10000}, 53.69587668799795)
INFO flwr 2024-04-07 00:56:26,978 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 00:56:26,978 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:36,325 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 00:56:37,615 | server.py:125 | fit progress: (5, 1.661657452583313, {'accuracy': 0.87, 'data_size': 10000}, 64.33303699898534)
INFO flwr 2024-04-07 00:56:37,615 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 00:56:37,615 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:47,223 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 00:56:48,790 | server.py:125 | fit progress: (6, 1.6591179370880127, {'accuracy': 0.8569, 'data_size': 10000}, 75.50830236601178)
INFO flwr 2024-04-07 00:56:48,791 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 00:56:48,791 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:56:58,631 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 00:57:00,309 | server.py:125 | fit progress: (7, 1.6332937479019165, {'accuracy': 0.8815, 'data_size': 10000}, 87.02707810298307)
INFO flwr 2024-04-07 00:57:00,309 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 00:57:00,309 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:57:09,872 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 00:57:11,659 | server.py:125 | fit progress: (8, 1.6323364973068237, {'accuracy': 0.881, 'data_size': 10000}, 98.37702817798709)
INFO flwr 2024-04-07 00:57:11,659 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 00:57:11,659 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:57:21,968 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 00:57:23,421 | server.py:125 | fit progress: (9, 1.6186577081680298, {'accuracy': 0.8885, 'data_size': 10000}, 110.1387299800117)
INFO flwr 2024-04-07 00:57:23,421 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 00:57:23,422 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 00:57:32,737 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 00:57:34,477 | server.py:125 | fit progress: (10, 1.6153367757797241, {'accuracy': 0.8894, 'data_size': 10000}, 121.19493555900408)
INFO flwr 2024-04-07 00:57:34,477 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 00:57:34,477 | server.py:153 | FL finished in 121.195485128992
INFO flwr 2024-04-07 00:57:34,478 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 00:57:34,478 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 00:57:34,478 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 00:57:34,478 | app.py:229 | app_fit: losses_centralized [(0, 2.3030409812927246), (1, 2.0356807708740234), (2, 1.8465449810028076), (3, 1.7300496101379395), (4, 1.6929175853729248), (5, 1.661657452583313), (6, 1.6591179370880127), (7, 1.6332937479019165), (8, 1.6323364973068237), (9, 1.6186577081680298), (10, 1.6153367757797241)]
INFO flwr 2024-04-07 00:57:34,478 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1002), (1, 0.643), (2, 0.7057), (3, 0.8201), (4, 0.8519), (5, 0.87), (6, 0.8569), (7, 0.8815), (8, 0.881), (9, 0.8885), (10, 0.8894)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8894
wandb:     loss 1.61534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_005513-b47qw7to
wandb: Find logs at: ./wandb/offline-run-20240407_005513-b47qw7to/logs
INFO flwr 2024-04-07 00:57:38,021 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:04:47,277 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1541627)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1541627)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:04:53,399	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:04:53,758	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:04:54,139	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f160c0e2e147f7db.zip' (10.80MiB) to Ray cluster...
2024-04-07 01:04:54,174	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f160c0e2e147f7db.zip'.
INFO flwr 2024-04-07 01:05:05,605 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 53088603340.0, 'memory': 113873407796.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 01:05:05,605 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:05:05,605 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:05:05,626 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:05:05,627 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:05:05,627 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:05:05,627 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:05:08,305 | server.py:94 | initial parameters (loss, other metrics): 2.3033287525177, {'accuracy': 0.0786, 'data_size': 10000}
INFO flwr 2024-04-07 01:05:08,305 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:05:08,306 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1549403)[0m 2024-04-07 01:05:11.015635: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1549403)[0m 2024-04-07 01:05:11.117127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1549403)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1549403)[0m 2024-04-07 01:05:13.375390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1549407)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1549407)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1549404)[0m 2024-04-07 01:05:12.434286: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1549404)[0m 2024-04-07 01:05:12.546763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1549404)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1549404)[0m 2024-04-07 01:05:14.608009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:05:27,181 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:05:27,214 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:05:28,605 | server.py:125 | fit progress: (1, 1.9845763444900513, {'accuracy': 0.5901, 'data_size': 10000}, 20.298661913984688)
INFO flwr 2024-04-07 01:05:28,605 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:05:28,605 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:05:38,556 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:05:39,809 | server.py:125 | fit progress: (2, 1.7870144844055176, {'accuracy': 0.7119, 'data_size': 10000}, 31.503274312999565)
INFO flwr 2024-04-07 01:05:39,809 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:05:39,810 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:05:49,459 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:05:50,734 | server.py:125 | fit progress: (3, 1.6754385232925415, {'accuracy': 0.8572, 'data_size': 10000}, 42.427879727998516)
INFO flwr 2024-04-07 01:05:50,734 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:05:50,734 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:05:59,870 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:01,139 | server.py:125 | fit progress: (4, 1.649289846420288, {'accuracy': 0.86, 'data_size': 10000}, 52.833030032983515)
INFO flwr 2024-04-07 01:06:01,139 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:01,139 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:06:10,109 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:11,763 | server.py:125 | fit progress: (5, 1.6255382299423218, {'accuracy': 0.875, 'data_size': 10000}, 63.457113571988884)
INFO flwr 2024-04-07 01:06:11,763 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:11,764 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:06:21,659 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:23,304 | server.py:125 | fit progress: (6, 1.6235297918319702, {'accuracy': 0.8755, 'data_size': 10000}, 74.9978193200077)
INFO flwr 2024-04-07 01:06:23,304 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:23,304 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:06:32,111 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:33,784 | server.py:125 | fit progress: (7, 1.6147949695587158, {'accuracy': 0.8782, 'data_size': 10000}, 85.47820080199745)
INFO flwr 2024-04-07 01:06:33,784 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:33,785 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:06:42,846 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:44,522 | server.py:125 | fit progress: (8, 1.6213161945343018, {'accuracy': 0.8668, 'data_size': 10000}, 96.21578976599267)
INFO flwr 2024-04-07 01:06:44,522 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:44,522 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:06:53,513 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:06:54,955 | server.py:125 | fit progress: (9, 1.5948337316513062, {'accuracy': 0.896, 'data_size': 10000}, 106.6489397349942)
INFO flwr 2024-04-07 01:06:54,955 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:06:54,955 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:07:04,681 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:07:06,146 | server.py:125 | fit progress: (10, 1.5893231630325317, {'accuracy': 0.8982, 'data_size': 10000}, 117.8405980670068)
INFO flwr 2024-04-07 01:07:06,147 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:07:06,147 | server.py:153 | FL finished in 117.84114893799415
INFO flwr 2024-04-07 01:07:06,147 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:07:06,147 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:07:06,147 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:07:06,147 | app.py:229 | app_fit: losses_centralized [(0, 2.3033287525177), (1, 1.9845763444900513), (2, 1.7870144844055176), (3, 1.6754385232925415), (4, 1.649289846420288), (5, 1.6255382299423218), (6, 1.6235297918319702), (7, 1.6147949695587158), (8, 1.6213161945343018), (9, 1.5948337316513062), (10, 1.5893231630325317)]
INFO flwr 2024-04-07 01:07:06,148 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0786), (1, 0.5901), (2, 0.7119), (3, 0.8572), (4, 0.86), (5, 0.875), (6, 0.8755), (7, 0.8782), (8, 0.8668), (9, 0.896), (10, 0.8982)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8982
wandb:     loss 1.58932
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_010446-wyxncpz4
wandb: Find logs at: ./wandb/offline-run-20240407_010446-wyxncpz4/logs
INFO flwr 2024-04-07 01:07:09,663 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:14:20,109 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1549408)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1549408)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:14:26,366	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:14:26,695	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:14:27,087	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_82c882e27abd8be9.zip' (10.81MiB) to Ray cluster...
2024-04-07 01:14:27,129	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_82c882e27abd8be9.zip'.
INFO flwr 2024-04-07 01:14:38,482 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 126429223527.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 58469667225.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 01:14:38,482 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:14:38,483 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:14:38,499 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:14:38,501 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:14:38,501 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:14:38,501 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:14:41,257 | server.py:94 | initial parameters (loss, other metrics): 2.3034565448760986, {'accuracy': 0.0994, 'data_size': 10000}
INFO flwr 2024-04-07 01:14:41,258 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:14:41,259 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1556506)[0m 2024-04-07 01:14:44.948067: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1556506)[0m 2024-04-07 01:14:45.049687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1556506)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1556502)[0m 2024-04-07 01:14:46.975122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1556516)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1556516)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1556509)[0m 2024-04-07 01:14:45.380783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1556509)[0m 2024-04-07 01:14:45.487579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1556509)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1556512)[0m 2024-04-07 01:14:47.808009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:15:11,062 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:15:11,097 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:15:12,520 | server.py:125 | fit progress: (1, 1.9381052255630493, {'accuracy': 0.6108, 'data_size': 10000}, 31.26087134098634)
INFO flwr 2024-04-07 01:15:12,520 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:12,520 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:23,023 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:15:24,488 | server.py:125 | fit progress: (2, 1.6959797143936157, {'accuracy': 0.828, 'data_size': 10000}, 43.22882101099822)
INFO flwr 2024-04-07 01:15:24,488 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:24,488 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:33,676 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:15:34,912 | server.py:125 | fit progress: (3, 1.6971114873886108, {'accuracy': 0.783, 'data_size': 10000}, 53.65311057199142)
INFO flwr 2024-04-07 01:15:34,912 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:34,912 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:44,386 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:15:45,650 | server.py:125 | fit progress: (4, 1.6384674310684204, {'accuracy': 0.8558, 'data_size': 10000}, 64.39115225200658)
INFO flwr 2024-04-07 01:15:45,650 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:45,650 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:15:55,710 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:15:57,311 | server.py:125 | fit progress: (5, 1.6004512310028076, {'accuracy': 0.8923, 'data_size': 10000}, 76.05185578699457)
INFO flwr 2024-04-07 01:15:57,311 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:15:57,311 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:07,114 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:08,713 | server.py:125 | fit progress: (6, 1.5961575508117676, {'accuracy': 0.8932, 'data_size': 10000}, 87.45432325499132)
INFO flwr 2024-04-07 01:16:08,713 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:08,714 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:17,864 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:19,504 | server.py:125 | fit progress: (7, 1.5886067152023315, {'accuracy': 0.8951, 'data_size': 10000}, 98.24489375198027)
INFO flwr 2024-04-07 01:16:19,504 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:19,504 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:29,179 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:30,823 | server.py:125 | fit progress: (8, 1.5905040502548218, {'accuracy': 0.8907, 'data_size': 10000}, 109.56408622398158)
INFO flwr 2024-04-07 01:16:30,823 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:30,823 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:40,946 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:42,379 | server.py:125 | fit progress: (9, 1.5779420137405396, {'accuracy': 0.9016, 'data_size': 10000}, 121.12054133799393)
INFO flwr 2024-04-07 01:16:42,380 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:16:42,380 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:16:52,090 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:16:53,567 | server.py:125 | fit progress: (10, 1.581327199935913, {'accuracy': 0.8973, 'data_size': 10000}, 132.3085068659857)
INFO flwr 2024-04-07 01:16:53,568 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:16:53,568 | server.py:153 | FL finished in 132.3089439960022
INFO flwr 2024-04-07 01:16:53,568 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:16:53,568 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:16:53,568 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:16:53,568 | app.py:229 | app_fit: losses_centralized [(0, 2.3034565448760986), (1, 1.9381052255630493), (2, 1.6959797143936157), (3, 1.6971114873886108), (4, 1.6384674310684204), (5, 1.6004512310028076), (6, 1.5961575508117676), (7, 1.5886067152023315), (8, 1.5905040502548218), (9, 1.5779420137405396), (10, 1.581327199935913)]
INFO flwr 2024-04-07 01:16:53,568 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0994), (1, 0.6108), (2, 0.828), (3, 0.783), (4, 0.8558), (5, 0.8923), (6, 0.8932), (7, 0.8951), (8, 0.8907), (9, 0.9016), (10, 0.8973)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8973
wandb:     loss 1.58133
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_011418-4ghccdjb
wandb: Find logs at: ./wandb/offline-run-20240407_011418-4ghccdjb/logs
INFO flwr 2024-04-07 01:16:57,152 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:24:06,054 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1556506)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1556506)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:24:11,422	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:24:11,791	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:24:12,128	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_310c0ddb4ad4c0a0.zip' (10.84MiB) to Ray cluster...
2024-04-07 01:24:12,152	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_310c0ddb4ad4c0a0.zip'.
INFO flwr 2024-04-07 01:24:23,247 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 122165573837.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56642388787.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 01:24:23,247 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:24:23,247 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:24:23,263 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:24:23,264 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:24:23,265 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:24:23,265 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:24:25,720 | server.py:94 | initial parameters (loss, other metrics): 2.30328106880188, {'accuracy': 0.102, 'data_size': 10000}
INFO flwr 2024-04-07 01:24:25,720 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:24:25,721 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1561435)[0m 2024-04-07 01:24:29.361472: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1561435)[0m 2024-04-07 01:24:29.462143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1561435)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1561434)[0m 2024-04-07 01:24:31.505617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1561437)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1561437)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1561439)[0m 2024-04-07 01:24:29.565715: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1561439)[0m 2024-04-07 01:24:29.657601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1561439)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1561432)[0m 2024-04-07 01:24:31.953930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:24:44,850 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:24:44,889 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:24:46,330 | server.py:125 | fit progress: (1, 1.8551291227340698, {'accuracy': 0.7033, 'data_size': 10000}, 20.609881668002345)
INFO flwr 2024-04-07 01:24:46,331 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:24:46,331 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:24:56,221 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:24:57,708 | server.py:125 | fit progress: (2, 1.718701720237732, {'accuracy': 0.7675, 'data_size': 10000}, 31.987312429992016)
INFO flwr 2024-04-07 01:24:57,708 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:24:57,708 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:25:07,074 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:25:08,414 | server.py:125 | fit progress: (3, 1.6738042831420898, {'accuracy': 0.8036, 'data_size': 10000}, 42.693912533984985)
INFO flwr 2024-04-07 01:25:08,415 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:25:08,415 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:25:18,425 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:25:20,128 | server.py:125 | fit progress: (4, 1.617882490158081, {'accuracy': 0.8719, 'data_size': 10000}, 54.40783119399566)
INFO flwr 2024-04-07 01:25:20,129 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:25:20,129 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:25:30,074 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:25:31,850 | server.py:125 | fit progress: (5, 1.6163567304611206, {'accuracy': 0.8632, 'data_size': 10000}, 66.12933823998901)
INFO flwr 2024-04-07 01:25:31,850 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:25:31,850 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:25:42,186 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:25:43,807 | server.py:125 | fit progress: (6, 1.5904457569122314, {'accuracy': 0.8911, 'data_size': 10000}, 78.08634131398867)
INFO flwr 2024-04-07 01:25:43,807 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:25:43,807 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:25:53,286 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:25:54,663 | server.py:125 | fit progress: (7, 1.5966864824295044, {'accuracy': 0.8817, 'data_size': 10000}, 88.9428122289828)
INFO flwr 2024-04-07 01:25:54,664 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:25:54,664 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:26:03,329 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:26:04,953 | server.py:125 | fit progress: (8, 1.579079031944275, {'accuracy': 0.8983, 'data_size': 10000}, 99.23208813800011)
INFO flwr 2024-04-07 01:26:04,953 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:26:04,953 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:26:13,739 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:26:15,223 | server.py:125 | fit progress: (9, 1.5791330337524414, {'accuracy': 0.8962, 'data_size': 10000}, 109.50235698799952)
INFO flwr 2024-04-07 01:26:15,223 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:26:15,223 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:26:25,604 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:26:27,098 | server.py:125 | fit progress: (10, 1.5725549459457397, {'accuracy': 0.9047, 'data_size': 10000}, 121.37724934399012)
INFO flwr 2024-04-07 01:26:27,098 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:26:27,098 | server.py:153 | FL finished in 121.37774469400756
INFO flwr 2024-04-07 01:26:27,098 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:26:27,098 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:26:27,099 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:26:27,099 | app.py:229 | app_fit: losses_centralized [(0, 2.30328106880188), (1, 1.8551291227340698), (2, 1.718701720237732), (3, 1.6738042831420898), (4, 1.617882490158081), (5, 1.6163567304611206), (6, 1.5904457569122314), (7, 1.5966864824295044), (8, 1.579079031944275), (9, 1.5791330337524414), (10, 1.5725549459457397)]
INFO flwr 2024-04-07 01:26:27,099 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.7033), (2, 0.7675), (3, 0.8036), (4, 0.8719), (5, 0.8632), (6, 0.8911), (7, 0.8817), (8, 0.8983), (9, 0.8962), (10, 0.9047)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9047
wandb:     loss 1.57255
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_012405-kh1hg9yi
wandb: Find logs at: ./wandb/offline-run-20240407_012405-kh1hg9yi/logs
INFO flwr 2024-04-07 01:26:30,717 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:33:39,711 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1561432)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1561432)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:33:45,683	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:33:46,023	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:33:46,363	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5f40dcc574fc06c9.zip' (10.86MiB) to Ray cluster...
2024-04-07 01:33:46,392	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5f40dcc574fc06c9.zip'.
INFO flwr 2024-04-07 01:33:57,867 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 116888360346.0, 'CPU': 64.0, 'object_store_memory': 54380725862.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 01:33:57,868 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:33:57,868 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:33:57,890 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:33:57,891 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:33:57,891 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:33:57,892 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:34:00,625 | server.py:94 | initial parameters (loss, other metrics): 2.298717498779297, {'accuracy': 0.1272, 'data_size': 10000}
INFO flwr 2024-04-07 01:34:00,626 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:34:00,627 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1569132)[0m 2024-04-07 01:34:04.493046: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1569132)[0m 2024-04-07 01:34:04.599741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1569132)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1569132)[0m 2024-04-07 01:34:06.634435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1569129)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1569129)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1569134)[0m 2024-04-07 01:34:04.619069: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1569134)[0m 2024-04-07 01:34:04.714938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1569134)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1569127)[0m 2024-04-07 01:34:06.860507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:34:21,386 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:34:21,425 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:34:22,807 | server.py:125 | fit progress: (1, 1.8258107900619507, {'accuracy': 0.7478, 'data_size': 10000}, 22.180748649989255)
INFO flwr 2024-04-07 01:34:22,808 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:34:22,808 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:34:32,823 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:34:34,643 | server.py:125 | fit progress: (2, 1.6777806282043457, {'accuracy': 0.8176, 'data_size': 10000}, 34.01693808400887)
INFO flwr 2024-04-07 01:34:34,644 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:34:34,644 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:34:44,162 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:34:45,766 | server.py:125 | fit progress: (3, 1.6255565881729126, {'accuracy': 0.864, 'data_size': 10000}, 45.139318036002805)
INFO flwr 2024-04-07 01:34:45,766 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:34:45,766 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:34:54,603 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:34:56,995 | server.py:125 | fit progress: (4, 1.598265290260315, {'accuracy': 0.8829, 'data_size': 10000}, 56.36875826999312)
INFO flwr 2024-04-07 01:34:56,996 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:34:56,996 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:06,845 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:08,190 | server.py:125 | fit progress: (5, 1.5925575494766235, {'accuracy': 0.8854, 'data_size': 10000}, 67.56308545600041)
INFO flwr 2024-04-07 01:35:08,190 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:08,190 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:17,012 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:18,573 | server.py:125 | fit progress: (6, 1.5960569381713867, {'accuracy': 0.8782, 'data_size': 10000}, 77.94648087999667)
INFO flwr 2024-04-07 01:35:18,573 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:18,573 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:28,384 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:29,851 | server.py:125 | fit progress: (7, 1.5857892036437988, {'accuracy': 0.8877, 'data_size': 10000}, 89.22406255398528)
INFO flwr 2024-04-07 01:35:29,851 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:29,851 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:39,802 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:41,201 | server.py:125 | fit progress: (8, 1.5780075788497925, {'accuracy': 0.8961, 'data_size': 10000}, 100.57407289798721)
INFO flwr 2024-04-07 01:35:41,201 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:41,201 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:35:50,512 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:35:52,399 | server.py:125 | fit progress: (9, 1.5763713121414185, {'accuracy': 0.895, 'data_size': 10000}, 111.77205242600758)
INFO flwr 2024-04-07 01:35:52,399 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:35:52,399 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:36:02,424 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:36:04,146 | server.py:125 | fit progress: (10, 1.5763455629348755, {'accuracy': 0.8944, 'data_size': 10000}, 123.51994207498501)
INFO flwr 2024-04-07 01:36:04,147 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:36:04,147 | server.py:153 | FL finished in 123.52036191400839
INFO flwr 2024-04-07 01:36:04,147 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:36:04,147 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:36:04,147 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:36:04,147 | app.py:229 | app_fit: losses_centralized [(0, 2.298717498779297), (1, 1.8258107900619507), (2, 1.6777806282043457), (3, 1.6255565881729126), (4, 1.598265290260315), (5, 1.5925575494766235), (6, 1.5960569381713867), (7, 1.5857892036437988), (8, 1.5780075788497925), (9, 1.5763713121414185), (10, 1.5763455629348755)]
INFO flwr 2024-04-07 01:36:04,147 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1272), (1, 0.7478), (2, 0.8176), (3, 0.864), (4, 0.8829), (5, 0.8854), (6, 0.8782), (7, 0.8877), (8, 0.8961), (9, 0.895), (10, 0.8944)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8944
wandb:     loss 1.57635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_013339-qmtycx45
wandb: Find logs at: ./wandb/offline-run-20240407_013339-qmtycx45/logs
INFO flwr 2024-04-07 01:36:07,741 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:43:16,778 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1569130)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1569130)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:43:21,565	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:43:22,046	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:43:22,516	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fbd150655c80c3ba.zip' (10.87MiB) to Ray cluster...
2024-04-07 01:43:22,557	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fbd150655c80c3ba.zip'.
INFO flwr 2024-04-07 01:43:33,866 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 121609047450.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 56403877478.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 01:43:33,867 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:43:33,867 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:43:33,884 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:43:33,886 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:43:33,886 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:43:33,886 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:43:35,951 | server.py:94 | initial parameters (loss, other metrics): 2.307264804840088, {'accuracy': 0.0483, 'data_size': 10000}
INFO flwr 2024-04-07 01:43:35,952 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:43:35,952 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1573570)[0m 2024-04-07 01:43:40.008909: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1573570)[0m 2024-04-07 01:43:40.096139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1573570)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1573559)[0m 2024-04-07 01:43:42.300247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1573559)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1573559)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1573565)[0m 2024-04-07 01:43:40.074405: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1573566)[0m 2024-04-07 01:43:40.254905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1573566)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1573565)[0m 2024-04-07 01:43:42.289480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:43:55,595 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:43:55,636 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:43:57,050 | server.py:125 | fit progress: (1, 1.9300965070724487, {'accuracy': 0.5476, 'data_size': 10000}, 21.097629672993207)
INFO flwr 2024-04-07 01:43:57,050 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:43:57,050 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:44:07,706 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:44:09,234 | server.py:125 | fit progress: (2, 1.6528252363204956, {'accuracy': 0.8339, 'data_size': 10000}, 33.281855257984716)
INFO flwr 2024-04-07 01:44:09,234 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:44:09,234 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:44:18,453 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:44:19,920 | server.py:125 | fit progress: (3, 1.6105093955993652, {'accuracy': 0.8693, 'data_size': 10000}, 43.96786896698177)
INFO flwr 2024-04-07 01:44:19,920 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:44:19,920 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:44:29,443 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:44:30,983 | server.py:125 | fit progress: (4, 1.6011239290237427, {'accuracy': 0.8755, 'data_size': 10000}, 55.031415874982486)
INFO flwr 2024-04-07 01:44:30,984 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:44:30,984 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:44:39,939 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:44:41,509 | server.py:125 | fit progress: (5, 1.5904757976531982, {'accuracy': 0.8849, 'data_size': 10000}, 65.55729974000133)
INFO flwr 2024-04-07 01:44:41,510 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:44:41,510 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:44:50,949 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:44:52,286 | server.py:125 | fit progress: (6, 1.583005428314209, {'accuracy': 0.8898, 'data_size': 10000}, 76.33405524399132)
INFO flwr 2024-04-07 01:44:52,286 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:44:52,287 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:45:01,339 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:45:02,698 | server.py:125 | fit progress: (7, 1.5809813737869263, {'accuracy': 0.8888, 'data_size': 10000}, 86.74586019400158)
INFO flwr 2024-04-07 01:45:02,698 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:45:02,698 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:45:12,359 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:45:13,743 | server.py:125 | fit progress: (8, 1.5733933448791504, {'accuracy': 0.8955, 'data_size': 10000}, 97.79072890599491)
INFO flwr 2024-04-07 01:45:13,743 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:45:13,743 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:45:22,940 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:45:24,606 | server.py:125 | fit progress: (9, 1.5802772045135498, {'accuracy': 0.888, 'data_size': 10000}, 108.65399178798543)
INFO flwr 2024-04-07 01:45:24,606 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:45:24,606 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:45:33,612 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:45:35,279 | server.py:125 | fit progress: (10, 1.5715453624725342, {'accuracy': 0.8954, 'data_size': 10000}, 119.32708495200495)
INFO flwr 2024-04-07 01:45:35,279 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:45:35,279 | server.py:153 | FL finished in 119.3274630009837
INFO flwr 2024-04-07 01:45:35,280 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:45:35,280 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:45:35,280 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:45:35,280 | app.py:229 | app_fit: losses_centralized [(0, 2.307264804840088), (1, 1.9300965070724487), (2, 1.6528252363204956), (3, 1.6105093955993652), (4, 1.6011239290237427), (5, 1.5904757976531982), (6, 1.583005428314209), (7, 1.5809813737869263), (8, 1.5733933448791504), (9, 1.5802772045135498), (10, 1.5715453624725342)]
INFO flwr 2024-04-07 01:45:35,280 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0483), (1, 0.5476), (2, 0.8339), (3, 0.8693), (4, 0.8755), (5, 0.8849), (6, 0.8898), (7, 0.8888), (8, 0.8955), (9, 0.888), (10, 0.8954)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8954
wandb:     loss 1.57155
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_014316-g1j4kh79
wandb: Find logs at: ./wandb/offline-run-20240407_014316-g1j4kh79/logs
INFO flwr 2024-04-07 01:45:38,832 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 01:52:47,702 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1573567)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1573567)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 01:52:53,780	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 01:52:54,132	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 01:52:54,489	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7f7e0a06041c59ce.zip' (10.89MiB) to Ray cluster...
2024-04-07 01:52:54,526	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7f7e0a06041c59ce.zip'.
INFO flwr 2024-04-07 01:53:05,876 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 121258053428.0, 'object_store_memory': 56253451468.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 01:53:05,876 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 01:53:05,876 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 01:53:05,895 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 01:53:05,896 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 01:53:05,896 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 01:53:05,896 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 01:53:08,123 | server.py:94 | initial parameters (loss, other metrics): 2.302804708480835, {'accuracy': 0.0685, 'data_size': 10000}
INFO flwr 2024-04-07 01:53:08,124 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 01:53:08,124 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1581004)[0m 2024-04-07 01:53:11.971066: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1581004)[0m 2024-04-07 01:53:12.085770: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1581004)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1581001)[0m 2024-04-07 01:53:14.280490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1581003)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1581003)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1580995)[0m 2024-04-07 01:53:12.449475: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1580995)[0m 2024-04-07 01:53:12.550635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1580995)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1580995)[0m 2024-04-07 01:53:14.847590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 01:53:32,474 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 01:53:32,541 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 01:53:33,692 | server.py:125 | fit progress: (1, 2.2597455978393555, {'accuracy': 0.3788, 'data_size': 10000}, 25.568190067016985)
INFO flwr 2024-04-07 01:53:33,693 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 01:53:33,693 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:53:47,425 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 01:53:48,867 | server.py:125 | fit progress: (2, 2.1831955909729004, {'accuracy': 0.5918, 'data_size': 10000}, 40.743430668022484)
INFO flwr 2024-04-07 01:53:48,868 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 01:53:48,868 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:54:01,841 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 01:54:03,369 | server.py:125 | fit progress: (3, 2.105936288833618, {'accuracy': 0.5849, 'data_size': 10000}, 55.244632771005854)
INFO flwr 2024-04-07 01:54:03,369 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 01:54:03,369 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:54:16,142 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 01:54:17,694 | server.py:125 | fit progress: (4, 2.0284652709960938, {'accuracy': 0.6632, 'data_size': 10000}, 69.5698793370102)
INFO flwr 2024-04-07 01:54:17,694 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 01:54:17,694 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:54:32,465 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 01:54:33,973 | server.py:125 | fit progress: (5, 1.9581882953643799, {'accuracy': 0.7025, 'data_size': 10000}, 85.8487699800171)
INFO flwr 2024-04-07 01:54:33,973 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 01:54:33,973 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:54:47,984 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 01:54:49,613 | server.py:125 | fit progress: (6, 1.9117119312286377, {'accuracy': 0.7784, 'data_size': 10000}, 101.48916574401665)
INFO flwr 2024-04-07 01:54:49,613 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 01:54:49,614 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:55:02,836 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 01:55:04,479 | server.py:125 | fit progress: (7, 1.888853907585144, {'accuracy': 0.7377, 'data_size': 10000}, 116.35466814402025)
INFO flwr 2024-04-07 01:55:04,479 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 01:55:04,479 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:55:17,520 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 01:55:19,177 | server.py:125 | fit progress: (8, 1.8464114665985107, {'accuracy': 0.7992, 'data_size': 10000}, 131.0532994090172)
INFO flwr 2024-04-07 01:55:19,178 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 01:55:19,178 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:55:32,204 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 01:55:33,657 | server.py:125 | fit progress: (9, 1.8240851163864136, {'accuracy': 0.8035, 'data_size': 10000}, 145.5329705470067)
INFO flwr 2024-04-07 01:55:33,657 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 01:55:33,658 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 01:55:45,463 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 01:55:47,203 | server.py:125 | fit progress: (10, 1.8024189472198486, {'accuracy': 0.836, 'data_size': 10000}, 159.0787760350213)
INFO flwr 2024-04-07 01:55:47,203 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 01:55:47,203 | server.py:153 | FL finished in 159.07919175902498
INFO flwr 2024-04-07 01:55:47,203 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 01:55:47,204 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 01:55:47,204 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 01:55:47,204 | app.py:229 | app_fit: losses_centralized [(0, 2.302804708480835), (1, 2.2597455978393555), (2, 2.1831955909729004), (3, 2.105936288833618), (4, 2.0284652709960938), (5, 1.9581882953643799), (6, 1.9117119312286377), (7, 1.888853907585144), (8, 1.8464114665985107), (9, 1.8240851163864136), (10, 1.8024189472198486)]
INFO flwr 2024-04-07 01:55:47,204 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0685), (1, 0.3788), (2, 0.5918), (3, 0.5849), (4, 0.6632), (5, 0.7025), (6, 0.7784), (7, 0.7377), (8, 0.7992), (9, 0.8035), (10, 0.836)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.836
wandb:     loss 1.80242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_015247-ka17qpxj
wandb: Find logs at: ./wandb/offline-run-20240407_015247-ka17qpxj/logs
INFO flwr 2024-04-07 01:55:50,838 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:03:00,340 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1580995)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1580995)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:03:05,109	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:03:05,678	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:03:06,132	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_75b5cf40db7648f7.zip' (10.91MiB) to Ray cluster...
2024-04-07 02:03:06,165	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_75b5cf40db7648f7.zip'.
INFO flwr 2024-04-07 02:03:17,218 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 126437529805.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58473227059.0, 'CPU': 64.0}
INFO flwr 2024-04-07 02:03:17,218 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:03:17,219 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:03:17,233 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:03:17,234 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:03:17,234 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:03:17,234 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:03:20,332 | server.py:94 | initial parameters (loss, other metrics): 2.3021957874298096, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-04-07 02:03:20,332 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:03:20,333 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1586186)[0m 2024-04-07 02:03:23.133041: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1586194)[0m 2024-04-07 02:03:23.321575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1586194)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1586200)[0m 2024-04-07 02:03:25.551374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1586194)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1586194)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1586199)[0m 2024-04-07 02:03:23.563494: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1586199)[0m 2024-04-07 02:03:23.647541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1586199)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1586186)[0m 2024-04-07 02:03:25.935209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:03:46,756 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:03:46,787 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:03:48,139 | server.py:125 | fit progress: (1, 1.9011573791503906, {'accuracy': 0.6018, 'data_size': 10000}, 27.806265833991347)
INFO flwr 2024-04-07 02:03:48,139 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:03:48,139 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:04:01,311 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:04:02,732 | server.py:125 | fit progress: (2, 1.7149105072021484, {'accuracy': 0.7789, 'data_size': 10000}, 42.399085854995064)
INFO flwr 2024-04-07 02:04:02,732 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:04:02,732 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:04:15,885 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:04:17,088 | server.py:125 | fit progress: (3, 1.6635351181030273, {'accuracy': 0.8379, 'data_size': 10000}, 56.755328282975825)
INFO flwr 2024-04-07 02:04:17,088 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:04:17,088 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:04:30,768 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:04:32,274 | server.py:125 | fit progress: (4, 1.602497935295105, {'accuracy': 0.899, 'data_size': 10000}, 71.94122016397887)
INFO flwr 2024-04-07 02:04:32,274 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:04:32,274 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:04:45,244 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:04:46,722 | server.py:125 | fit progress: (5, 1.5932189226150513, {'accuracy': 0.8996, 'data_size': 10000}, 86.38945727798273)
INFO flwr 2024-04-07 02:04:46,722 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:04:46,722 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:04:59,356 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:05:01,124 | server.py:125 | fit progress: (6, 1.5846832990646362, {'accuracy': 0.9049, 'data_size': 10000}, 100.79121393698733)
INFO flwr 2024-04-07 02:05:01,124 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:05:01,124 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:05:15,364 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:05:16,731 | server.py:125 | fit progress: (7, 1.6048392057418823, {'accuracy': 0.8807, 'data_size': 10000}, 116.39880294998875)
INFO flwr 2024-04-07 02:05:16,732 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:05:16,732 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:05:28,974 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:05:30,645 | server.py:125 | fit progress: (8, 1.5753662586212158, {'accuracy': 0.9101, 'data_size': 10000}, 130.3130740269844)
INFO flwr 2024-04-07 02:05:30,646 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:05:30,646 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:05:43,976 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:05:45,429 | server.py:125 | fit progress: (9, 1.5725926160812378, {'accuracy': 0.9101, 'data_size': 10000}, 145.09666528299567)
INFO flwr 2024-04-07 02:05:45,429 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:05:45,430 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:05:58,654 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:06:00,147 | server.py:125 | fit progress: (10, 1.5721211433410645, {'accuracy': 0.9084, 'data_size': 10000}, 159.81454922497505)
INFO flwr 2024-04-07 02:06:00,147 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:06:00,147 | server.py:153 | FL finished in 159.81505575499614
INFO flwr 2024-04-07 02:06:00,148 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:06:00,148 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:06:00,148 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:06:00,148 | app.py:229 | app_fit: losses_centralized [(0, 2.3021957874298096), (1, 1.9011573791503906), (2, 1.7149105072021484), (3, 1.6635351181030273), (4, 1.602497935295105), (5, 1.5932189226150513), (6, 1.5846832990646362), (7, 1.6048392057418823), (8, 1.5753662586212158), (9, 1.5725926160812378), (10, 1.5721211433410645)]
INFO flwr 2024-04-07 02:06:00,148 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.6018), (2, 0.7789), (3, 0.8379), (4, 0.899), (5, 0.8996), (6, 0.9049), (7, 0.8807), (8, 0.9101), (9, 0.9101), (10, 0.9084)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9084
wandb:     loss 1.57212
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_020259-o8euza1o
wandb: Find logs at: ./wandb/offline-run-20240407_020259-o8euza1o/logs
INFO flwr 2024-04-07 02:06:03,811 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:13:13,707 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1586186)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1586186)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:13:19,759	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:13:20,193	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:13:20,642	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3d98125f15fadccc.zip' (10.93MiB) to Ray cluster...
2024-04-07 02:13:20,686	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3d98125f15fadccc.zip'.
INFO flwr 2024-04-07 02:13:31,890 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 113443984384.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 52904564736.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 02:13:31,890 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:13:31,890 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:13:31,905 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:13:31,906 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:13:31,906 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:13:31,906 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:13:33,994 | server.py:94 | initial parameters (loss, other metrics): 2.3001112937927246, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-07 02:13:33,995 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:13:33,995 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1593667)[0m 2024-04-07 02:13:39.591542: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1593667)[0m 2024-04-07 02:13:39.652965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1593667)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1593667)[0m 2024-04-07 02:13:42.383849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1593667)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1593667)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1593677)[0m 2024-04-07 02:13:40.652880: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593677)[0m 2024-04-07 02:13:40.755544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593677)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1593671)[0m 2024-04-07 02:13:43.219798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:14:01,858 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:14:01,905 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:14:03,285 | server.py:125 | fit progress: (1, 1.8231868743896484, {'accuracy': 0.6512, 'data_size': 10000}, 29.289571238972712)
INFO flwr 2024-04-07 02:14:03,285 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:14:03,285 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:14:18,211 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:14:19,666 | server.py:125 | fit progress: (2, 1.6587110757827759, {'accuracy': 0.8449, 'data_size': 10000}, 45.6712487579789)
INFO flwr 2024-04-07 02:14:19,667 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:14:19,667 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:14:32,138 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:14:33,845 | server.py:125 | fit progress: (3, 1.607328176498413, {'accuracy': 0.8873, 'data_size': 10000}, 59.850409325998044)
INFO flwr 2024-04-07 02:14:33,846 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:14:33,846 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:14:46,634 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:14:48,138 | server.py:125 | fit progress: (4, 1.5913370847702026, {'accuracy': 0.8928, 'data_size': 10000}, 74.14281627399032)
INFO flwr 2024-04-07 02:14:48,138 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:14:48,138 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:15:01,467 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:15:03,051 | server.py:125 | fit progress: (5, 1.5800716876983643, {'accuracy': 0.9016, 'data_size': 10000}, 89.05569827198633)
INFO flwr 2024-04-07 02:15:03,051 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:15:03,051 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:15:16,919 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:15:18,551 | server.py:125 | fit progress: (6, 1.571484088897705, {'accuracy': 0.9081, 'data_size': 10000}, 104.55547888399451)
INFO flwr 2024-04-07 02:15:18,551 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:15:18,551 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:15:31,015 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:15:32,386 | server.py:125 | fit progress: (7, 1.5708489418029785, {'accuracy': 0.9065, 'data_size': 10000}, 118.39101795799797)
INFO flwr 2024-04-07 02:15:32,386 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:15:32,387 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:15:45,653 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:15:47,288 | server.py:125 | fit progress: (8, 1.5730528831481934, {'accuracy': 0.9023, 'data_size': 10000}, 133.2927497169876)
INFO flwr 2024-04-07 02:15:47,288 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:15:47,288 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:16:01,608 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:16:03,074 | server.py:125 | fit progress: (9, 1.5666917562484741, {'accuracy': 0.9081, 'data_size': 10000}, 149.0790244039963)
INFO flwr 2024-04-07 02:16:03,074 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:16:03,075 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:16:18,014 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:16:19,519 | server.py:125 | fit progress: (10, 1.5689036846160889, {'accuracy': 0.9105, 'data_size': 10000}, 165.52358742797514)
INFO flwr 2024-04-07 02:16:19,519 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:16:19,519 | server.py:153 | FL finished in 165.52416979699046
INFO flwr 2024-04-07 02:16:19,519 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:16:19,520 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:16:19,520 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:16:19,520 | app.py:229 | app_fit: losses_centralized [(0, 2.3001112937927246), (1, 1.8231868743896484), (2, 1.6587110757827759), (3, 1.607328176498413), (4, 1.5913370847702026), (5, 1.5800716876983643), (6, 1.571484088897705), (7, 1.5708489418029785), (8, 1.5730528831481934), (9, 1.5666917562484741), (10, 1.5689036846160889)]
INFO flwr 2024-04-07 02:16:19,520 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.6512), (2, 0.8449), (3, 0.8873), (4, 0.8928), (5, 0.9016), (6, 0.9081), (7, 0.9065), (8, 0.9023), (9, 0.9081), (10, 0.9105)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9105
wandb:     loss 1.5689
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_021313-ihlsnere
wandb: Find logs at: ./wandb/offline-run-20240407_021313-ihlsnere/logs
INFO flwr 2024-04-07 02:16:23,020 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:23:32,953 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1593671)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1593671)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:23:37,803	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:23:38,090	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:23:38,447	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9f065983c164e502.zip' (10.95MiB) to Ray cluster...
2024-04-07 02:23:38,478	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9f065983c164e502.zip'.
INFO flwr 2024-04-07 02:23:49,804 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 57299903692.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 123699775284.0}
INFO flwr 2024-04-07 02:23:49,804 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:23:49,805 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:23:49,830 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:23:49,833 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:23:49,833 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:23:49,834 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:23:51,831 | server.py:94 | initial parameters (loss, other metrics): 2.3038723468780518, {'accuracy': 0.091, 'data_size': 10000}
INFO flwr 2024-04-07 02:23:51,831 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:23:51,832 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1601057)[0m 2024-04-07 02:23:55.750591: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1601057)[0m 2024-04-07 02:23:55.842149: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1601057)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1601057)[0m 2024-04-07 02:23:57.667493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1601067)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1601067)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1601064)[0m 2024-04-07 02:23:56.462551: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1601064)[0m 2024-04-07 02:23:56.554264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1601064)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1601069)[0m 2024-04-07 02:23:58.883666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:24:15,635 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:24:15,686 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:24:17,094 | server.py:125 | fit progress: (1, 1.8076456785202026, {'accuracy': 0.6835, 'data_size': 10000}, 25.262419134000083)
INFO flwr 2024-04-07 02:24:17,094 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:24:17,095 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:24:32,024 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:24:33,484 | server.py:125 | fit progress: (2, 1.6206485033035278, {'accuracy': 0.8718, 'data_size': 10000}, 41.652346963004675)
INFO flwr 2024-04-07 02:24:33,484 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:24:33,484 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:24:46,738 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:24:47,930 | server.py:125 | fit progress: (3, 1.603227138519287, {'accuracy': 0.8804, 'data_size': 10000}, 56.09826913100551)
INFO flwr 2024-04-07 02:24:47,930 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:24:47,931 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:01,349 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:25:02,826 | server.py:125 | fit progress: (4, 1.5766280889511108, {'accuracy': 0.9025, 'data_size': 10000}, 70.9943252959929)
INFO flwr 2024-04-07 02:25:02,826 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:25:02,826 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:15,232 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:25:16,761 | server.py:125 | fit progress: (5, 1.5785990953445435, {'accuracy': 0.8977, 'data_size': 10000}, 84.92941319398233)
INFO flwr 2024-04-07 02:25:16,761 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:25:16,762 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:29,303 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:25:30,884 | server.py:125 | fit progress: (6, 1.5680493116378784, {'accuracy': 0.9041, 'data_size': 10000}, 99.05198455398204)
INFO flwr 2024-04-07 02:25:30,884 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:25:30,884 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:44,490 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:25:45,850 | server.py:125 | fit progress: (7, 1.5629087686538696, {'accuracy': 0.9103, 'data_size': 10000}, 114.01822802898823)
INFO flwr 2024-04-07 02:25:45,850 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:25:45,850 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:25:58,361 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:26:00,015 | server.py:125 | fit progress: (8, 1.5596377849578857, {'accuracy': 0.9133, 'data_size': 10000}, 128.18329190500663)
INFO flwr 2024-04-07 02:26:00,015 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:26:00,015 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:26:13,442 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:26:14,881 | server.py:125 | fit progress: (9, 1.5717500448226929, {'accuracy': 0.9015, 'data_size': 10000}, 143.04972628099495)
INFO flwr 2024-04-07 02:26:14,882 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:26:14,882 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:26:28,446 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:26:29,904 | server.py:125 | fit progress: (10, 1.5653374195098877, {'accuracy': 0.9061, 'data_size': 10000}, 158.07260552200023)
INFO flwr 2024-04-07 02:26:29,904 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:26:29,905 | server.py:153 | FL finished in 158.0730363719922
INFO flwr 2024-04-07 02:26:29,905 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:26:29,905 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:26:29,905 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:26:29,905 | app.py:229 | app_fit: losses_centralized [(0, 2.3038723468780518), (1, 1.8076456785202026), (2, 1.6206485033035278), (3, 1.603227138519287), (4, 1.5766280889511108), (5, 1.5785990953445435), (6, 1.5680493116378784), (7, 1.5629087686538696), (8, 1.5596377849578857), (9, 1.5717500448226929), (10, 1.5653374195098877)]
INFO flwr 2024-04-07 02:26:29,905 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.091), (1, 0.6835), (2, 0.8718), (3, 0.8804), (4, 0.9025), (5, 0.8977), (6, 0.9041), (7, 0.9103), (8, 0.9133), (9, 0.9015), (10, 0.9061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9061
wandb:     loss 1.56534
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_022332-67s584q4
wandb: Find logs at: ./wandb/offline-run-20240407_022332-67s584q4/logs
INFO flwr 2024-04-07 02:26:33,530 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:33:43,766 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1601069)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1601069)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:33:50,536	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:33:51,043	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:33:51,411	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9fe827f1852f9934.zip' (10.97MiB) to Ray cluster...
2024-04-07 02:33:51,448	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9fe827f1852f9934.zip'.
INFO flwr 2024-04-07 02:34:02,508 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 125602119476.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58115194060.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 02:34:02,508 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:34:02,508 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:34:02,525 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:34:02,526 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:34:02,527 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:34:02,527 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:34:05,199 | server.py:94 | initial parameters (loss, other metrics): 2.304962635040283, {'accuracy': 0.1067, 'data_size': 10000}
INFO flwr 2024-04-07 02:34:05,200 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:34:05,200 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1605883)[0m 2024-04-07 02:34:08.727878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1605883)[0m 2024-04-07 02:34:08.827528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1605883)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1605881)[0m 2024-04-07 02:34:11.016302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1605881)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1605881)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1605879)[0m 2024-04-07 02:34:08.908629: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605879)[0m 2024-04-07 02:34:08.983867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605879)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1605882)[0m 2024-04-07 02:34:11.049137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:34:32,506 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:34:32,553 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:34:33,910 | server.py:125 | fit progress: (1, 1.820802092552185, {'accuracy': 0.6712, 'data_size': 10000}, 28.709896028012736)
INFO flwr 2024-04-07 02:34:33,910 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:34:33,911 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:34:48,364 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:34:49,857 | server.py:125 | fit progress: (2, 1.6335630416870117, {'accuracy': 0.8515, 'data_size': 10000}, 44.657280198007356)
INFO flwr 2024-04-07 02:34:49,858 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:34:49,858 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:35:02,876 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:35:04,099 | server.py:125 | fit progress: (3, 1.6454720497131348, {'accuracy': 0.8248, 'data_size': 10000}, 58.89912674299558)
INFO flwr 2024-04-07 02:35:04,100 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:35:04,100 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:35:16,755 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:35:18,257 | server.py:125 | fit progress: (4, 1.5765403509140015, {'accuracy': 0.8989, 'data_size': 10000}, 73.05691770699923)
INFO flwr 2024-04-07 02:35:18,257 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:35:18,258 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:35:31,795 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:35:33,380 | server.py:125 | fit progress: (5, 1.575280785560608, {'accuracy': 0.8981, 'data_size': 10000}, 88.18028052899172)
INFO flwr 2024-04-07 02:35:33,381 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:35:33,382 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:35:46,287 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:35:47,909 | server.py:125 | fit progress: (6, 1.5731151103973389, {'accuracy': 0.8986, 'data_size': 10000}, 102.70860654101125)
INFO flwr 2024-04-07 02:35:47,909 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:35:47,909 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:36:01,532 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:36:02,863 | server.py:125 | fit progress: (7, 1.5616751909255981, {'accuracy': 0.9086, 'data_size': 10000}, 117.66266175801866)
INFO flwr 2024-04-07 02:36:02,863 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:36:02,863 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:36:15,779 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:36:17,521 | server.py:125 | fit progress: (8, 1.5577551126480103, {'accuracy': 0.9127, 'data_size': 10000}, 132.32095535000553)
INFO flwr 2024-04-07 02:36:17,521 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:36:17,522 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:36:31,469 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:36:32,872 | server.py:125 | fit progress: (9, 1.569136142730713, {'accuracy': 0.8988, 'data_size': 10000}, 147.6717543550185)
INFO flwr 2024-04-07 02:36:32,872 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:36:32,872 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:36:46,602 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:36:48,007 | server.py:125 | fit progress: (10, 1.5550042390823364, {'accuracy': 0.9128, 'data_size': 10000}, 162.80671017599525)
INFO flwr 2024-04-07 02:36:48,007 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:36:48,007 | server.py:153 | FL finished in 162.80717222898966
INFO flwr 2024-04-07 02:36:48,008 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:36:48,008 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:36:48,008 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:36:48,008 | app.py:229 | app_fit: losses_centralized [(0, 2.304962635040283), (1, 1.820802092552185), (2, 1.6335630416870117), (3, 1.6454720497131348), (4, 1.5765403509140015), (5, 1.575280785560608), (6, 1.5731151103973389), (7, 1.5616751909255981), (8, 1.5577551126480103), (9, 1.569136142730713), (10, 1.5550042390823364)]
INFO flwr 2024-04-07 02:36:48,008 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1067), (1, 0.6712), (2, 0.8515), (3, 0.8248), (4, 0.8989), (5, 0.8981), (6, 0.8986), (7, 0.9086), (8, 0.9127), (9, 0.8988), (10, 0.9128)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9128
wandb:     loss 1.555
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_023342-0mjd0ekc
wandb: Find logs at: ./wandb/offline-run-20240407_023342-0mjd0ekc/logs
INFO flwr 2024-04-07 02:36:51,577 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:44:01,593 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1605882)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1605882)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:44:06,613	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:44:07,177	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:44:07,677	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5e2598ebf5a1e1f3.zip' (10.99MiB) to Ray cluster...
2024-04-07 02:44:07,704	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5e2598ebf5a1e1f3.zip'.
INFO flwr 2024-04-07 02:44:19,313 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 115569491354.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 53815496294.0}
INFO flwr 2024-04-07 02:44:19,313 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:44:19,313 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:44:19,329 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:44:19,330 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:44:19,331 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:44:19,331 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:44:21,389 | server.py:94 | initial parameters (loss, other metrics): 2.301093339920044, {'accuracy': 0.0993, 'data_size': 10000}
INFO flwr 2024-04-07 02:44:21,389 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:44:21,390 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1613300)[0m 2024-04-07 02:44:25.579418: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1613300)[0m 2024-04-07 02:44:25.679257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1613300)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1613298)[0m 2024-04-07 02:44:27.756267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1613303)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1613303)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1613299)[0m 2024-04-07 02:44:25.946931: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1613299)[0m 2024-04-07 02:44:26.042894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1613299)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1613303)[0m 2024-04-07 02:44:28.084894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:44:52,492 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:44:52,522 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:44:53,939 | server.py:125 | fit progress: (1, 1.7675050497055054, {'accuracy': 0.7243, 'data_size': 10000}, 32.54918016400188)
INFO flwr 2024-04-07 02:44:53,939 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:44:53,939 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:45:08,768 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:45:10,254 | server.py:125 | fit progress: (2, 1.6354674100875854, {'accuracy': 0.8445, 'data_size': 10000}, 48.86426887498237)
INFO flwr 2024-04-07 02:45:10,254 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:45:10,254 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:45:24,420 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:45:25,622 | server.py:125 | fit progress: (3, 1.6074302196502686, {'accuracy': 0.8689, 'data_size': 10000}, 64.23227062699152)
INFO flwr 2024-04-07 02:45:25,622 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:45:25,622 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:45:39,795 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:45:41,310 | server.py:125 | fit progress: (4, 1.5694698095321655, {'accuracy': 0.903, 'data_size': 10000}, 79.92029280998395)
INFO flwr 2024-04-07 02:45:41,310 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:45:41,311 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:45:54,654 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:45:56,231 | server.py:125 | fit progress: (5, 1.5735875368118286, {'accuracy': 0.8964, 'data_size': 10000}, 94.84087291700416)
INFO flwr 2024-04-07 02:45:56,231 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:45:56,231 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:46:10,943 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:46:12,515 | server.py:125 | fit progress: (6, 1.576857566833496, {'accuracy': 0.8932, 'data_size': 10000}, 111.12588583599427)
INFO flwr 2024-04-07 02:46:12,516 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:46:12,516 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:46:26,550 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:46:27,935 | server.py:125 | fit progress: (7, 1.5627520084381104, {'accuracy': 0.9043, 'data_size': 10000}, 126.5456754170009)
INFO flwr 2024-04-07 02:46:27,936 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:46:27,936 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:46:41,067 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:46:42,729 | server.py:125 | fit progress: (8, 1.5588432550430298, {'accuracy': 0.9086, 'data_size': 10000}, 141.33911818498746)
INFO flwr 2024-04-07 02:46:42,729 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:46:42,729 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:46:56,826 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:46:58,232 | server.py:125 | fit progress: (9, 1.559706687927246, {'accuracy': 0.9071, 'data_size': 10000}, 156.84221152600367)
INFO flwr 2024-04-07 02:46:58,232 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:46:58,232 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:47:12,145 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:47:13,610 | server.py:125 | fit progress: (10, 1.5578904151916504, {'accuracy': 0.9097, 'data_size': 10000}, 172.22048407999682)
INFO flwr 2024-04-07 02:47:13,610 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:47:13,610 | server.py:153 | FL finished in 172.22089762898395
INFO flwr 2024-04-07 02:47:13,611 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:47:13,611 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:47:13,611 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:47:13,611 | app.py:229 | app_fit: losses_centralized [(0, 2.301093339920044), (1, 1.7675050497055054), (2, 1.6354674100875854), (3, 1.6074302196502686), (4, 1.5694698095321655), (5, 1.5735875368118286), (6, 1.576857566833496), (7, 1.5627520084381104), (8, 1.5588432550430298), (9, 1.559706687927246), (10, 1.5578904151916504)]
INFO flwr 2024-04-07 02:47:13,611 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0993), (1, 0.7243), (2, 0.8445), (3, 0.8689), (4, 0.903), (5, 0.8964), (6, 0.8932), (7, 0.9043), (8, 0.9086), (9, 0.9071), (10, 0.9097)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9097
wandb:     loss 1.55789
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_024401-yxwmow3l
wandb: Find logs at: ./wandb/offline-run-20240407_024401-yxwmow3l/logs
INFO flwr 2024-04-07 02:47:17,146 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 02:54:27,052 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1613297)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1613297)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 02:54:33,376	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 02:54:33,790	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 02:54:34,133	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c4f78cb0cf6add22.zip' (11.01MiB) to Ray cluster...
2024-04-07 02:54:34,157	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c4f78cb0cf6add22.zip'.
INFO flwr 2024-04-07 02:54:45,215 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 119830812877.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55641776947.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 02:54:45,216 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 02:54:45,216 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 02:54:45,233 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 02:54:45,234 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 02:54:45,234 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 02:54:45,234 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 02:54:47,331 | server.py:94 | initial parameters (loss, other metrics): 2.2989494800567627, {'accuracy': 0.1601, 'data_size': 10000}
INFO flwr 2024-04-07 02:54:47,331 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 02:54:47,332 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1617802)[0m 2024-04-07 02:54:51.074776: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1617725)[0m 2024-04-07 02:54:51.199331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1617725)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1617802)[0m 2024-04-07 02:54:53.509761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1617802)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1617802)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1617723)[0m 2024-04-07 02:54:51.881056: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1617723)[0m 2024-04-07 02:54:51.976740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1617723)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1617723)[0m 2024-04-07 02:54:54.364011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 02:55:12,884 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 02:55:12,925 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 02:55:14,254 | server.py:125 | fit progress: (1, 1.7258915901184082, {'accuracy': 0.7762, 'data_size': 10000}, 26.922585722000804)
INFO flwr 2024-04-07 02:55:14,254 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 02:55:14,255 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:55:28,089 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 02:55:29,519 | server.py:125 | fit progress: (2, 1.6000028848648071, {'accuracy': 0.8776, 'data_size': 10000}, 42.18733139001415)
INFO flwr 2024-04-07 02:55:29,519 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 02:55:29,519 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:55:43,505 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 02:55:45,000 | server.py:125 | fit progress: (3, 1.5894182920455933, {'accuracy': 0.8815, 'data_size': 10000}, 57.668725911004)
INFO flwr 2024-04-07 02:55:45,001 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 02:55:45,001 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:55:58,976 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 02:56:00,501 | server.py:125 | fit progress: (4, 1.6031345129013062, {'accuracy': 0.8666, 'data_size': 10000}, 73.16922201201669)
INFO flwr 2024-04-07 02:56:00,501 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 02:56:00,501 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:56:16,380 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 02:56:17,620 | server.py:125 | fit progress: (5, 1.5619863271713257, {'accuracy': 0.9048, 'data_size': 10000}, 90.288607754017)
INFO flwr 2024-04-07 02:56:17,620 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 02:56:17,621 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:56:32,258 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 02:56:33,539 | server.py:125 | fit progress: (6, 1.5633660554885864, {'accuracy': 0.9041, 'data_size': 10000}, 106.20760920402245)
INFO flwr 2024-04-07 02:56:33,540 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 02:56:33,540 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:56:47,375 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 02:56:48,699 | server.py:125 | fit progress: (7, 1.557698130607605, {'accuracy': 0.908, 'data_size': 10000}, 121.36774688999867)
INFO flwr 2024-04-07 02:56:48,700 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 02:56:48,700 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:57:02,227 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 02:57:03,845 | server.py:125 | fit progress: (8, 1.558803915977478, {'accuracy': 0.9073, 'data_size': 10000}, 136.51306833702256)
INFO flwr 2024-04-07 02:57:03,845 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 02:57:03,845 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:57:17,519 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 02:57:19,194 | server.py:125 | fit progress: (9, 1.552141547203064, {'accuracy': 0.9135, 'data_size': 10000}, 151.86223902000347)
INFO flwr 2024-04-07 02:57:19,194 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 02:57:19,194 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 02:57:32,892 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 02:57:34,612 | server.py:125 | fit progress: (10, 1.555568814277649, {'accuracy': 0.9087, 'data_size': 10000}, 167.28066232602578)
INFO flwr 2024-04-07 02:57:34,613 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 02:57:34,613 | server.py:153 | FL finished in 167.28109204900102
INFO flwr 2024-04-07 02:57:34,613 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 02:57:34,613 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 02:57:34,613 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 02:57:34,613 | app.py:229 | app_fit: losses_centralized [(0, 2.2989494800567627), (1, 1.7258915901184082), (2, 1.6000028848648071), (3, 1.5894182920455933), (4, 1.6031345129013062), (5, 1.5619863271713257), (6, 1.5633660554885864), (7, 1.557698130607605), (8, 1.558803915977478), (9, 1.552141547203064), (10, 1.555568814277649)]
INFO flwr 2024-04-07 02:57:34,613 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1601), (1, 0.7762), (2, 0.8776), (3, 0.8815), (4, 0.8666), (5, 0.9048), (6, 0.9041), (7, 0.908), (8, 0.9073), (9, 0.9135), (10, 0.9087)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9087
wandb:     loss 1.55557
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_025426-q6ecw4v0
wandb: Find logs at: ./wandb/offline-run-20240407_025426-q6ecw4v0/logs
INFO flwr 2024-04-07 02:57:38,153 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:04:46,980 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1617721)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1617721)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 03:04:51,972	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:04:52,530	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:04:53,064	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_78764294835231b7.zip' (11.03MiB) to Ray cluster...
2024-04-07 03:04:53,089	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_78764294835231b7.zip'.
INFO flwr 2024-04-07 03:05:04,316 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 54816272793.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 117904636519.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 03:05:04,316 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:05:04,316 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:05:04,332 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:05:04,333 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:05:04,333 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:05:04,333 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 03:05:06,399 | server.py:94 | initial parameters (loss, other metrics): 2.3026866912841797, {'accuracy': 0.1045, 'data_size': 10000}
INFO flwr 2024-04-07 03:05:06,400 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:05:06,400 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1625525)[0m 2024-04-07 03:05:11.017840: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1625525)[0m 2024-04-07 03:05:11.123878: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1625525)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1625528)[0m 2024-04-07 03:05:13.525970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1625528)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1625528)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1625522)[0m 2024-04-07 03:05:11.252802: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1625522)[0m 2024-04-07 03:05:11.345686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1625522)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1625527)[0m 2024-04-07 03:05:13.823329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 03:05:43,443 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:05:43,511 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:05:44,968 | server.py:125 | fit progress: (1, 2.1910438537597656, {'accuracy': 0.3738, 'data_size': 10000}, 38.568489847006276)
INFO flwr 2024-04-07 03:05:44,969 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:05:44,969 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:06:05,831 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:06:07,281 | server.py:125 | fit progress: (2, 2.090291976928711, {'accuracy': 0.4475, 'data_size': 10000}, 60.88101722497959)
INFO flwr 2024-04-07 03:06:07,281 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:06:07,281 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:06:27,644 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:06:29,156 | server.py:125 | fit progress: (3, 1.9732258319854736, {'accuracy': 0.689, 'data_size': 10000}, 82.75612035099766)
INFO flwr 2024-04-07 03:06:29,156 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:06:29,157 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:06:49,556 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:06:51,087 | server.py:125 | fit progress: (4, 1.8974007368087769, {'accuracy': 0.7757, 'data_size': 10000}, 104.68750346300658)
INFO flwr 2024-04-07 03:06:51,088 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:06:51,088 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:07:10,691 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:07:11,971 | server.py:125 | fit progress: (5, 1.8368406295776367, {'accuracy': 0.806, 'data_size': 10000}, 125.57095677198959)
INFO flwr 2024-04-07 03:07:11,971 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:07:11,971 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:07:33,142 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:07:34,629 | server.py:125 | fit progress: (6, 1.7955747842788696, {'accuracy': 0.813, 'data_size': 10000}, 148.2287019979849)
INFO flwr 2024-04-07 03:07:34,629 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:07:34,629 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:07:56,620 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:07:57,967 | server.py:125 | fit progress: (7, 1.7629311084747314, {'accuracy': 0.8416, 'data_size': 10000}, 171.56666344299447)
INFO flwr 2024-04-07 03:07:57,967 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:07:57,967 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:08:19,083 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:08:20,823 | server.py:125 | fit progress: (8, 1.754028081893921, {'accuracy': 0.8337, 'data_size': 10000}, 194.42283641599352)
INFO flwr 2024-04-07 03:08:20,823 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:08:20,823 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:08:42,151 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:08:43,896 | server.py:125 | fit progress: (9, 1.7436025142669678, {'accuracy': 0.8345, 'data_size': 10000}, 217.49567076499807)
INFO flwr 2024-04-07 03:08:43,896 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:08:43,896 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:09:07,551 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:09:09,417 | server.py:125 | fit progress: (10, 1.71830415725708, {'accuracy': 0.8577, 'data_size': 10000}, 243.0168784239795)
INFO flwr 2024-04-07 03:09:09,417 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:09:09,417 | server.py:153 | FL finished in 243.0174979379808
INFO flwr 2024-04-07 03:09:09,418 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:09:09,418 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:09:09,418 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:09:09,418 | app.py:229 | app_fit: losses_centralized [(0, 2.3026866912841797), (1, 2.1910438537597656), (2, 2.090291976928711), (3, 1.9732258319854736), (4, 1.8974007368087769), (5, 1.8368406295776367), (6, 1.7955747842788696), (7, 1.7629311084747314), (8, 1.754028081893921), (9, 1.7436025142669678), (10, 1.71830415725708)]
INFO flwr 2024-04-07 03:09:09,418 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1045), (1, 0.3738), (2, 0.4475), (3, 0.689), (4, 0.7757), (5, 0.806), (6, 0.813), (7, 0.8416), (8, 0.8337), (9, 0.8345), (10, 0.8577)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8577
wandb:     loss 1.7183
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_030446-k2h4hrpy
wandb: Find logs at: ./wandb/offline-run-20240407_030446-k2h4hrpy/logs
INFO flwr 2024-04-07 03:09:13,040 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:16:21,889 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1625524)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1625524)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 03:16:27,649	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:16:28,049	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:16:28,384	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_abbb12c780f01ccf.zip' (11.05MiB) to Ray cluster...
2024-04-07 03:16:28,417	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_abbb12c780f01ccf.zip'.
INFO flwr 2024-04-07 03:16:39,825 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55237362892.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 118887180084.0}
INFO flwr 2024-04-07 03:16:39,825 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:16:39,825 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:16:39,843 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:16:39,845 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:16:39,846 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:16:39,846 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 03:16:41,889 | server.py:94 | initial parameters (loss, other metrics): 2.303633689880371, {'accuracy': 0.0891, 'data_size': 10000}
INFO flwr 2024-04-07 03:16:41,890 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:16:41,890 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1634212)[0m 2024-04-07 03:16:46.162302: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1634212)[0m 2024-04-07 03:16:46.261424: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1634212)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1634212)[0m 2024-04-07 03:16:48.333225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1634211)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1634211)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1634215)[0m 2024-04-07 03:16:46.576102: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1634215)[0m 2024-04-07 03:16:46.674501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1634215)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1634208)[0m 2024-04-07 03:16:48.997838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 03:17:17,841 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:17:17,875 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:17:19,030 | server.py:125 | fit progress: (1, 1.8128811120986938, {'accuracy': 0.7145, 'data_size': 10000}, 37.139920034998795)
INFO flwr 2024-04-07 03:17:19,030 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:17:19,030 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:17:40,744 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:17:42,274 | server.py:125 | fit progress: (2, 1.6738476753234863, {'accuracy': 0.8206, 'data_size': 10000}, 60.383757528994465)
INFO flwr 2024-04-07 03:17:42,274 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:17:42,274 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:18:01,665 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:18:03,181 | server.py:125 | fit progress: (3, 1.6359196901321411, {'accuracy': 0.8549, 'data_size': 10000}, 81.29126801001257)
INFO flwr 2024-04-07 03:18:03,182 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:18:03,182 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:18:26,706 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:18:28,297 | server.py:125 | fit progress: (4, 1.5852669477462769, {'accuracy': 0.9035, 'data_size': 10000}, 106.4075994080049)
INFO flwr 2024-04-07 03:18:28,298 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:18:28,298 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:18:52,452 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:18:53,744 | server.py:125 | fit progress: (5, 1.600236415863037, {'accuracy': 0.8854, 'data_size': 10000}, 131.853775622003)
INFO flwr 2024-04-07 03:18:53,744 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:18:53,744 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:19:14,303 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:19:15,618 | server.py:125 | fit progress: (6, 1.5723559856414795, {'accuracy': 0.9101, 'data_size': 10000}, 153.72854806800024)
INFO flwr 2024-04-07 03:19:15,619 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:19:15,619 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:19:38,886 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:19:40,519 | server.py:125 | fit progress: (7, 1.5734302997589111, {'accuracy': 0.9048, 'data_size': 10000}, 178.6294294600084)
INFO flwr 2024-04-07 03:19:40,520 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:19:40,520 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:20:02,063 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:20:03,793 | server.py:125 | fit progress: (8, 1.5695388317108154, {'accuracy': 0.9093, 'data_size': 10000}, 201.9032796100073)
INFO flwr 2024-04-07 03:20:03,793 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:20:03,794 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:20:23,624 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:20:25,401 | server.py:125 | fit progress: (9, 1.574278473854065, {'accuracy': 0.9038, 'data_size': 10000}, 223.51072808899335)
INFO flwr 2024-04-07 03:20:25,401 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:20:25,401 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:20:48,984 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:20:50,777 | server.py:125 | fit progress: (10, 1.5622104406356812, {'accuracy': 0.9147, 'data_size': 10000}, 248.88754180501564)
INFO flwr 2024-04-07 03:20:50,778 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:20:50,778 | server.py:153 | FL finished in 248.88797258900013
INFO flwr 2024-04-07 03:20:50,778 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:20:50,778 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:20:50,778 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:20:50,778 | app.py:229 | app_fit: losses_centralized [(0, 2.303633689880371), (1, 1.8128811120986938), (2, 1.6738476753234863), (3, 1.6359196901321411), (4, 1.5852669477462769), (5, 1.600236415863037), (6, 1.5723559856414795), (7, 1.5734302997589111), (8, 1.5695388317108154), (9, 1.574278473854065), (10, 1.5622104406356812)]
INFO flwr 2024-04-07 03:20:50,779 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0891), (1, 0.7145), (2, 0.8206), (3, 0.8549), (4, 0.9035), (5, 0.8854), (6, 0.9101), (7, 0.9048), (8, 0.9093), (9, 0.9038), (10, 0.9147)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9147
wandb:     loss 1.56221
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_031621-pubrbtq6
wandb: Find logs at: ./wandb/offline-run-20240407_031621-pubrbtq6/logs
INFO flwr 2024-04-07 03:20:54,424 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:28:03,423 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1634208)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1634208)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 03:28:10,079	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:28:10,564	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:28:11,057	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6c001197f56a42c1.zip' (11.07MiB) to Ray cluster...
2024-04-07 03:28:11,090	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6c001197f56a42c1.zip'.
INFO flwr 2024-04-07 03:28:22,150 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 124732044698.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57742304870.0}
INFO flwr 2024-04-07 03:28:22,150 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:28:22,151 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:28:22,168 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:28:22,169 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:28:22,169 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:28:22,169 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 03:28:24,293 | server.py:94 | initial parameters (loss, other metrics): 2.3023431301116943, {'accuracy': 0.0648, 'data_size': 10000}
INFO flwr 2024-04-07 03:28:24,294 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:28:24,294 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1641000)[0m 2024-04-07 03:28:28.360413: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1641012)[0m 2024-04-07 03:28:28.494018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1641012)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1641000)[0m 2024-04-07 03:28:30.570309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1641000)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1641000)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1641011)[0m 2024-04-07 03:28:28.601510: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1641011)[0m 2024-04-07 03:28:28.696289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1641011)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1641011)[0m 2024-04-07 03:28:30.991322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 03:29:00,156 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:29:00,188 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:29:01,322 | server.py:125 | fit progress: (1, 1.9410127401351929, {'accuracy': 0.5095, 'data_size': 10000}, 37.02856193599291)
INFO flwr 2024-04-07 03:29:01,323 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:29:01,323 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:29:23,859 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:29:25,287 | server.py:125 | fit progress: (2, 1.6452710628509521, {'accuracy': 0.8487, 'data_size': 10000}, 60.992885654006386)
INFO flwr 2024-04-07 03:29:25,287 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:29:25,287 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:29:48,181 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:29:49,642 | server.py:125 | fit progress: (3, 1.5893025398254395, {'accuracy': 0.8946, 'data_size': 10000}, 85.34860570999444)
INFO flwr 2024-04-07 03:29:49,643 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:29:49,643 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:30:11,794 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:30:13,310 | server.py:125 | fit progress: (4, 1.6027910709381104, {'accuracy': 0.8789, 'data_size': 10000}, 109.01643191100447)
INFO flwr 2024-04-07 03:30:13,310 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:30:13,311 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:30:33,355 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:30:34,647 | server.py:125 | fit progress: (5, 1.5776904821395874, {'accuracy': 0.9002, 'data_size': 10000}, 130.3537383429939)
INFO flwr 2024-04-07 03:30:34,648 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:30:34,648 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:30:56,761 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:30:58,085 | server.py:125 | fit progress: (6, 1.5716928243637085, {'accuracy': 0.9044, 'data_size': 10000}, 153.79160634599975)
INFO flwr 2024-04-07 03:30:58,086 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:30:58,086 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:31:20,045 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:31:21,649 | server.py:125 | fit progress: (7, 1.5620341300964355, {'accuracy': 0.9134, 'data_size': 10000}, 177.3551566459937)
INFO flwr 2024-04-07 03:31:21,649 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:31:21,649 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:31:48,405 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:31:50,017 | server.py:125 | fit progress: (8, 1.5602073669433594, {'accuracy': 0.9128, 'data_size': 10000}, 205.7233544900082)
INFO flwr 2024-04-07 03:31:50,017 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:31:50,018 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:32:10,698 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:32:12,398 | server.py:125 | fit progress: (9, 1.5609718561172485, {'accuracy': 0.9101, 'data_size': 10000}, 228.10423485498177)
INFO flwr 2024-04-07 03:32:12,398 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:32:12,399 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:32:34,809 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:32:36,547 | server.py:125 | fit progress: (10, 1.5611178874969482, {'accuracy': 0.9112, 'data_size': 10000}, 252.2533915519889)
INFO flwr 2024-04-07 03:32:36,547 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:32:36,548 | server.py:153 | FL finished in 252.2538286820054
INFO flwr 2024-04-07 03:32:36,548 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:32:36,548 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:32:36,548 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:32:36,548 | app.py:229 | app_fit: losses_centralized [(0, 2.3023431301116943), (1, 1.9410127401351929), (2, 1.6452710628509521), (3, 1.5893025398254395), (4, 1.6027910709381104), (5, 1.5776904821395874), (6, 1.5716928243637085), (7, 1.5620341300964355), (8, 1.5602073669433594), (9, 1.5609718561172485), (10, 1.5611178874969482)]
INFO flwr 2024-04-07 03:32:36,548 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0648), (1, 0.5095), (2, 0.8487), (3, 0.8946), (4, 0.8789), (5, 0.9002), (6, 0.9044), (7, 0.9134), (8, 0.9128), (9, 0.9101), (10, 0.9112)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9112
wandb:     loss 1.56112
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_032802-verpytcp
wandb: Find logs at: ./wandb/offline-run-20240407_032802-verpytcp/logs
INFO flwr 2024-04-07 03:32:40,087 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:39:51,789 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1640998)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1640998)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 03:40:00,114	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:40:01,527	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:40:02,004	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d4559a7a4f3796a6.zip' (11.09MiB) to Ray cluster...
2024-04-07 03:40:02,033	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d4559a7a4f3796a6.zip'.
INFO flwr 2024-04-07 03:40:13,222 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 51868817817.0, 'memory': 111027241575.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 03:40:13,222 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:40:13,222 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:40:13,239 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:40:13,240 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:40:13,241 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:40:13,241 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 03:40:15,416 | server.py:94 | initial parameters (loss, other metrics): 2.303743839263916, {'accuracy': 0.0817, 'data_size': 10000}
INFO flwr 2024-04-07 03:40:15,417 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:40:15,417 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1651552)[0m 2024-04-07 03:40:19.794932: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1651552)[0m 2024-04-07 03:40:19.897278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1651552)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1651552)[0m 2024-04-07 03:40:22.541700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1651548)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1651548)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1651556)[0m 2024-04-07 03:40:20.079142: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1651556)[0m 2024-04-07 03:40:20.191996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1651556)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1651590)[0m 2024-04-07 03:40:22.542779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 03:40:56,425 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:40:56,462 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:40:57,856 | server.py:125 | fit progress: (1, 1.8130128383636475, {'accuracy': 0.6802, 'data_size': 10000}, 42.438576043001376)
INFO flwr 2024-04-07 03:40:57,856 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:40:57,856 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:41:18,812 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:41:20,254 | server.py:125 | fit progress: (2, 1.604573369026184, {'accuracy': 0.8867, 'data_size': 10000}, 64.83697730198037)
INFO flwr 2024-04-07 03:41:20,255 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:41:20,255 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:41:40,347 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:41:41,852 | server.py:125 | fit progress: (3, 1.594340443611145, {'accuracy': 0.8836, 'data_size': 10000}, 86.43422551098047)
INFO flwr 2024-04-07 03:41:41,852 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:41:41,852 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:42:02,248 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:42:03,762 | server.py:125 | fit progress: (4, 1.5758017301559448, {'accuracy': 0.9003, 'data_size': 10000}, 108.34467648199643)
INFO flwr 2024-04-07 03:42:03,762 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:42:03,763 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:42:26,480 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:42:27,725 | server.py:125 | fit progress: (5, 1.5864993333816528, {'accuracy': 0.8875, 'data_size': 10000}, 132.30795135197695)
INFO flwr 2024-04-07 03:42:27,726 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:42:27,726 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:42:50,158 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:42:51,461 | server.py:125 | fit progress: (6, 1.5641001462936401, {'accuracy': 0.9077, 'data_size': 10000}, 156.04323668798315)
INFO flwr 2024-04-07 03:42:51,461 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:42:51,461 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:43:11,720 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:43:13,307 | server.py:125 | fit progress: (7, 1.5640525817871094, {'accuracy': 0.9084, 'data_size': 10000}, 177.88959744398016)
INFO flwr 2024-04-07 03:43:13,307 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:43:13,307 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:43:34,228 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:43:35,855 | server.py:125 | fit progress: (8, 1.5601328611373901, {'accuracy': 0.9094, 'data_size': 10000}, 200.43804536998505)
INFO flwr 2024-04-07 03:43:35,856 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:43:35,856 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:43:57,471 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:43:59,118 | server.py:125 | fit progress: (9, 1.568050503730774, {'accuracy': 0.9015, 'data_size': 10000}, 223.70086317998357)
INFO flwr 2024-04-07 03:43:59,118 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:43:59,119 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:44:20,631 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:44:22,411 | server.py:125 | fit progress: (10, 1.557876706123352, {'accuracy': 0.9112, 'data_size': 10000}, 246.99333650700282)
INFO flwr 2024-04-07 03:44:22,411 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:44:22,411 | server.py:153 | FL finished in 246.99380427398137
INFO flwr 2024-04-07 03:44:22,411 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:44:22,411 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:44:22,412 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:44:22,412 | app.py:229 | app_fit: losses_centralized [(0, 2.303743839263916), (1, 1.8130128383636475), (2, 1.604573369026184), (3, 1.594340443611145), (4, 1.5758017301559448), (5, 1.5864993333816528), (6, 1.5641001462936401), (7, 1.5640525817871094), (8, 1.5601328611373901), (9, 1.568050503730774), (10, 1.557876706123352)]
INFO flwr 2024-04-07 03:44:22,412 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0817), (1, 0.6802), (2, 0.8867), (3, 0.8836), (4, 0.9003), (5, 0.8875), (6, 0.9077), (7, 0.9084), (8, 0.9094), (9, 0.9015), (10, 0.9112)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9112
wandb:     loss 1.55788
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_033951-ax1b2vu2
wandb: Find logs at: ./wandb/offline-run-20240407_033951-ax1b2vu2/logs
INFO flwr 2024-04-07 03:44:26,025 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 03:51:34,954 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1651590)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1651590)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 03:51:39,829	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 03:51:40,284	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 03:51:40,652	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_79b070aa97cb3190.zip' (11.11MiB) to Ray cluster...
2024-04-07 03:51:40,689	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_79b070aa97cb3190.zip'.
INFO flwr 2024-04-07 03:51:51,928 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 122099023258.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56613867110.0}
INFO flwr 2024-04-07 03:51:51,928 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 03:51:51,928 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 03:51:51,947 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 03:51:51,948 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 03:51:51,948 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 03:51:51,949 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 03:51:53,899 | server.py:94 | initial parameters (loss, other metrics): 2.302584409713745, {'accuracy': 0.1028, 'data_size': 10000}
INFO flwr 2024-04-07 03:51:53,900 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 03:51:53,900 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1667133)[0m 2024-04-07 03:51:58.035166: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1667133)[0m 2024-04-07 03:51:58.131922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1667133)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1667133)[0m 2024-04-07 03:52:00.584580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1667132)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1667132)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1667132)[0m 2024-04-07 03:51:58.455962: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1667132)[0m 2024-04-07 03:51:58.552648: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1667132)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1667132)[0m 2024-04-07 03:52:00.999804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 03:52:31,246 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 03:52:31,286 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 03:52:32,429 | server.py:125 | fit progress: (1, 1.7183345556259155, {'accuracy': 0.7734, 'data_size': 10000}, 38.52882423900883)
INFO flwr 2024-04-07 03:52:32,429 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 03:52:32,429 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:52:54,667 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 03:52:56,150 | server.py:125 | fit progress: (2, 1.6197344064712524, {'accuracy': 0.8578, 'data_size': 10000}, 62.250467789010145)
INFO flwr 2024-04-07 03:52:56,151 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 03:52:56,151 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:53:19,653 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 03:53:21,120 | server.py:125 | fit progress: (3, 1.5762075185775757, {'accuracy': 0.8989, 'data_size': 10000}, 87.21967768599279)
INFO flwr 2024-04-07 03:53:21,120 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 03:53:21,120 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:53:42,970 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 03:53:44,897 | server.py:125 | fit progress: (4, 1.5676101446151733, {'accuracy': 0.9045, 'data_size': 10000}, 110.99741376799648)
INFO flwr 2024-04-07 03:53:44,898 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 03:53:44,898 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:54:08,353 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 03:54:09,632 | server.py:125 | fit progress: (5, 1.563412070274353, {'accuracy': 0.909, 'data_size': 10000}, 135.73214205200202)
INFO flwr 2024-04-07 03:54:09,632 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 03:54:09,633 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:54:29,896 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 03:54:31,235 | server.py:125 | fit progress: (6, 1.5672045946121216, {'accuracy': 0.9011, 'data_size': 10000}, 157.33514548299718)
INFO flwr 2024-04-07 03:54:31,235 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 03:54:31,236 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:54:52,860 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 03:54:55,514 | server.py:125 | fit progress: (7, 1.560286283493042, {'accuracy': 0.9088, 'data_size': 10000}, 181.61412331700558)
INFO flwr 2024-04-07 03:54:55,514 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 03:54:55,515 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:55:18,371 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 03:55:20,057 | server.py:125 | fit progress: (8, 1.5539296865463257, {'accuracy': 0.9131, 'data_size': 10000}, 206.1566378819989)
INFO flwr 2024-04-07 03:55:20,057 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 03:55:20,057 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:55:40,202 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 03:55:41,893 | server.py:125 | fit progress: (9, 1.5550554990768433, {'accuracy': 0.9125, 'data_size': 10000}, 227.99278364799102)
INFO flwr 2024-04-07 03:55:41,893 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 03:55:41,893 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 03:56:03,957 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 03:56:05,707 | server.py:125 | fit progress: (10, 1.5558066368103027, {'accuracy': 0.9117, 'data_size': 10000}, 251.80655522100278)
INFO flwr 2024-04-07 03:56:05,707 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 03:56:05,707 | server.py:153 | FL finished in 251.806940718001
INFO flwr 2024-04-07 03:56:05,707 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 03:56:05,707 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 03:56:05,707 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 03:56:05,707 | app.py:229 | app_fit: losses_centralized [(0, 2.302584409713745), (1, 1.7183345556259155), (2, 1.6197344064712524), (3, 1.5762075185775757), (4, 1.5676101446151733), (5, 1.563412070274353), (6, 1.5672045946121216), (7, 1.560286283493042), (8, 1.5539296865463257), (9, 1.5550554990768433), (10, 1.5558066368103027)]
INFO flwr 2024-04-07 03:56:05,707 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1028), (1, 0.7734), (2, 0.8578), (3, 0.8989), (4, 0.9045), (5, 0.909), (6, 0.9011), (7, 0.9088), (8, 0.9131), (9, 0.9125), (10, 0.9117)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9117
wandb:     loss 1.55581
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_035134-u2zt2u14
wandb: Find logs at: ./wandb/offline-run-20240407_035134-u2zt2u14/logs
INFO flwr 2024-04-07 03:56:09,296 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:03:17,914 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1667133)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1667133)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 04:03:23,236	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:03:23,733	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:03:24,113	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_70b2044fca43afd5.zip' (11.13MiB) to Ray cluster...
2024-04-07 04:03:24,152	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_70b2044fca43afd5.zip'.
INFO flwr 2024-04-07 04:03:35,139 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58939625472.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 127525792768.0}
INFO flwr 2024-04-07 04:03:35,139 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:03:35,139 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:03:35,155 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:03:35,156 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:03:35,156 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:03:35,156 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 04:03:36,933 | server.py:94 | initial parameters (loss, other metrics): 2.3018460273742676, {'accuracy': 0.0899, 'data_size': 10000}
INFO flwr 2024-04-07 04:03:36,934 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:03:36,934 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1671901)[0m 2024-04-07 04:03:41.664538: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1671898)[0m 2024-04-07 04:03:41.796314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1671898)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1671898)[0m 2024-04-07 04:03:43.883666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1671902)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1671902)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1671897)[0m 2024-04-07 04:03:41.888025: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1671897)[0m 2024-04-07 04:03:41.987170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1671897)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1671897)[0m 2024-04-07 04:03:44.146546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:04:13,819 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:04:13,866 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:04:14,981 | server.py:125 | fit progress: (1, 1.8070974349975586, {'accuracy': 0.6949, 'data_size': 10000}, 38.04723235400161)
INFO flwr 2024-04-07 04:04:14,982 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:04:14,982 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:04:36,578 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:04:38,024 | server.py:125 | fit progress: (2, 1.6878763437271118, {'accuracy': 0.7773, 'data_size': 10000}, 61.09004090298549)
INFO flwr 2024-04-07 04:04:38,024 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:04:38,025 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:04:58,166 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:04:59,619 | server.py:125 | fit progress: (3, 1.5878751277923584, {'accuracy': 0.8858, 'data_size': 10000}, 82.68448182599968)
INFO flwr 2024-04-07 04:04:59,619 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:04:59,619 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:05:19,309 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:05:20,813 | server.py:125 | fit progress: (4, 1.5799239873886108, {'accuracy': 0.8888, 'data_size': 10000}, 103.87855417098035)
INFO flwr 2024-04-07 04:05:20,813 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:05:20,813 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:05:41,181 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:05:42,486 | server.py:125 | fit progress: (5, 1.563652753829956, {'accuracy': 0.9046, 'data_size': 10000}, 125.551595566998)
INFO flwr 2024-04-07 04:05:42,486 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:05:42,486 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:06:05,150 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:06:06,476 | server.py:125 | fit progress: (6, 1.559592843055725, {'accuracy': 0.9073, 'data_size': 10000}, 149.5421409010014)
INFO flwr 2024-04-07 04:06:06,476 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:06:06,477 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:06:28,002 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:06:29,653 | server.py:125 | fit progress: (7, 1.567462682723999, {'accuracy': 0.9005, 'data_size': 10000}, 172.7191382079909)
INFO flwr 2024-04-07 04:06:29,653 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:06:29,654 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:06:50,584 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:06:52,270 | server.py:125 | fit progress: (8, 1.5673940181732178, {'accuracy': 0.8992, 'data_size': 10000}, 195.3356727270002)
INFO flwr 2024-04-07 04:06:52,270 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:06:52,270 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:07:14,920 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:07:16,644 | server.py:125 | fit progress: (9, 1.5549697875976562, {'accuracy': 0.9106, 'data_size': 10000}, 219.71021258298424)
INFO flwr 2024-04-07 04:07:16,645 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:07:16,645 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:07:37,915 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:07:39,633 | server.py:125 | fit progress: (10, 1.5563653707504272, {'accuracy': 0.9098, 'data_size': 10000}, 242.6992773009988)
INFO flwr 2024-04-07 04:07:39,634 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:07:39,634 | server.py:153 | FL finished in 242.69967372098472
INFO flwr 2024-04-07 04:07:39,634 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:07:39,634 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:07:39,634 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:07:39,634 | app.py:229 | app_fit: losses_centralized [(0, 2.3018460273742676), (1, 1.8070974349975586), (2, 1.6878763437271118), (3, 1.5878751277923584), (4, 1.5799239873886108), (5, 1.563652753829956), (6, 1.559592843055725), (7, 1.567462682723999), (8, 1.5673940181732178), (9, 1.5549697875976562), (10, 1.5563653707504272)]
INFO flwr 2024-04-07 04:07:39,634 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0899), (1, 0.6949), (2, 0.7773), (3, 0.8858), (4, 0.8888), (5, 0.9046), (6, 0.9073), (7, 0.9005), (8, 0.8992), (9, 0.9106), (10, 0.9098)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9098
wandb:     loss 1.55637
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_040317-4ph99gx2
wandb: Find logs at: ./wandb/offline-run-20240407_040317-4ph99gx2/logs
INFO flwr 2024-04-07 04:07:43,196 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:14:52,328 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1671896)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1671896)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 04:14:58,020	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:14:58,452	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:14:58,859	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9187e6c67415306d.zip' (11.15MiB) to Ray cluster...
2024-04-07 04:14:58,902	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9187e6c67415306d.zip'.
INFO flwr 2024-04-07 04:15:10,162 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 113112209204.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 52762375372.0, 'CPU': 64.0}
INFO flwr 2024-04-07 04:15:10,162 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:15:10,163 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:15:10,185 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:15:10,186 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:15:10,186 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:15:10,187 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 04:15:12,268 | server.py:94 | initial parameters (loss, other metrics): 2.3045008182525635, {'accuracy': 0.0786, 'data_size': 10000}
INFO flwr 2024-04-07 04:15:12,268 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:15:12,269 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1680053)[0m 2024-04-07 04:15:16.495935: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1680053)[0m 2024-04-07 04:15:16.601752: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1680053)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1680051)[0m 2024-04-07 04:15:18.670125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1680048)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1680048)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1680054)[0m 2024-04-07 04:15:16.783597: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1680054)[0m 2024-04-07 04:15:16.876237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1680054)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1680049)[0m 2024-04-07 04:15:19.546441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:15:49,719 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:15:49,763 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:15:50,878 | server.py:125 | fit progress: (1, 1.6791423559188843, {'accuracy': 0.8354, 'data_size': 10000}, 38.60898537602043)
INFO flwr 2024-04-07 04:15:50,878 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:15:50,878 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:16:11,862 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:16:13,296 | server.py:125 | fit progress: (2, 1.6155781745910645, {'accuracy': 0.8562, 'data_size': 10000}, 61.02688429900445)
INFO flwr 2024-04-07 04:16:13,296 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:16:13,296 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:16:37,187 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:16:38,645 | server.py:125 | fit progress: (3, 1.5830934047698975, {'accuracy': 0.8882, 'data_size': 10000}, 86.37624103800044)
INFO flwr 2024-04-07 04:16:38,645 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:16:38,646 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:16:58,513 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:17:00,037 | server.py:125 | fit progress: (4, 1.5696085691452026, {'accuracy': 0.8993, 'data_size': 10000}, 107.76862965102191)
INFO flwr 2024-04-07 04:17:00,038 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:17:00,038 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:17:21,338 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:17:22,608 | server.py:125 | fit progress: (5, 1.5644150972366333, {'accuracy': 0.9034, 'data_size': 10000}, 130.33964364099666)
INFO flwr 2024-04-07 04:17:22,609 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:17:22,609 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:17:40,373 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:17:41,660 | server.py:125 | fit progress: (6, 1.5574538707733154, {'accuracy': 0.9103, 'data_size': 10000}, 149.39096052502282)
INFO flwr 2024-04-07 04:17:41,660 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:17:41,660 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:18:01,300 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:18:02,909 | server.py:125 | fit progress: (7, 1.5638620853424072, {'accuracy': 0.9011, 'data_size': 10000}, 170.6404292540101)
INFO flwr 2024-04-07 04:18:02,909 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:18:02,910 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:18:23,196 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:18:24,810 | server.py:125 | fit progress: (8, 1.5519109964370728, {'accuracy': 0.9142, 'data_size': 10000}, 192.5414107250108)
INFO flwr 2024-04-07 04:18:24,810 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:18:24,811 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:18:45,739 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:18:47,407 | server.py:125 | fit progress: (9, 1.561234951019287, {'accuracy': 0.9018, 'data_size': 10000}, 215.1378043910081)
INFO flwr 2024-04-07 04:18:47,407 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:18:47,407 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:19:11,671 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:19:13,348 | server.py:125 | fit progress: (10, 1.553804636001587, {'accuracy': 0.9119, 'data_size': 10000}, 241.07908266200684)
INFO flwr 2024-04-07 04:19:13,348 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:19:13,348 | server.py:153 | FL finished in 241.07951315800892
INFO flwr 2024-04-07 04:19:13,348 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:19:13,349 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:19:13,349 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:19:13,349 | app.py:229 | app_fit: losses_centralized [(0, 2.3045008182525635), (1, 1.6791423559188843), (2, 1.6155781745910645), (3, 1.5830934047698975), (4, 1.5696085691452026), (5, 1.5644150972366333), (6, 1.5574538707733154), (7, 1.5638620853424072), (8, 1.5519109964370728), (9, 1.561234951019287), (10, 1.553804636001587)]
INFO flwr 2024-04-07 04:19:13,349 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0786), (1, 0.8354), (2, 0.8562), (3, 0.8882), (4, 0.8993), (5, 0.9034), (6, 0.9103), (7, 0.9011), (8, 0.9142), (9, 0.9018), (10, 0.9119)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9119
wandb:     loss 1.5538
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_041451-ml18rzjs
wandb: Find logs at: ./wandb/offline-run-20240407_041451-ml18rzjs/logs
INFO flwr 2024-04-07 04:19:16,881 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:26:35,223 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1680050)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1680050)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 04:26:41,081	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:26:41,431	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:26:41,799	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4ebf537ff17c6432.zip' (11.17MiB) to Ray cluster...
2024-04-07 04:26:41,831	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4ebf537ff17c6432.zip'.
INFO flwr 2024-04-07 04:26:53,254 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55756308480.0, 'CPU': 64.0, 'memory': 120098053120.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 04:26:53,254 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:26:53,254 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:26:53,270 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:26:53,271 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:26:53,271 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:26:53,271 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 04:26:55,296 | server.py:94 | initial parameters (loss, other metrics): 2.3000147342681885, {'accuracy': 0.1103, 'data_size': 10000}
INFO flwr 2024-04-07 04:26:55,296 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:26:55,297 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1687525)[0m 2024-04-07 04:26:59.714267: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1687525)[0m 2024-04-07 04:26:59.805803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1687525)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1687529)[0m 2024-04-07 04:27:02.455301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1687529)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1687529)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1687533)[0m 2024-04-07 04:27:00.412516: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1687533)[0m 2024-04-07 04:27:00.525554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1687533)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1687531)[0m 2024-04-07 04:27:02.649608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:27:52,533 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:27:52,567 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:27:53,711 | server.py:125 | fit progress: (1, 2.1206467151641846, {'accuracy': 0.4802, 'data_size': 10000}, 58.41400517601869)
INFO flwr 2024-04-07 04:27:53,711 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:27:53,711 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:28:29,200 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:28:30,633 | server.py:125 | fit progress: (2, 1.9471551179885864, {'accuracy': 0.6079, 'data_size': 10000}, 95.33596847901936)
INFO flwr 2024-04-07 04:28:30,633 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:28:30,633 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:29:10,126 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:29:11,626 | server.py:125 | fit progress: (3, 1.8578248023986816, {'accuracy': 0.7264, 'data_size': 10000}, 136.32957187099964)
INFO flwr 2024-04-07 04:29:11,627 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:29:11,627 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:29:48,932 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:29:50,676 | server.py:125 | fit progress: (4, 1.7860281467437744, {'accuracy': 0.7929, 'data_size': 10000}, 175.37908681901172)
INFO flwr 2024-04-07 04:29:50,676 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:29:50,676 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:30:31,046 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:30:32,328 | server.py:125 | fit progress: (5, 1.7581331729888916, {'accuracy': 0.8055, 'data_size': 10000}, 217.03132758199354)
INFO flwr 2024-04-07 04:30:32,328 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:30:32,329 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:31:14,623 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:31:16,276 | server.py:125 | fit progress: (6, 1.7132939100265503, {'accuracy': 0.8555, 'data_size': 10000}, 260.9792523130018)
INFO flwr 2024-04-07 04:31:16,286 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:31:16,286 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:31:58,123 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:31:59,753 | server.py:125 | fit progress: (7, 1.696470856666565, {'accuracy': 0.8685, 'data_size': 10000}, 304.4558976730041)
INFO flwr 2024-04-07 04:31:59,753 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:31:59,753 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:32:37,933 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:32:39,570 | server.py:125 | fit progress: (8, 1.684064269065857, {'accuracy': 0.8661, 'data_size': 10000}, 344.2737653090153)
INFO flwr 2024-04-07 04:32:39,571 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:32:39,571 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:33:20,381 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:33:22,258 | server.py:125 | fit progress: (9, 1.672119379043579, {'accuracy': 0.8761, 'data_size': 10000}, 386.96148111499497)
INFO flwr 2024-04-07 04:33:22,258 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:33:22,259 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:34:00,678 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:34:02,464 | server.py:125 | fit progress: (10, 1.6760165691375732, {'accuracy': 0.8572, 'data_size': 10000}, 427.16726846201345)
INFO flwr 2024-04-07 04:34:02,464 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:34:02,464 | server.py:153 | FL finished in 427.1677590190084
INFO flwr 2024-04-07 04:34:02,465 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:34:02,465 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:34:02,465 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:34:02,465 | app.py:229 | app_fit: losses_centralized [(0, 2.3000147342681885), (1, 2.1206467151641846), (2, 1.9471551179885864), (3, 1.8578248023986816), (4, 1.7860281467437744), (5, 1.7581331729888916), (6, 1.7132939100265503), (7, 1.696470856666565), (8, 1.684064269065857), (9, 1.672119379043579), (10, 1.6760165691375732)]
INFO flwr 2024-04-07 04:34:02,465 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1103), (1, 0.4802), (2, 0.6079), (3, 0.7264), (4, 0.7929), (5, 0.8055), (6, 0.8555), (7, 0.8685), (8, 0.8661), (9, 0.8761), (10, 0.8572)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8572
wandb:     loss 1.67602
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_042634-taaqrm7w
wandb: Find logs at: ./wandb/offline-run-20240407_042634-taaqrm7w/logs
INFO flwr 2024-04-07 04:34:06,036 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:41:15,785 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1687531)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1687531)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 04:41:20,912	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:41:21,425	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:41:21,830	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6a84a16e94eb6802.zip' (11.19MiB) to Ray cluster...
2024-04-07 04:41:21,864	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6a84a16e94eb6802.zip'.
INFO flwr 2024-04-07 04:41:32,871 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 121187789824.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 56223338496.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 04:41:32,871 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:41:32,872 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:41:32,889 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:41:32,890 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:41:32,891 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:41:32,891 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 04:41:35,081 | server.py:94 | initial parameters (loss, other metrics): 2.3059470653533936, {'accuracy': 0.0799, 'data_size': 10000}
INFO flwr 2024-04-07 04:41:35,082 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:41:35,082 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1692872)[0m 2024-04-07 04:41:39.004586: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1692872)[0m 2024-04-07 04:41:39.099013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1692872)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1692871)[0m 2024-04-07 04:41:41.340212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1692873)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1692873)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1692866)[0m 2024-04-07 04:41:39.366162: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1692866)[0m 2024-04-07 04:41:39.475254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1692866)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1692866)[0m 2024-04-07 04:41:41.608521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:42:29,237 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:42:29,275 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:42:30,410 | server.py:125 | fit progress: (1, 1.8217841386795044, {'accuracy': 0.6587, 'data_size': 10000}, 55.32819875900168)
INFO flwr 2024-04-07 04:42:30,410 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:42:30,411 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:43:10,673 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:43:12,127 | server.py:125 | fit progress: (2, 1.6599267721176147, {'accuracy': 0.8358, 'data_size': 10000}, 97.04495331598446)
INFO flwr 2024-04-07 04:43:12,127 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:43:12,127 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:43:53,559 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:43:55,051 | server.py:125 | fit progress: (3, 1.5883162021636963, {'accuracy': 0.8988, 'data_size': 10000}, 139.96887104699272)
INFO flwr 2024-04-07 04:43:55,051 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:43:55,051 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:44:31,652 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:44:33,160 | server.py:125 | fit progress: (4, 1.578756332397461, {'accuracy': 0.9023, 'data_size': 10000}, 178.07788196898764)
INFO flwr 2024-04-07 04:44:33,160 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:44:33,160 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:45:19,746 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 04:45:21,048 | server.py:125 | fit progress: (5, 1.5751559734344482, {'accuracy': 0.9039, 'data_size': 10000}, 225.96567767998204)
INFO flwr 2024-04-07 04:45:21,048 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 04:45:21,048 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:46:04,111 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 04:46:05,431 | server.py:125 | fit progress: (6, 1.5673407316207886, {'accuracy': 0.9067, 'data_size': 10000}, 270.34912580798846)
INFO flwr 2024-04-07 04:46:05,431 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 04:46:05,432 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:46:41,928 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 04:46:43,702 | server.py:125 | fit progress: (7, 1.5622832775115967, {'accuracy': 0.9126, 'data_size': 10000}, 308.6204764499853)
INFO flwr 2024-04-07 04:46:43,703 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 04:46:43,703 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:47:24,610 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 04:47:26,321 | server.py:125 | fit progress: (8, 1.5670617818832397, {'accuracy': 0.9075, 'data_size': 10000}, 351.23860072597745)
INFO flwr 2024-04-07 04:47:26,321 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 04:47:26,321 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:48:06,642 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 04:48:08,381 | server.py:125 | fit progress: (9, 1.5584439039230347, {'accuracy': 0.9152, 'data_size': 10000}, 393.29952289597713)
INFO flwr 2024-04-07 04:48:08,382 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 04:48:08,382 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:48:49,574 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 04:48:52,566 | server.py:125 | fit progress: (10, 1.5682049989700317, {'accuracy': 0.9036, 'data_size': 10000}, 437.48455848699086)
INFO flwr 2024-04-07 04:48:52,567 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 04:48:52,567 | server.py:153 | FL finished in 437.4850320550031
INFO flwr 2024-04-07 04:48:52,567 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 04:48:52,567 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 04:48:52,567 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 04:48:52,567 | app.py:229 | app_fit: losses_centralized [(0, 2.3059470653533936), (1, 1.8217841386795044), (2, 1.6599267721176147), (3, 1.5883162021636963), (4, 1.578756332397461), (5, 1.5751559734344482), (6, 1.5673407316207886), (7, 1.5622832775115967), (8, 1.5670617818832397), (9, 1.5584439039230347), (10, 1.5682049989700317)]
INFO flwr 2024-04-07 04:48:52,568 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0799), (1, 0.6587), (2, 0.8358), (3, 0.8988), (4, 0.9023), (5, 0.9039), (6, 0.9067), (7, 0.9126), (8, 0.9075), (9, 0.9152), (10, 0.9036)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9036
wandb:     loss 1.5682
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_044114-5bx4rhi9
wandb: Find logs at: ./wandb/offline-run-20240407_044114-5bx4rhi9/logs
INFO flwr 2024-04-07 04:48:56,085 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 04:56:04,715 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1692866)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1692866)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 04:56:10,203	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 04:56:10,686	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 04:56:11,053	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_06c104cc843e39e4.zip' (11.21MiB) to Ray cluster...
2024-04-07 04:56:11,083	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_06c104cc843e39e4.zip'.
INFO flwr 2024-04-07 04:56:22,584 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 126538759168.0, 'object_store_memory': 58516611072.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 04:56:22,584 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 04:56:22,584 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 04:56:22,598 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 04:56:22,599 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 04:56:22,599 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 04:56:22,599 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 04:56:24,629 | server.py:94 | initial parameters (loss, other metrics): 2.300560235977173, {'accuracy': 0.1405, 'data_size': 10000}
INFO flwr 2024-04-07 04:56:24,629 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 04:56:24,630 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1701013)[0m 2024-04-07 04:56:28.879927: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1701017)[0m 2024-04-07 04:56:28.956970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1701017)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1701011)[0m 2024-04-07 04:56:31.278680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1701011)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1701011)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1701012)[0m 2024-04-07 04:56:29.096887: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1701012)[0m 2024-04-07 04:56:29.178749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1701012)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1701015)[0m 2024-04-07 04:56:31.276746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 04:57:20,115 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 04:57:20,327 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 04:57:21,440 | server.py:125 | fit progress: (1, 1.774840235710144, {'accuracy': 0.7339, 'data_size': 10000}, 56.80977428201004)
INFO flwr 2024-04-07 04:57:21,440 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 04:57:21,440 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:58:03,980 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 04:58:05,460 | server.py:125 | fit progress: (2, 1.6110806465148926, {'accuracy': 0.8703, 'data_size': 10000}, 100.82989750700654)
INFO flwr 2024-04-07 04:58:05,460 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 04:58:05,460 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:58:46,923 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 04:58:48,490 | server.py:125 | fit progress: (3, 1.5706164836883545, {'accuracy': 0.9076, 'data_size': 10000}, 143.8605556830007)
INFO flwr 2024-04-07 04:58:48,491 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 04:58:48,491 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 04:59:25,596 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 04:59:27,114 | server.py:125 | fit progress: (4, 1.5709446668624878, {'accuracy': 0.9028, 'data_size': 10000}, 182.48456783400616)
INFO flwr 2024-04-07 04:59:27,115 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 04:59:27,115 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:00:04,507 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:00:05,808 | server.py:125 | fit progress: (5, 1.5693861246109009, {'accuracy': 0.9026, 'data_size': 10000}, 221.17791718500666)
INFO flwr 2024-04-07 05:00:05,808 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:00:05,808 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:00:46,680 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:00:48,024 | server.py:125 | fit progress: (6, 1.562422752380371, {'accuracy': 0.9093, 'data_size': 10000}, 263.3938726680062)
INFO flwr 2024-04-07 05:00:48,024 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:00:48,024 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:01:25,758 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:01:27,318 | server.py:125 | fit progress: (7, 1.56017005443573, {'accuracy': 0.9095, 'data_size': 10000}, 302.68804377899505)
INFO flwr 2024-04-07 05:01:27,318 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:01:27,318 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:02:06,006 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:02:07,631 | server.py:125 | fit progress: (8, 1.5638232231140137, {'accuracy': 0.9071, 'data_size': 10000}, 343.00105144199915)
INFO flwr 2024-04-07 05:02:07,631 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:02:07,632 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:02:48,655 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:02:50,383 | server.py:125 | fit progress: (9, 1.5565019845962524, {'accuracy': 0.9114, 'data_size': 10000}, 385.75332637000247)
INFO flwr 2024-04-07 05:02:50,384 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:02:50,384 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:03:31,401 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:03:33,145 | server.py:125 | fit progress: (10, 1.5510884523391724, {'accuracy': 0.9179, 'data_size': 10000}, 428.51527317700675)
INFO flwr 2024-04-07 05:03:33,145 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:03:33,146 | server.py:153 | FL finished in 428.51573985401774
INFO flwr 2024-04-07 05:03:33,146 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:03:33,146 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:03:33,146 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:03:33,146 | app.py:229 | app_fit: losses_centralized [(0, 2.300560235977173), (1, 1.774840235710144), (2, 1.6110806465148926), (3, 1.5706164836883545), (4, 1.5709446668624878), (5, 1.5693861246109009), (6, 1.562422752380371), (7, 1.56017005443573), (8, 1.5638232231140137), (9, 1.5565019845962524), (10, 1.5510884523391724)]
INFO flwr 2024-04-07 05:03:33,146 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1405), (1, 0.7339), (2, 0.8703), (3, 0.9076), (4, 0.9028), (5, 0.9026), (6, 0.9093), (7, 0.9095), (8, 0.9071), (9, 0.9114), (10, 0.9179)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9179
wandb:     loss 1.55109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_045604-4q9ryjjz
wandb: Find logs at: ./wandb/offline-run-20240407_045604-4q9ryjjz/logs
INFO flwr 2024-04-07 05:03:36,638 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:10:45,343 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1701012)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1701012)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 05:10:51,658	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:10:52,200	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:10:52,554	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_845b2492442c3a17.zip' (11.24MiB) to Ray cluster...
2024-04-07 05:10:52,591	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_845b2492442c3a17.zip'.
INFO flwr 2024-04-07 05:11:03,546 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 120291915981.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55839392563.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 05:11:03,547 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:11:03,547 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:11:03,562 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:11:03,563 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:11:03,563 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:11:03,563 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 05:11:05,523 | server.py:94 | initial parameters (loss, other metrics): 2.302933931350708, {'accuracy': 0.1035, 'data_size': 10000}
INFO flwr 2024-04-07 05:11:05,523 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:11:05,524 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1709457)[0m 2024-04-07 05:11:09.695639: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1709457)[0m 2024-04-07 05:11:09.794762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1709457)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1709460)[0m 2024-04-07 05:11:11.667350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1709463)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1709463)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1709456)[0m 2024-04-07 05:11:10.150568: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1709456)[0m 2024-04-07 05:11:10.242225: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1709456)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1709458)[0m 2024-04-07 05:11:12.739234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 05:11:57,715 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:11:57,750 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:11:58,883 | server.py:125 | fit progress: (1, 1.7371134757995605, {'accuracy': 0.7467, 'data_size': 10000}, 53.35887785599334)
INFO flwr 2024-04-07 05:11:58,883 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:11:58,883 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:12:41,679 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:12:43,143 | server.py:125 | fit progress: (2, 1.644681692123413, {'accuracy': 0.8219, 'data_size': 10000}, 97.61909589997958)
INFO flwr 2024-04-07 05:12:43,143 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:12:43,143 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:13:23,890 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:13:25,368 | server.py:125 | fit progress: (3, 1.5727347135543823, {'accuracy': 0.9016, 'data_size': 10000}, 139.84419871098362)
INFO flwr 2024-04-07 05:13:25,368 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:13:25,368 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:14:06,214 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:14:07,452 | server.py:125 | fit progress: (4, 1.5823712348937988, {'accuracy': 0.8896, 'data_size': 10000}, 181.92852386899176)
INFO flwr 2024-04-07 05:14:07,453 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:14:07,453 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:14:43,044 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:14:44,318 | server.py:125 | fit progress: (5, 1.5609731674194336, {'accuracy': 0.9089, 'data_size': 10000}, 218.79399777000071)
INFO flwr 2024-04-07 05:14:44,318 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:14:44,318 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:15:24,866 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:15:26,458 | server.py:125 | fit progress: (6, 1.5675653219223022, {'accuracy': 0.9023, 'data_size': 10000}, 260.93430747798993)
INFO flwr 2024-04-07 05:15:26,458 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:15:26,458 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:16:11,949 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:16:13,616 | server.py:125 | fit progress: (7, 1.5723717212677002, {'accuracy': 0.896, 'data_size': 10000}, 308.0924793119775)
INFO flwr 2024-04-07 05:16:13,616 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:16:13,617 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:16:54,577 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:16:56,242 | server.py:125 | fit progress: (8, 1.5708335638046265, {'accuracy': 0.898, 'data_size': 10000}, 350.71826590699493)
INFO flwr 2024-04-07 05:16:56,242 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:16:56,242 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:17:34,055 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:17:35,433 | server.py:125 | fit progress: (9, 1.558243989944458, {'accuracy': 0.9099, 'data_size': 10000}, 389.9090839010023)
INFO flwr 2024-04-07 05:17:35,433 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:17:35,433 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:18:17,041 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:18:18,734 | server.py:125 | fit progress: (10, 1.5518205165863037, {'accuracy': 0.9156, 'data_size': 10000}, 433.21019752899883)
INFO flwr 2024-04-07 05:18:18,734 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:18:18,734 | server.py:153 | FL finished in 433.2106222769944
INFO flwr 2024-04-07 05:18:18,734 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:18:18,735 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:18:18,735 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:18:18,735 | app.py:229 | app_fit: losses_centralized [(0, 2.302933931350708), (1, 1.7371134757995605), (2, 1.644681692123413), (3, 1.5727347135543823), (4, 1.5823712348937988), (5, 1.5609731674194336), (6, 1.5675653219223022), (7, 1.5723717212677002), (8, 1.5708335638046265), (9, 1.558243989944458), (10, 1.5518205165863037)]
INFO flwr 2024-04-07 05:18:18,735 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1035), (1, 0.7467), (2, 0.8219), (3, 0.9016), (4, 0.8896), (5, 0.9089), (6, 0.9023), (7, 0.896), (8, 0.898), (9, 0.9099), (10, 0.9156)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9156
wandb:     loss 1.55182
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_051044-zk614l97
wandb: Find logs at: ./wandb/offline-run-20240407_051044-zk614l97/logs
INFO flwr 2024-04-07 05:18:22,308 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:25:40,225 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1709452)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1709452)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 05:25:48,716	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:25:49,069	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:25:49,586	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_904a800c48d1f7ab.zip' (11.26MiB) to Ray cluster...
2024-04-07 05:25:49,622	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_904a800c48d1f7ab.zip'.
INFO flwr 2024-04-07 05:26:00,856 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 55926187622.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 120494437786.0}
INFO flwr 2024-04-07 05:26:00,857 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:26:00,857 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:26:00,872 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:26:00,873 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:26:00,874 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:26:00,874 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 05:26:03,278 | server.py:94 | initial parameters (loss, other metrics): 2.302145481109619, {'accuracy': 0.1196, 'data_size': 10000}
INFO flwr 2024-04-07 05:26:03,278 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:26:03,279 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1717624)[0m 2024-04-07 05:26:07.632620: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1717624)[0m 2024-04-07 05:26:07.730110: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1717624)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1717625)[0m 2024-04-07 05:26:10.438402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1717622)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1717622)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1717625)[0m 2024-04-07 05:26:07.868280: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1717625)[0m 2024-04-07 05:26:07.957067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1717625)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1717626)[0m 2024-04-07 05:26:10.445232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 05:26:59,944 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:26:59,984 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:27:01,116 | server.py:125 | fit progress: (1, 1.6383943557739258, {'accuracy': 0.8646, 'data_size': 10000}, 57.83764502298436)
INFO flwr 2024-04-07 05:27:01,116 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:27:01,117 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:27:42,406 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:27:43,929 | server.py:125 | fit progress: (2, 1.5885926485061646, {'accuracy': 0.888, 'data_size': 10000}, 100.6505043389916)
INFO flwr 2024-04-07 05:27:43,929 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:27:43,929 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:28:23,382 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:28:25,542 | server.py:125 | fit progress: (3, 1.590234637260437, {'accuracy': 0.8815, 'data_size': 10000}, 142.26333882499603)
INFO flwr 2024-04-07 05:28:25,542 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:28:25,542 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:29:02,185 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:29:04,898 | server.py:125 | fit progress: (4, 1.5747337341308594, {'accuracy': 0.895, 'data_size': 10000}, 181.6199155729846)
INFO flwr 2024-04-07 05:29:04,899 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:29:04,899 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:29:42,311 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:29:43,606 | server.py:125 | fit progress: (5, 1.5624356269836426, {'accuracy': 0.9078, 'data_size': 10000}, 220.32797991897678)
INFO flwr 2024-04-07 05:29:43,607 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:29:43,607 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:30:22,074 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:30:23,395 | server.py:125 | fit progress: (6, 1.5863940715789795, {'accuracy': 0.8806, 'data_size': 10000}, 260.11644882598193)
INFO flwr 2024-04-07 05:30:23,395 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:30:23,395 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:31:06,501 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:31:08,620 | server.py:125 | fit progress: (7, 1.558210015296936, {'accuracy': 0.9091, 'data_size': 10000}, 305.3421074829821)
INFO flwr 2024-04-07 05:31:08,621 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:31:08,621 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:31:44,923 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:31:46,627 | server.py:125 | fit progress: (8, 1.5540680885314941, {'accuracy': 0.9134, 'data_size': 10000}, 343.34901552897645)
INFO flwr 2024-04-07 05:31:46,628 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:31:46,628 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:32:27,785 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:32:29,542 | server.py:125 | fit progress: (9, 1.5535809993743896, {'accuracy': 0.9136, 'data_size': 10000}, 386.263583389984)
INFO flwr 2024-04-07 05:32:29,542 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:32:29,543 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:33:07,967 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:33:09,689 | server.py:125 | fit progress: (10, 1.5569298267364502, {'accuracy': 0.9102, 'data_size': 10000}, 426.4110536839871)
INFO flwr 2024-04-07 05:33:09,690 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:33:09,690 | server.py:153 | FL finished in 426.4115105889796
INFO flwr 2024-04-07 05:33:09,690 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:33:09,690 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:33:09,690 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:33:09,690 | app.py:229 | app_fit: losses_centralized [(0, 2.302145481109619), (1, 1.6383943557739258), (2, 1.5885926485061646), (3, 1.590234637260437), (4, 1.5747337341308594), (5, 1.5624356269836426), (6, 1.5863940715789795), (7, 1.558210015296936), (8, 1.5540680885314941), (9, 1.5535809993743896), (10, 1.5569298267364502)]
INFO flwr 2024-04-07 05:33:09,691 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1196), (1, 0.8646), (2, 0.888), (3, 0.8815), (4, 0.895), (5, 0.9078), (6, 0.8806), (7, 0.9091), (8, 0.9134), (9, 0.9136), (10, 0.9102)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9102
wandb:     loss 1.55693
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_052539-c9tkixpo
wandb: Find logs at: ./wandb/offline-run-20240407_052539-c9tkixpo/logs
INFO flwr 2024-04-07 05:33:13,225 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:40:22,551 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1717624)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1717624)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 05:40:27,473	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:40:27,902	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:40:28,272	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f1ab75c8a4079aa9.zip' (11.28MiB) to Ray cluster...
2024-04-07 05:40:28,302	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f1ab75c8a4079aa9.zip'.
INFO flwr 2024-04-07 05:40:40,436 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 116289359258.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 54124011110.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 05:40:40,437 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:40:40,437 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:40:40,452 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:40:40,453 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:40:40,453 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:40:40,454 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 05:40:42,467 | server.py:94 | initial parameters (loss, other metrics): 2.302846670150757, {'accuracy': 0.1046, 'data_size': 10000}
INFO flwr 2024-04-07 05:40:42,468 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:40:42,468 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1725977)[0m 2024-04-07 05:40:46.142119: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1725977)[0m 2024-04-07 05:40:46.242987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1725977)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1725977)[0m 2024-04-07 05:40:49.219088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1725985)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1725985)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1725985)[0m 2024-04-07 05:40:47.542623: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1725985)[0m 2024-04-07 05:40:47.666172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1725985)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1725985)[0m 2024-04-07 05:40:50.112710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 05:41:37,579 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:41:37,617 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:41:38,737 | server.py:125 | fit progress: (1, 1.705310583114624, {'accuracy': 0.7799, 'data_size': 10000}, 56.26943617800134)
INFO flwr 2024-04-07 05:41:38,738 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:41:38,738 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:42:19,794 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:42:22,733 | server.py:125 | fit progress: (2, 1.6043927669525146, {'accuracy': 0.8684, 'data_size': 10000}, 100.26527700998122)
INFO flwr 2024-04-07 05:42:22,733 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:42:22,734 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:43:00,304 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:43:02,694 | server.py:125 | fit progress: (3, 1.5755252838134766, {'accuracy': 0.8936, 'data_size': 10000}, 140.22622589499224)
INFO flwr 2024-04-07 05:43:02,694 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:43:02,695 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:43:43,715 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:43:45,241 | server.py:125 | fit progress: (4, 1.5634093284606934, {'accuracy': 0.906, 'data_size': 10000}, 182.77280406298814)
INFO flwr 2024-04-07 05:43:45,241 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:43:45,241 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:44:23,811 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:44:27,501 | server.py:125 | fit progress: (5, 1.5726228952407837, {'accuracy': 0.8946, 'data_size': 10000}, 225.0327062419965)
INFO flwr 2024-04-07 05:44:27,501 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:44:27,501 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:45:05,409 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 05:45:08,769 | server.py:125 | fit progress: (6, 1.5616967678070068, {'accuracy': 0.9052, 'data_size': 10000}, 266.3015708369785)
INFO flwr 2024-04-07 05:45:08,770 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 05:45:08,770 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:45:49,654 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 05:45:51,976 | server.py:125 | fit progress: (7, 1.5652977228164673, {'accuracy': 0.9025, 'data_size': 10000}, 309.50805236797896)
INFO flwr 2024-04-07 05:45:51,976 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 05:45:51,976 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:46:32,601 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 05:46:34,282 | server.py:125 | fit progress: (8, 1.561362862586975, {'accuracy': 0.9055, 'data_size': 10000}, 351.8145271219837)
INFO flwr 2024-04-07 05:46:34,283 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 05:46:34,283 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:47:15,351 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 05:47:17,062 | server.py:125 | fit progress: (9, 1.5493967533111572, {'accuracy': 0.9169, 'data_size': 10000}, 394.59369242299)
INFO flwr 2024-04-07 05:47:17,062 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 05:47:17,062 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:47:58,184 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 05:48:03,476 | server.py:125 | fit progress: (10, 1.550481915473938, {'accuracy': 0.9156, 'data_size': 10000}, 441.008264781005)
INFO flwr 2024-04-07 05:48:03,476 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 05:48:03,477 | server.py:153 | FL finished in 441.0087015070021
INFO flwr 2024-04-07 05:48:03,477 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 05:48:03,477 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 05:48:03,477 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 05:48:03,477 | app.py:229 | app_fit: losses_centralized [(0, 2.302846670150757), (1, 1.705310583114624), (2, 1.6043927669525146), (3, 1.5755252838134766), (4, 1.5634093284606934), (5, 1.5726228952407837), (6, 1.5616967678070068), (7, 1.5652977228164673), (8, 1.561362862586975), (9, 1.5493967533111572), (10, 1.550481915473938)]
INFO flwr 2024-04-07 05:48:03,477 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1046), (1, 0.7799), (2, 0.8684), (3, 0.8936), (4, 0.906), (5, 0.8946), (6, 0.9052), (7, 0.9025), (8, 0.9055), (9, 0.9169), (10, 0.9156)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9156
wandb:     loss 1.55048
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_054022-5960jmcs
wandb: Find logs at: ./wandb/offline-run-20240407_054022-5960jmcs/logs
INFO flwr 2024-04-07 05:48:07,033 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 1
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 05:55:15,648 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1725987)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1725987)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 05:55:21,665	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 05:55:22,230	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 05:55:22,591	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ea4275c1748d6a4f.zip' (11.30MiB) to Ray cluster...
2024-04-07 05:55:22,623	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ea4275c1748d6a4f.zip'.
INFO flwr 2024-04-07 05:55:34,229 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 119834907239.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55643531673.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 05:55:34,229 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 05:55:34,230 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 05:55:34,247 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 05:55:34,248 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 05:55:34,249 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 05:55:34,249 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 05:55:36,207 | server.py:94 | initial parameters (loss, other metrics): 2.3029398918151855, {'accuracy': 0.0852, 'data_size': 10000}
INFO flwr 2024-04-07 05:55:36,208 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 05:55:36,208 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1731141)[0m 2024-04-07 05:55:40.163469: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1731141)[0m 2024-04-07 05:55:40.331913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1731141)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1731141)[0m 2024-04-07 05:55:42.472117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1731144)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1731144)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1731142)[0m 2024-04-07 05:55:40.749925: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1731142)[0m 2024-04-07 05:55:40.867164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1731142)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1731144)[0m 2024-04-07 05:55:43.099132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 05:56:26,172 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 05:56:26,207 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 05:56:27,327 | server.py:125 | fit progress: (1, 1.684876799583435, {'accuracy': 0.7897, 'data_size': 10000}, 51.11926914798096)
INFO flwr 2024-04-07 05:56:27,327 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 05:56:27,328 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:57:13,704 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 05:57:15,167 | server.py:125 | fit progress: (2, 1.575603723526001, {'accuracy': 0.8952, 'data_size': 10000}, 98.95884524699068)
INFO flwr 2024-04-07 05:57:15,167 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 05:57:15,167 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:57:52,469 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 05:57:54,018 | server.py:125 | fit progress: (3, 1.5728956460952759, {'accuracy': 0.8944, 'data_size': 10000}, 137.8101472829876)
INFO flwr 2024-04-07 05:57:54,018 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 05:57:54,019 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:58:31,569 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 05:58:33,150 | server.py:125 | fit progress: (4, 1.5608854293823242, {'accuracy': 0.9062, 'data_size': 10000}, 176.9425215649826)
INFO flwr 2024-04-07 05:58:33,151 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 05:58:33,151 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 05:59:15,663 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 05:59:16,948 | server.py:125 | fit progress: (5, 1.56153404712677, {'accuracy': 0.9031, 'data_size': 10000}, 220.74030933500035)
INFO flwr 2024-04-07 05:59:16,949 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 05:59:16,949 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:00:00,166 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:00:01,511 | server.py:125 | fit progress: (6, 1.5545501708984375, {'accuracy': 0.9098, 'data_size': 10000}, 265.30279047598015)
INFO flwr 2024-04-07 06:00:01,511 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:00:01,511 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:00:43,069 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:00:44,764 | server.py:125 | fit progress: (7, 1.5532655715942383, {'accuracy': 0.9116, 'data_size': 10000}, 308.5558717479871)
INFO flwr 2024-04-07 06:00:44,764 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:00:44,764 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:01:27,907 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:01:29,610 | server.py:125 | fit progress: (8, 1.5492372512817383, {'accuracy': 0.9156, 'data_size': 10000}, 353.4020235219796)
INFO flwr 2024-04-07 06:01:29,610 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:01:29,611 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:02:11,020 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:02:12,818 | server.py:125 | fit progress: (9, 1.5726618766784668, {'accuracy': 0.8927, 'data_size': 10000}, 396.60961110398057)
INFO flwr 2024-04-07 06:02:12,818 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:02:12,818 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:02:55,222 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:02:57,017 | server.py:125 | fit progress: (10, 1.5494256019592285, {'accuracy': 0.914, 'data_size': 10000}, 440.809090085997)
INFO flwr 2024-04-07 06:02:57,017 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:02:57,017 | server.py:153 | FL finished in 440.80957217499963
INFO flwr 2024-04-07 06:02:57,018 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:02:57,018 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:02:57,018 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:02:57,018 | app.py:229 | app_fit: losses_centralized [(0, 2.3029398918151855), (1, 1.684876799583435), (2, 1.575603723526001), (3, 1.5728956460952759), (4, 1.5608854293823242), (5, 1.56153404712677), (6, 1.5545501708984375), (7, 1.5532655715942383), (8, 1.5492372512817383), (9, 1.5726618766784668), (10, 1.5494256019592285)]
INFO flwr 2024-04-07 06:02:57,018 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0852), (1, 0.7897), (2, 0.8952), (3, 0.8944), (4, 0.9062), (5, 0.9031), (6, 0.9098), (7, 0.9116), (8, 0.9156), (9, 0.8927), (10, 0.914)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.914
wandb:     loss 1.54943
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_055515-fd5pxlcw
wandb: Find logs at: ./wandb/offline-run-20240407_055515-fd5pxlcw/logs
INFO flwr 2024-04-07 06:03:00,628 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:10:23,885 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1731139)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1731139)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:10:28,785	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:10:29,236	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:10:29,678	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_82b7cbfffcfa8681.zip' (11.32MiB) to Ray cluster...
2024-04-07 06:10:29,719	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_82b7cbfffcfa8681.zip'.
INFO flwr 2024-04-07 06:10:41,024 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 118931733504.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55256457216.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 06:10:41,024 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:10:41,025 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:10:41,041 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:10:41,042 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:10:41,042 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:10:41,043 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:10:43,041 | server.py:94 | initial parameters (loss, other metrics): 2.3060481548309326, {'accuracy': 0.0384, 'data_size': 10000}
INFO flwr 2024-04-07 06:10:43,042 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:10:43,042 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1739601)[0m 2024-04-07 06:10:47.763904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1739601)[0m 2024-04-07 06:10:47.861450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1739601)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1739607)[0m 2024-04-07 06:10:50.300982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1739602)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1739602)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1739603)[0m 2024-04-07 06:10:47.977691: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1739603)[0m 2024-04-07 06:10:48.068025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1739603)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1739608)[0m 2024-04-07 06:10:50.544152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:11:04,619 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:11:04,652 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:11:07,222 | server.py:125 | fit progress: (1, 2.3053667545318604, {'accuracy': 0.041, 'data_size': 10000}, 24.179660306021105)
INFO flwr 2024-04-07 06:11:07,222 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:11:07,222 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:11:16,785 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:11:19,845 | server.py:125 | fit progress: (2, 2.304422616958618, {'accuracy': 0.0466, 'data_size': 10000}, 36.802627808996476)
INFO flwr 2024-04-07 06:11:19,845 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:11:19,845 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:11:28,497 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:11:30,934 | server.py:125 | fit progress: (3, 2.3033533096313477, {'accuracy': 0.0562, 'data_size': 10000}, 47.891508075001184)
INFO flwr 2024-04-07 06:11:30,934 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:11:30,934 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:11:39,604 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:11:42,222 | server.py:125 | fit progress: (4, 2.3024487495422363, {'accuracy': 0.06, 'data_size': 10000}, 59.17932540801121)
INFO flwr 2024-04-07 06:11:42,222 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:11:42,222 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:11:50,848 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:11:52,497 | server.py:125 | fit progress: (5, 2.301591157913208, {'accuracy': 0.069, 'data_size': 10000}, 69.45456983300392)
INFO flwr 2024-04-07 06:11:52,497 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:11:52,498 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:12:02,003 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:12:03,903 | server.py:125 | fit progress: (6, 2.3005967140197754, {'accuracy': 0.077, 'data_size': 10000}, 80.8608382960083)
INFO flwr 2024-04-07 06:12:03,904 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:12:03,905 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:12:12,912 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:12:15,138 | server.py:125 | fit progress: (7, 2.299686908721924, {'accuracy': 0.082, 'data_size': 10000}, 92.09532892602147)
INFO flwr 2024-04-07 06:12:15,138 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:12:15,138 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:12:23,689 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:12:25,653 | server.py:125 | fit progress: (8, 2.298762798309326, {'accuracy': 0.0936, 'data_size': 10000}, 102.61118799299584)
INFO flwr 2024-04-07 06:12:25,654 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:12:25,654 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:12:34,936 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:12:37,811 | server.py:125 | fit progress: (9, 2.2977375984191895, {'accuracy': 0.1137, 'data_size': 10000}, 114.76866956500453)
INFO flwr 2024-04-07 06:12:37,811 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:12:37,811 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:12:46,686 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:12:48,706 | server.py:125 | fit progress: (10, 2.296780586242676, {'accuracy': 0.1218, 'data_size': 10000}, 125.66381907000323)
INFO flwr 2024-04-07 06:12:48,707 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:12:48,707 | server.py:153 | FL finished in 125.66450123500545
INFO flwr 2024-04-07 06:12:48,707 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:12:48,707 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:12:48,707 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:12:48,707 | app.py:229 | app_fit: losses_centralized [(0, 2.3060481548309326), (1, 2.3053667545318604), (2, 2.304422616958618), (3, 2.3033533096313477), (4, 2.3024487495422363), (5, 2.301591157913208), (6, 2.3005967140197754), (7, 2.299686908721924), (8, 2.298762798309326), (9, 2.2977375984191895), (10, 2.296780586242676)]
INFO flwr 2024-04-07 06:12:48,707 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0384), (1, 0.041), (2, 0.0466), (3, 0.0562), (4, 0.06), (5, 0.069), (6, 0.077), (7, 0.082), (8, 0.0936), (9, 0.1137), (10, 0.1218)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1218
wandb:     loss 2.29678
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_061021-o6phd7vt
wandb: Find logs at: ./wandb/offline-run-20240407_061021-o6phd7vt/logs
INFO flwr 2024-04-07 06:12:52,248 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:20:01,751 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1739603)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1739603)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:20:07,203	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:20:07,681	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:20:08,092	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_847f20727b6354fc.zip' (11.33MiB) to Ray cluster...
2024-04-07 06:20:08,131	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_847f20727b6354fc.zip'.
INFO flwr 2024-04-07 06:20:19,260 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 54944308838.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 118203387290.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 06:20:19,260 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:20:19,260 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:20:19,286 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:20:19,287 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:20:19,288 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:20:19,288 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:20:21,302 | server.py:94 | initial parameters (loss, other metrics): 2.3039090633392334, {'accuracy': 0.0686, 'data_size': 10000}
INFO flwr 2024-04-07 06:20:21,303 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:20:21,303 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1743988)[0m 2024-04-07 06:20:25.517722: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1743988)[0m 2024-04-07 06:20:25.610060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1743988)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1743993)[0m 2024-04-07 06:20:27.700085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1743993)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1743993)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1743992)[0m 2024-04-07 06:20:25.738304: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1743992)[0m 2024-04-07 06:20:25.828761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1743992)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1743992)[0m 2024-04-07 06:20:28.314161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1743995)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1743995)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-07 06:20:42,187 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:20:42,226 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:20:43,730 | server.py:125 | fit progress: (1, 2.2199440002441406, {'accuracy': 0.4061, 'data_size': 10000}, 22.427043659001356)
INFO flwr 2024-04-07 06:20:43,730 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:20:43,731 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:20:54,066 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:20:55,591 | server.py:125 | fit progress: (2, 2.1078901290893555, {'accuracy': 0.5152, 'data_size': 10000}, 34.28778291499475)
INFO flwr 2024-04-07 06:20:55,591 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:20:55,591 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:03,498 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:04,755 | server.py:125 | fit progress: (3, 2.043687582015991, {'accuracy': 0.5356, 'data_size': 10000}, 43.451627754984656)
INFO flwr 2024-04-07 06:21:04,755 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:04,755 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:13,047 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:14,548 | server.py:125 | fit progress: (4, 1.9762718677520752, {'accuracy': 0.6216, 'data_size': 10000}, 53.2447757019836)
INFO flwr 2024-04-07 06:21:14,548 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:14,548 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:23,120 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:24,419 | server.py:125 | fit progress: (5, 1.9236736297607422, {'accuracy': 0.6642, 'data_size': 10000}, 63.11621509399265)
INFO flwr 2024-04-07 06:21:24,420 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:24,420 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:33,570 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:34,944 | server.py:125 | fit progress: (6, 1.8832509517669678, {'accuracy': 0.7551, 'data_size': 10000}, 73.6410255600058)
INFO flwr 2024-04-07 06:21:34,944 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:34,945 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:43,404 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:45,026 | server.py:125 | fit progress: (7, 1.8632161617279053, {'accuracy': 0.7309, 'data_size': 10000}, 83.72336528700544)
INFO flwr 2024-04-07 06:21:45,027 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:45,027 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:21:53,722 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:21:55,420 | server.py:125 | fit progress: (8, 1.8225886821746826, {'accuracy': 0.8191, 'data_size': 10000}, 94.11681502001011)
INFO flwr 2024-04-07 06:21:55,420 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:21:55,420 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:22:04,195 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:22:05,943 | server.py:125 | fit progress: (9, 1.798270344734192, {'accuracy': 0.8178, 'data_size': 10000}, 104.64001747700968)
INFO flwr 2024-04-07 06:22:05,943 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:22:05,943 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:22:14,489 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:22:15,915 | server.py:125 | fit progress: (10, 1.7756904363632202, {'accuracy': 0.8395, 'data_size': 10000}, 114.6123746910016)
INFO flwr 2024-04-07 06:22:15,916 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:22:15,916 | server.py:153 | FL finished in 114.61281044900534
INFO flwr 2024-04-07 06:22:15,916 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:22:15,916 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:22:15,916 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:22:15,916 | app.py:229 | app_fit: losses_centralized [(0, 2.3039090633392334), (1, 2.2199440002441406), (2, 2.1078901290893555), (3, 2.043687582015991), (4, 1.9762718677520752), (5, 1.9236736297607422), (6, 1.8832509517669678), (7, 1.8632161617279053), (8, 1.8225886821746826), (9, 1.798270344734192), (10, 1.7756904363632202)]
INFO flwr 2024-04-07 06:22:15,916 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0686), (1, 0.4061), (2, 0.5152), (3, 0.5356), (4, 0.6216), (5, 0.6642), (6, 0.7551), (7, 0.7309), (8, 0.8191), (9, 0.8178), (10, 0.8395)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8395
wandb:     loss 1.77569
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_062001-5qp1ujpi
wandb: Find logs at: ./wandb/offline-run-20240407_062001-5qp1ujpi/logs
INFO flwr 2024-04-07 06:22:19,421 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:29:37,265 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1743983)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1743983)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-07 06:29:43,495	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:29:44,075	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:29:44,424	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9f13c078edac76d4.zip' (11.35MiB) to Ray cluster...
2024-04-07 06:29:44,464	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9f13c078edac76d4.zip'.
INFO flwr 2024-04-07 06:29:55,710 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 117876787405.0, 'CPU': 64.0, 'object_store_memory': 54804337459.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 06:29:55,711 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:29:55,711 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:29:55,727 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:29:55,729 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:29:55,729 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:29:55,729 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:29:58,259 | server.py:94 | initial parameters (loss, other metrics): 2.30361270904541, {'accuracy': 0.0681, 'data_size': 10000}
INFO flwr 2024-04-07 06:29:58,259 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:29:58,260 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1751701)[0m 2024-04-07 06:30:02.534444: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1751701)[0m 2024-04-07 06:30:02.635484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1751701)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1751707)[0m 2024-04-07 06:30:05.136080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1751700)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1751700)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1751699)[0m 2024-04-07 06:30:02.781998: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1751700)[0m 2024-04-07 06:30:02.843499: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1751700)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1751699)[0m 2024-04-07 06:30:05.226967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:30:19,437 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:30:19,474 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:30:24,020 | server.py:125 | fit progress: (1, 2.176072597503662, {'accuracy': 0.5008, 'data_size': 10000}, 25.760240379022434)
INFO flwr 2024-04-07 06:30:24,020 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:30:24,020 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:30:33,552 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:30:34,935 | server.py:125 | fit progress: (2, 2.0261383056640625, {'accuracy': 0.5309, 'data_size': 10000}, 36.67498872100259)
INFO flwr 2024-04-07 06:30:34,935 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:30:34,935 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:30:42,908 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:30:44,362 | server.py:125 | fit progress: (3, 1.9183968305587769, {'accuracy': 0.6888, 'data_size': 10000}, 46.102668575011194)
INFO flwr 2024-04-07 06:30:44,363 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:30:44,363 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:30:52,832 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:30:54,101 | server.py:125 | fit progress: (4, 1.8678162097930908, {'accuracy': 0.7208, 'data_size': 10000}, 55.84174567100126)
INFO flwr 2024-04-07 06:30:54,102 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:30:54,102 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:02,328 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:03,629 | server.py:125 | fit progress: (5, 1.8164010047912598, {'accuracy': 0.7836, 'data_size': 10000}, 65.36955934102298)
INFO flwr 2024-04-07 06:31:03,629 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:31:03,630 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:12,075 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:13,613 | server.py:125 | fit progress: (6, 1.8027719259262085, {'accuracy': 0.7543, 'data_size': 10000}, 75.35358618802275)
INFO flwr 2024-04-07 06:31:13,613 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:31:13,614 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:22,007 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:23,595 | server.py:125 | fit progress: (7, 1.7695066928863525, {'accuracy': 0.8151, 'data_size': 10000}, 85.33583752001869)
INFO flwr 2024-04-07 06:31:23,596 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:31:23,596 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:32,318 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:35,079 | server.py:125 | fit progress: (8, 1.7371398210525513, {'accuracy': 0.8419, 'data_size': 10000}, 96.81907187201432)
INFO flwr 2024-04-07 06:31:35,079 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:31:35,079 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:43,723 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:47,003 | server.py:125 | fit progress: (9, 1.7365392446517944, {'accuracy': 0.8199, 'data_size': 10000}, 108.74356348000583)
INFO flwr 2024-04-07 06:31:47,003 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:31:47,004 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:31:55,427 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:31:59,431 | server.py:125 | fit progress: (10, 1.7089366912841797, {'accuracy': 0.8497, 'data_size': 10000}, 121.1714066590066)
INFO flwr 2024-04-07 06:31:59,431 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:31:59,431 | server.py:153 | FL finished in 121.17185564001556
INFO flwr 2024-04-07 06:31:59,432 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:31:59,432 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:31:59,432 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:31:59,432 | app.py:229 | app_fit: losses_centralized [(0, 2.30361270904541), (1, 2.176072597503662), (2, 2.0261383056640625), (3, 1.9183968305587769), (4, 1.8678162097930908), (5, 1.8164010047912598), (6, 1.8027719259262085), (7, 1.7695066928863525), (8, 1.7371398210525513), (9, 1.7365392446517944), (10, 1.7089366912841797)]
INFO flwr 2024-04-07 06:31:59,432 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0681), (1, 0.5008), (2, 0.5309), (3, 0.6888), (4, 0.7208), (5, 0.7836), (6, 0.7543), (7, 0.8151), (8, 0.8419), (9, 0.8199), (10, 0.8497)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8497
wandb:     loss 1.70894
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_062936-kc4c1qzy
wandb: Find logs at: ./wandb/offline-run-20240407_062936-kc4c1qzy/logs
INFO flwr 2024-04-07 06:32:02,934 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:39:26,101 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1751702)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1751702)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:39:32,789	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:39:33,210	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:39:33,627	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ffd23dea9f6f94a5.zip' (11.37MiB) to Ray cluster...
2024-04-07 06:39:33,669	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ffd23dea9f6f94a5.zip'.
INFO flwr 2024-04-07 06:39:44,946 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 110883394151.0, 'object_store_memory': 51807168921.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 06:39:44,946 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:39:44,946 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:39:44,961 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:39:44,962 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:39:44,962 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:39:44,962 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:39:48,715 | server.py:94 | initial parameters (loss, other metrics): 2.3072807788848877, {'accuracy': 0.0542, 'data_size': 10000}
INFO flwr 2024-04-07 06:39:48,716 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:39:48,716 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1756148)[0m 2024-04-07 06:39:51.492132: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1756148)[0m 2024-04-07 06:39:51.600843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1756148)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1756148)[0m 2024-04-07 06:39:53.963522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1756148)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1756148)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1756144)[0m 2024-04-07 06:39:51.731342: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1756144)[0m 2024-04-07 06:39:51.828974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1756144)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1756146)[0m 2024-04-07 06:39:54.094766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:40:08,505 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:40:08,536 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:40:09,769 | server.py:125 | fit progress: (1, 2.1494429111480713, {'accuracy': 0.5781, 'data_size': 10000}, 21.05323037499329)
INFO flwr 2024-04-07 06:40:09,770 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:40:09,770 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:40:19,377 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:40:20,660 | server.py:125 | fit progress: (2, 1.9688136577606201, {'accuracy': 0.6788, 'data_size': 10000}, 31.94456541101681)
INFO flwr 2024-04-07 06:40:20,661 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:40:20,661 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:40:30,274 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:40:31,895 | server.py:125 | fit progress: (3, 1.8388761281967163, {'accuracy': 0.8002, 'data_size': 10000}, 43.179130839009304)
INFO flwr 2024-04-07 06:40:31,895 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:40:31,896 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:40:40,792 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:40:42,400 | server.py:125 | fit progress: (4, 1.7703025341033936, {'accuracy': 0.8368, 'data_size': 10000}, 53.68431711901212)
INFO flwr 2024-04-07 06:40:42,401 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:40:42,401 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:40:51,699 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:40:53,357 | server.py:125 | fit progress: (5, 1.7379783391952515, {'accuracy': 0.8339, 'data_size': 10000}, 64.64095373600139)
INFO flwr 2024-04-07 06:40:53,357 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:40:53,357 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:41:02,336 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:41:03,705 | server.py:125 | fit progress: (6, 1.7116093635559082, {'accuracy': 0.8574, 'data_size': 10000}, 74.98911232501268)
INFO flwr 2024-04-07 06:41:03,705 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:41:03,706 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:41:13,098 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:41:14,830 | server.py:125 | fit progress: (7, 1.7008298635482788, {'accuracy': 0.847, 'data_size': 10000}, 86.11364244000288)
INFO flwr 2024-04-07 06:41:14,830 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:41:14,830 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:41:24,129 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:41:25,865 | server.py:125 | fit progress: (8, 1.679269552230835, {'accuracy': 0.8705, 'data_size': 10000}, 97.14892604300985)
INFO flwr 2024-04-07 06:41:25,865 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:41:25,865 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:41:34,540 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:41:36,365 | server.py:125 | fit progress: (9, 1.6705472469329834, {'accuracy': 0.8752, 'data_size': 10000}, 107.64911154299625)
INFO flwr 2024-04-07 06:41:36,365 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:41:36,366 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:41:45,952 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:41:47,447 | server.py:125 | fit progress: (10, 1.6632040739059448, {'accuracy': 0.875, 'data_size': 10000}, 118.73077216101228)
INFO flwr 2024-04-07 06:41:47,447 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:41:47,447 | server.py:153 | FL finished in 118.73118603602052
INFO flwr 2024-04-07 06:41:47,447 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:41:47,447 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:41:47,447 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:41:47,448 | app.py:229 | app_fit: losses_centralized [(0, 2.3072807788848877), (1, 2.1494429111480713), (2, 1.9688136577606201), (3, 1.8388761281967163), (4, 1.7703025341033936), (5, 1.7379783391952515), (6, 1.7116093635559082), (7, 1.7008298635482788), (8, 1.679269552230835), (9, 1.6705472469329834), (10, 1.6632040739059448)]
INFO flwr 2024-04-07 06:41:47,448 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0542), (1, 0.5781), (2, 0.6788), (3, 0.8002), (4, 0.8368), (5, 0.8339), (6, 0.8574), (7, 0.847), (8, 0.8705), (9, 0.8752), (10, 0.875)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.875
wandb:     loss 1.6632
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_063925-uzmfolbn
wandb: Find logs at: ./wandb/offline-run-20240407_063925-uzmfolbn/logs
INFO flwr 2024-04-07 06:41:50,995 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:49:00,246 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1756135)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1756135)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:49:05,441	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:49:05,706	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:49:06,163	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5c0747664e08b64f.zip' (11.39MiB) to Ray cluster...
2024-04-07 06:49:06,208	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5c0747664e08b64f.zip'.
INFO flwr 2024-04-07 06:49:17,129 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 116917439488.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'object_store_memory': 54393188352.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 06:49:17,129 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:49:17,129 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:49:17,144 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:49:17,145 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:49:17,145 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:49:17,145 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:49:19,878 | server.py:94 | initial parameters (loss, other metrics): 2.3005738258361816, {'accuracy': 0.1305, 'data_size': 10000}
INFO flwr 2024-04-07 06:49:19,879 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:49:19,880 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1763048)[0m 2024-04-07 06:49:22.944251: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1763048)[0m 2024-04-07 06:49:23.032075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1763048)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1763052)[0m 2024-04-07 06:49:25.131676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1763053)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1763053)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1763046)[0m 2024-04-07 06:49:23.781799: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1763046)[0m 2024-04-07 06:49:23.877777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1763046)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1763057)[0m 2024-04-07 06:49:25.918793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:49:37,960 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:49:38,010 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:49:39,404 | server.py:125 | fit progress: (1, 2.0705556869506836, {'accuracy': 0.5441, 'data_size': 10000}, 19.524511108989827)
INFO flwr 2024-04-07 06:49:39,404 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:49:39,405 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:49:48,676 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:49:50,184 | server.py:125 | fit progress: (2, 1.9407007694244385, {'accuracy': 0.607, 'data_size': 10000}, 30.304840700002387)
INFO flwr 2024-04-07 06:49:50,185 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:49:50,185 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:49:58,225 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:49:59,675 | server.py:125 | fit progress: (3, 1.8173136711120605, {'accuracy': 0.7619, 'data_size': 10000}, 39.79564688599203)
INFO flwr 2024-04-07 06:49:59,676 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:49:59,676 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:07,731 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:08,998 | server.py:125 | fit progress: (4, 1.7460888624191284, {'accuracy': 0.8343, 'data_size': 10000}, 49.11859020899283)
INFO flwr 2024-04-07 06:50:08,998 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:08,999 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:17,834 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:19,186 | server.py:125 | fit progress: (5, 1.7202929258346558, {'accuracy': 0.838, 'data_size': 10000}, 59.3068041089864)
INFO flwr 2024-04-07 06:50:19,187 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:19,187 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:27,398 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:28,968 | server.py:125 | fit progress: (6, 1.6891257762908936, {'accuracy': 0.8675, 'data_size': 10000}, 69.08860057900893)
INFO flwr 2024-04-07 06:50:28,968 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:28,969 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:37,337 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:38,944 | server.py:125 | fit progress: (7, 1.6789724826812744, {'accuracy': 0.8629, 'data_size': 10000}, 79.06455329500022)
INFO flwr 2024-04-07 06:50:38,944 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:38,945 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:47,930 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:49,417 | server.py:125 | fit progress: (8, 1.6603541374206543, {'accuracy': 0.8752, 'data_size': 10000}, 89.53709880801034)
INFO flwr 2024-04-07 06:50:49,417 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:49,417 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:50:58,123 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 06:50:59,843 | server.py:125 | fit progress: (9, 1.654587745666504, {'accuracy': 0.8757, 'data_size': 10000}, 99.96340204699663)
INFO flwr 2024-04-07 06:50:59,843 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 06:50:59,844 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:51:08,124 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 06:51:09,587 | server.py:125 | fit progress: (10, 1.639128565788269, {'accuracy': 0.8855, 'data_size': 10000}, 109.70776499100612)
INFO flwr 2024-04-07 06:51:09,588 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 06:51:09,588 | server.py:153 | FL finished in 109.70838096999796
INFO flwr 2024-04-07 06:51:09,588 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 06:51:09,588 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 06:51:09,588 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 06:51:09,589 | app.py:229 | app_fit: losses_centralized [(0, 2.3005738258361816), (1, 2.0705556869506836), (2, 1.9407007694244385), (3, 1.8173136711120605), (4, 1.7460888624191284), (5, 1.7202929258346558), (6, 1.6891257762908936), (7, 1.6789724826812744), (8, 1.6603541374206543), (9, 1.654587745666504), (10, 1.639128565788269)]
INFO flwr 2024-04-07 06:51:09,589 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1305), (1, 0.5441), (2, 0.607), (3, 0.7619), (4, 0.8343), (5, 0.838), (6, 0.8675), (7, 0.8629), (8, 0.8752), (9, 0.8757), (10, 0.8855)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8855
wandb:     loss 1.63913
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_064859-4oe6mb3w
wandb: Find logs at: ./wandb/offline-run-20240407_064859-4oe6mb3w/logs
INFO flwr 2024-04-07 06:51:13,160 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 06:58:36,510 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1763046)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1763046)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 06:58:41,543	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 06:58:42,454	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 06:58:42,799	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b7dda2f1fef7adf0.zip' (11.40MiB) to Ray cluster...
2024-04-07 06:58:42,835	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b7dda2f1fef7adf0.zip'.
INFO flwr 2024-04-07 06:58:53,849 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 117409820877.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54604208947.0}
INFO flwr 2024-04-07 06:58:53,850 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 06:58:53,851 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 06:58:53,870 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 06:58:53,873 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 06:58:53,874 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 06:58:53,874 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 06:58:56,170 | server.py:94 | initial parameters (loss, other metrics): 2.3013083934783936, {'accuracy': 0.0975, 'data_size': 10000}
INFO flwr 2024-04-07 06:58:56,171 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 06:58:56,171 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1767611)[0m 2024-04-07 06:59:00.319421: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1767619)[0m 2024-04-07 06:59:00.459986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1767619)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1767619)[0m 2024-04-07 06:59:02.717614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1767608)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1767608)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1767616)[0m 2024-04-07 06:59:00.500895: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1767616)[0m 2024-04-07 06:59:00.599380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1767616)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1767613)[0m 2024-04-07 06:59:02.831966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 06:59:15,572 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 06:59:15,602 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 06:59:17,735 | server.py:125 | fit progress: (1, 2.060441732406616, {'accuracy': 0.5033, 'data_size': 10000}, 21.563736001000507)
INFO flwr 2024-04-07 06:59:17,735 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 06:59:17,735 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:59:27,322 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 06:59:29,604 | server.py:125 | fit progress: (2, 1.917748212814331, {'accuracy': 0.645, 'data_size': 10000}, 33.43264793898561)
INFO flwr 2024-04-07 06:59:29,604 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 06:59:29,605 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:59:38,046 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 06:59:40,035 | server.py:125 | fit progress: (3, 1.8001290559768677, {'accuracy': 0.7434, 'data_size': 10000}, 43.863997582986485)
INFO flwr 2024-04-07 06:59:40,035 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 06:59:40,036 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:59:48,263 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 06:59:50,681 | server.py:125 | fit progress: (4, 1.7442048788070679, {'accuracy': 0.8008, 'data_size': 10000}, 54.5102256659884)
INFO flwr 2024-04-07 06:59:50,682 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 06:59:50,682 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 06:59:59,073 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:01,076 | server.py:125 | fit progress: (5, 1.6995145082473755, {'accuracy': 0.8568, 'data_size': 10000}, 64.90449357000762)
INFO flwr 2024-04-07 07:00:01,076 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:00:01,076 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:00:08,918 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:11,414 | server.py:125 | fit progress: (6, 1.6778361797332764, {'accuracy': 0.8646, 'data_size': 10000}, 75.24304574501002)
INFO flwr 2024-04-07 07:00:11,414 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:00:11,415 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:00:19,545 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:21,525 | server.py:125 | fit progress: (7, 1.6552437543869019, {'accuracy': 0.8773, 'data_size': 10000}, 85.35410908499034)
INFO flwr 2024-04-07 07:00:21,526 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:00:21,526 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:00:29,911 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:31,301 | server.py:125 | fit progress: (8, 1.6479986906051636, {'accuracy': 0.8801, 'data_size': 10000}, 95.13016119899112)
INFO flwr 2024-04-07 07:00:31,302 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:00:31,302 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:00:39,816 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:41,243 | server.py:125 | fit progress: (9, 1.6422345638275146, {'accuracy': 0.8771, 'data_size': 10000}, 105.07215247000568)
INFO flwr 2024-04-07 07:00:41,244 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:00:41,244 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:00:49,826 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:00:51,566 | server.py:125 | fit progress: (10, 1.6313024759292603, {'accuracy': 0.8846, 'data_size': 10000}, 115.39456476698979)
INFO flwr 2024-04-07 07:00:51,566 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:00:51,566 | server.py:153 | FL finished in 115.39506592400721
INFO flwr 2024-04-07 07:00:51,566 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:00:51,566 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:00:51,567 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:00:51,567 | app.py:229 | app_fit: losses_centralized [(0, 2.3013083934783936), (1, 2.060441732406616), (2, 1.917748212814331), (3, 1.8001290559768677), (4, 1.7442048788070679), (5, 1.6995145082473755), (6, 1.6778361797332764), (7, 1.6552437543869019), (8, 1.6479986906051636), (9, 1.6422345638275146), (10, 1.6313024759292603)]
INFO flwr 2024-04-07 07:00:51,567 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0975), (1, 0.5033), (2, 0.645), (3, 0.7434), (4, 0.8008), (5, 0.8568), (6, 0.8646), (7, 0.8773), (8, 0.8801), (9, 0.8771), (10, 0.8846)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8846
wandb:     loss 1.6313
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_065833-c62tfi5l
wandb: Find logs at: ./wandb/offline-run-20240407_065833-c62tfi5l/logs
INFO flwr 2024-04-07 07:00:55,189 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:08:04,313 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1767613)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1767613)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:08:09,650	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:08:10,106	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:08:10,558	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_98eb38907571f276.zip' (11.42MiB) to Ray cluster...
2024-04-07 07:08:10,598	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_98eb38907571f276.zip'.
INFO flwr 2024-04-07 07:08:21,417 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54396359884.0, 'memory': 116924839732.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 07:08:21,418 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:08:21,418 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:08:21,439 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:08:21,441 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:08:21,441 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:08:21,441 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:08:23,939 | server.py:94 | initial parameters (loss, other metrics): 2.304145574569702, {'accuracy': 0.0875, 'data_size': 10000}
INFO flwr 2024-04-07 07:08:23,939 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:08:23,940 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1772178)[0m 2024-04-07 07:08:27.289901: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1772178)[0m 2024-04-07 07:08:27.388500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1772178)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1772169)[0m 2024-04-07 07:08:29.589927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1772177)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1772177)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1772172)[0m 2024-04-07 07:08:27.944862: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1772172)[0m 2024-04-07 07:08:28.053000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1772172)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1772172)[0m 2024-04-07 07:08:30.306697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:08:42,353 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:08:42,393 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:08:43,804 | server.py:125 | fit progress: (1, 2.1032912731170654, {'accuracy': 0.5677, 'data_size': 10000}, 19.863731065008324)
INFO flwr 2024-04-07 07:08:43,804 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:08:43,804 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:08:52,996 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:08:54,208 | server.py:125 | fit progress: (2, 1.896016240119934, {'accuracy': 0.7083, 'data_size': 10000}, 30.26830723200692)
INFO flwr 2024-04-07 07:08:54,209 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:08:54,209 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:02,291 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:03,738 | server.py:125 | fit progress: (3, 1.7623175382614136, {'accuracy': 0.8226, 'data_size': 10000}, 39.79806502198335)
INFO flwr 2024-04-07 07:09:03,738 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:03,738 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:11,525 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:13,025 | server.py:125 | fit progress: (4, 1.741241216659546, {'accuracy': 0.7855, 'data_size': 10000}, 49.08498515401152)
INFO flwr 2024-04-07 07:09:13,025 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:13,025 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:21,668 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:23,200 | server.py:125 | fit progress: (5, 1.685254454612732, {'accuracy': 0.8485, 'data_size': 10000}, 59.26055765498313)
INFO flwr 2024-04-07 07:09:23,201 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:23,201 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:31,454 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:32,777 | server.py:125 | fit progress: (6, 1.657637357711792, {'accuracy': 0.878, 'data_size': 10000}, 68.83720242299023)
INFO flwr 2024-04-07 07:09:32,777 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:32,778 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:41,267 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:42,923 | server.py:125 | fit progress: (7, 1.6423242092132568, {'accuracy': 0.882, 'data_size': 10000}, 78.9826912629942)
INFO flwr 2024-04-07 07:09:42,923 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:42,923 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:09:50,986 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:09:52,654 | server.py:125 | fit progress: (8, 1.6307199001312256, {'accuracy': 0.8877, 'data_size': 10000}, 88.71371136198286)
INFO flwr 2024-04-07 07:09:52,654 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:09:52,654 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:10:01,132 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:10:02,843 | server.py:125 | fit progress: (9, 1.6237064599990845, {'accuracy': 0.8884, 'data_size': 10000}, 98.90316040898324)
INFO flwr 2024-04-07 07:10:02,843 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:10:02,843 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:10:11,239 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:10:12,726 | server.py:125 | fit progress: (10, 1.6246577501296997, {'accuracy': 0.8848, 'data_size': 10000}, 108.78617177999695)
INFO flwr 2024-04-07 07:10:12,726 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:10:12,726 | server.py:153 | FL finished in 108.78667117800796
INFO flwr 2024-04-07 07:10:12,727 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:10:12,727 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:10:12,727 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:10:12,727 | app.py:229 | app_fit: losses_centralized [(0, 2.304145574569702), (1, 2.1032912731170654), (2, 1.896016240119934), (3, 1.7623175382614136), (4, 1.741241216659546), (5, 1.685254454612732), (6, 1.657637357711792), (7, 1.6423242092132568), (8, 1.6307199001312256), (9, 1.6237064599990845), (10, 1.6246577501296997)]
INFO flwr 2024-04-07 07:10:12,727 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0875), (1, 0.5677), (2, 0.7083), (3, 0.8226), (4, 0.7855), (5, 0.8485), (6, 0.878), (7, 0.882), (8, 0.8877), (9, 0.8884), (10, 0.8848)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8848
wandb:     loss 1.62466
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_070803-2x610tnx
wandb: Find logs at: ./wandb/offline-run-20240407_070803-2x610tnx/logs
INFO flwr 2024-04-07 07:10:16,278 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:17:28,959 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1772172)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1772172)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:17:39,752	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:17:40,666	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:17:41,145	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_899ed6f9330b3cbf.zip' (11.44MiB) to Ray cluster...
2024-04-07 07:17:41,182	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_899ed6f9330b3cbf.zip'.
INFO flwr 2024-04-07 07:17:52,106 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 117175865959.0, 'node:__internal_head__': 1.0, 'object_store_memory': 54503942553.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 07:17:52,106 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:17:52,106 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:17:52,130 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:17:52,131 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:17:52,131 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:17:52,131 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:17:54,971 | server.py:94 | initial parameters (loss, other metrics): 2.3045389652252197, {'accuracy': 0.0831, 'data_size': 10000}
INFO flwr 2024-04-07 07:17:54,972 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:17:54,972 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1779680)[0m 2024-04-07 07:17:59.161501: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1779680)[0m 2024-04-07 07:17:59.261433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1779680)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1779689)[0m 2024-04-07 07:18:02.081578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1779689)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1779689)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1779677)[0m 2024-04-07 07:17:59.399696: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1779677)[0m 2024-04-07 07:17:59.481629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1779677)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1779677)[0m 2024-04-07 07:18:02.106016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:18:27,341 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:18:27,385 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:18:31,367 | server.py:125 | fit progress: (1, 2.2998874187469482, {'accuracy': 0.1461, 'data_size': 10000}, 36.39427351401537)
INFO flwr 2024-04-07 07:18:31,367 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:18:31,367 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:18:40,261 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:18:42,082 | server.py:125 | fit progress: (2, 2.2940328121185303, {'accuracy': 0.2118, 'data_size': 10000}, 47.10991625499446)
INFO flwr 2024-04-07 07:18:42,082 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:18:42,083 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:18:51,217 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:18:53,257 | server.py:125 | fit progress: (3, 2.2855453491210938, {'accuracy': 0.1393, 'data_size': 10000}, 58.28481566300616)
INFO flwr 2024-04-07 07:18:53,257 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:18:53,258 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:03,112 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:04,827 | server.py:125 | fit progress: (4, 2.28066349029541, {'accuracy': 0.1477, 'data_size': 10000}, 69.85487805801677)
INFO flwr 2024-04-07 07:19:04,827 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:04,828 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:14,817 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:16,089 | server.py:125 | fit progress: (5, 2.273484945297241, {'accuracy': 0.2208, 'data_size': 10000}, 81.11712907301262)
INFO flwr 2024-04-07 07:19:16,090 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:16,090 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:25,714 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:27,077 | server.py:125 | fit progress: (6, 2.26548433303833, {'accuracy': 0.3037, 'data_size': 10000}, 92.10498021301464)
INFO flwr 2024-04-07 07:19:27,078 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:27,078 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:36,259 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:37,957 | server.py:125 | fit progress: (7, 2.256026268005371, {'accuracy': 0.2937, 'data_size': 10000}, 102.9850926460058)
INFO flwr 2024-04-07 07:19:37,958 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:37,958 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:46,616 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:48,353 | server.py:125 | fit progress: (8, 2.246283769607544, {'accuracy': 0.3336, 'data_size': 10000}, 113.38042690901784)
INFO flwr 2024-04-07 07:19:48,353 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:48,353 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:19:57,010 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:19:58,458 | server.py:125 | fit progress: (9, 2.2364726066589355, {'accuracy': 0.2531, 'data_size': 10000}, 123.48565092199715)
INFO flwr 2024-04-07 07:19:58,458 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:19:58,458 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:20:07,112 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:20:08,622 | server.py:125 | fit progress: (10, 2.227128028869629, {'accuracy': 0.2903, 'data_size': 10000}, 133.6499228850007)
INFO flwr 2024-04-07 07:20:08,622 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:20:08,623 | server.py:153 | FL finished in 133.65042251101113
INFO flwr 2024-04-07 07:20:08,623 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:20:08,623 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:20:08,623 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:20:08,623 | app.py:229 | app_fit: losses_centralized [(0, 2.3045389652252197), (1, 2.2998874187469482), (2, 2.2940328121185303), (3, 2.2855453491210938), (4, 2.28066349029541), (5, 2.273484945297241), (6, 2.26548433303833), (7, 2.256026268005371), (8, 2.246283769607544), (9, 2.2364726066589355), (10, 2.227128028869629)]
INFO flwr 2024-04-07 07:20:08,623 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0831), (1, 0.1461), (2, 0.2118), (3, 0.1393), (4, 0.1477), (5, 0.2208), (6, 0.3037), (7, 0.2937), (8, 0.3336), (9, 0.2531), (10, 0.2903)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2903
wandb:     loss 2.22713
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_071727-nmabty01
wandb: Find logs at: ./wandb/offline-run-20240407_071727-nmabty01/logs
INFO flwr 2024-04-07 07:20:12,210 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:27:22,465 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1779680)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1779680)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:27:27,915	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:27:28,266	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:27:28,732	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_967d11b1b3ae4991.zip' (11.46MiB) to Ray cluster...
2024-04-07 07:27:28,764	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_967d11b1b3ae4991.zip'.
INFO flwr 2024-04-07 07:27:40,853 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 125523529524.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 58081512652.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 07:27:40,853 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:27:40,854 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:27:40,870 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:27:40,871 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:27:40,871 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:27:40,871 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:27:43,515 | server.py:94 | initial parameters (loss, other metrics): 2.2993783950805664, {'accuracy': 0.1101, 'data_size': 10000}
INFO flwr 2024-04-07 07:27:43,515 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:27:43,515 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1785514)[0m 2024-04-07 07:27:47.697080: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1785514)[0m 2024-04-07 07:27:47.798488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1785514)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1785525)[0m 2024-04-07 07:27:50.133886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1785520)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1785520)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1785519)[0m 2024-04-07 07:27:47.948803: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785519)[0m 2024-04-07 07:27:48.056740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785519)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1785510)[0m 2024-04-07 07:27:50.156063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:28:07,050 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:28:07,085 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:28:08,436 | server.py:125 | fit progress: (1, 2.1028010845184326, {'accuracy': 0.5199, 'data_size': 10000}, 24.920369553001365)
INFO flwr 2024-04-07 07:28:08,436 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:08,436 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:18,037 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:19,502 | server.py:125 | fit progress: (2, 1.9104232788085938, {'accuracy': 0.6553, 'data_size': 10000}, 35.986572095018346)
INFO flwr 2024-04-07 07:28:19,502 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:19,502 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:27,605 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:29,060 | server.py:125 | fit progress: (3, 1.7960927486419678, {'accuracy': 0.7897, 'data_size': 10000}, 45.54474119300721)
INFO flwr 2024-04-07 07:28:29,060 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:29,061 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:37,695 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:39,240 | server.py:125 | fit progress: (4, 1.7458125352859497, {'accuracy': 0.7978, 'data_size': 10000}, 55.72440124800778)
INFO flwr 2024-04-07 07:28:39,240 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:39,240 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:47,946 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:49,220 | server.py:125 | fit progress: (5, 1.7102059125900269, {'accuracy': 0.84, 'data_size': 10000}, 65.70445170300081)
INFO flwr 2024-04-07 07:28:49,220 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:49,220 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:28:58,011 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:28:59,572 | server.py:125 | fit progress: (6, 1.699971079826355, {'accuracy': 0.8244, 'data_size': 10000}, 76.05670744401868)
INFO flwr 2024-04-07 07:28:59,572 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:28:59,573 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:08,411 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:09,742 | server.py:125 | fit progress: (7, 1.6668180227279663, {'accuracy': 0.8707, 'data_size': 10000}, 86.2267378050019)
INFO flwr 2024-04-07 07:29:09,742 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:29:09,743 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:18,960 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:20,370 | server.py:125 | fit progress: (8, 1.6668550968170166, {'accuracy': 0.859, 'data_size': 10000}, 96.85420015201089)
INFO flwr 2024-04-07 07:29:20,370 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:29:20,370 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:29,195 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:30,856 | server.py:125 | fit progress: (9, 1.64725661277771, {'accuracy': 0.878, 'data_size': 10000}, 107.34020950802369)
INFO flwr 2024-04-07 07:29:30,856 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:29:30,856 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:29:39,448 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:29:41,177 | server.py:125 | fit progress: (10, 1.6447144746780396, {'accuracy': 0.8771, 'data_size': 10000}, 117.66159745800542)
INFO flwr 2024-04-07 07:29:41,177 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:29:41,177 | server.py:153 | FL finished in 117.66198622301454
INFO flwr 2024-04-07 07:29:41,177 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:29:41,178 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:29:41,178 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:29:41,178 | app.py:229 | app_fit: losses_centralized [(0, 2.2993783950805664), (1, 2.1028010845184326), (2, 1.9104232788085938), (3, 1.7960927486419678), (4, 1.7458125352859497), (5, 1.7102059125900269), (6, 1.699971079826355), (7, 1.6668180227279663), (8, 1.6668550968170166), (9, 1.64725661277771), (10, 1.6447144746780396)]
INFO flwr 2024-04-07 07:29:41,178 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1101), (1, 0.5199), (2, 0.6553), (3, 0.7897), (4, 0.7978), (5, 0.84), (6, 0.8244), (7, 0.8707), (8, 0.859), (9, 0.878), (10, 0.8771)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8771
wandb:     loss 1.64471
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_072720-bkzsxmoq
wandb: Find logs at: ./wandb/offline-run-20240407_072720-bkzsxmoq/logs
INFO flwr 2024-04-07 07:29:44,712 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:37:03,338 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1785514)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1785514)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:37:09,495	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:37:13,325	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:37:13,699	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1bbb00ab16305fdf.zip' (11.48MiB) to Ray cluster...
2024-04-07 07:37:13,741	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1bbb00ab16305fdf.zip'.
INFO flwr 2024-04-07 07:37:25,869 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 50855584972.0, 'memory': 108663031604.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 07:37:25,870 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:37:25,870 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:37:25,885 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:37:25,886 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:37:25,886 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:37:25,887 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:37:27,971 | server.py:94 | initial parameters (loss, other metrics): 2.29986310005188, {'accuracy': 0.1558, 'data_size': 10000}
INFO flwr 2024-04-07 07:37:27,971 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:37:27,972 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1791853)[0m 2024-04-07 07:37:35.907105: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1791853)[0m 2024-04-07 07:37:36.011303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1791853)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1791851)[0m 2024-04-07 07:37:38.923814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1791848)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1791848)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1791850)[0m 2024-04-07 07:37:36.084545: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1791850)[0m 2024-04-07 07:37:36.150885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1791850)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1791854)[0m 2024-04-07 07:37:38.924743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:37:58,042 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:37:58,116 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:37:59,522 | server.py:125 | fit progress: (1, 2.032515525817871, {'accuracy': 0.5468, 'data_size': 10000}, 31.55054262498743)
INFO flwr 2024-04-07 07:37:59,523 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:37:59,523 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:38:09,081 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:38:10,545 | server.py:125 | fit progress: (2, 1.7952734231948853, {'accuracy': 0.8082, 'data_size': 10000}, 42.57370679799351)
INFO flwr 2024-04-07 07:38:10,546 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:38:10,546 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:38:19,751 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:38:21,220 | server.py:125 | fit progress: (3, 1.7104002237319946, {'accuracy': 0.8323, 'data_size': 10000}, 53.248127299011685)
INFO flwr 2024-04-07 07:38:21,220 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:38:21,220 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:38:30,194 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:38:31,742 | server.py:125 | fit progress: (4, 1.6642560958862305, {'accuracy': 0.8713, 'data_size': 10000}, 63.76999387599062)
INFO flwr 2024-04-07 07:38:31,742 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:38:31,742 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:38:40,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:38:42,247 | server.py:125 | fit progress: (5, 1.6428461074829102, {'accuracy': 0.8793, 'data_size': 10000}, 74.27480594199733)
INFO flwr 2024-04-07 07:38:42,247 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:38:42,247 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:38:51,280 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:38:52,567 | server.py:125 | fit progress: (6, 1.6555070877075195, {'accuracy': 0.8609, 'data_size': 10000}, 84.5955629539967)
INFO flwr 2024-04-07 07:38:52,568 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:38:52,568 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:39:01,780 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:39:03,151 | server.py:125 | fit progress: (7, 1.6404668092727661, {'accuracy': 0.8749, 'data_size': 10000}, 95.17886465400807)
INFO flwr 2024-04-07 07:39:03,151 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:39:03,151 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:39:12,326 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:39:13,712 | server.py:125 | fit progress: (8, 1.6212579011917114, {'accuracy': 0.8906, 'data_size': 10000}, 105.74063210299937)
INFO flwr 2024-04-07 07:39:13,713 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:39:13,713 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:39:22,447 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:39:24,117 | server.py:125 | fit progress: (9, 1.6264700889587402, {'accuracy': 0.8808, 'data_size': 10000}, 116.14570362199447)
INFO flwr 2024-04-07 07:39:24,118 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:39:24,118 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:39:33,511 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:39:35,187 | server.py:125 | fit progress: (10, 1.607973575592041, {'accuracy': 0.8945, 'data_size': 10000}, 127.21523162900121)
INFO flwr 2024-04-07 07:39:35,187 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:39:35,187 | server.py:153 | FL finished in 127.21569438101142
INFO flwr 2024-04-07 07:39:35,188 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:39:35,188 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:39:35,188 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:39:35,188 | app.py:229 | app_fit: losses_centralized [(0, 2.29986310005188), (1, 2.032515525817871), (2, 1.7952734231948853), (3, 1.7104002237319946), (4, 1.6642560958862305), (5, 1.6428461074829102), (6, 1.6555070877075195), (7, 1.6404668092727661), (8, 1.6212579011917114), (9, 1.6264700889587402), (10, 1.607973575592041)]
INFO flwr 2024-04-07 07:39:35,188 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1558), (1, 0.5468), (2, 0.8082), (3, 0.8323), (4, 0.8713), (5, 0.8793), (6, 0.8609), (7, 0.8749), (8, 0.8906), (9, 0.8808), (10, 0.8945)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8945
wandb:     loss 1.60797
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_073701-rqhrncly
wandb: Find logs at: ./wandb/offline-run-20240407_073701-rqhrncly/logs
INFO flwr 2024-04-07 07:39:38,700 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:46:47,630 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1791853)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1791853)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 07:46:52,968	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:46:53,544	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:46:54,004	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9e2d713b5a24839a.zip' (11.50MiB) to Ray cluster...
2024-04-07 07:46:54,049	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9e2d713b5a24839a.zip'.
INFO flwr 2024-04-07 07:47:05,082 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 116790680576.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 54338863104.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 07:47:05,083 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:47:05,083 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:47:05,100 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:47:05,100 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:47:05,101 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:47:05,101 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:47:07,644 | server.py:94 | initial parameters (loss, other metrics): 2.304189682006836, {'accuracy': 0.105, 'data_size': 10000}
INFO flwr 2024-04-07 07:47:07,645 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:47:07,646 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1799231)[0m 2024-04-07 07:47:11.246096: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1799231)[0m 2024-04-07 07:47:11.337753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1799231)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1799223)[0m 2024-04-07 07:47:13.398644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1799231)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1799231)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1799221)[0m 2024-04-07 07:47:11.545422: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1799221)[0m 2024-04-07 07:47:11.658677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1799221)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1799229)[0m 2024-04-07 07:47:13.657932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1799233)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=1799233)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
DEBUG flwr 2024-04-07 07:47:30,577 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:47:30,615 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:47:31,735 | server.py:125 | fit progress: (1, 1.952924370765686, {'accuracy': 0.6218, 'data_size': 10000}, 24.08981559801032)
INFO flwr 2024-04-07 07:47:31,736 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:47:31,736 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:47:41,106 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:47:42,583 | server.py:125 | fit progress: (2, 1.7635599374771118, {'accuracy': 0.7806, 'data_size': 10000}, 34.93709190899972)
INFO flwr 2024-04-07 07:47:42,583 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:47:42,583 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:47:51,744 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:47:53,226 | server.py:125 | fit progress: (3, 1.681731104850769, {'accuracy': 0.8541, 'data_size': 10000}, 45.580309408018366)
INFO flwr 2024-04-07 07:47:53,226 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:47:53,226 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:01,445 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:02,688 | server.py:125 | fit progress: (4, 1.6360232830047607, {'accuracy': 0.8876, 'data_size': 10000}, 55.04283186700195)
INFO flwr 2024-04-07 07:48:02,689 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:02,689 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:11,732 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:13,017 | server.py:125 | fit progress: (5, 1.6272574663162231, {'accuracy': 0.8817, 'data_size': 10000}, 65.37112663500011)
INFO flwr 2024-04-07 07:48:13,017 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:13,017 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:21,541 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:22,876 | server.py:125 | fit progress: (6, 1.619916558265686, {'accuracy': 0.8832, 'data_size': 10000}, 75.23084851301974)
INFO flwr 2024-04-07 07:48:22,877 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:22,877 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:32,018 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:33,657 | server.py:125 | fit progress: (7, 1.6045432090759277, {'accuracy': 0.8987, 'data_size': 10000}, 86.01137044100324)
INFO flwr 2024-04-07 07:48:33,657 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:33,657 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:42,917 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:44,638 | server.py:125 | fit progress: (8, 1.6049984693527222, {'accuracy': 0.8942, 'data_size': 10000}, 96.99223324601189)
INFO flwr 2024-04-07 07:48:44,638 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:44,638 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:48:53,317 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:48:55,069 | server.py:125 | fit progress: (9, 1.6016260385513306, {'accuracy': 0.8961, 'data_size': 10000}, 107.42306572699454)
INFO flwr 2024-04-07 07:48:55,069 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:48:55,069 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:49:03,793 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:49:05,479 | server.py:125 | fit progress: (10, 1.5934935808181763, {'accuracy': 0.901, 'data_size': 10000}, 117.83359604599536)
INFO flwr 2024-04-07 07:49:05,479 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:49:05,479 | server.py:153 | FL finished in 117.83399126300355
INFO flwr 2024-04-07 07:49:05,480 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:49:05,480 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:49:05,480 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:49:05,480 | app.py:229 | app_fit: losses_centralized [(0, 2.304189682006836), (1, 1.952924370765686), (2, 1.7635599374771118), (3, 1.681731104850769), (4, 1.6360232830047607), (5, 1.6272574663162231), (6, 1.619916558265686), (7, 1.6045432090759277), (8, 1.6049984693527222), (9, 1.6016260385513306), (10, 1.5934935808181763)]
INFO flwr 2024-04-07 07:49:05,480 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.105), (1, 0.6218), (2, 0.7806), (3, 0.8541), (4, 0.8876), (5, 0.8817), (6, 0.8832), (7, 0.8987), (8, 0.8942), (9, 0.8961), (10, 0.901)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.901
wandb:     loss 1.59349
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_074647-k0p9svgv
wandb: Find logs at: ./wandb/offline-run-20240407_074647-k0p9svgv/logs
INFO flwr 2024-04-07 07:49:09,100 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 07:56:23,042 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1799229)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=1799229)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
2024-04-07 07:56:29,945	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 07:56:30,591	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 07:56:30,952	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_30ba781b8e1e26e7.zip' (11.52MiB) to Ray cluster...
2024-04-07 07:56:30,985	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_30ba781b8e1e26e7.zip'.
INFO flwr 2024-04-07 07:56:41,784 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 54404573184.0, 'node:10.20.240.18': 1.0, 'memory': 116944004096.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 07:56:41,785 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 07:56:41,785 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 07:56:41,799 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 07:56:41,800 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 07:56:41,801 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 07:56:41,801 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 07:56:43,765 | server.py:94 | initial parameters (loss, other metrics): 2.3055524826049805, {'accuracy': 0.0545, 'data_size': 10000}
INFO flwr 2024-04-07 07:56:43,765 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 07:56:43,766 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1803732)[0m 2024-04-07 07:56:48.732901: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1803732)[0m 2024-04-07 07:56:48.835709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1803732)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1803732)[0m 2024-04-07 07:56:52.101884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1803732)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1803732)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1803733)[0m 2024-04-07 07:56:48.821946: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1803733)[0m 2024-04-07 07:56:48.926752: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1803733)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1803731)[0m 2024-04-07 07:56:52.102182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 07:57:08,309 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 07:57:08,339 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 07:57:09,723 | server.py:125 | fit progress: (1, 1.9664887189865112, {'accuracy': 0.6125, 'data_size': 10000}, 25.956721491005737)
INFO flwr 2024-04-07 07:57:09,723 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 07:57:09,723 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:57:19,449 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 07:57:20,895 | server.py:125 | fit progress: (2, 1.7283819913864136, {'accuracy': 0.8396, 'data_size': 10000}, 37.12887335100095)
INFO flwr 2024-04-07 07:57:20,895 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 07:57:20,895 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:57:29,454 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 07:57:30,916 | server.py:125 | fit progress: (3, 1.67923903465271, {'accuracy': 0.8439, 'data_size': 10000}, 47.15051384101389)
INFO flwr 2024-04-07 07:57:30,917 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 07:57:30,917 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:57:39,386 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 07:57:40,615 | server.py:125 | fit progress: (4, 1.650390863418579, {'accuracy': 0.8608, 'data_size': 10000}, 56.84925005299738)
INFO flwr 2024-04-07 07:57:40,615 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 07:57:40,616 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:57:49,295 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 07:57:50,815 | server.py:125 | fit progress: (5, 1.6325188875198364, {'accuracy': 0.8745, 'data_size': 10000}, 67.04876569000771)
INFO flwr 2024-04-07 07:57:50,815 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 07:57:50,815 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:57:59,672 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 07:58:01,214 | server.py:125 | fit progress: (6, 1.6062841415405273, {'accuracy': 0.8968, 'data_size': 10000}, 77.44801730601466)
INFO flwr 2024-04-07 07:58:01,214 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 07:58:01,214 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:58:09,738 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 07:58:11,339 | server.py:125 | fit progress: (7, 1.6017491817474365, {'accuracy': 0.8976, 'data_size': 10000}, 87.57306436399813)
INFO flwr 2024-04-07 07:58:11,339 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 07:58:11,339 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:58:20,380 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 07:58:21,749 | server.py:125 | fit progress: (8, 1.6052006483078003, {'accuracy': 0.8928, 'data_size': 10000}, 97.98310612500063)
INFO flwr 2024-04-07 07:58:21,749 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 07:58:21,749 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:58:30,839 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 07:58:32,497 | server.py:125 | fit progress: (9, 1.5934321880340576, {'accuracy': 0.8994, 'data_size': 10000}, 108.73074241899303)
INFO flwr 2024-04-07 07:58:32,497 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 07:58:32,497 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 07:58:41,232 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 07:58:42,916 | server.py:125 | fit progress: (10, 1.599082112312317, {'accuracy': 0.893, 'data_size': 10000}, 119.14981094599352)
INFO flwr 2024-04-07 07:58:42,916 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 07:58:42,916 | server.py:153 | FL finished in 119.15022712000064
INFO flwr 2024-04-07 07:58:42,916 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 07:58:42,916 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 07:58:42,916 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 07:58:42,917 | app.py:229 | app_fit: losses_centralized [(0, 2.3055524826049805), (1, 1.9664887189865112), (2, 1.7283819913864136), (3, 1.67923903465271), (4, 1.650390863418579), (5, 1.6325188875198364), (6, 1.6062841415405273), (7, 1.6017491817474365), (8, 1.6052006483078003), (9, 1.5934321880340576), (10, 1.599082112312317)]
INFO flwr 2024-04-07 07:58:42,917 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0545), (1, 0.6125), (2, 0.8396), (3, 0.8439), (4, 0.8608), (5, 0.8745), (6, 0.8968), (7, 0.8976), (8, 0.8928), (9, 0.8994), (10, 0.893)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.893
wandb:     loss 1.59908
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_075617-5yw8r5cp
wandb: Find logs at: ./wandb/offline-run-20240407_075617-5yw8r5cp/logs
INFO flwr 2024-04-07 07:58:46,442 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:05:55,487 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1803731)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1803731)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:06:02,048	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:06:04,147	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:06:04,506	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f6d13c9e5148c116.zip' (11.53MiB) to Ray cluster...
2024-04-07 08:06:04,547	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f6d13c9e5148c116.zip'.
INFO flwr 2024-04-07 08:06:15,573 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'memory': 113567357133.0, 'object_store_memory': 52957438771.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 08:06:15,574 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:06:15,574 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:06:15,591 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:06:15,592 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:06:15,592 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:06:15,602 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:06:18,517 | server.py:94 | initial parameters (loss, other metrics): 2.2976372241973877, {'accuracy': 0.1731, 'data_size': 10000}
INFO flwr 2024-04-07 08:06:18,517 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:06:18,517 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1811366)[0m 2024-04-07 08:06:21.518752: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1811366)[0m 2024-04-07 08:06:21.616619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1811366)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1811366)[0m 2024-04-07 08:06:23.711627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1811366)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1811366)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1811367)[0m 2024-04-07 08:06:22.221925: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1811361)[0m 2024-04-07 08:06:22.308866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1811361)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1811361)[0m 2024-04-07 08:06:24.519324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:06:36,361 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:06:36,411 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:06:37,858 | server.py:125 | fit progress: (1, 1.9371830224990845, {'accuracy': 0.5586, 'data_size': 10000}, 19.340481396007817)
INFO flwr 2024-04-07 08:06:37,858 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:06:37,858 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:06:47,338 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:06:48,575 | server.py:125 | fit progress: (2, 1.7027909755706787, {'accuracy': 0.8334, 'data_size': 10000}, 30.05816815799335)
INFO flwr 2024-04-07 08:06:48,576 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:06:48,576 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:06:57,263 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:06:58,714 | server.py:125 | fit progress: (3, 1.6554967164993286, {'accuracy': 0.8632, 'data_size': 10000}, 40.197155061003286)
INFO flwr 2024-04-07 08:06:58,715 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:06:58,715 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:07,836 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:09,422 | server.py:125 | fit progress: (4, 1.6131517887115479, {'accuracy': 0.8905, 'data_size': 10000}, 50.90518985799281)
INFO flwr 2024-04-07 08:07:09,423 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:09,423 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:17,992 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:19,519 | server.py:125 | fit progress: (5, 1.6086078882217407, {'accuracy': 0.8924, 'data_size': 10000}, 61.00204083099379)
INFO flwr 2024-04-07 08:07:19,519 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:19,520 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:27,979 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:29,317 | server.py:125 | fit progress: (6, 1.5956209897994995, {'accuracy': 0.9008, 'data_size': 10000}, 70.79966523000621)
INFO flwr 2024-04-07 08:07:29,317 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:29,317 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:37,989 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:39,557 | server.py:125 | fit progress: (7, 1.6030575037002563, {'accuracy': 0.891, 'data_size': 10000}, 81.039493512013)
INFO flwr 2024-04-07 08:07:39,557 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:39,557 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:48,465 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:07:50,136 | server.py:125 | fit progress: (8, 1.5870729684829712, {'accuracy': 0.9018, 'data_size': 10000}, 91.61907522298861)
INFO flwr 2024-04-07 08:07:50,137 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:07:50,137 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:07:59,351 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:08:01,076 | server.py:125 | fit progress: (9, 1.5839171409606934, {'accuracy': 0.9056, 'data_size': 10000}, 102.5592655960063)
INFO flwr 2024-04-07 08:08:01,077 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:08:01,077 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:08:10,017 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:08:11,493 | server.py:125 | fit progress: (10, 1.584167242050171, {'accuracy': 0.9047, 'data_size': 10000}, 112.97582899700501)
INFO flwr 2024-04-07 08:08:11,493 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:08:11,493 | server.py:153 | FL finished in 112.97621346899541
INFO flwr 2024-04-07 08:08:11,494 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:08:11,494 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:08:11,494 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:08:11,494 | app.py:229 | app_fit: losses_centralized [(0, 2.2976372241973877), (1, 1.9371830224990845), (2, 1.7027909755706787), (3, 1.6554967164993286), (4, 1.6131517887115479), (5, 1.6086078882217407), (6, 1.5956209897994995), (7, 1.6030575037002563), (8, 1.5870729684829712), (9, 1.5839171409606934), (10, 1.584167242050171)]
INFO flwr 2024-04-07 08:08:11,494 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1731), (1, 0.5586), (2, 0.8334), (3, 0.8632), (4, 0.8905), (5, 0.8924), (6, 0.9008), (7, 0.891), (8, 0.9018), (9, 0.9056), (10, 0.9047)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9047
wandb:     loss 1.58417
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_080555-li7qe2ks
wandb: Find logs at: ./wandb/offline-run-20240407_080555-li7qe2ks/logs
INFO flwr 2024-04-07 08:08:15,140 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:15:25,053 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1811361)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1811361)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:15:31,457	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:15:32,043	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:15:32,418	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_301c827db1676fea.zip' (11.55MiB) to Ray cluster...
2024-04-07 08:15:32,443	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_301c827db1676fea.zip'.
INFO flwr 2024-04-07 08:15:43,518 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 116607062221.0, 'CPU': 64.0, 'object_store_memory': 54260169523.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 08:15:43,518 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:15:43,518 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:15:43,536 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:15:43,537 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:15:43,537 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:15:43,538 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:15:46,975 | server.py:94 | initial parameters (loss, other metrics): 2.3003292083740234, {'accuracy': 0.1099, 'data_size': 10000}
INFO flwr 2024-04-07 08:15:46,975 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:15:46,975 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1815922)[0m 2024-04-07 08:15:50.128795: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1815923)[0m 2024-04-07 08:15:50.246687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1815923)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1815923)[0m 2024-04-07 08:15:52.902642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1815923)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1815923)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1815915)[0m 2024-04-07 08:15:50.222783: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1815915)[0m 2024-04-07 08:15:50.323986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1815915)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1815922)[0m 2024-04-07 08:15:52.919926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:16:13,992 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:16:14,034 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:16:15,426 | server.py:125 | fit progress: (1, 1.9104490280151367, {'accuracy': 0.6596, 'data_size': 10000}, 28.45067303397809)
INFO flwr 2024-04-07 08:16:15,426 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:16:15,426 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:16:25,133 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:16:26,597 | server.py:125 | fit progress: (2, 1.7121591567993164, {'accuracy': 0.7924, 'data_size': 10000}, 39.62212011599331)
INFO flwr 2024-04-07 08:16:26,598 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:16:26,598 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:16:35,653 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:16:37,173 | server.py:125 | fit progress: (3, 1.6555285453796387, {'accuracy': 0.8564, 'data_size': 10000}, 50.197769535996486)
INFO flwr 2024-04-07 08:16:37,173 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:16:37,174 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:16:45,998 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:16:47,279 | server.py:125 | fit progress: (4, 1.620676875114441, {'accuracy': 0.8839, 'data_size': 10000}, 60.30372499200166)
INFO flwr 2024-04-07 08:16:47,279 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:16:47,280 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:16:56,632 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:16:57,949 | server.py:125 | fit progress: (5, 1.609297513961792, {'accuracy': 0.8886, 'data_size': 10000}, 70.97404027500306)
INFO flwr 2024-04-07 08:16:57,950 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:16:57,950 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:17:07,138 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:17:08,473 | server.py:125 | fit progress: (6, 1.6050060987472534, {'accuracy': 0.8891, 'data_size': 10000}, 81.49779706998379)
INFO flwr 2024-04-07 08:17:08,473 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:17:08,474 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:17:17,742 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:17:19,359 | server.py:125 | fit progress: (7, 1.5905953645706177, {'accuracy': 0.8986, 'data_size': 10000}, 92.38349179099896)
INFO flwr 2024-04-07 08:17:19,359 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:17:19,359 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:17:28,518 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:17:30,200 | server.py:125 | fit progress: (8, 1.5825549364089966, {'accuracy': 0.9043, 'data_size': 10000}, 103.22431816099561)
INFO flwr 2024-04-07 08:17:30,200 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:17:30,200 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:17:39,244 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:17:40,968 | server.py:125 | fit progress: (9, 1.5790956020355225, {'accuracy': 0.9071, 'data_size': 10000}, 113.99284884499502)
INFO flwr 2024-04-07 08:17:40,968 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:17:40,969 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:17:49,892 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:17:51,576 | server.py:125 | fit progress: (10, 1.5777961015701294, {'accuracy': 0.9074, 'data_size': 10000}, 124.60046519999742)
INFO flwr 2024-04-07 08:17:51,576 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:17:51,576 | server.py:153 | FL finished in 124.60094717697939
INFO flwr 2024-04-07 08:17:51,576 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:17:51,577 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:17:51,577 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:17:51,577 | app.py:229 | app_fit: losses_centralized [(0, 2.3003292083740234), (1, 1.9104490280151367), (2, 1.7121591567993164), (3, 1.6555285453796387), (4, 1.620676875114441), (5, 1.609297513961792), (6, 1.6050060987472534), (7, 1.5905953645706177), (8, 1.5825549364089966), (9, 1.5790956020355225), (10, 1.5777961015701294)]
INFO flwr 2024-04-07 08:17:51,577 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1099), (1, 0.6596), (2, 0.7924), (3, 0.8564), (4, 0.8839), (5, 0.8886), (6, 0.8891), (7, 0.8986), (8, 0.9043), (9, 0.9071), (10, 0.9074)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9074
wandb:     loss 1.5778
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_081523-9dy79fcp
wandb: Find logs at: ./wandb/offline-run-20240407_081523-9dy79fcp/logs
INFO flwr 2024-04-07 08:17:55,085 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:25:03,927 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1815918)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1815918)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:25:10,563	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:25:10,977	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:25:11,361	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f5795e68bee07648.zip' (11.57MiB) to Ray cluster...
2024-04-07 08:25:11,394	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f5795e68bee07648.zip'.
INFO flwr 2024-04-07 08:25:22,553 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 116582490317.0, 'object_store_memory': 54249638707.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 08:25:22,554 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:25:22,554 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:25:22,576 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:25:22,578 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:25:22,578 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:25:22,579 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:25:25,487 | server.py:94 | initial parameters (loss, other metrics): 2.30330753326416, {'accuracy': 0.0551, 'data_size': 10000}
INFO flwr 2024-04-07 08:25:25,491 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:25:25,499 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1823416)[0m 2024-04-07 08:25:28.979875: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1823416)[0m 2024-04-07 08:25:29.098757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1823416)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1823416)[0m 2024-04-07 08:25:31.401065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1823417)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1823417)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1823421)[0m 2024-04-07 08:25:29.214073: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1823421)[0m 2024-04-07 08:25:29.315664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1823421)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1823420)[0m 2024-04-07 08:25:31.645537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:25:46,876 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:25:46,915 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:25:48,360 | server.py:125 | fit progress: (1, 2.2884230613708496, {'accuracy': 0.1749, 'data_size': 10000}, 22.86065981001593)
INFO flwr 2024-04-07 08:25:48,360 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:25:48,360 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:25:58,478 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:00,051 | server.py:125 | fit progress: (2, 2.2683122158050537, {'accuracy': 0.282, 'data_size': 10000}, 34.551754429005086)
INFO flwr 2024-04-07 08:26:00,051 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:00,051 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:09,512 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:11,060 | server.py:125 | fit progress: (3, 2.2441165447235107, {'accuracy': 0.2792, 'data_size': 10000}, 45.56092250102665)
INFO flwr 2024-04-07 08:26:11,060 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:11,061 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:20,935 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:22,266 | server.py:125 | fit progress: (4, 2.226796865463257, {'accuracy': 0.3548, 'data_size': 10000}, 56.76657901602448)
INFO flwr 2024-04-07 08:26:22,266 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:22,267 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:32,235 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:33,840 | server.py:125 | fit progress: (5, 2.208662509918213, {'accuracy': 0.4731, 'data_size': 10000}, 68.34123577401624)
INFO flwr 2024-04-07 08:26:33,841 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:33,841 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:44,234 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:46,783 | server.py:125 | fit progress: (6, 2.182447671890259, {'accuracy': 0.5565, 'data_size': 10000}, 81.28432062701904)
INFO flwr 2024-04-07 08:26:46,784 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:46,784 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:26:56,064 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:26:57,705 | server.py:125 | fit progress: (7, 2.1585006713867188, {'accuracy': 0.5713, 'data_size': 10000}, 92.20569286600221)
INFO flwr 2024-04-07 08:26:57,705 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:26:57,705 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:27:06,899 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:27:08,285 | server.py:125 | fit progress: (8, 2.1340529918670654, {'accuracy': 0.6012, 'data_size': 10000}, 102.78601049800636)
INFO flwr 2024-04-07 08:27:08,285 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:27:08,286 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:27:18,122 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:27:19,841 | server.py:125 | fit progress: (9, 2.1079938411712646, {'accuracy': 0.5506, 'data_size': 10000}, 114.34211462701205)
INFO flwr 2024-04-07 08:27:19,841 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:27:19,842 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:27:29,221 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:27:30,931 | server.py:125 | fit progress: (10, 2.083033800125122, {'accuracy': 0.5686, 'data_size': 10000}, 125.43202005600324)
INFO flwr 2024-04-07 08:27:30,931 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:27:30,932 | server.py:153 | FL finished in 125.43244921500445
INFO flwr 2024-04-07 08:27:30,932 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:27:30,932 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:27:30,932 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:27:30,932 | app.py:229 | app_fit: losses_centralized [(0, 2.30330753326416), (1, 2.2884230613708496), (2, 2.2683122158050537), (3, 2.2441165447235107), (4, 2.226796865463257), (5, 2.208662509918213), (6, 2.182447671890259), (7, 2.1585006713867188), (8, 2.1340529918670654), (9, 2.1079938411712646), (10, 2.083033800125122)]
INFO flwr 2024-04-07 08:27:30,932 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0551), (1, 0.1749), (2, 0.282), (3, 0.2792), (4, 0.3548), (5, 0.4731), (6, 0.5565), (7, 0.5713), (8, 0.6012), (9, 0.5506), (10, 0.5686)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.5686
wandb:     loss 2.08303
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_082503-n9mc2qx5
wandb: Find logs at: ./wandb/offline-run-20240407_082503-n9mc2qx5/logs
INFO flwr 2024-04-07 08:27:34,557 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:34:44,737 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1823416)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1823416)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:34:50,930	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:34:51,451	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:34:51,887	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1ae4b0e6b4fae27f.zip' (11.59MiB) to Ray cluster...
2024-04-07 08:34:51,928	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1ae4b0e6b4fae27f.zip'.
INFO flwr 2024-04-07 08:35:02,732 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 116507146036.0, 'object_store_memory': 54217348300.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 08:35:02,732 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:35:02,732 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:35:02,750 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:35:02,752 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:35:02,752 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:35:02,752 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:35:05,297 | server.py:94 | initial parameters (loss, other metrics): 2.3016932010650635, {'accuracy': 0.0532, 'data_size': 10000}
INFO flwr 2024-04-07 08:35:05,297 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:35:05,298 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1827527)[0m 2024-04-07 08:35:08.482328: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1827527)[0m 2024-04-07 08:35:08.544367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1827527)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1827521)[0m 2024-04-07 08:35:10.944157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1827522)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1827522)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1827523)[0m 2024-04-07 08:35:09.070463: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1827523)[0m 2024-04-07 08:35:09.170672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1827523)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1827525)[0m 2024-04-07 08:35:11.513775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:35:25,943 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:35:25,981 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:35:27,386 | server.py:125 | fit progress: (1, 1.9712902307510376, {'accuracy': 0.6499, 'data_size': 10000}, 22.088266072998522)
INFO flwr 2024-04-07 08:35:27,387 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:35:27,387 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:35:37,526 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:35:38,756 | server.py:125 | fit progress: (2, 1.7846649885177612, {'accuracy': 0.7864, 'data_size': 10000}, 33.45806334298686)
INFO flwr 2024-04-07 08:35:38,756 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:35:38,756 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:35:47,845 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:35:49,348 | server.py:125 | fit progress: (3, 1.7309083938598633, {'accuracy': 0.8011, 'data_size': 10000}, 44.05055482900934)
INFO flwr 2024-04-07 08:35:49,349 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:35:49,349 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:35:59,304 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:00,782 | server.py:125 | fit progress: (4, 1.6803234815597534, {'accuracy': 0.8598, 'data_size': 10000}, 55.48387843600358)
INFO flwr 2024-04-07 08:36:00,782 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:00,782 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:36:10,057 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:11,650 | server.py:125 | fit progress: (5, 1.6564396619796753, {'accuracy': 0.8773, 'data_size': 10000}, 66.35243933400488)
INFO flwr 2024-04-07 08:36:11,651 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:11,651 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:36:21,658 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:23,095 | server.py:125 | fit progress: (6, 1.635998249053955, {'accuracy': 0.8832, 'data_size': 10000}, 77.79698040199582)
INFO flwr 2024-04-07 08:36:23,095 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:23,095 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:36:32,823 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:34,468 | server.py:125 | fit progress: (7, 1.6323941946029663, {'accuracy': 0.8837, 'data_size': 10000}, 89.17033554898808)
INFO flwr 2024-04-07 08:36:34,469 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:34,469 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:36:43,944 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:45,651 | server.py:125 | fit progress: (8, 1.6152915954589844, {'accuracy': 0.8949, 'data_size': 10000}, 100.35337747598533)
INFO flwr 2024-04-07 08:36:45,652 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:45,652 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:36:54,833 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:36:56,616 | server.py:125 | fit progress: (9, 1.6197259426116943, {'accuracy': 0.8888, 'data_size': 10000}, 111.31805015599821)
INFO flwr 2024-04-07 08:36:56,616 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:36:56,616 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:37:06,588 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:37:08,023 | server.py:125 | fit progress: (10, 1.6101365089416504, {'accuracy': 0.8958, 'data_size': 10000}, 122.72525856399443)
INFO flwr 2024-04-07 08:37:08,023 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:37:08,024 | server.py:153 | FL finished in 122.72573728399584
INFO flwr 2024-04-07 08:37:08,024 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:37:08,024 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:37:08,024 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:37:08,024 | app.py:229 | app_fit: losses_centralized [(0, 2.3016932010650635), (1, 1.9712902307510376), (2, 1.7846649885177612), (3, 1.7309083938598633), (4, 1.6803234815597534), (5, 1.6564396619796753), (6, 1.635998249053955), (7, 1.6323941946029663), (8, 1.6152915954589844), (9, 1.6197259426116943), (10, 1.6101365089416504)]
INFO flwr 2024-04-07 08:37:08,024 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0532), (1, 0.6499), (2, 0.7864), (3, 0.8011), (4, 0.8598), (5, 0.8773), (6, 0.8832), (7, 0.8837), (8, 0.8949), (9, 0.8888), (10, 0.8958)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8958
wandb:     loss 1.61014
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_083442-sroi9vwx
wandb: Find logs at: ./wandb/offline-run-20240407_083442-sroi9vwx/logs
INFO flwr 2024-04-07 08:37:11,577 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:44:19,103 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1827521)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1827521)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:44:24,093	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:44:24,478	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:44:24,955	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f43565d580360676.zip' (11.61MiB) to Ray cluster...
2024-04-07 08:44:25,004	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f43565d580360676.zip'.
INFO flwr 2024-04-07 08:44:35,908 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 112474781901.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 52489192243.0, 'CPU': 64.0}
INFO flwr 2024-04-07 08:44:35,908 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:44:35,908 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:44:35,929 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:44:35,930 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:44:35,930 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:44:35,931 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:44:38,686 | server.py:94 | initial parameters (loss, other metrics): 2.30074405670166, {'accuracy': 0.1815, 'data_size': 10000}
INFO flwr 2024-04-07 08:44:38,686 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:44:38,687 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1834940)[0m 2024-04-07 08:44:41.766768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1834940)[0m 2024-04-07 08:44:41.863463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1834940)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1834949)[0m 2024-04-07 08:44:44.242019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1834942)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1834942)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1834943)[0m 2024-04-07 08:44:42.301305: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1834943)[0m 2024-04-07 08:44:42.393025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1834943)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1834939)[0m 2024-04-07 08:44:44.459658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:44:58,278 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:44:58,315 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:44:59,697 | server.py:125 | fit progress: (1, 1.9125250577926636, {'accuracy': 0.6264, 'data_size': 10000}, 21.01059869900928)
INFO flwr 2024-04-07 08:44:59,697 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:44:59,698 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:09,452 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:10,940 | server.py:125 | fit progress: (2, 1.7814570665359497, {'accuracy': 0.7158, 'data_size': 10000}, 32.253687540011015)
INFO flwr 2024-04-07 08:45:10,941 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:10,941 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:20,207 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:21,707 | server.py:125 | fit progress: (3, 1.6945359706878662, {'accuracy': 0.8251, 'data_size': 10000}, 43.020861244003754)
INFO flwr 2024-04-07 08:45:21,708 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:21,708 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:30,468 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:31,969 | server.py:125 | fit progress: (4, 1.6413577795028687, {'accuracy': 0.8751, 'data_size': 10000}, 53.282374501024606)
INFO flwr 2024-04-07 08:45:31,969 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:31,969 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:41,854 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:43,150 | server.py:125 | fit progress: (5, 1.6188838481903076, {'accuracy': 0.8891, 'data_size': 10000}, 64.46351524701458)
INFO flwr 2024-04-07 08:45:43,150 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:43,151 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:45:53,442 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:45:55,022 | server.py:125 | fit progress: (6, 1.60349440574646, {'accuracy': 0.8973, 'data_size': 10000}, 76.3355605120014)
INFO flwr 2024-04-07 08:45:55,022 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:45:55,023 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:46:04,596 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:46:07,301 | server.py:125 | fit progress: (7, 1.6292766332626343, {'accuracy': 0.8663, 'data_size': 10000}, 88.61441665500752)
INFO flwr 2024-04-07 08:46:07,301 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:46:07,302 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:46:17,447 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:46:19,217 | server.py:125 | fit progress: (8, 1.5950731039047241, {'accuracy': 0.8995, 'data_size': 10000}, 100.53022024701932)
INFO flwr 2024-04-07 08:46:19,217 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:46:19,217 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:46:28,553 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:46:30,198 | server.py:125 | fit progress: (9, 1.5930852890014648, {'accuracy': 0.8999, 'data_size': 10000}, 111.51146133002476)
INFO flwr 2024-04-07 08:46:30,198 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:46:30,198 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:46:39,065 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:46:40,790 | server.py:125 | fit progress: (10, 1.6126290559768677, {'accuracy': 0.8807, 'data_size': 10000}, 122.10318455001106)
INFO flwr 2024-04-07 08:46:40,790 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:46:40,790 | server.py:153 | FL finished in 122.10361042100703
INFO flwr 2024-04-07 08:46:40,790 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:46:40,790 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:46:40,791 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:46:40,791 | app.py:229 | app_fit: losses_centralized [(0, 2.30074405670166), (1, 1.9125250577926636), (2, 1.7814570665359497), (3, 1.6945359706878662), (4, 1.6413577795028687), (5, 1.6188838481903076), (6, 1.60349440574646), (7, 1.6292766332626343), (8, 1.5950731039047241), (9, 1.5930852890014648), (10, 1.6126290559768677)]
INFO flwr 2024-04-07 08:46:40,791 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1815), (1, 0.6264), (2, 0.7158), (3, 0.8251), (4, 0.8751), (5, 0.8891), (6, 0.8973), (7, 0.8663), (8, 0.8995), (9, 0.8999), (10, 0.8807)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8807
wandb:     loss 1.61263
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_084418-roayoutl
wandb: Find logs at: ./wandb/offline-run-20240407_084418-roayoutl/logs
INFO flwr 2024-04-07 08:46:45,159 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 08:53:53,982 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1834937)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1834937)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 08:53:59,830	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 08:54:00,623	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 08:54:00,967	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_aaeac01797be47b5.zip' (11.62MiB) to Ray cluster...
2024-04-07 08:54:00,994	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_aaeac01797be47b5.zip'.
INFO flwr 2024-04-07 08:54:11,870 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54089861529.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 116209676903.0}
INFO flwr 2024-04-07 08:54:11,870 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 08:54:11,870 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 08:54:11,888 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 08:54:11,889 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 08:54:11,889 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 08:54:11,889 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 08:54:16,535 | server.py:94 | initial parameters (loss, other metrics): 2.298464059829712, {'accuracy': 0.1477, 'data_size': 10000}
INFO flwr 2024-04-07 08:54:16,535 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 08:54:16,536 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1839356)[0m 2024-04-07 08:54:17.681493: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1839356)[0m 2024-04-07 08:54:17.785755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1839356)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1839350)[0m 2024-04-07 08:54:19.990679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1839355)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1839355)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1839360)[0m 2024-04-07 08:54:18.313265: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1839360)[0m 2024-04-07 08:54:18.404437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1839360)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1839360)[0m 2024-04-07 08:54:20.477538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1839349)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1839349)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 08:54:35,114 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 08:54:35,143 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 08:54:36,622 | server.py:125 | fit progress: (1, 1.8946576118469238, {'accuracy': 0.6878, 'data_size': 10000}, 20.086454914999194)
INFO flwr 2024-04-07 08:54:36,622 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 08:54:36,623 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:54:46,510 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 08:54:47,944 | server.py:125 | fit progress: (2, 1.6972885131835938, {'accuracy': 0.8434, 'data_size': 10000}, 31.408223562000785)
INFO flwr 2024-04-07 08:54:47,944 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 08:54:47,945 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:54:57,428 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 08:54:58,981 | server.py:125 | fit progress: (3, 1.6460970640182495, {'accuracy': 0.871, 'data_size': 10000}, 42.445139066985575)
INFO flwr 2024-04-07 08:54:58,981 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 08:54:58,981 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:55:08,551 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 08:55:10,166 | server.py:125 | fit progress: (4, 1.6147078275680542, {'accuracy': 0.8927, 'data_size': 10000}, 53.630280112993205)
INFO flwr 2024-04-07 08:55:10,166 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 08:55:10,166 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:55:19,903 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 08:55:21,514 | server.py:125 | fit progress: (5, 1.6097230911254883, {'accuracy': 0.8904, 'data_size': 10000}, 64.9783239789831)
INFO flwr 2024-04-07 08:55:21,514 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 08:55:21,514 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:55:31,054 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 08:55:32,389 | server.py:125 | fit progress: (6, 1.6003588438034058, {'accuracy': 0.8963, 'data_size': 10000}, 75.85302400999353)
INFO flwr 2024-04-07 08:55:32,389 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 08:55:32,389 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:55:42,630 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 08:55:44,035 | server.py:125 | fit progress: (7, 1.6038546562194824, {'accuracy': 0.8866, 'data_size': 10000}, 87.49902117697638)
INFO flwr 2024-04-07 08:55:44,035 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 08:55:44,035 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:55:53,782 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 08:55:55,160 | server.py:125 | fit progress: (8, 1.5835005044937134, {'accuracy': 0.9047, 'data_size': 10000}, 98.62400676298421)
INFO flwr 2024-04-07 08:55:55,160 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 08:55:55,160 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:56:05,257 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 08:56:06,939 | server.py:125 | fit progress: (9, 1.5832786560058594, {'accuracy': 0.9042, 'data_size': 10000}, 110.40310012799455)
INFO flwr 2024-04-07 08:56:06,939 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 08:56:06,939 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 08:56:16,505 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 08:56:18,232 | server.py:125 | fit progress: (10, 1.5812963247299194, {'accuracy': 0.9052, 'data_size': 10000}, 121.6965805599757)
INFO flwr 2024-04-07 08:56:18,232 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 08:56:18,233 | server.py:153 | FL finished in 121.69704731798265
INFO flwr 2024-04-07 08:56:18,233 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 08:56:18,233 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 08:56:18,233 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 08:56:18,233 | app.py:229 | app_fit: losses_centralized [(0, 2.298464059829712), (1, 1.8946576118469238), (2, 1.6972885131835938), (3, 1.6460970640182495), (4, 1.6147078275680542), (5, 1.6097230911254883), (6, 1.6003588438034058), (7, 1.6038546562194824), (8, 1.5835005044937134), (9, 1.5832786560058594), (10, 1.5812963247299194)]
INFO flwr 2024-04-07 08:56:18,233 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1477), (1, 0.6878), (2, 0.8434), (3, 0.871), (4, 0.8927), (5, 0.8904), (6, 0.8963), (7, 0.8866), (8, 0.9047), (9, 0.9042), (10, 0.9052)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9052
wandb:     loss 1.5813
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_085353-02k1bmun
wandb: Find logs at: ./wandb/offline-run-20240407_085353-02k1bmun/logs
INFO flwr 2024-04-07 08:56:21,912 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:03:30,783 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 09:03:36,005	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:03:36,370	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:03:36,746	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_8899526e88d18890.zip' (11.64MiB) to Ray cluster...
2024-04-07 09:03:36,787	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_8899526e88d18890.zip'.
INFO flwr 2024-04-07 09:03:47,817 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 51971297280.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 111266360320.0}
INFO flwr 2024-04-07 09:03:47,817 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:03:47,818 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:03:47,838 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:03:47,839 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:03:47,839 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:03:47,840 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:03:49,878 | server.py:94 | initial parameters (loss, other metrics): 2.3036158084869385, {'accuracy': 0.1139, 'data_size': 10000}
INFO flwr 2024-04-07 09:03:49,878 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:03:49,879 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1847101)[0m 2024-04-07 09:03:54.110072: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1847101)[0m 2024-04-07 09:03:54.205941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1847101)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1847093)[0m 2024-04-07 09:03:56.709252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1847086)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1847086)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1847105)[0m 2024-04-07 09:03:54.533132: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1847105)[0m 2024-04-07 09:03:54.641092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1847105)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1847105)[0m 2024-04-07 09:03:56.924287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:04:15,247 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:04:15,286 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:04:16,467 | server.py:125 | fit progress: (1, 1.9304003715515137, {'accuracy': 0.6031, 'data_size': 10000}, 26.58882528700633)
INFO flwr 2024-04-07 09:04:16,468 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:04:16,468 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:04:27,282 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:04:29,198 | server.py:125 | fit progress: (2, 1.7217406034469604, {'accuracy': 0.7967, 'data_size': 10000}, 39.3189654389862)
INFO flwr 2024-04-07 09:04:29,198 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:04:29,198 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:04:38,418 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:04:40,035 | server.py:125 | fit progress: (3, 1.6332106590270996, {'accuracy': 0.8764, 'data_size': 10000}, 50.156512062996626)
INFO flwr 2024-04-07 09:04:40,035 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:04:40,036 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:04:49,889 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:04:51,145 | server.py:125 | fit progress: (4, 1.6047698259353638, {'accuracy': 0.8967, 'data_size': 10000}, 61.2659552039986)
INFO flwr 2024-04-07 09:04:51,145 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:04:51,145 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:01,133 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:02,494 | server.py:125 | fit progress: (5, 1.5999094247817993, {'accuracy': 0.8934, 'data_size': 10000}, 72.61560812298558)
INFO flwr 2024-04-07 09:05:02,495 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:02,495 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:12,120 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:13,426 | server.py:125 | fit progress: (6, 1.589568018913269, {'accuracy': 0.8986, 'data_size': 10000}, 83.5477994820103)
INFO flwr 2024-04-07 09:05:13,427 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:13,427 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:22,932 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:24,644 | server.py:125 | fit progress: (7, 1.5828096866607666, {'accuracy': 0.9057, 'data_size': 10000}, 94.7657552229939)
INFO flwr 2024-04-07 09:05:24,645 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:24,645 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:34,323 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:35,962 | server.py:125 | fit progress: (8, 1.591192364692688, {'accuracy': 0.8935, 'data_size': 10000}, 106.08385578900925)
INFO flwr 2024-04-07 09:05:35,963 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:35,963 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:45,955 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:05:48,397 | server.py:125 | fit progress: (9, 1.5751209259033203, {'accuracy': 0.9097, 'data_size': 10000}, 118.51827225199668)
INFO flwr 2024-04-07 09:05:48,397 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:05:48,397 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:05:58,317 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:06:01,813 | server.py:125 | fit progress: (10, 1.5742216110229492, {'accuracy': 0.9087, 'data_size': 10000}, 131.93389976100298)
INFO flwr 2024-04-07 09:06:01,813 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:06:01,813 | server.py:153 | FL finished in 131.9346104389988
INFO flwr 2024-04-07 09:06:01,813 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:06:01,813 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:06:01,814 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:06:01,814 | app.py:229 | app_fit: losses_centralized [(0, 2.3036158084869385), (1, 1.9304003715515137), (2, 1.7217406034469604), (3, 1.6332106590270996), (4, 1.6047698259353638), (5, 1.5999094247817993), (6, 1.589568018913269), (7, 1.5828096866607666), (8, 1.591192364692688), (9, 1.5751209259033203), (10, 1.5742216110229492)]
INFO flwr 2024-04-07 09:06:01,814 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1139), (1, 0.6031), (2, 0.7967), (3, 0.8764), (4, 0.8967), (5, 0.8934), (6, 0.8986), (7, 0.9057), (8, 0.8935), (9, 0.9097), (10, 0.9087)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9087
wandb:     loss 1.57422
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_090330-fys9sv2b
wandb: Find logs at: ./wandb/offline-run-20240407_090330-fys9sv2b/logs
INFO flwr 2024-04-07 09:06:05,398 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:13:23,678 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1847102)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1847102)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 09:13:36,521	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:13:39,052	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:13:39,408	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ad601414f7ca0b07.zip' (11.66MiB) to Ray cluster...
2024-04-07 09:13:39,448	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ad601414f7ca0b07.zip'.
INFO flwr 2024-04-07 09:13:50,180 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 116313607168.0, 'object_store_memory': 54134403072.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 09:13:50,180 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:13:50,181 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:13:50,198 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:13:50,199 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:13:50,199 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:13:50,199 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:13:52,839 | server.py:94 | initial parameters (loss, other metrics): 2.303546190261841, {'accuracy': 0.0802, 'data_size': 10000}
INFO flwr 2024-04-07 09:13:52,840 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:13:52,840 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1851512)[0m 2024-04-07 09:13:57.507666: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1851512)[0m 2024-04-07 09:13:57.606261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1851512)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1851521)[0m 2024-04-07 09:14:01.243757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1851512)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1851512)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1851518)[0m 2024-04-07 09:13:57.659960: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1851518)[0m 2024-04-07 09:13:57.725611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1851518)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1851512)[0m 2024-04-07 09:14:01.281752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:14:22,951 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:14:22,989 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:14:24,471 | server.py:125 | fit progress: (1, 1.9494950771331787, {'accuracy': 0.5661, 'data_size': 10000}, 31.630408907018136)
INFO flwr 2024-04-07 09:14:24,471 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:14:24,471 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:14:34,677 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:14:36,165 | server.py:125 | fit progress: (2, 1.6551507711410522, {'accuracy': 0.8674, 'data_size': 10000}, 43.32533561901073)
INFO flwr 2024-04-07 09:14:36,166 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:14:36,166 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:14:46,241 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:14:47,765 | server.py:125 | fit progress: (3, 1.618666410446167, {'accuracy': 0.8834, 'data_size': 10000}, 54.92491561299539)
INFO flwr 2024-04-07 09:14:47,765 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:14:47,765 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:14:56,369 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:14:57,886 | server.py:125 | fit progress: (4, 1.5920077562332153, {'accuracy': 0.9017, 'data_size': 10000}, 65.04625044300337)
INFO flwr 2024-04-07 09:14:57,887 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:14:57,887 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:15:07,235 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:15:08,556 | server.py:125 | fit progress: (5, 1.5923734903335571, {'accuracy': 0.8961, 'data_size': 10000}, 75.71624974699807)
INFO flwr 2024-04-07 09:15:08,557 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:15:08,557 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:15:18,014 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:15:19,620 | server.py:125 | fit progress: (6, 1.5831053256988525, {'accuracy': 0.9036, 'data_size': 10000}, 86.78004607200273)
INFO flwr 2024-04-07 09:15:19,620 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:15:19,621 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:15:28,998 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:15:30,384 | server.py:125 | fit progress: (7, 1.5791053771972656, {'accuracy': 0.9038, 'data_size': 10000}, 97.5439673899964)
INFO flwr 2024-04-07 09:15:30,384 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:15:30,385 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:15:40,489 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:15:41,882 | server.py:125 | fit progress: (8, 1.5862183570861816, {'accuracy': 0.8965, 'data_size': 10000}, 109.04183435201412)
INFO flwr 2024-04-07 09:15:41,882 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:15:41,883 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:15:51,147 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:15:52,823 | server.py:125 | fit progress: (9, 1.5692028999328613, {'accuracy': 0.9112, 'data_size': 10000}, 119.98267217900138)
INFO flwr 2024-04-07 09:15:52,823 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:15:52,823 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:16:02,486 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:16:04,240 | server.py:125 | fit progress: (10, 1.569648027420044, {'accuracy': 0.9105, 'data_size': 10000}, 131.39992097901995)
INFO flwr 2024-04-07 09:16:04,240 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:16:04,241 | server.py:153 | FL finished in 131.4003750840202
INFO flwr 2024-04-07 09:16:04,241 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:16:04,241 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:16:04,241 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:16:04,241 | app.py:229 | app_fit: losses_centralized [(0, 2.303546190261841), (1, 1.9494950771331787), (2, 1.6551507711410522), (3, 1.618666410446167), (4, 1.5920077562332153), (5, 1.5923734903335571), (6, 1.5831053256988525), (7, 1.5791053771972656), (8, 1.5862183570861816), (9, 1.5692028999328613), (10, 1.569648027420044)]
INFO flwr 2024-04-07 09:16:04,241 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0802), (1, 0.5661), (2, 0.8674), (3, 0.8834), (4, 0.9017), (5, 0.8961), (6, 0.9036), (7, 0.9038), (8, 0.8965), (9, 0.9112), (10, 0.9105)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9105
wandb:     loss 1.56965
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_091320-14l8ub65
wandb: Find logs at: ./wandb/offline-run-20240407_091320-14l8ub65/logs
INFO flwr 2024-04-07 09:16:07,788 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:23:16,794 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1851518)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1851518)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 09:23:22,081	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:23:22,570	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:23:22,966	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5f49c6cc6212845d.zip' (11.68MiB) to Ray cluster...
2024-04-07 09:23:23,003	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5f49c6cc6212845d.zip'.
INFO flwr 2024-04-07 09:23:34,048 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 116352787456.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54151194624.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 09:23:34,049 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:23:34,049 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:23:34,071 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:23:34,073 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:23:34,073 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:23:34,074 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:23:36,802 | server.py:94 | initial parameters (loss, other metrics): 2.3028182983398438, {'accuracy': 0.0882, 'data_size': 10000}
INFO flwr 2024-04-07 09:23:36,803 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:23:36,803 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1858902)[0m 2024-04-07 09:23:40.144579: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1858902)[0m 2024-04-07 09:23:40.236137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1858902)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1858898)[0m 2024-04-07 09:23:42.269519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1858900)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1858900)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1858903)[0m 2024-04-07 09:23:40.422590: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1858903)[0m 2024-04-07 09:23:40.524718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1858903)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1858899)[0m 2024-04-07 09:23:42.636791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:23:56,251 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:23:56,325 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:23:57,746 | server.py:125 | fit progress: (1, 1.889174461364746, {'accuracy': 0.6197, 'data_size': 10000}, 20.942608725017635)
INFO flwr 2024-04-07 09:23:57,746 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:23:57,746 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:08,130 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:09,613 | server.py:125 | fit progress: (2, 1.6601532697677612, {'accuracy': 0.8589, 'data_size': 10000}, 32.80992912701913)
INFO flwr 2024-04-07 09:24:09,613 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:09,614 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:18,810 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:20,327 | server.py:125 | fit progress: (3, 1.6274840831756592, {'accuracy': 0.8699, 'data_size': 10000}, 43.52391390001867)
INFO flwr 2024-04-07 09:24:20,327 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:20,328 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:29,549 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:31,065 | server.py:125 | fit progress: (4, 1.59066903591156, {'accuracy': 0.8989, 'data_size': 10000}, 54.26206896401709)
INFO flwr 2024-04-07 09:24:31,066 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:31,066 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:41,008 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:42,596 | server.py:125 | fit progress: (5, 1.5809355974197388, {'accuracy': 0.9068, 'data_size': 10000}, 65.79301597300218)
INFO flwr 2024-04-07 09:24:42,596 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:42,597 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:24:52,361 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:24:53,756 | server.py:125 | fit progress: (6, 1.6043535470962524, {'accuracy': 0.8818, 'data_size': 10000}, 76.95245488602086)
INFO flwr 2024-04-07 09:24:53,756 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:24:53,756 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:25:03,623 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:25:05,011 | server.py:125 | fit progress: (7, 1.5781095027923584, {'accuracy': 0.901, 'data_size': 10000}, 88.20771624401095)
INFO flwr 2024-04-07 09:25:05,011 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:25:05,012 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:25:14,779 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:25:16,251 | server.py:125 | fit progress: (8, 1.572860836982727, {'accuracy': 0.9081, 'data_size': 10000}, 99.44736800601822)
INFO flwr 2024-04-07 09:25:16,251 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:25:16,251 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:25:26,027 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:25:27,723 | server.py:125 | fit progress: (9, 1.5778166055679321, {'accuracy': 0.9023, 'data_size': 10000}, 110.92018811299931)
INFO flwr 2024-04-07 09:25:27,724 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:25:27,724 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:25:37,434 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:25:41,096 | server.py:125 | fit progress: (10, 1.5688375234603882, {'accuracy': 0.9086, 'data_size': 10000}, 124.29284302602173)
INFO flwr 2024-04-07 09:25:41,096 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:25:41,096 | server.py:153 | FL finished in 124.29331778502092
INFO flwr 2024-04-07 09:25:41,097 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:25:41,097 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:25:41,097 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:25:41,097 | app.py:229 | app_fit: losses_centralized [(0, 2.3028182983398438), (1, 1.889174461364746), (2, 1.6601532697677612), (3, 1.6274840831756592), (4, 1.59066903591156), (5, 1.5809355974197388), (6, 1.6043535470962524), (7, 1.5781095027923584), (8, 1.572860836982727), (9, 1.5778166055679321), (10, 1.5688375234603882)]
INFO flwr 2024-04-07 09:25:41,097 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0882), (1, 0.6197), (2, 0.8589), (3, 0.8699), (4, 0.8989), (5, 0.9068), (6, 0.8818), (7, 0.901), (8, 0.9081), (9, 0.9023), (10, 0.9086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9086
wandb:     loss 1.56884
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_092316-c1hn8pf6
wandb: Find logs at: ./wandb/offline-run-20240407_092316-c1hn8pf6/logs
INFO flwr 2024-04-07 09:25:44,691 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:33:00,052 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1858897)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1858897)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 09:33:21,056	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:33:24,102	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:33:26,111	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1648d6a39526ade1.zip' (11.69MiB) to Ray cluster...
2024-04-07 09:33:26,170	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1648d6a39526ade1.zip'.
INFO flwr 2024-04-07 09:34:05,679 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 107758329652.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 50467855564.0}
INFO flwr 2024-04-07 09:34:05,680 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:34:05,680 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:34:05,704 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:34:05,705 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:34:05,705 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:34:05,705 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:34:07,959 | server.py:94 | initial parameters (loss, other metrics): 2.3042218685150146, {'accuracy': 0.052, 'data_size': 10000}
INFO flwr 2024-04-07 09:34:07,960 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:34:07,960 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1863668)[0m 2024-04-07 09:34:13.451715: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1863668)[0m 2024-04-07 09:34:13.550031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1863668)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1863668)[0m 2024-04-07 09:34:17.682405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1863668)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1863668)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1863661)[0m 2024-04-07 09:34:13.670010: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1863661)[0m 2024-04-07 09:34:13.738050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1863661)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1863661)[0m 2024-04-07 09:34:17.682405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:34:48,268 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:34:48,312 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:34:49,481 | server.py:125 | fit progress: (1, 2.272172689437866, {'accuracy': 0.2725, 'data_size': 10000}, 41.5208282090025)
INFO flwr 2024-04-07 09:34:49,481 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:34:49,481 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:35:01,766 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:35:03,404 | server.py:125 | fit progress: (2, 2.2298407554626465, {'accuracy': 0.3414, 'data_size': 10000}, 55.443557910999516)
INFO flwr 2024-04-07 09:35:03,404 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:35:03,404 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:35:14,878 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:35:16,508 | server.py:125 | fit progress: (3, 2.1906070709228516, {'accuracy': 0.4707, 'data_size': 10000}, 68.54787461599335)
INFO flwr 2024-04-07 09:35:16,508 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:35:16,509 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:35:27,143 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:35:28,503 | server.py:125 | fit progress: (4, 2.149376392364502, {'accuracy': 0.5007, 'data_size': 10000}, 80.54280956299044)
INFO flwr 2024-04-07 09:35:28,503 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:35:28,504 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:35:40,326 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:35:41,692 | server.py:125 | fit progress: (5, 2.1094911098480225, {'accuracy': 0.512, 'data_size': 10000}, 93.73198077100096)
INFO flwr 2024-04-07 09:35:41,692 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:35:41,693 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:35:53,637 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:35:54,994 | server.py:125 | fit progress: (6, 2.0636422634124756, {'accuracy': 0.6619, 'data_size': 10000}, 107.03385499399155)
INFO flwr 2024-04-07 09:35:54,994 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:35:54,995 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:36:06,660 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:36:08,360 | server.py:125 | fit progress: (7, 2.017911434173584, {'accuracy': 0.6882, 'data_size': 10000}, 120.40028225499555)
INFO flwr 2024-04-07 09:36:08,361 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:36:08,361 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:36:19,830 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:36:21,617 | server.py:125 | fit progress: (8, 1.985061526298523, {'accuracy': 0.6925, 'data_size': 10000}, 133.6569063399802)
INFO flwr 2024-04-07 09:36:21,617 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:36:21,617 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:36:33,358 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:36:34,817 | server.py:125 | fit progress: (9, 1.9537923336029053, {'accuracy': 0.7057, 'data_size': 10000}, 146.85705255999346)
INFO flwr 2024-04-07 09:36:34,817 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:36:34,818 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:36:46,363 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:36:47,831 | server.py:125 | fit progress: (10, 1.9322093725204468, {'accuracy': 0.7039, 'data_size': 10000}, 159.87044538199552)
INFO flwr 2024-04-07 09:36:47,831 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:36:47,831 | server.py:153 | FL finished in 159.8709426909918
INFO flwr 2024-04-07 09:36:47,831 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:36:47,831 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:36:47,831 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:36:47,831 | app.py:229 | app_fit: losses_centralized [(0, 2.3042218685150146), (1, 2.272172689437866), (2, 2.2298407554626465), (3, 2.1906070709228516), (4, 2.149376392364502), (5, 2.1094911098480225), (6, 2.0636422634124756), (7, 2.017911434173584), (8, 1.985061526298523), (9, 1.9537923336029053), (10, 1.9322093725204468)]
INFO flwr 2024-04-07 09:36:47,832 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.052), (1, 0.2725), (2, 0.3414), (3, 0.4707), (4, 0.5007), (5, 0.512), (6, 0.6619), (7, 0.6882), (8, 0.6925), (9, 0.7057), (10, 0.7039)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7039
wandb:     loss 1.93221
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_093257-6i6urmdw
wandb: Find logs at: ./wandb/offline-run-20240407_093257-6i6urmdw/logs
INFO flwr 2024-04-07 09:36:51,377 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:43:59,989 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1863661)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1863661)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 09:44:07,727	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:44:08,144	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:44:08,532	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0798b19ef50061c3.zip' (11.71MiB) to Ray cluster...
2024-04-07 09:44:08,566	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0798b19ef50061c3.zip'.
INFO flwr 2024-04-07 09:44:19,623 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 54158721024.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 116370349056.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 09:44:19,623 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:44:19,624 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:44:19,643 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:44:19,644 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:44:19,644 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:44:19,644 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:44:23,691 | server.py:94 | initial parameters (loss, other metrics): 2.3044025897979736, {'accuracy': 0.0855, 'data_size': 10000}
INFO flwr 2024-04-07 09:44:23,692 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:44:23,692 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1871057)[0m 2024-04-07 09:44:25.456065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1871057)[0m 2024-04-07 09:44:25.552912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1871057)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1871068)[0m 2024-04-07 09:44:27.602927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1871061)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1871061)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1871069)[0m 2024-04-07 09:44:25.816409: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1871069)[0m 2024-04-07 09:44:25.921536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1871069)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1871069)[0m 2024-04-07 09:44:28.020265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:44:43,350 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:44:43,389 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:44:44,593 | server.py:125 | fit progress: (1, 1.955562710762024, {'accuracy': 0.642, 'data_size': 10000}, 20.901669221988413)
INFO flwr 2024-04-07 09:44:44,594 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:44:44,594 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:44:56,916 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:44:58,482 | server.py:125 | fit progress: (2, 1.756345510482788, {'accuracy': 0.7597, 'data_size': 10000}, 34.79009580699494)
INFO flwr 2024-04-07 09:44:58,482 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:44:58,483 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:10,404 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:45:12,024 | server.py:125 | fit progress: (3, 1.667986512184143, {'accuracy': 0.8602, 'data_size': 10000}, 48.33256268501282)
INFO flwr 2024-04-07 09:45:12,025 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:45:12,025 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:23,074 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:45:24,674 | server.py:125 | fit progress: (4, 1.6373157501220703, {'accuracy': 0.8756, 'data_size': 10000}, 60.982137582002906)
INFO flwr 2024-04-07 09:45:24,674 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:45:24,674 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:36,068 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:45:37,385 | server.py:125 | fit progress: (5, 1.6280996799468994, {'accuracy': 0.8804, 'data_size': 10000}, 73.6933139830071)
INFO flwr 2024-04-07 09:45:37,385 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:45:37,386 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:45:49,903 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:45:51,296 | server.py:125 | fit progress: (6, 1.6278266906738281, {'accuracy': 0.8737, 'data_size': 10000}, 87.60400422499515)
INFO flwr 2024-04-07 09:45:51,296 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:45:51,296 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:46:03,767 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:46:05,501 | server.py:125 | fit progress: (7, 1.6007260084152222, {'accuracy': 0.9002, 'data_size': 10000}, 101.80903832201147)
INFO flwr 2024-04-07 09:46:05,501 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:46:05,501 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:46:17,229 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:46:19,972 | server.py:125 | fit progress: (8, 1.6016907691955566, {'accuracy': 0.8962, 'data_size': 10000}, 116.2805888550065)
INFO flwr 2024-04-07 09:46:19,973 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:46:19,973 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:46:31,431 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:46:33,233 | server.py:125 | fit progress: (9, 1.5936137437820435, {'accuracy': 0.9026, 'data_size': 10000}, 129.54105442899163)
INFO flwr 2024-04-07 09:46:33,233 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:46:33,233 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:46:44,838 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:46:46,700 | server.py:125 | fit progress: (10, 1.5938565731048584, {'accuracy': 0.8978, 'data_size': 10000}, 143.00811063899891)
INFO flwr 2024-04-07 09:46:46,700 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:46:46,700 | server.py:153 | FL finished in 143.0085151870153
INFO flwr 2024-04-07 09:46:46,700 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:46:46,700 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:46:46,701 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:46:46,701 | app.py:229 | app_fit: losses_centralized [(0, 2.3044025897979736), (1, 1.955562710762024), (2, 1.756345510482788), (3, 1.667986512184143), (4, 1.6373157501220703), (5, 1.6280996799468994), (6, 1.6278266906738281), (7, 1.6007260084152222), (8, 1.6016907691955566), (9, 1.5936137437820435), (10, 1.5938565731048584)]
INFO flwr 2024-04-07 09:46:46,701 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0855), (1, 0.642), (2, 0.7597), (3, 0.8602), (4, 0.8756), (5, 0.8804), (6, 0.8737), (7, 0.9002), (8, 0.8962), (9, 0.9026), (10, 0.8978)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8978
wandb:     loss 1.59386
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_094359-sewz3fhw
wandb: Find logs at: ./wandb/offline-run-20240407_094359-sewz3fhw/logs
INFO flwr 2024-04-07 09:46:50,247 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 09:54:16,895 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1871056)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1871056)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 09:54:32,148	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 09:54:34,023	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 09:54:34,770	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2d8ab54712c3812a.zip' (11.73MiB) to Ray cluster...
2024-04-07 09:54:34,822	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2d8ab54712c3812a.zip'.
INFO flwr 2024-04-07 09:54:49,564 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54046096588.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 116107558708.0}
INFO flwr 2024-04-07 09:54:49,565 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 09:54:49,565 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 09:54:49,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 09:54:49,587 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 09:54:49,588 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 09:54:49,588 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 09:54:51,757 | server.py:94 | initial parameters (loss, other metrics): 2.3015620708465576, {'accuracy': 0.1224, 'data_size': 10000}
INFO flwr 2024-04-07 09:54:51,757 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 09:54:51,758 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1875969)[0m 2024-04-07 09:54:59.769018: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1875969)[0m 2024-04-07 09:54:59.872095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1875969)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1875969)[0m 2024-04-07 09:55:04.889756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1875981)[0m 2024-04-07 09:54:59.865972: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1875969)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1875969)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1875981)[0m 2024-04-07 09:54:59.970545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1875981)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1875981)[0m 2024-04-07 09:55:04.872859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 09:55:31,177 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 09:55:31,219 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 09:55:32,678 | server.py:125 | fit progress: (1, 1.8600544929504395, {'accuracy': 0.6801, 'data_size': 10000}, 40.9206517470011)
INFO flwr 2024-04-07 09:55:32,678 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 09:55:32,679 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:55:44,219 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 09:55:45,713 | server.py:125 | fit progress: (2, 1.6831119060516357, {'accuracy': 0.8355, 'data_size': 10000}, 53.95592621600372)
INFO flwr 2024-04-07 09:55:45,714 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 09:55:45,714 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:55:56,358 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 09:55:57,893 | server.py:125 | fit progress: (3, 1.6219677925109863, {'accuracy': 0.8921, 'data_size': 10000}, 66.13598322798498)
INFO flwr 2024-04-07 09:55:57,894 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 09:55:57,894 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:56:09,044 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 09:56:10,604 | server.py:125 | fit progress: (4, 1.614019513130188, {'accuracy': 0.8865, 'data_size': 10000}, 78.84632854899974)
INFO flwr 2024-04-07 09:56:10,604 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 09:56:10,604 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:56:21,251 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 09:56:22,538 | server.py:125 | fit progress: (5, 1.5925214290618896, {'accuracy': 0.9013, 'data_size': 10000}, 90.78072995500406)
INFO flwr 2024-04-07 09:56:22,538 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 09:56:22,539 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:56:34,196 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 09:56:35,531 | server.py:125 | fit progress: (6, 1.604257583618164, {'accuracy': 0.8859, 'data_size': 10000}, 103.77317865300574)
INFO flwr 2024-04-07 09:56:35,531 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 09:56:35,531 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:56:47,187 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 09:56:48,842 | server.py:125 | fit progress: (7, 1.582920789718628, {'accuracy': 0.9032, 'data_size': 10000}, 117.08433427099953)
INFO flwr 2024-04-07 09:56:48,842 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 09:56:48,842 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:56:59,685 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 09:57:01,435 | server.py:125 | fit progress: (8, 1.5784995555877686, {'accuracy': 0.9071, 'data_size': 10000}, 129.67742592500872)
INFO flwr 2024-04-07 09:57:01,435 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 09:57:01,435 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:57:12,027 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 09:57:13,472 | server.py:125 | fit progress: (9, 1.5928261280059814, {'accuracy': 0.8921, 'data_size': 10000}, 141.71462993699242)
INFO flwr 2024-04-07 09:57:13,472 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 09:57:13,473 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 09:57:24,095 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 09:57:25,807 | server.py:125 | fit progress: (10, 1.5759357213974, {'accuracy': 0.9081, 'data_size': 10000}, 154.04932749000727)
INFO flwr 2024-04-07 09:57:25,807 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 09:57:25,807 | server.py:153 | FL finished in 154.04991503499332
INFO flwr 2024-04-07 09:57:25,808 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 09:57:25,808 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 09:57:25,808 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 09:57:25,808 | app.py:229 | app_fit: losses_centralized [(0, 2.3015620708465576), (1, 1.8600544929504395), (2, 1.6831119060516357), (3, 1.6219677925109863), (4, 1.614019513130188), (5, 1.5925214290618896), (6, 1.604257583618164), (7, 1.582920789718628), (8, 1.5784995555877686), (9, 1.5928261280059814), (10, 1.5759357213974)]
INFO flwr 2024-04-07 09:57:25,808 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1224), (1, 0.6801), (2, 0.8355), (3, 0.8921), (4, 0.8865), (5, 0.9013), (6, 0.8859), (7, 0.9032), (8, 0.9071), (9, 0.8921), (10, 0.9081)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9081
wandb:     loss 1.57594
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_095359-2qeb1xjt
wandb: Find logs at: ./wandb/offline-run-20240407_095359-2qeb1xjt/logs
INFO flwr 2024-04-07 09:57:29,348 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:04:39,056 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1875981)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1875981)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:04:45,145	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:04:45,501	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:04:45,860	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ed17d8fbc1694ea9.zip' (11.75MiB) to Ray cluster...
2024-04-07 10:04:45,898	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ed17d8fbc1694ea9.zip'.
INFO flwr 2024-04-07 10:04:56,839 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 111957874484.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 52267660492.0}
INFO flwr 2024-04-07 10:04:56,839 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:04:56,839 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:04:56,857 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:04:56,858 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:04:56,858 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:04:56,859 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:04:58,937 | server.py:94 | initial parameters (loss, other metrics): 2.305995225906372, {'accuracy': 0.0691, 'data_size': 10000}
INFO flwr 2024-04-07 10:04:58,938 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:04:58,939 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1883683)[0m 2024-04-07 10:05:02.574110: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1883683)[0m 2024-04-07 10:05:02.666612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1883683)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1883683)[0m 2024-04-07 10:05:05.128341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1883692)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1883692)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1883692)[0m 2024-04-07 10:05:03.334426: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1883692)[0m 2024-04-07 10:05:03.430059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1883692)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1883692)[0m 2024-04-07 10:05:05.928858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:05:21,747 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:05:21,784 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:05:24,643 | server.py:125 | fit progress: (1, 1.9678435325622559, {'accuracy': 0.4873, 'data_size': 10000}, 25.704085461009527)
INFO flwr 2024-04-07 10:05:24,643 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:05:24,643 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:05:36,287 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:05:37,736 | server.py:125 | fit progress: (2, 1.7029842138290405, {'accuracy': 0.7931, 'data_size': 10000}, 38.79781715600984)
INFO flwr 2024-04-07 10:05:37,737 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:05:37,737 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:05:49,054 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:05:50,309 | server.py:125 | fit progress: (3, 1.612621784210205, {'accuracy': 0.887, 'data_size': 10000}, 51.37042819999624)
INFO flwr 2024-04-07 10:05:50,309 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:05:50,310 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:00,302 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:01,767 | server.py:125 | fit progress: (4, 1.5940548181533813, {'accuracy': 0.8953, 'data_size': 10000}, 62.828762663004454)
INFO flwr 2024-04-07 10:06:01,768 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:01,768 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:12,510 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:14,076 | server.py:125 | fit progress: (5, 1.592245101928711, {'accuracy': 0.8965, 'data_size': 10000}, 75.13757697399706)
INFO flwr 2024-04-07 10:06:14,077 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:14,077 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:24,676 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:26,245 | server.py:125 | fit progress: (6, 1.5863838195800781, {'accuracy': 0.8989, 'data_size': 10000}, 87.30661415299983)
INFO flwr 2024-04-07 10:06:26,246 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:26,246 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:37,488 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:41,879 | server.py:125 | fit progress: (7, 1.5749751329421997, {'accuracy': 0.9084, 'data_size': 10000}, 102.94057228098973)
INFO flwr 2024-04-07 10:06:41,880 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:41,880 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:06:52,984 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:06:56,192 | server.py:125 | fit progress: (8, 1.5729167461395264, {'accuracy': 0.9086, 'data_size': 10000}, 117.25378411699785)
INFO flwr 2024-04-07 10:06:56,193 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:06:56,193 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:07:07,072 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:07:09,811 | server.py:125 | fit progress: (9, 1.5764306783676147, {'accuracy': 0.9054, 'data_size': 10000}, 130.8724470490124)
INFO flwr 2024-04-07 10:07:09,811 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:07:09,811 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:07:21,398 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:07:24,151 | server.py:125 | fit progress: (10, 1.582269549369812, {'accuracy': 0.8994, 'data_size': 10000}, 145.21226723701693)
INFO flwr 2024-04-07 10:07:24,151 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:07:24,151 | server.py:153 | FL finished in 145.21270489599556
INFO flwr 2024-04-07 10:07:24,151 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:07:24,152 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:07:24,152 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:07:24,152 | app.py:229 | app_fit: losses_centralized [(0, 2.305995225906372), (1, 1.9678435325622559), (2, 1.7029842138290405), (3, 1.612621784210205), (4, 1.5940548181533813), (5, 1.592245101928711), (6, 1.5863838195800781), (7, 1.5749751329421997), (8, 1.5729167461395264), (9, 1.5764306783676147), (10, 1.582269549369812)]
INFO flwr 2024-04-07 10:07:24,152 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0691), (1, 0.4873), (2, 0.7931), (3, 0.887), (4, 0.8953), (5, 0.8965), (6, 0.8989), (7, 0.9084), (8, 0.9086), (9, 0.9054), (10, 0.8994)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8994
wandb:     loss 1.58227
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_100438-c2q63uqa
wandb: Find logs at: ./wandb/offline-run-20240407_100438-c2q63uqa/logs
INFO flwr 2024-04-07 10:07:27,709 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:14:40,223 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1883690)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1883690)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:14:50,042	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:14:58,145	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:14:58,527	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4c3a359d1d096271.zip' (11.76MiB) to Ray cluster...
2024-04-07 10:14:58,567	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4c3a359d1d096271.zip'.
INFO flwr 2024-04-07 10:15:09,807 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 113150818919.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 52778922393.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 10:15:09,807 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:15:09,808 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:15:09,828 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:15:09,829 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:15:09,829 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:15:09,829 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:15:14,113 | server.py:94 | initial parameters (loss, other metrics): 2.301478147506714, {'accuracy': 0.1227, 'data_size': 10000}
INFO flwr 2024-04-07 10:15:14,113 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:15:14,113 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1888141)[0m 2024-04-07 10:15:18.173174: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1888141)[0m 2024-04-07 10:15:18.281008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1888141)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1888141)[0m 2024-04-07 10:15:22.424818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1888141)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1888141)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1888148)[0m 2024-04-07 10:15:18.313758: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1888148)[0m 2024-04-07 10:15:18.408673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1888148)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1888145)[0m 2024-04-07 10:15:22.436126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:15:49,166 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:15:49,212 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:15:51,852 | server.py:125 | fit progress: (1, 1.7311580181121826, {'accuracy': 0.825, 'data_size': 10000}, 37.73839641400264)
INFO flwr 2024-04-07 10:15:51,852 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:15:51,852 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:16:02,751 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:16:06,915 | server.py:125 | fit progress: (2, 1.6254026889801025, {'accuracy': 0.8764, 'data_size': 10000}, 52.80202232400188)
INFO flwr 2024-04-07 10:16:06,916 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:16:06,916 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:16:16,852 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:16:20,786 | server.py:125 | fit progress: (3, 1.6076233386993408, {'accuracy': 0.8859, 'data_size': 10000}, 66.67262794298586)
INFO flwr 2024-04-07 10:16:20,786 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:16:20,786 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:16:30,925 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:16:35,163 | server.py:125 | fit progress: (4, 1.5874487161636353, {'accuracy': 0.8993, 'data_size': 10000}, 81.05015918900608)
INFO flwr 2024-04-07 10:16:35,164 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:16:35,164 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:16:45,514 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:16:49,806 | server.py:125 | fit progress: (5, 1.5819884538650513, {'accuracy': 0.9021, 'data_size': 10000}, 95.69311694600037)
INFO flwr 2024-04-07 10:16:49,807 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:16:49,807 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:17:00,976 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:02,601 | server.py:125 | fit progress: (6, 1.5845496654510498, {'accuracy': 0.8956, 'data_size': 10000}, 108.48766529400018)
INFO flwr 2024-04-07 10:17:02,601 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:17:02,601 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:17:13,337 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:14,735 | server.py:125 | fit progress: (7, 1.5732024908065796, {'accuracy': 0.9071, 'data_size': 10000}, 120.62174369298737)
INFO flwr 2024-04-07 10:17:14,735 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:17:14,736 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:17:25,187 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:26,597 | server.py:125 | fit progress: (8, 1.5688120126724243, {'accuracy': 0.9123, 'data_size': 10000}, 132.48353825998493)
INFO flwr 2024-04-07 10:17:26,597 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:17:26,597 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:17:37,333 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:39,042 | server.py:125 | fit progress: (9, 1.5827369689941406, {'accuracy': 0.8971, 'data_size': 10000}, 144.92836198798614)
INFO flwr 2024-04-07 10:17:39,042 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:17:39,042 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:17:49,268 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:17:51,025 | server.py:125 | fit progress: (10, 1.5668381452560425, {'accuracy': 0.9087, 'data_size': 10000}, 156.91209193400573)
INFO flwr 2024-04-07 10:17:51,026 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:17:51,026 | server.py:153 | FL finished in 156.91262556400034
INFO flwr 2024-04-07 10:17:51,026 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:17:51,026 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:17:51,026 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:17:51,026 | app.py:229 | app_fit: losses_centralized [(0, 2.301478147506714), (1, 1.7311580181121826), (2, 1.6254026889801025), (3, 1.6076233386993408), (4, 1.5874487161636353), (5, 1.5819884538650513), (6, 1.5845496654510498), (7, 1.5732024908065796), (8, 1.5688120126724243), (9, 1.5827369689941406), (10, 1.5668381452560425)]
INFO flwr 2024-04-07 10:17:51,026 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1227), (1, 0.825), (2, 0.8764), (3, 0.8859), (4, 0.8993), (5, 0.9021), (6, 0.8956), (7, 0.9071), (8, 0.9123), (9, 0.8971), (10, 0.9087)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9087
wandb:     loss 1.56684
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_101437-m9pbhugb
wandb: Find logs at: ./wandb/offline-run-20240407_101437-m9pbhugb/logs
INFO flwr 2024-04-07 10:17:54,624 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:25:06,011 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1888144)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1888144)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:25:12,619	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:25:15,699	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:25:16,050	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_7cf7350aa09d18a3.zip' (11.78MiB) to Ray cluster...
2024-04-07 10:25:16,092	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_7cf7350aa09d18a3.zip'.
INFO flwr 2024-04-07 10:25:28,154 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 123887743181.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57380461363.0}
INFO flwr 2024-04-07 10:25:28,155 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:25:28,155 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:25:28,172 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:25:28,173 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:25:28,173 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:25:28,173 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:25:30,348 | server.py:94 | initial parameters (loss, other metrics): 2.3012304306030273, {'accuracy': 0.12, 'data_size': 10000}
INFO flwr 2024-04-07 10:25:30,349 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:25:30,349 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1893884)[0m 2024-04-07 10:25:35.520995: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1893884)[0m 2024-04-07 10:25:35.639000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1893884)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1893884)[0m 2024-04-07 10:25:38.388402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1893922)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1893922)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1893917)[0m 2024-04-07 10:25:35.698647: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893917)[0m 2024-04-07 10:25:35.807677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893917)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1893874)[0m 2024-04-07 10:25:38.389108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:25:59,537 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:25:59,579 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:26:00,986 | server.py:125 | fit progress: (1, 1.8264163732528687, {'accuracy': 0.6845, 'data_size': 10000}, 30.63676024499)
INFO flwr 2024-04-07 10:26:00,986 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:00,986 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:12,137 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:26:13,575 | server.py:125 | fit progress: (2, 1.6496849060058594, {'accuracy': 0.8445, 'data_size': 10000}, 43.22561111600953)
INFO flwr 2024-04-07 10:26:13,575 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:13,575 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:23,874 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:26:25,326 | server.py:125 | fit progress: (3, 1.5925350189208984, {'accuracy': 0.8977, 'data_size': 10000}, 54.97684213300818)
INFO flwr 2024-04-07 10:26:25,326 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:25,326 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:36,101 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:26:37,617 | server.py:125 | fit progress: (4, 1.5816738605499268, {'accuracy': 0.9037, 'data_size': 10000}, 67.26801676399191)
INFO flwr 2024-04-07 10:26:37,617 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:37,618 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:47,883 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:26:49,176 | server.py:125 | fit progress: (5, 1.578338384628296, {'accuracy': 0.9025, 'data_size': 10000}, 78.82712896898738)
INFO flwr 2024-04-07 10:26:49,177 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:26:49,177 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:26:59,926 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:01,245 | server.py:125 | fit progress: (6, 1.576440453529358, {'accuracy': 0.9038, 'data_size': 10000}, 90.89595152399852)
INFO flwr 2024-04-07 10:27:01,245 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:01,246 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:12,092 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:13,408 | server.py:125 | fit progress: (7, 1.5717172622680664, {'accuracy': 0.9077, 'data_size': 10000}, 103.05845506500918)
INFO flwr 2024-04-07 10:27:13,408 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:13,408 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:25,510 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:27,334 | server.py:125 | fit progress: (8, 1.5776598453521729, {'accuracy': 0.8974, 'data_size': 10000}, 116.98476467898581)
INFO flwr 2024-04-07 10:27:27,334 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:27,334 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:38,490 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:40,328 | server.py:125 | fit progress: (9, 1.573848009109497, {'accuracy': 0.9025, 'data_size': 10000}, 129.97836479000398)
INFO flwr 2024-04-07 10:27:40,328 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:27:40,328 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:27:51,512 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:27:52,865 | server.py:125 | fit progress: (10, 1.5702488422393799, {'accuracy': 0.9053, 'data_size': 10000}, 142.51599830100895)
INFO flwr 2024-04-07 10:27:52,865 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:27:52,866 | server.py:153 | FL finished in 142.51643506300752
INFO flwr 2024-04-07 10:27:52,866 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:27:52,866 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:27:52,866 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:27:52,866 | app.py:229 | app_fit: losses_centralized [(0, 2.3012304306030273), (1, 1.8264163732528687), (2, 1.6496849060058594), (3, 1.5925350189208984), (4, 1.5816738605499268), (5, 1.578338384628296), (6, 1.576440453529358), (7, 1.5717172622680664), (8, 1.5776598453521729), (9, 1.573848009109497), (10, 1.5702488422393799)]
INFO flwr 2024-04-07 10:27:52,866 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.12), (1, 0.6845), (2, 0.8445), (3, 0.8977), (4, 0.9037), (5, 0.9025), (6, 0.9038), (7, 0.9077), (8, 0.8974), (9, 0.9025), (10, 0.9053)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9053
wandb:     loss 1.57025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_102504-5semlqja
wandb: Find logs at: ./wandb/offline-run-20240407_102504-5semlqja/logs
INFO flwr 2024-04-07 10:27:56,476 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 8
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:35:04,776 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1893891)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1893891)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:35:09,747	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:35:10,338	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:35:10,790	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_31b90922c87a3f07.zip' (11.80MiB) to Ray cluster...
2024-04-07 10:35:10,837	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_31b90922c87a3f07.zip'.
INFO flwr 2024-04-07 10:35:21,880 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 109462656410.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 51198281318.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 10:35:21,880 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:35:21,880 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:35:21,895 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:35:21,896 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:35:21,896 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:35:21,896 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:35:24,867 | server.py:94 | initial parameters (loss, other metrics): 2.300086259841919, {'accuracy': 0.1141, 'data_size': 10000}
INFO flwr 2024-04-07 10:35:24,867 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:35:24,868 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1900259)[0m 2024-04-07 10:35:28.230528: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1900259)[0m 2024-04-07 10:35:28.332592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1900259)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1900259)[0m 2024-04-07 10:35:30.485897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1900267)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1900267)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1900258)[0m 2024-04-07 10:35:28.423929: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1900258)[0m 2024-04-07 10:35:28.519561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1900258)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1900262)[0m 2024-04-07 10:35:30.697135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:35:47,290 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:35:47,327 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:35:48,387 | server.py:125 | fit progress: (1, 1.8104822635650635, {'accuracy': 0.712, 'data_size': 10000}, 23.519326268025907)
INFO flwr 2024-04-07 10:35:48,387 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:35:48,388 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:35:59,582 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:06,380 | server.py:125 | fit progress: (2, 1.622310996055603, {'accuracy': 0.8727, 'data_size': 10000}, 41.51269457000308)
INFO flwr 2024-04-07 10:36:06,381 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:36:06,381 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:36:16,082 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:17,596 | server.py:125 | fit progress: (3, 1.590728998184204, {'accuracy': 0.8961, 'data_size': 10000}, 52.72835087502608)
INFO flwr 2024-04-07 10:36:17,596 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:36:17,596 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:36:26,872 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:28,427 | server.py:125 | fit progress: (4, 1.5812548398971558, {'accuracy': 0.9021, 'data_size': 10000}, 63.55928573102574)
INFO flwr 2024-04-07 10:36:28,427 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:36:28,427 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:36:38,669 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:39,922 | server.py:125 | fit progress: (5, 1.573923945426941, {'accuracy': 0.9063, 'data_size': 10000}, 75.05405367602361)
INFO flwr 2024-04-07 10:36:39,922 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:36:39,922 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:36:49,397 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:36:50,886 | server.py:125 | fit progress: (6, 1.5693589448928833, {'accuracy': 0.909, 'data_size': 10000}, 86.01856608400703)
INFO flwr 2024-04-07 10:36:50,887 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:36:50,887 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:37:00,599 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:37:01,904 | server.py:125 | fit progress: (7, 1.5661581754684448, {'accuracy': 0.9113, 'data_size': 10000}, 97.03664066400961)
INFO flwr 2024-04-07 10:37:01,905 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:37:01,905 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:37:12,994 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:37:14,972 | server.py:125 | fit progress: (8, 1.5658761262893677, {'accuracy': 0.9099, 'data_size': 10000}, 110.10435792300268)
INFO flwr 2024-04-07 10:37:14,972 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:37:14,973 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:37:25,010 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:37:26,561 | server.py:125 | fit progress: (9, 1.565230131149292, {'accuracy': 0.9085, 'data_size': 10000}, 121.69282503201975)
INFO flwr 2024-04-07 10:37:26,561 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:37:26,561 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:37:35,796 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:37:37,648 | server.py:125 | fit progress: (10, 1.5700018405914307, {'accuracy': 0.9046, 'data_size': 10000}, 132.7803414700029)
INFO flwr 2024-04-07 10:37:37,648 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:37:37,648 | server.py:153 | FL finished in 132.78077379701426
INFO flwr 2024-04-07 10:37:37,649 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:37:37,649 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:37:37,649 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:37:37,649 | app.py:229 | app_fit: losses_centralized [(0, 2.300086259841919), (1, 1.8104822635650635), (2, 1.622310996055603), (3, 1.590728998184204), (4, 1.5812548398971558), (5, 1.573923945426941), (6, 1.5693589448928833), (7, 1.5661581754684448), (8, 1.5658761262893677), (9, 1.565230131149292), (10, 1.5700018405914307)]
INFO flwr 2024-04-07 10:37:37,649 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1141), (1, 0.712), (2, 0.8727), (3, 0.8961), (4, 0.9021), (5, 0.9063), (6, 0.909), (7, 0.9113), (8, 0.9099), (9, 0.9085), (10, 0.9046)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9046
wandb:     loss 1.57
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_103504-hvclprip
wandb: Find logs at: ./wandb/offline-run-20240407_103504-hvclprip/logs
INFO flwr 2024-04-07 10:37:41,200 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:44:51,367 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1900258)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1900258)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:44:56,230	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:44:56,638	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:44:57,064	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_38aa02435d11a912.zip' (11.82MiB) to Ray cluster...
2024-04-07 10:44:57,098	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_38aa02435d11a912.zip'.
INFO flwr 2024-04-07 10:45:08,120 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 117056535962.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 54452801126.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 10:45:08,120 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:45:08,120 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:45:08,137 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:45:08,138 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:45:08,138 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:45:08,138 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:45:10,921 | server.py:94 | initial parameters (loss, other metrics): 2.3009493350982666, {'accuracy': 0.1242, 'data_size': 10000}
INFO flwr 2024-04-07 10:45:10,925 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:45:10,926 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1907630)[0m 2024-04-07 10:45:14.092702: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1907623)[0m 2024-04-07 10:45:14.219846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1907623)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1907630)[0m 2024-04-07 10:45:15.975658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1907625)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1907625)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1907626)[0m 2024-04-07 10:45:14.821576: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1907626)[0m 2024-04-07 10:45:14.912644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1907626)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1907626)[0m 2024-04-07 10:45:16.948207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:45:28,268 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:45:28,298 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:45:29,834 | server.py:125 | fit progress: (1, 2.3005237579345703, {'accuracy': 0.1298, 'data_size': 10000}, 18.9085042369843)
INFO flwr 2024-04-07 10:45:29,834 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:29,835 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:45:38,251 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:45:39,601 | server.py:125 | fit progress: (2, 2.3000385761260986, {'accuracy': 0.1348, 'data_size': 10000}, 28.675773399998434)
INFO flwr 2024-04-07 10:45:39,602 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:39,602 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:45:46,773 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:45:48,198 | server.py:125 | fit progress: (3, 2.299628257751465, {'accuracy': 0.139, 'data_size': 10000}, 37.27228244298021)
INFO flwr 2024-04-07 10:45:48,198 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:48,199 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:45:56,139 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:45:57,581 | server.py:125 | fit progress: (4, 2.299190044403076, {'accuracy': 0.1434, 'data_size': 10000}, 46.65556683798786)
INFO flwr 2024-04-07 10:45:57,582 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:45:57,582 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:05,208 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:06,926 | server.py:125 | fit progress: (5, 2.2986361980438232, {'accuracy': 0.1553, 'data_size': 10000}, 55.999885122990236)
INFO flwr 2024-04-07 10:46:06,926 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:06,926 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:16,192 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:17,561 | server.py:125 | fit progress: (6, 2.2981324195861816, {'accuracy': 0.1613, 'data_size': 10000}, 66.63529943098547)
INFO flwr 2024-04-07 10:46:17,561 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:17,562 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:25,943 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:27,198 | server.py:125 | fit progress: (7, 2.297598361968994, {'accuracy': 0.1679, 'data_size': 10000}, 76.27210226599709)
INFO flwr 2024-04-07 10:46:27,198 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:27,198 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:34,942 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:36,253 | server.py:125 | fit progress: (8, 2.2971043586730957, {'accuracy': 0.1752, 'data_size': 10000}, 85.32750668199151)
INFO flwr 2024-04-07 10:46:36,254 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:36,254 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:44,329 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:45,949 | server.py:125 | fit progress: (9, 2.2966103553771973, {'accuracy': 0.1809, 'data_size': 10000}, 95.02314653998474)
INFO flwr 2024-04-07 10:46:45,949 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:46:45,949 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:46:53,599 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:46:55,333 | server.py:125 | fit progress: (10, 2.2960758209228516, {'accuracy': 0.1919, 'data_size': 10000}, 104.4073795589793)
INFO flwr 2024-04-07 10:46:55,334 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:46:55,334 | server.py:153 | FL finished in 104.4081065839855
INFO flwr 2024-04-07 10:46:55,334 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:46:55,334 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:46:55,334 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:46:55,334 | app.py:229 | app_fit: losses_centralized [(0, 2.3009493350982666), (1, 2.3005237579345703), (2, 2.3000385761260986), (3, 2.299628257751465), (4, 2.299190044403076), (5, 2.2986361980438232), (6, 2.2981324195861816), (7, 2.297598361968994), (8, 2.2971043586730957), (9, 2.2966103553771973), (10, 2.2960758209228516)]
INFO flwr 2024-04-07 10:46:55,334 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1242), (1, 0.1298), (2, 0.1348), (3, 0.139), (4, 0.1434), (5, 0.1553), (6, 0.1613), (7, 0.1679), (8, 0.1752), (9, 0.1809), (10, 0.1919)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1919
wandb:     loss 2.29608
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_104450-464zbaad
wandb: Find logs at: ./wandb/offline-run-20240407_104450-464zbaad/logs
INFO flwr 2024-04-07 10:46:58,875 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 10:54:09,876 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1907633)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1907633)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 10:54:16,507	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 10:54:22,855	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 10:54:23,199	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_06f16161d816d37a.zip' (11.84MiB) to Ray cluster...
2024-04-07 10:54:23,239	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_06f16161d816d37a.zip'.
INFO flwr 2024-04-07 10:54:34,095 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 108019525837.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 50579796787.0}
INFO flwr 2024-04-07 10:54:34,095 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 10:54:34,096 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 10:54:34,112 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 10:54:34,112 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 10:54:34,113 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 10:54:34,113 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 10:54:36,556 | server.py:94 | initial parameters (loss, other metrics): 2.306201457977295, {'accuracy': 0.0772, 'data_size': 10000}
INFO flwr 2024-04-07 10:54:36,557 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 10:54:36,557 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1912116)[0m 2024-04-07 10:54:40.681974: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1912116)[0m 2024-04-07 10:54:40.782815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1912116)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1912115)[0m 2024-04-07 10:54:43.764137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1912118)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1912118)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1912118)[0m 2024-04-07 10:54:40.830863: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1912118)[0m 2024-04-07 10:54:40.931262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1912118)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1912118)[0m 2024-04-07 10:54:43.806201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 10:55:35,274 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 10:55:35,314 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 10:55:36,670 | server.py:125 | fit progress: (1, 2.271496057510376, {'accuracy': 0.2633, 'data_size': 10000}, 60.11368255800335)
INFO flwr 2024-04-07 10:55:36,671 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 10:55:36,671 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:55:46,410 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 10:55:47,830 | server.py:125 | fit progress: (2, 2.2245709896087646, {'accuracy': 0.4507, 'data_size': 10000}, 71.27281655199477)
INFO flwr 2024-04-07 10:55:47,830 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 10:55:47,830 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:55:56,480 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 10:55:58,016 | server.py:125 | fit progress: (3, 2.1654560565948486, {'accuracy': 0.5344, 'data_size': 10000}, 81.45882447200711)
INFO flwr 2024-04-07 10:55:58,016 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 10:55:58,016 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:06,234 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:07,830 | server.py:125 | fit progress: (4, 2.115992784500122, {'accuracy': 0.618, 'data_size': 10000}, 91.27365677399212)
INFO flwr 2024-04-07 10:56:07,831 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:07,831 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:16,363 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:18,221 | server.py:125 | fit progress: (5, 2.0629701614379883, {'accuracy': 0.5779, 'data_size': 10000}, 101.66423187300097)
INFO flwr 2024-04-07 10:56:18,221 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:18,222 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:26,604 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:28,410 | server.py:125 | fit progress: (6, 2.0121676921844482, {'accuracy': 0.6671, 'data_size': 10000}, 111.85348130200873)
INFO flwr 2024-04-07 10:56:28,411 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:28,411 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:36,720 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:38,331 | server.py:125 | fit progress: (7, 1.9661284685134888, {'accuracy': 0.7004, 'data_size': 10000}, 121.77460505999625)
INFO flwr 2024-04-07 10:56:38,332 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:38,332 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:47,205 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:49,330 | server.py:125 | fit progress: (8, 1.9363605976104736, {'accuracy': 0.6917, 'data_size': 10000}, 132.7733800020069)
INFO flwr 2024-04-07 10:56:49,330 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:49,331 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:56:57,928 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 10:56:59,354 | server.py:125 | fit progress: (9, 1.90827476978302, {'accuracy': 0.7158, 'data_size': 10000}, 142.79743380201398)
INFO flwr 2024-04-07 10:56:59,354 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 10:56:59,355 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 10:57:07,961 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 10:57:09,711 | server.py:125 | fit progress: (10, 1.8881933689117432, {'accuracy': 0.722, 'data_size': 10000}, 153.15436012399732)
INFO flwr 2024-04-07 10:57:09,711 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 10:57:09,712 | server.py:153 | FL finished in 153.15480537200347
INFO flwr 2024-04-07 10:57:09,712 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 10:57:09,712 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 10:57:09,712 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 10:57:09,712 | app.py:229 | app_fit: losses_centralized [(0, 2.306201457977295), (1, 2.271496057510376), (2, 2.2245709896087646), (3, 2.1654560565948486), (4, 2.115992784500122), (5, 2.0629701614379883), (6, 2.0121676921844482), (7, 1.9661284685134888), (8, 1.9363605976104736), (9, 1.90827476978302), (10, 1.8881933689117432)]
INFO flwr 2024-04-07 10:57:09,712 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0772), (1, 0.2633), (2, 0.4507), (3, 0.5344), (4, 0.618), (5, 0.5779), (6, 0.6671), (7, 0.7004), (8, 0.6917), (9, 0.7158), (10, 0.722)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.722
wandb:     loss 1.88819
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_105407-0lviv6q5
wandb: Find logs at: ./wandb/offline-run-20240407_105407-0lviv6q5/logs
INFO flwr 2024-04-07 10:57:13,254 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:04:21,429 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1912119)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1912119)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:04:29,259	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:04:31,068	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:04:31,402	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4eed0a18c8acf950.zip' (11.85MiB) to Ray cluster...
2024-04-07 11:04:31,441	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4eed0a18c8acf950.zip'.
INFO flwr 2024-04-07 11:04:42,320 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 120340841882.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 55860360806.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 11:04:42,320 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:04:42,320 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:04:42,333 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:04:42,335 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:04:42,335 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:04:42,335 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:04:46,851 | server.py:94 | initial parameters (loss, other metrics): 2.302257537841797, {'accuracy': 0.1089, 'data_size': 10000}
INFO flwr 2024-04-07 11:04:46,852 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:04:46,852 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1916765)[0m 2024-04-07 11:04:48.690287: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1916773)[0m 2024-04-07 11:04:48.811738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1916773)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1916773)[0m 2024-04-07 11:04:51.568150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1916764)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1916764)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1916770)[0m 2024-04-07 11:04:48.901727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1916770)[0m 2024-04-07 11:04:48.982121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1916770)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1916764)[0m 2024-04-07 11:04:51.597224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:05:05,058 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:05:05,115 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:05:06,398 | server.py:125 | fit progress: (1, 2.244947671890259, {'accuracy': 0.2932, 'data_size': 10000}, 19.545838232006645)
INFO flwr 2024-04-07 11:05:06,398 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:06,398 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:05:14,686 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:05:16,438 | server.py:125 | fit progress: (2, 2.159604787826538, {'accuracy': 0.5488, 'data_size': 10000}, 29.586526305007283)
INFO flwr 2024-04-07 11:05:16,439 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:16,439 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:05:24,591 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:05:25,749 | server.py:125 | fit progress: (3, 2.083157539367676, {'accuracy': 0.5752, 'data_size': 10000}, 38.897651765990304)
INFO flwr 2024-04-07 11:05:25,750 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:25,750 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:05:33,329 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:05:35,111 | server.py:125 | fit progress: (4, 1.9802902936935425, {'accuracy': 0.7178, 'data_size': 10000}, 48.25878859098884)
INFO flwr 2024-04-07 11:05:35,111 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:35,111 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:05:42,860 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:05:44,334 | server.py:125 | fit progress: (5, 1.9069308042526245, {'accuracy': 0.7821, 'data_size': 10000}, 57.482537670002785)
INFO flwr 2024-04-07 11:05:44,335 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:44,335 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:05:51,911 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:05:53,772 | server.py:125 | fit progress: (6, 1.8662017583847046, {'accuracy': 0.7944, 'data_size': 10000}, 66.92066868999973)
INFO flwr 2024-04-07 11:05:53,773 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:05:53,773 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:06:01,527 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:06:02,974 | server.py:125 | fit progress: (7, 1.8389660120010376, {'accuracy': 0.7859, 'data_size': 10000}, 76.12214325799141)
INFO flwr 2024-04-07 11:06:02,974 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:06:02,975 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:06:10,753 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:06:12,275 | server.py:125 | fit progress: (8, 1.8030476570129395, {'accuracy': 0.8328, 'data_size': 10000}, 85.42339067699504)
INFO flwr 2024-04-07 11:06:12,275 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:06:12,276 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:06:20,589 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:06:22,048 | server.py:125 | fit progress: (9, 1.7809829711914062, {'accuracy': 0.8332, 'data_size': 10000}, 95.19644363899715)
INFO flwr 2024-04-07 11:06:22,049 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:06:22,049 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:06:30,250 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:06:31,829 | server.py:125 | fit progress: (10, 1.761092185974121, {'accuracy': 0.8457, 'data_size': 10000}, 104.97693984099897)
INFO flwr 2024-04-07 11:06:31,829 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:06:31,829 | server.py:153 | FL finished in 104.97741399999359
INFO flwr 2024-04-07 11:06:31,829 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:06:31,829 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:06:31,830 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:06:31,830 | app.py:229 | app_fit: losses_centralized [(0, 2.302257537841797), (1, 2.244947671890259), (2, 2.159604787826538), (3, 2.083157539367676), (4, 1.9802902936935425), (5, 1.9069308042526245), (6, 1.8662017583847046), (7, 1.8389660120010376), (8, 1.8030476570129395), (9, 1.7809829711914062), (10, 1.761092185974121)]
INFO flwr 2024-04-07 11:06:31,830 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1089), (1, 0.2932), (2, 0.5488), (3, 0.5752), (4, 0.7178), (5, 0.7821), (6, 0.7944), (7, 0.7859), (8, 0.8328), (9, 0.8332), (10, 0.8457)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8457
wandb:     loss 1.76109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_110420-3jh44v3t
wandb: Find logs at: ./wandb/offline-run-20240407_110420-3jh44v3t/logs
INFO flwr 2024-04-07 11:06:35,444 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:13:41,980 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1916774)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1916774)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:13:47,061	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:13:47,515	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:13:48,006	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ade9105eae159092.zip' (11.86MiB) to Ray cluster...
2024-04-07 11:13:48,043	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ade9105eae159092.zip'.
INFO flwr 2024-04-07 11:13:59,051 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 55766318284.0, 'node:__internal_head__': 1.0, 'memory': 120121409332.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 11:13:59,051 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:13:59,051 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:13:59,073 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:13:59,074 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:13:59,074 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:13:59,074 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:14:02,770 | server.py:94 | initial parameters (loss, other metrics): 2.30629301071167, {'accuracy': 0.0896, 'data_size': 10000}
INFO flwr 2024-04-07 11:14:02,771 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:14:02,771 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1922495)[0m 2024-04-07 11:14:04.755057: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1922495)[0m 2024-04-07 11:14:04.856438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1922495)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1922491)[0m 2024-04-07 11:14:07.136704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1922494)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1922494)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1922494)[0m 2024-04-07 11:14:05.387217: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1922494)[0m 2024-04-07 11:14:05.482764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1922494)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1922485)[0m 2024-04-07 11:14:07.774278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:14:19,759 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:14:19,803 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:14:21,131 | server.py:125 | fit progress: (1, 2.2225377559661865, {'accuracy': 0.4788, 'data_size': 10000}, 18.359609698993154)
INFO flwr 2024-04-07 11:14:21,131 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:14:21,131 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:14:29,417 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:14:30,931 | server.py:125 | fit progress: (2, 2.069537878036499, {'accuracy': 0.5882, 'data_size': 10000}, 28.15954143399722)
INFO flwr 2024-04-07 11:14:30,931 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:14:30,931 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:14:38,389 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:14:39,813 | server.py:125 | fit progress: (3, 1.9781193733215332, {'accuracy': 0.6984, 'data_size': 10000}, 37.04197756497888)
INFO flwr 2024-04-07 11:14:39,813 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:14:39,814 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:14:47,523 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:14:48,908 | server.py:125 | fit progress: (4, 1.9014335870742798, {'accuracy': 0.7678, 'data_size': 10000}, 46.13723075899179)
INFO flwr 2024-04-07 11:14:48,909 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:14:48,909 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:14:57,468 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:14:58,725 | server.py:125 | fit progress: (5, 1.8616756200790405, {'accuracy': 0.7344, 'data_size': 10000}, 55.95393100299407)
INFO flwr 2024-04-07 11:14:58,725 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:14:58,726 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:15:06,887 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:15:08,365 | server.py:125 | fit progress: (6, 1.80702543258667, {'accuracy': 0.8193, 'data_size': 10000}, 65.59354204498231)
INFO flwr 2024-04-07 11:15:08,365 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:15:08,365 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:15:16,273 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:15:17,627 | server.py:125 | fit progress: (7, 1.776807188987732, {'accuracy': 0.8337, 'data_size': 10000}, 74.85597641699133)
INFO flwr 2024-04-07 11:15:17,627 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:15:17,628 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:15:25,502 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:15:26,892 | server.py:125 | fit progress: (8, 1.7497923374176025, {'accuracy': 0.8414, 'data_size': 10000}, 84.12117543397471)
INFO flwr 2024-04-07 11:15:26,893 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:15:26,893 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:15:34,999 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:15:36,652 | server.py:125 | fit progress: (9, 1.7357951402664185, {'accuracy': 0.8368, 'data_size': 10000}, 93.88072907199967)
INFO flwr 2024-04-07 11:15:36,652 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:15:36,652 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:15:44,705 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:15:46,363 | server.py:125 | fit progress: (10, 1.7183644771575928, {'accuracy': 0.8595, 'data_size': 10000}, 103.59186170299654)
INFO flwr 2024-04-07 11:15:46,363 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:15:46,363 | server.py:153 | FL finished in 103.5922695369809
INFO flwr 2024-04-07 11:15:46,364 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:15:46,364 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:15:46,364 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:15:46,364 | app.py:229 | app_fit: losses_centralized [(0, 2.30629301071167), (1, 2.2225377559661865), (2, 2.069537878036499), (3, 1.9781193733215332), (4, 1.9014335870742798), (5, 1.8616756200790405), (6, 1.80702543258667), (7, 1.776807188987732), (8, 1.7497923374176025), (9, 1.7357951402664185), (10, 1.7183644771575928)]
INFO flwr 2024-04-07 11:15:46,364 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0896), (1, 0.4788), (2, 0.5882), (3, 0.6984), (4, 0.7678), (5, 0.7344), (6, 0.8193), (7, 0.8337), (8, 0.8414), (9, 0.8368), (10, 0.8595)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8595
wandb:     loss 1.71836
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_111341-uiuhlnqr
wandb: Find logs at: ./wandb/offline-run-20240407_111341-uiuhlnqr/logs
INFO flwr 2024-04-07 11:15:49,895 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:23:09,375 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1922484)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1922484)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:23:33,312	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-07 11:23:33,314	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-07 11:23:33,314	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/tmp/ray/session_2024-04-07_11-23-11_847859_1357177/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-04-07 11:23:33,315	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-04-07 11:23:33,463	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:23:45,758	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:23:46,135	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4a76cf59ff804633.zip' (11.88MiB) to Ray cluster...
2024-04-07 11:23:46,174	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4a76cf59ff804633.zip'.
INFO flwr 2024-04-07 11:23:57,081 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 59279957606.0, 'node:10.20.240.18': 1.0, 'memory': 128319901082.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 11:23:57,081 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:23:57,081 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:23:57,097 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:23:57,098 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:23:57,098 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:23:57,098 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:23:59,389 | server.py:94 | initial parameters (loss, other metrics): 2.301058769226074, {'accuracy': 0.1481, 'data_size': 10000}
INFO flwr 2024-04-07 11:23:59,390 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:23:59,390 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1927850)[0m 2024-04-07 11:24:08.567771: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1927850)[0m 2024-04-07 11:24:08.693624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1927850)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1927850)[0m 2024-04-07 11:24:48.943875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=1927844)[0m 2024-04-07 11:24:08.570081: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1927844)[0m 2024-04-07 11:24:08.704093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1927844)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1927846)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1927846)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1927844)[0m 2024-04-07 11:24:48.944462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:26:35,289 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:26:35,331 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:26:36,563 | server.py:125 | fit progress: (1, 2.178330421447754, {'accuracy': 0.4131, 'data_size': 10000}, 157.17355566300103)
INFO flwr 2024-04-07 11:26:36,564 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:26:36,564 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:26:44,831 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:26:46,172 | server.py:125 | fit progress: (2, 2.0223004817962646, {'accuracy': 0.6095, 'data_size': 10000}, 166.78240996101522)
INFO flwr 2024-04-07 11:26:46,173 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:26:46,173 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:26:54,057 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:26:55,477 | server.py:125 | fit progress: (3, 1.9144021272659302, {'accuracy': 0.7018, 'data_size': 10000}, 176.08708558802027)
INFO flwr 2024-04-07 11:26:55,477 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:26:55,477 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:02,780 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:04,155 | server.py:125 | fit progress: (4, 1.8454984426498413, {'accuracy': 0.7594, 'data_size': 10000}, 184.7651929600106)
INFO flwr 2024-04-07 11:27:04,155 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:04,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:11,573 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:12,999 | server.py:125 | fit progress: (5, 1.819437861442566, {'accuracy': 0.7828, 'data_size': 10000}, 193.60871888001566)
INFO flwr 2024-04-07 11:27:12,999 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:12,999 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:20,344 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:21,566 | server.py:125 | fit progress: (6, 1.78242826461792, {'accuracy': 0.8077, 'data_size': 10000}, 202.1757688600046)
INFO flwr 2024-04-07 11:27:21,566 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:21,566 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:29,151 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:30,413 | server.py:125 | fit progress: (7, 1.753967046737671, {'accuracy': 0.8171, 'data_size': 10000}, 211.0230115769955)
INFO flwr 2024-04-07 11:27:30,413 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:30,413 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:38,222 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:39,519 | server.py:125 | fit progress: (8, 1.7391996383666992, {'accuracy': 0.8252, 'data_size': 10000}, 220.12908866201178)
INFO flwr 2024-04-07 11:27:39,519 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:39,519 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:47,570 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:49,200 | server.py:125 | fit progress: (9, 1.7264957427978516, {'accuracy': 0.8316, 'data_size': 10000}, 229.81022971100174)
INFO flwr 2024-04-07 11:27:49,200 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:27:49,201 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:27:56,750 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:27:58,333 | server.py:125 | fit progress: (10, 1.7075921297073364, {'accuracy': 0.8475, 'data_size': 10000}, 238.94272118600202)
INFO flwr 2024-04-07 11:27:58,333 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:27:58,333 | server.py:153 | FL finished in 238.9431317000126
INFO flwr 2024-04-07 11:27:58,333 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:27:58,333 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:27:58,333 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:27:58,334 | app.py:229 | app_fit: losses_centralized [(0, 2.301058769226074), (1, 2.178330421447754), (2, 2.0223004817962646), (3, 1.9144021272659302), (4, 1.8454984426498413), (5, 1.819437861442566), (6, 1.78242826461792), (7, 1.753967046737671), (8, 1.7391996383666992), (9, 1.7264957427978516), (10, 1.7075921297073364)]
INFO flwr 2024-04-07 11:27:58,334 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1481), (1, 0.4131), (2, 0.6095), (3, 0.7018), (4, 0.7594), (5, 0.7828), (6, 0.8077), (7, 0.8171), (8, 0.8252), (9, 0.8316), (10, 0.8475)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8475
wandb:     loss 1.70759
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_112305-sz3owyo7
wandb: Find logs at: ./wandb/offline-run-20240407_112305-sz3owyo7/logs
INFO flwr 2024-04-07 11:28:01,860 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:35:08,309 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1927851)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1927851)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:35:16,159	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:35:16,837	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:35:17,210	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b728713dafe08281.zip' (11.89MiB) to Ray cluster...
2024-04-07 11:35:17,240	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b728713dafe08281.zip'.
INFO flwr 2024-04-07 11:35:28,278 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 128076134605.0, 'CPU': 64.0, 'object_store_memory': 59175486259.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 11:35:28,278 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:35:28,279 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:35:28,297 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:35:28,298 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:35:28,299 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:35:28,299 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:35:30,437 | server.py:94 | initial parameters (loss, other metrics): 2.2983670234680176, {'accuracy': 0.1205, 'data_size': 10000}
INFO flwr 2024-04-07 11:35:30,437 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:35:30,438 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1932534)[0m 2024-04-07 11:35:34.665001: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1932537)[0m 2024-04-07 11:35:34.747083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1932537)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1932534)[0m 2024-04-07 11:35:37.348872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1932534)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1932534)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1932540)[0m 2024-04-07 11:35:34.701711: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1932540)[0m 2024-04-07 11:35:34.799935: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1932540)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1932539)[0m 2024-04-07 11:35:37.348882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:35:51,535 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:35:51,590 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:35:52,627 | server.py:125 | fit progress: (1, 2.1566853523254395, {'accuracy': 0.4074, 'data_size': 10000}, 22.189250385999912)
INFO flwr 2024-04-07 11:35:52,627 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:35:52,628 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:01,061 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:02,364 | server.py:125 | fit progress: (2, 1.9910240173339844, {'accuracy': 0.6858, 'data_size': 10000}, 31.926535487989895)
INFO flwr 2024-04-07 11:36:02,365 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:02,365 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:09,960 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:11,301 | server.py:125 | fit progress: (3, 1.8790955543518066, {'accuracy': 0.7437, 'data_size': 10000}, 40.86279380400083)
INFO flwr 2024-04-07 11:36:11,301 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:11,301 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:19,015 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:20,486 | server.py:125 | fit progress: (4, 1.8277093172073364, {'accuracy': 0.7562, 'data_size': 10000}, 50.04861826598062)
INFO flwr 2024-04-07 11:36:20,487 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:20,487 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:28,539 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:29,792 | server.py:125 | fit progress: (5, 1.7836189270019531, {'accuracy': 0.7981, 'data_size': 10000}, 59.353784926002845)
INFO flwr 2024-04-07 11:36:29,792 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:29,792 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:38,063 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:39,292 | server.py:125 | fit progress: (6, 1.7538347244262695, {'accuracy': 0.8114, 'data_size': 10000}, 68.85394603799796)
INFO flwr 2024-04-07 11:36:39,292 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:39,292 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:47,127 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:48,620 | server.py:125 | fit progress: (7, 1.7308896780014038, {'accuracy': 0.8434, 'data_size': 10000}, 78.18222968900227)
INFO flwr 2024-04-07 11:36:48,620 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:48,620 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:36:56,192 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:36:57,694 | server.py:125 | fit progress: (8, 1.7003746032714844, {'accuracy': 0.8567, 'data_size': 10000}, 87.25576019199798)
INFO flwr 2024-04-07 11:36:57,694 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:36:57,694 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:37:05,571 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:37:07,149 | server.py:125 | fit progress: (9, 1.6914732456207275, {'accuracy': 0.8603, 'data_size': 10000}, 96.71081168300589)
INFO flwr 2024-04-07 11:37:07,149 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:37:07,149 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:37:15,300 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:37:16,889 | server.py:125 | fit progress: (10, 1.6774611473083496, {'accuracy': 0.8735, 'data_size': 10000}, 106.45167303198832)
INFO flwr 2024-04-07 11:37:16,890 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:37:16,890 | server.py:153 | FL finished in 106.45209428598173
INFO flwr 2024-04-07 11:37:16,890 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:37:16,890 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:37:16,890 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:37:16,890 | app.py:229 | app_fit: losses_centralized [(0, 2.2983670234680176), (1, 2.1566853523254395), (2, 1.9910240173339844), (3, 1.8790955543518066), (4, 1.8277093172073364), (5, 1.7836189270019531), (6, 1.7538347244262695), (7, 1.7308896780014038), (8, 1.7003746032714844), (9, 1.6914732456207275), (10, 1.6774611473083496)]
INFO flwr 2024-04-07 11:37:16,891 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1205), (1, 0.4074), (2, 0.6858), (3, 0.7437), (4, 0.7562), (5, 0.7981), (6, 0.8114), (7, 0.8434), (8, 0.8567), (9, 0.8603), (10, 0.8735)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8735
wandb:     loss 1.67746
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_113507-nj00xzwe
wandb: Find logs at: ./wandb/offline-run-20240407_113507-nj00xzwe/logs
INFO flwr 2024-04-07 11:37:20,480 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:44:27,017 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1932539)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1932539)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:44:37,054	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:44:37,689	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:44:38,098	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d874976642ba8e4b.zip' (11.90MiB) to Ray cluster...
2024-04-07 11:44:38,120	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d874976642ba8e4b.zip'.
INFO flwr 2024-04-07 11:44:48,936 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 59138114764.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 127988934452.0}
INFO flwr 2024-04-07 11:44:48,937 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:44:48,937 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:44:48,953 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:44:48,953 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:44:48,953 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:44:48,954 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:44:50,978 | server.py:94 | initial parameters (loss, other metrics): 2.302539825439453, {'accuracy': 0.1459, 'data_size': 10000}
INFO flwr 2024-04-07 11:44:50,978 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:44:50,979 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1936998)[0m 2024-04-07 11:44:55.202255: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1936998)[0m 2024-04-07 11:44:55.332719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1936998)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1936998)[0m 2024-04-07 11:44:58.146309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1936998)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1936998)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1936999)[0m 2024-04-07 11:44:55.329384: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1936999)[0m 2024-04-07 11:44:55.428796: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1936999)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1936999)[0m 2024-04-07 11:44:58.146309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:45:13,214 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:45:13,250 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:45:14,510 | server.py:125 | fit progress: (1, 2.115077257156372, {'accuracy': 0.5181, 'data_size': 10000}, 23.531837716000155)
INFO flwr 2024-04-07 11:45:14,511 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:14,511 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:45:22,971 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:45:24,319 | server.py:125 | fit progress: (2, 1.9766497611999512, {'accuracy': 0.6133, 'data_size': 10000}, 33.34052099697874)
INFO flwr 2024-04-07 11:45:24,319 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:24,320 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:45:32,045 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:45:33,199 | server.py:125 | fit progress: (3, 1.8783549070358276, {'accuracy': 0.6763, 'data_size': 10000}, 42.22071129299002)
INFO flwr 2024-04-07 11:45:33,200 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:33,200 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:45:40,334 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:45:41,702 | server.py:125 | fit progress: (4, 1.802075982093811, {'accuracy': 0.7919, 'data_size': 10000}, 50.72292729100445)
INFO flwr 2024-04-07 11:45:41,702 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:41,702 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:45:49,505 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:45:50,691 | server.py:125 | fit progress: (5, 1.7632511854171753, {'accuracy': 0.8171, 'data_size': 10000}, 59.71279719698941)
INFO flwr 2024-04-07 11:45:50,692 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:50,692 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:45:58,600 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:45:59,844 | server.py:125 | fit progress: (6, 1.7298189401626587, {'accuracy': 0.8536, 'data_size': 10000}, 68.86489396297839)
INFO flwr 2024-04-07 11:45:59,844 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:45:59,844 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:46:07,387 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:46:08,881 | server.py:125 | fit progress: (7, 1.7169685363769531, {'accuracy': 0.8442, 'data_size': 10000}, 77.9025744269893)
INFO flwr 2024-04-07 11:46:08,881 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:46:08,882 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:46:16,849 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:46:18,379 | server.py:125 | fit progress: (8, 1.6898523569107056, {'accuracy': 0.8649, 'data_size': 10000}, 87.40087339800084)
INFO flwr 2024-04-07 11:46:18,380 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:46:18,380 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:46:26,080 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:46:27,599 | server.py:125 | fit progress: (9, 1.6714733839035034, {'accuracy': 0.874, 'data_size': 10000}, 96.62026320298901)
INFO flwr 2024-04-07 11:46:27,599 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:46:27,599 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:46:35,394 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:46:36,752 | server.py:125 | fit progress: (10, 1.6695599555969238, {'accuracy': 0.8728, 'data_size': 10000}, 105.77355435298523)
INFO flwr 2024-04-07 11:46:36,752 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:46:36,753 | server.py:153 | FL finished in 105.77394866000395
INFO flwr 2024-04-07 11:46:36,753 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:46:36,753 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:46:36,753 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:46:36,753 | app.py:229 | app_fit: losses_centralized [(0, 2.302539825439453), (1, 2.115077257156372), (2, 1.9766497611999512), (3, 1.8783549070358276), (4, 1.802075982093811), (5, 1.7632511854171753), (6, 1.7298189401626587), (7, 1.7169685363769531), (8, 1.6898523569107056), (9, 1.6714733839035034), (10, 1.6695599555969238)]
INFO flwr 2024-04-07 11:46:36,753 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1459), (1, 0.5181), (2, 0.6133), (3, 0.6763), (4, 0.7919), (5, 0.8171), (6, 0.8536), (7, 0.8442), (8, 0.8649), (9, 0.874), (10, 0.8728)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8728
wandb:     loss 1.66956
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_114426-pisy7rqq
wandb: Find logs at: ./wandb/offline-run-20240407_114426-pisy7rqq/logs
INFO flwr 2024-04-07 11:46:40,326 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 11:53:46,133 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1936999)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1936999)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 11:53:52,106	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 11:53:52,460	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 11:53:52,827	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f562094c230113a7.zip' (11.91MiB) to Ray cluster...
2024-04-07 11:53:52,869	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f562094c230113a7.zip'.
INFO flwr 2024-04-07 11:54:03,781 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 127401333351.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58886285721.0}
INFO flwr 2024-04-07 11:54:03,782 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 11:54:03,782 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 11:54:03,801 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 11:54:03,802 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 11:54:03,802 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 11:54:03,803 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 11:54:07,544 | server.py:94 | initial parameters (loss, other metrics): 2.3054745197296143, {'accuracy': 0.0666, 'data_size': 10000}
INFO flwr 2024-04-07 11:54:07,545 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 11:54:07,547 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1941321)[0m 2024-04-07 11:54:09.273928: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1941321)[0m 2024-04-07 11:54:09.373683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1941321)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1941321)[0m 2024-04-07 11:54:11.762445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1941323)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1941323)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1941316)[0m 2024-04-07 11:54:10.318824: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1941316)[0m 2024-04-07 11:54:10.416594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1941316)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1941316)[0m 2024-04-07 11:54:12.295408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 11:54:24,249 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 11:54:24,306 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 11:54:25,582 | server.py:125 | fit progress: (1, 2.3031792640686035, {'accuracy': 0.1004, 'data_size': 10000}, 18.03540238601272)
INFO flwr 2024-04-07 11:54:25,582 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 11:54:25,583 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:54:34,219 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 11:54:35,338 | server.py:125 | fit progress: (2, 2.3012073040008545, {'accuracy': 0.1269, 'data_size': 10000}, 27.791152788995532)
INFO flwr 2024-04-07 11:54:35,338 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 11:54:35,338 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:54:43,052 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 11:54:44,402 | server.py:125 | fit progress: (3, 2.298046588897705, {'accuracy': 0.1637, 'data_size': 10000}, 36.854940835008165)
INFO flwr 2024-04-07 11:54:44,402 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 11:54:44,402 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:54:51,797 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 11:54:53,158 | server.py:125 | fit progress: (4, 2.295243978500366, {'accuracy': 0.1903, 'data_size': 10000}, 45.61170818598475)
INFO flwr 2024-04-07 11:54:53,159 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 11:54:53,159 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:01,145 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:02,539 | server.py:125 | fit progress: (5, 2.292917013168335, {'accuracy': 0.2122, 'data_size': 10000}, 54.99260519599193)
INFO flwr 2024-04-07 11:55:02,540 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 11:55:02,540 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:10,566 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:11,758 | server.py:125 | fit progress: (6, 2.288679838180542, {'accuracy': 0.2085, 'data_size': 10000}, 64.211530347995)
INFO flwr 2024-04-07 11:55:11,758 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 11:55:11,759 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:19,587 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:21,076 | server.py:125 | fit progress: (7, 2.2851197719573975, {'accuracy': 0.2443, 'data_size': 10000}, 73.5293012780021)
INFO flwr 2024-04-07 11:55:21,076 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 11:55:21,076 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:29,541 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:30,789 | server.py:125 | fit progress: (8, 2.281067132949829, {'accuracy': 0.261, 'data_size': 10000}, 83.24225720198592)
INFO flwr 2024-04-07 11:55:30,789 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 11:55:30,789 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:38,990 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:40,314 | server.py:125 | fit progress: (9, 2.276223659515381, {'accuracy': 0.2435, 'data_size': 10000}, 92.76778616799857)
INFO flwr 2024-04-07 11:55:40,315 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 11:55:40,315 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 11:55:48,535 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 11:55:50,097 | server.py:125 | fit progress: (10, 2.2724485397338867, {'accuracy': 0.2746, 'data_size': 10000}, 102.5499152590055)
INFO flwr 2024-04-07 11:55:50,097 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 11:55:50,097 | server.py:153 | FL finished in 102.55026341800112
INFO flwr 2024-04-07 11:55:50,097 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 11:55:50,097 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 11:55:50,097 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 11:55:50,097 | app.py:229 | app_fit: losses_centralized [(0, 2.3054745197296143), (1, 2.3031792640686035), (2, 2.3012073040008545), (3, 2.298046588897705), (4, 2.295243978500366), (5, 2.292917013168335), (6, 2.288679838180542), (7, 2.2851197719573975), (8, 2.281067132949829), (9, 2.276223659515381), (10, 2.2724485397338867)]
INFO flwr 2024-04-07 11:55:50,097 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0666), (1, 0.1004), (2, 0.1269), (3, 0.1637), (4, 0.1903), (5, 0.2122), (6, 0.2085), (7, 0.2443), (8, 0.261), (9, 0.2435), (10, 0.2746)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2746
wandb:     loss 2.27245
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_115345-4baezfk7
wandb: Find logs at: ./wandb/offline-run-20240407_115345-4baezfk7/logs
INFO flwr 2024-04-07 11:55:53,643 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:02:59,756 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1941313)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1941313)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:03:05,444	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:03:05,848	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:03:06,248	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c081c448980dbf69.zip' (11.92MiB) to Ray cluster...
2024-04-07 12:03:06,290	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c081c448980dbf69.zip'.
INFO flwr 2024-04-07 12:03:17,217 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 127056197018.0, 'object_store_memory': 58738370150.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 12:03:17,217 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:03:17,217 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:03:17,231 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:03:17,231 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:03:17,231 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:03:17,232 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:03:19,872 | server.py:94 | initial parameters (loss, other metrics): 2.2977893352508545, {'accuracy': 0.1196, 'data_size': 10000}
INFO flwr 2024-04-07 12:03:19,873 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:03:19,874 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1945911)[0m 2024-04-07 12:03:23.310102: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1945906)[0m 2024-04-07 12:03:23.340664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1945906)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1945906)[0m 2024-04-07 12:03:25.350552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1945912)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1945912)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1945908)[0m 2024-04-07 12:03:23.579037: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1945908)[0m 2024-04-07 12:03:23.678980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1945908)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1945913)[0m 2024-04-07 12:03:25.825205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:03:37,837 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:03:37,879 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:03:39,198 | server.py:125 | fit progress: (1, 2.1475284099578857, {'accuracy': 0.4552, 'data_size': 10000}, 19.324439894000534)
INFO flwr 2024-04-07 12:03:39,199 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:03:39,199 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:03:48,065 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:03:49,234 | server.py:125 | fit progress: (2, 2.00072979927063, {'accuracy': 0.532, 'data_size': 10000}, 29.359829241002444)
INFO flwr 2024-04-07 12:03:49,234 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:03:49,234 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:03:57,282 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:03:58,386 | server.py:125 | fit progress: (3, 1.909966230392456, {'accuracy': 0.6389, 'data_size': 10000}, 38.51179807499284)
INFO flwr 2024-04-07 12:03:58,386 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:03:58,386 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:06,507 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:07,724 | server.py:125 | fit progress: (4, 1.8404431343078613, {'accuracy': 0.761, 'data_size': 10000}, 47.85031957901083)
INFO flwr 2024-04-07 12:04:07,725 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:07,725 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:15,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:17,137 | server.py:125 | fit progress: (5, 1.7975863218307495, {'accuracy': 0.7563, 'data_size': 10000}, 57.26322500099195)
INFO flwr 2024-04-07 12:04:17,137 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:17,138 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:25,176 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:26,651 | server.py:125 | fit progress: (6, 1.7698270082473755, {'accuracy': 0.7736, 'data_size': 10000}, 66.77691266901093)
INFO flwr 2024-04-07 12:04:26,651 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:26,651 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:34,567 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:35,805 | server.py:125 | fit progress: (7, 1.7212486267089844, {'accuracy': 0.858, 'data_size': 10000}, 75.93074078299105)
INFO flwr 2024-04-07 12:04:35,805 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:35,805 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:43,761 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:45,287 | server.py:125 | fit progress: (8, 1.7026722431182861, {'accuracy': 0.8603, 'data_size': 10000}, 85.41265243000817)
INFO flwr 2024-04-07 12:04:45,287 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:45,287 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:04:53,444 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:04:55,012 | server.py:125 | fit progress: (9, 1.6894632577896118, {'accuracy': 0.8686, 'data_size': 10000}, 95.13837738800794)
INFO flwr 2024-04-07 12:04:55,013 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:04:55,013 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:05:03,085 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:05:04,646 | server.py:125 | fit progress: (10, 1.6748212575912476, {'accuracy': 0.8761, 'data_size': 10000}, 104.77205968598719)
INFO flwr 2024-04-07 12:05:04,646 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:05:04,646 | server.py:153 | FL finished in 104.77245115098776
INFO flwr 2024-04-07 12:05:04,647 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:05:04,647 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:05:04,647 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:05:04,647 | app.py:229 | app_fit: losses_centralized [(0, 2.2977893352508545), (1, 2.1475284099578857), (2, 2.00072979927063), (3, 1.909966230392456), (4, 1.8404431343078613), (5, 1.7975863218307495), (6, 1.7698270082473755), (7, 1.7212486267089844), (8, 1.7026722431182861), (9, 1.6894632577896118), (10, 1.6748212575912476)]
INFO flwr 2024-04-07 12:05:04,647 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1196), (1, 0.4552), (2, 0.532), (3, 0.6389), (4, 0.761), (5, 0.7563), (6, 0.7736), (7, 0.858), (8, 0.8603), (9, 0.8686), (10, 0.8761)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8761
wandb:     loss 1.67482
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_120259-5n73cylj
wandb: Find logs at: ./wandb/offline-run-20240407_120259-5n73cylj/logs
INFO flwr 2024-04-07 12:05:08,227 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:12:14,773 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1945906)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1945906)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:12:21,373	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:12:21,854	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:12:22,212	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ec31d144c763730a.zip' (11.93MiB) to Ray cluster...
2024-04-07 12:12:22,242	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ec31d144c763730a.zip'.
INFO flwr 2024-04-07 12:12:33,117 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 126224711885.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58382019379.0}
INFO flwr 2024-04-07 12:12:33,118 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:12:33,118 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:12:33,139 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:12:33,141 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:12:33,141 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:12:33,141 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:12:35,563 | server.py:94 | initial parameters (loss, other metrics): 2.302748918533325, {'accuracy': 0.0903, 'data_size': 10000}
INFO flwr 2024-04-07 12:12:35,564 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:12:35,564 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1950216)[0m 2024-04-07 12:12:39.797386: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1950214)[0m 2024-04-07 12:12:39.832978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1950214)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1950216)[0m 2024-04-07 12:12:42.287944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1950216)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1950216)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1950213)[0m 2024-04-07 12:12:39.864919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1950209)[0m 2024-04-07 12:12:39.901078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1950209)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1950209)[0m 2024-04-07 12:12:42.335914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:12:54,622 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:12:54,664 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:12:56,216 | server.py:125 | fit progress: (1, 2.069476842880249, {'accuracy': 0.451, 'data_size': 10000}, 20.652175297000213)
INFO flwr 2024-04-07 12:12:56,216 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:12:56,217 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:06,672 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:08,019 | server.py:125 | fit progress: (2, 1.884883999824524, {'accuracy': 0.6959, 'data_size': 10000}, 32.455374530982226)
INFO flwr 2024-04-07 12:13:08,020 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:08,020 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:16,086 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:17,466 | server.py:125 | fit progress: (3, 1.7844570875167847, {'accuracy': 0.7791, 'data_size': 10000}, 41.90238338598283)
INFO flwr 2024-04-07 12:13:17,467 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:17,467 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:25,332 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:26,509 | server.py:125 | fit progress: (4, 1.7150734663009644, {'accuracy': 0.8531, 'data_size': 10000}, 50.94476479900186)
INFO flwr 2024-04-07 12:13:26,509 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:26,509 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:34,777 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:36,191 | server.py:125 | fit progress: (5, 1.6871886253356934, {'accuracy': 0.8718, 'data_size': 10000}, 60.62722642798326)
INFO flwr 2024-04-07 12:13:36,191 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:36,192 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:44,070 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:45,515 | server.py:125 | fit progress: (6, 1.6714668273925781, {'accuracy': 0.8714, 'data_size': 10000}, 69.95141484600026)
INFO flwr 2024-04-07 12:13:45,516 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:45,516 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:13:53,414 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:13:54,940 | server.py:125 | fit progress: (7, 1.65361487865448, {'accuracy': 0.8792, 'data_size': 10000}, 79.37574994598981)
INFO flwr 2024-04-07 12:13:54,940 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:13:54,940 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:14:02,751 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:14:04,068 | server.py:125 | fit progress: (8, 1.6437039375305176, {'accuracy': 0.8841, 'data_size': 10000}, 88.5041149109893)
INFO flwr 2024-04-07 12:14:04,068 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:14:04,068 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:14:11,967 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:14:13,506 | server.py:125 | fit progress: (9, 1.6483668088912964, {'accuracy': 0.8744, 'data_size': 10000}, 97.94189181999536)
INFO flwr 2024-04-07 12:14:13,506 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:14:13,506 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:14:21,827 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:14:23,435 | server.py:125 | fit progress: (10, 1.6380606889724731, {'accuracy': 0.8802, 'data_size': 10000}, 107.87116605200572)
INFO flwr 2024-04-07 12:14:23,435 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:14:23,435 | server.py:153 | FL finished in 107.87163827801123
INFO flwr 2024-04-07 12:14:23,436 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:14:23,436 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:14:23,436 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:14:23,436 | app.py:229 | app_fit: losses_centralized [(0, 2.302748918533325), (1, 2.069476842880249), (2, 1.884883999824524), (3, 1.7844570875167847), (4, 1.7150734663009644), (5, 1.6871886253356934), (6, 1.6714668273925781), (7, 1.65361487865448), (8, 1.6437039375305176), (9, 1.6483668088912964), (10, 1.6380606889724731)]
INFO flwr 2024-04-07 12:14:23,436 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0903), (1, 0.451), (2, 0.6959), (3, 0.7791), (4, 0.8531), (5, 0.8718), (6, 0.8714), (7, 0.8792), (8, 0.8841), (9, 0.8744), (10, 0.8802)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8802
wandb:     loss 1.63806
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_121214-dkvzp1nu
wandb: Find logs at: ./wandb/offline-run-20240407_121214-dkvzp1nu/logs
INFO flwr 2024-04-07 12:14:26,956 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:21:32,948 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1950218)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1950218)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:21:37,669	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:21:38,082	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:21:38,414	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_15f214c90f780241.zip' (11.95MiB) to Ray cluster...
2024-04-07 12:21:38,443	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_15f214c90f780241.zip'.
INFO flwr 2024-04-07 12:21:49,527 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 58330157875.0, 'memory': 126103701709.0}
INFO flwr 2024-04-07 12:21:49,527 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:21:49,527 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:21:49,541 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:21:49,542 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:21:49,542 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:21:49,542 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:21:52,001 | server.py:94 | initial parameters (loss, other metrics): 2.3050355911254883, {'accuracy': 0.0425, 'data_size': 10000}
INFO flwr 2024-04-07 12:21:52,001 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:21:52,002 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1954528)[0m 2024-04-07 12:21:55.382660: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1954528)[0m 2024-04-07 12:21:55.477507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1954528)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1954528)[0m 2024-04-07 12:21:57.597016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1954528)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1954528)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1954534)[0m 2024-04-07 12:21:56.299263: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1954534)[0m 2024-04-07 12:21:56.395689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1954534)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1954534)[0m 2024-04-07 12:21:58.723862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:22:10,989 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:22:11,035 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:22:12,351 | server.py:125 | fit progress: (1, 2.0583930015563965, {'accuracy': 0.4603, 'data_size': 10000}, 20.349880800989922)
INFO flwr 2024-04-07 12:22:12,352 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:22:12,352 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:22:21,243 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:22:22,551 | server.py:125 | fit progress: (2, 1.8553155660629272, {'accuracy': 0.6897, 'data_size': 10000}, 30.549182204995304)
INFO flwr 2024-04-07 12:22:22,551 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:22:22,552 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:22:32,402 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:22:34,025 | server.py:125 | fit progress: (3, 1.7690659761428833, {'accuracy': 0.7748, 'data_size': 10000}, 42.0231393299764)
INFO flwr 2024-04-07 12:22:34,025 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:22:34,025 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:22:43,470 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:22:44,939 | server.py:125 | fit progress: (4, 1.706017017364502, {'accuracy': 0.8388, 'data_size': 10000}, 52.937518600025214)
INFO flwr 2024-04-07 12:22:44,939 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:22:44,939 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:22:53,125 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:22:54,571 | server.py:125 | fit progress: (5, 1.6715072393417358, {'accuracy': 0.8626, 'data_size': 10000}, 62.56931604701094)
INFO flwr 2024-04-07 12:22:54,571 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:22:54,571 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:23:03,024 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:23:04,288 | server.py:125 | fit progress: (6, 1.6659770011901855, {'accuracy': 0.8602, 'data_size': 10000}, 72.28706389898434)
INFO flwr 2024-04-07 12:23:04,289 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:23:04,289 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:23:12,588 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:23:14,142 | server.py:125 | fit progress: (7, 1.6408807039260864, {'accuracy': 0.8782, 'data_size': 10000}, 82.14091937901685)
INFO flwr 2024-04-07 12:23:14,143 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:23:14,143 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:23:22,651 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:23:24,209 | server.py:125 | fit progress: (8, 1.6266849040985107, {'accuracy': 0.8885, 'data_size': 10000}, 92.20712440798525)
INFO flwr 2024-04-07 12:23:24,209 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:23:24,209 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:23:32,281 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:23:33,824 | server.py:125 | fit progress: (9, 1.621859073638916, {'accuracy': 0.8927, 'data_size': 10000}, 101.82231693499489)
INFO flwr 2024-04-07 12:23:33,824 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:23:33,824 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:23:42,047 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:23:43,403 | server.py:125 | fit progress: (10, 1.6165320873260498, {'accuracy': 0.8947, 'data_size': 10000}, 111.40152755501913)
INFO flwr 2024-04-07 12:23:43,403 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:23:43,403 | server.py:153 | FL finished in 111.40196306002326
INFO flwr 2024-04-07 12:23:43,404 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:23:43,404 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:23:43,404 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:23:43,404 | app.py:229 | app_fit: losses_centralized [(0, 2.3050355911254883), (1, 2.0583930015563965), (2, 1.8553155660629272), (3, 1.7690659761428833), (4, 1.706017017364502), (5, 1.6715072393417358), (6, 1.6659770011901855), (7, 1.6408807039260864), (8, 1.6266849040985107), (9, 1.621859073638916), (10, 1.6165320873260498)]
INFO flwr 2024-04-07 12:23:43,404 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0425), (1, 0.4603), (2, 0.6897), (3, 0.7748), (4, 0.8388), (5, 0.8626), (6, 0.8602), (7, 0.8782), (8, 0.8885), (9, 0.8927), (10, 0.8947)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8947
wandb:     loss 1.61653
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_122132-5t9x3k2b
wandb: Find logs at: ./wandb/offline-run-20240407_122132-5t9x3k2b/logs
INFO flwr 2024-04-07 12:23:46,999 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:30:52,859 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1954534)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1954534)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:30:59,572	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:31:00,016	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:31:00,443	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5af9072e17eef2e2.zip' (11.96MiB) to Ray cluster...
2024-04-07 12:31:00,475	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5af9072e17eef2e2.zip'.
INFO flwr 2024-04-07 12:31:11,335 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 125628965069.0, 'object_store_memory': 58126699315.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 12:31:11,335 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:31:11,335 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:31:11,350 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:31:11,351 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:31:11,351 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:31:11,351 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:31:15,524 | server.py:94 | initial parameters (loss, other metrics): 2.3066885471343994, {'accuracy': 0.0505, 'data_size': 10000}
INFO flwr 2024-04-07 12:31:15,525 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:31:15,525 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1958521)[0m 2024-04-07 12:31:16.574295: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1958521)[0m 2024-04-07 12:31:16.686261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1958521)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1958521)[0m 2024-04-07 12:31:19.124169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1958517)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1958517)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1958517)[0m 2024-04-07 12:31:17.524723: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1958517)[0m 2024-04-07 12:31:17.635676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1958517)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1958519)[0m 2024-04-07 12:31:19.877388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:31:31,436 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:31:31,465 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:31:32,703 | server.py:125 | fit progress: (1, 2.0811145305633545, {'accuracy': 0.3907, 'data_size': 10000}, 17.177783485036343)
INFO flwr 2024-04-07 12:31:32,703 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:31:32,703 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:31:41,447 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:31:42,792 | server.py:125 | fit progress: (2, 1.868760585784912, {'accuracy': 0.668, 'data_size': 10000}, 27.267393059039023)
INFO flwr 2024-04-07 12:31:42,793 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:31:42,793 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:31:50,482 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:31:51,905 | server.py:125 | fit progress: (3, 1.7737699747085571, {'accuracy': 0.7276, 'data_size': 10000}, 36.38018291402841)
INFO flwr 2024-04-07 12:31:51,905 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:31:51,906 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:31:59,570 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:00,937 | server.py:125 | fit progress: (4, 1.6958822011947632, {'accuracy': 0.8369, 'data_size': 10000}, 45.41193421400385)
INFO flwr 2024-04-07 12:32:00,937 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:00,937 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:09,000 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:10,201 | server.py:125 | fit progress: (5, 1.6458748579025269, {'accuracy': 0.8882, 'data_size': 10000}, 54.67576914705569)
INFO flwr 2024-04-07 12:32:10,201 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:10,201 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:18,004 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:19,436 | server.py:125 | fit progress: (6, 1.6385515928268433, {'accuracy': 0.8847, 'data_size': 10000}, 63.91146371100331)
INFO flwr 2024-04-07 12:32:19,437 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:19,437 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:27,387 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:28,668 | server.py:125 | fit progress: (7, 1.637636423110962, {'accuracy': 0.875, 'data_size': 10000}, 73.1431043370394)
INFO flwr 2024-04-07 12:32:28,668 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:28,669 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:36,541 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:37,835 | server.py:125 | fit progress: (8, 1.6225942373275757, {'accuracy': 0.8875, 'data_size': 10000}, 82.31026645604288)
INFO flwr 2024-04-07 12:32:37,836 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:37,836 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:45,685 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:47,251 | server.py:125 | fit progress: (9, 1.6136667728424072, {'accuracy': 0.8955, 'data_size': 10000}, 91.7260063640424)
INFO flwr 2024-04-07 12:32:47,251 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:32:47,251 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:32:55,277 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:32:56,886 | server.py:125 | fit progress: (10, 1.6119333505630493, {'accuracy': 0.8928, 'data_size': 10000}, 101.36073393502738)
INFO flwr 2024-04-07 12:32:56,886 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:32:56,886 | server.py:153 | FL finished in 101.36116652702913
INFO flwr 2024-04-07 12:32:56,886 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:32:56,886 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:32:56,887 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:32:56,887 | app.py:229 | app_fit: losses_centralized [(0, 2.3066885471343994), (1, 2.0811145305633545), (2, 1.868760585784912), (3, 1.7737699747085571), (4, 1.6958822011947632), (5, 1.6458748579025269), (6, 1.6385515928268433), (7, 1.637636423110962), (8, 1.6225942373275757), (9, 1.6136667728424072), (10, 1.6119333505630493)]
INFO flwr 2024-04-07 12:32:56,887 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0505), (1, 0.3907), (2, 0.668), (3, 0.7276), (4, 0.8369), (5, 0.8882), (6, 0.8847), (7, 0.875), (8, 0.8875), (9, 0.8955), (10, 0.8928)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8928
wandb:     loss 1.61193
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_123052-7yz9rsxt
wandb: Find logs at: ./wandb/offline-run-20240407_123052-7yz9rsxt/logs
INFO flwr 2024-04-07 12:33:00,456 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:40:06,385 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1958511)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1958511)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:40:12,646	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:40:13,008	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:40:13,430	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c52431dc496a876d.zip' (11.97MiB) to Ray cluster...
2024-04-07 12:40:13,457	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c52431dc496a876d.zip'.
INFO flwr 2024-04-07 12:40:24,368 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 125046989415.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57877281177.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 12:40:24,368 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:40:24,368 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:40:24,392 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:40:24,394 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:40:24,394 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:40:24,394 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:40:27,172 | server.py:94 | initial parameters (loss, other metrics): 2.3050174713134766, {'accuracy': 0.0858, 'data_size': 10000}
INFO flwr 2024-04-07 12:40:27,173 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:40:27,173 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1962830)[0m 2024-04-07 12:40:30.562691: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1962830)[0m 2024-04-07 12:40:30.655318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1962830)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1962830)[0m 2024-04-07 12:40:32.934261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1962826)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1962826)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1962833)[0m 2024-04-07 12:40:30.784827: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1962833)[0m 2024-04-07 12:40:30.880009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1962833)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1962826)[0m 2024-04-07 12:40:33.123938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:40:46,471 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:40:46,506 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:40:47,776 | server.py:125 | fit progress: (1, 2.037698268890381, {'accuracy': 0.4607, 'data_size': 10000}, 20.602640964032616)
INFO flwr 2024-04-07 12:40:47,776 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:40:47,776 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:40:56,102 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:40:57,426 | server.py:125 | fit progress: (2, 1.8007042407989502, {'accuracy': 0.7551, 'data_size': 10000}, 30.253208914014976)
INFO flwr 2024-04-07 12:40:57,426 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:40:57,427 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:05,505 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:06,873 | server.py:125 | fit progress: (3, 1.7014025449752808, {'accuracy': 0.8399, 'data_size': 10000}, 39.699974546034355)
INFO flwr 2024-04-07 12:41:06,873 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:06,873 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:14,852 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:16,243 | server.py:125 | fit progress: (4, 1.6865323781967163, {'accuracy': 0.8299, 'data_size': 10000}, 49.07045947102597)
INFO flwr 2024-04-07 12:41:16,244 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:16,244 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:23,972 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:25,398 | server.py:125 | fit progress: (5, 1.6493042707443237, {'accuracy': 0.8706, 'data_size': 10000}, 58.22516428702511)
INFO flwr 2024-04-07 12:41:25,398 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:25,399 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:33,413 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:34,654 | server.py:125 | fit progress: (6, 1.6223646402359009, {'accuracy': 0.8906, 'data_size': 10000}, 67.4813269310398)
INFO flwr 2024-04-07 12:41:34,654 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:34,655 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:42,722 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:44,010 | server.py:125 | fit progress: (7, 1.6133344173431396, {'accuracy': 0.8945, 'data_size': 10000}, 76.83685963402968)
INFO flwr 2024-04-07 12:41:44,010 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:44,010 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:41:52,134 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:41:53,455 | server.py:125 | fit progress: (8, 1.6072876453399658, {'accuracy': 0.8995, 'data_size': 10000}, 86.28236633201595)
INFO flwr 2024-04-07 12:41:53,456 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:41:53,456 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:42:01,384 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:42:02,938 | server.py:125 | fit progress: (9, 1.6019549369812012, {'accuracy': 0.8991, 'data_size': 10000}, 95.76489851099905)
INFO flwr 2024-04-07 12:42:02,938 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:42:02,938 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:42:10,875 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:42:12,512 | server.py:125 | fit progress: (10, 1.5984840393066406, {'accuracy': 0.9004, 'data_size': 10000}, 105.33860886702314)
INFO flwr 2024-04-07 12:42:12,512 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:42:12,512 | server.py:153 | FL finished in 105.33904503803933
INFO flwr 2024-04-07 12:42:12,512 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:42:12,512 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:42:12,512 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:42:12,513 | app.py:229 | app_fit: losses_centralized [(0, 2.3050174713134766), (1, 2.037698268890381), (2, 1.8007042407989502), (3, 1.7014025449752808), (4, 1.6865323781967163), (5, 1.6493042707443237), (6, 1.6223646402359009), (7, 1.6133344173431396), (8, 1.6072876453399658), (9, 1.6019549369812012), (10, 1.5984840393066406)]
INFO flwr 2024-04-07 12:42:12,513 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0858), (1, 0.4607), (2, 0.7551), (3, 0.8399), (4, 0.8299), (5, 0.8706), (6, 0.8906), (7, 0.8945), (8, 0.8995), (9, 0.8991), (10, 0.9004)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9004
wandb:     loss 1.59848
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_124005-pc4cpvz8
wandb: Find logs at: ./wandb/offline-run-20240407_124005-pc4cpvz8/logs
INFO flwr 2024-04-07 12:42:16,057 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:49:21,991 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1962833)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1962833)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:49:30,431	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:49:30,838	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:49:31,162	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9ccc6b52e6801568.zip' (11.98MiB) to Ray cluster...
2024-04-07 12:49:31,193	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9ccc6b52e6801568.zip'.
INFO flwr 2024-04-07 12:49:42,175 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 124583885005.0, 'object_store_memory': 57678807859.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 12:49:42,176 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:49:42,176 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:49:42,190 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:49:42,191 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:49:42,191 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:49:42,191 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:49:44,418 | server.py:94 | initial parameters (loss, other metrics): 2.299117088317871, {'accuracy': 0.1409, 'data_size': 10000}
INFO flwr 2024-04-07 12:49:44,419 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:49:44,419 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1967133)[0m 2024-04-07 12:49:48.777069: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1967133)[0m 2024-04-07 12:49:48.877187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1967133)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1967133)[0m 2024-04-07 12:49:52.468047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1967139)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1967139)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1967134)[0m 2024-04-07 12:49:48.953654: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1967137)[0m 2024-04-07 12:49:48.962720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1967137)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1967136)[0m 2024-04-07 12:49:52.457988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:50:13,764 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:50:13,808 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:50:15,097 | server.py:125 | fit progress: (1, 1.934574007987976, {'accuracy': 0.6351, 'data_size': 10000}, 30.677580024988856)
INFO flwr 2024-04-07 12:50:15,097 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:50:15,097 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:50:23,612 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:50:25,206 | server.py:125 | fit progress: (2, 1.7343679666519165, {'accuracy': 0.8067, 'data_size': 10000}, 40.78683480800828)
INFO flwr 2024-04-07 12:50:25,206 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:50:25,206 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:50:33,003 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:50:34,371 | server.py:125 | fit progress: (3, 1.6600724458694458, {'accuracy': 0.8695, 'data_size': 10000}, 49.95206545299152)
INFO flwr 2024-04-07 12:50:34,371 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:50:34,371 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:50:42,215 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:50:43,391 | server.py:125 | fit progress: (4, 1.634142279624939, {'accuracy': 0.8862, 'data_size': 10000}, 58.972455897019245)
INFO flwr 2024-04-07 12:50:43,392 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:50:43,392 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:50:51,403 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 12:50:52,813 | server.py:125 | fit progress: (5, 1.6251554489135742, {'accuracy': 0.8857, 'data_size': 10000}, 68.39444644004107)
INFO flwr 2024-04-07 12:50:52,814 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 12:50:52,814 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:51:00,628 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 12:51:02,071 | server.py:125 | fit progress: (6, 1.6141436100006104, {'accuracy': 0.887, 'data_size': 10000}, 77.65196961199399)
INFO flwr 2024-04-07 12:51:02,071 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 12:51:02,071 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:51:10,257 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 12:51:11,799 | server.py:125 | fit progress: (7, 1.6197577714920044, {'accuracy': 0.8826, 'data_size': 10000}, 87.38031975901686)
INFO flwr 2024-04-07 12:51:11,799 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 12:51:11,800 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:51:19,825 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 12:51:21,155 | server.py:125 | fit progress: (8, 1.5990973711013794, {'accuracy': 0.8995, 'data_size': 10000}, 96.73636463203002)
INFO flwr 2024-04-07 12:51:21,156 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 12:51:21,156 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:51:29,320 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 12:51:30,876 | server.py:125 | fit progress: (9, 1.5963265895843506, {'accuracy': 0.8979, 'data_size': 10000}, 106.45701076602563)
INFO flwr 2024-04-07 12:51:30,876 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 12:51:30,876 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:51:38,504 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 12:51:40,084 | server.py:125 | fit progress: (10, 1.6009116172790527, {'accuracy': 0.8941, 'data_size': 10000}, 115.6655441900366)
INFO flwr 2024-04-07 12:51:40,085 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 12:51:40,085 | server.py:153 | FL finished in 115.66611742600799
INFO flwr 2024-04-07 12:51:40,085 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 12:51:40,085 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 12:51:40,085 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 12:51:40,086 | app.py:229 | app_fit: losses_centralized [(0, 2.299117088317871), (1, 1.934574007987976), (2, 1.7343679666519165), (3, 1.6600724458694458), (4, 1.634142279624939), (5, 1.6251554489135742), (6, 1.6141436100006104), (7, 1.6197577714920044), (8, 1.5990973711013794), (9, 1.5963265895843506), (10, 1.6009116172790527)]
INFO flwr 2024-04-07 12:51:40,086 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1409), (1, 0.6351), (2, 0.8067), (3, 0.8695), (4, 0.8862), (5, 0.8857), (6, 0.887), (7, 0.8826), (8, 0.8995), (9, 0.8979), (10, 0.8941)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8941
wandb:     loss 1.60091
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_124921-dycu8r6j
wandb: Find logs at: ./wandb/offline-run-20240407_124921-dycu8r6j/logs
INFO flwr 2024-04-07 12:51:43,616 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 12:58:49,634 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1967136)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1967136)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 12:58:54,709	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 12:58:55,158	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 12:58:55,491	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_60346c4c8c065035.zip' (11.99MiB) to Ray cluster...
2024-04-07 12:58:55,523	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_60346c4c8c065035.zip'.
INFO flwr 2024-04-07 12:59:06,560 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57433452134.0, 'memory': 124011388314.0, 'CPU': 64.0}
INFO flwr 2024-04-07 12:59:06,560 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 12:59:06,561 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 12:59:06,579 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 12:59:06,580 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 12:59:06,580 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 12:59:06,581 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 12:59:08,869 | server.py:94 | initial parameters (loss, other metrics): 2.3044211864471436, {'accuracy': 0.1039, 'data_size': 10000}
INFO flwr 2024-04-07 12:59:08,869 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 12:59:08,870 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1971699)[0m 2024-04-07 12:59:12.727939: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1971698)[0m 2024-04-07 12:59:12.832961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1971698)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1971704)[0m 2024-04-07 12:59:14.855502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1971700)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1971700)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1971696)[0m 2024-04-07 12:59:12.867056: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1971694)[0m 2024-04-07 12:59:12.972806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1971694)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1971694)[0m 2024-04-07 12:59:15.471249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 12:59:28,099 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 12:59:28,140 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 12:59:29,209 | server.py:125 | fit progress: (1, 2.2993578910827637, {'accuracy': 0.1363, 'data_size': 10000}, 20.339590762974694)
INFO flwr 2024-04-07 12:59:29,209 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 12:59:29,210 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:59:38,385 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 12:59:39,723 | server.py:125 | fit progress: (2, 2.2942991256713867, {'accuracy': 0.1891, 'data_size': 10000}, 30.853289152961224)
INFO flwr 2024-04-07 12:59:39,723 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 12:59:39,723 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:59:47,774 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 12:59:49,129 | server.py:125 | fit progress: (3, 2.287050485610962, {'accuracy': 0.2377, 'data_size': 10000}, 40.2598261529929)
INFO flwr 2024-04-07 12:59:49,130 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 12:59:49,130 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 12:59:57,885 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 12:59:59,295 | server.py:125 | fit progress: (4, 2.2814812660217285, {'accuracy': 0.3065, 'data_size': 10000}, 50.42591511196224)
INFO flwr 2024-04-07 12:59:59,296 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 12:59:59,296 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:07,371 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:08,573 | server.py:125 | fit progress: (5, 2.2758102416992188, {'accuracy': 0.3555, 'data_size': 10000}, 59.703163536963984)
INFO flwr 2024-04-07 13:00:08,573 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:00:08,573 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:17,151 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:18,589 | server.py:125 | fit progress: (6, 2.2664012908935547, {'accuracy': 0.3607, 'data_size': 10000}, 69.719950847968)
INFO flwr 2024-04-07 13:00:18,590 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:00:18,590 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:26,877 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:28,332 | server.py:125 | fit progress: (7, 2.2586188316345215, {'accuracy': 0.389, 'data_size': 10000}, 79.46276634000242)
INFO flwr 2024-04-07 13:00:28,332 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:00:28,333 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:37,059 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:38,607 | server.py:125 | fit progress: (8, 2.2501091957092285, {'accuracy': 0.403, 'data_size': 10000}, 89.73747881496092)
INFO flwr 2024-04-07 13:00:38,607 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:00:38,607 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:46,847 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:48,146 | server.py:125 | fit progress: (9, 2.242257833480835, {'accuracy': 0.426, 'data_size': 10000}, 99.27612523798598)
INFO flwr 2024-04-07 13:00:48,146 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:00:48,146 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:00:56,414 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:00:57,765 | server.py:125 | fit progress: (10, 2.230546712875366, {'accuracy': 0.4435, 'data_size': 10000}, 108.89569289295468)
INFO flwr 2024-04-07 13:00:57,765 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:00:57,766 | server.py:153 | FL finished in 108.89610595896374
INFO flwr 2024-04-07 13:00:57,766 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:00:57,766 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:00:57,766 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:00:57,766 | app.py:229 | app_fit: losses_centralized [(0, 2.3044211864471436), (1, 2.2993578910827637), (2, 2.2942991256713867), (3, 2.287050485610962), (4, 2.2814812660217285), (5, 2.2758102416992188), (6, 2.2664012908935547), (7, 2.2586188316345215), (8, 2.2501091957092285), (9, 2.242257833480835), (10, 2.230546712875366)]
INFO flwr 2024-04-07 13:00:57,766 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1039), (1, 0.1363), (2, 0.1891), (3, 0.2377), (4, 0.3065), (5, 0.3555), (6, 0.3607), (7, 0.389), (8, 0.403), (9, 0.426), (10, 0.4435)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4435
wandb:     loss 2.23055
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_125849-2anvm49y
wandb: Find logs at: ./wandb/offline-run-20240407_125849-2anvm49y/logs
INFO flwr 2024-04-07 13:01:01,348 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:08:08,186 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1971694)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1971694)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 13:08:13,187	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:08:13,633	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:08:14,099	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_6b0b1b58e76c5124.zip' (12.01MiB) to Ray cluster...
2024-04-07 13:08:14,126	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_6b0b1b58e76c5124.zip'.
INFO flwr 2024-04-07 13:08:25,137 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 123452754535.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 57194037657.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 13:08:25,137 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:08:25,137 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:08:25,151 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:08:25,153 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:08:25,153 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:08:25,153 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:08:28,476 | server.py:94 | initial parameters (loss, other metrics): 2.3049814701080322, {'accuracy': 0.0688, 'data_size': 10000}
INFO flwr 2024-04-07 13:08:28,476 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:08:28,477 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1976007)[0m 2024-04-07 13:08:30.712884: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1976007)[0m 2024-04-07 13:08:30.802676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1976007)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1976007)[0m 2024-04-07 13:08:33.041289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1976004)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1976004)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1976003)[0m 2024-04-07 13:08:31.385567: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1976006)[0m 2024-04-07 13:08:31.530126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1976006)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1975999)[0m 2024-04-07 13:08:33.590208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:08:46,231 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:08:46,270 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:08:47,568 | server.py:125 | fit progress: (1, 2.0909886360168457, {'accuracy': 0.4722, 'data_size': 10000}, 19.090997224033345)
INFO flwr 2024-04-07 13:08:47,568 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:08:47,568 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:08:56,271 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:08:57,589 | server.py:125 | fit progress: (2, 1.8854132890701294, {'accuracy': 0.7265, 'data_size': 10000}, 29.112097741046455)
INFO flwr 2024-04-07 13:08:57,589 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:08:57,589 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:05,638 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:06,738 | server.py:125 | fit progress: (3, 1.778102159500122, {'accuracy': 0.8066, 'data_size': 10000}, 38.26144481502706)
INFO flwr 2024-04-07 13:09:06,738 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:06,738 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:15,072 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:16,564 | server.py:125 | fit progress: (4, 1.7227474451065063, {'accuracy': 0.8391, 'data_size': 10000}, 48.08733618300175)
INFO flwr 2024-04-07 13:09:16,564 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:16,564 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:24,741 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:26,258 | server.py:125 | fit progress: (5, 1.6838568449020386, {'accuracy': 0.8644, 'data_size': 10000}, 57.78179016604554)
INFO flwr 2024-04-07 13:09:26,259 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:26,259 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:34,485 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:36,084 | server.py:125 | fit progress: (6, 1.6742221117019653, {'accuracy': 0.8698, 'data_size': 10000}, 67.60771137900883)
INFO flwr 2024-04-07 13:09:36,084 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:36,085 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:44,746 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:46,088 | server.py:125 | fit progress: (7, 1.6519759893417358, {'accuracy': 0.8801, 'data_size': 10000}, 77.61113167001167)
INFO flwr 2024-04-07 13:09:46,088 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:46,088 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:09:54,167 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:09:55,815 | server.py:125 | fit progress: (8, 1.6435819864273071, {'accuracy': 0.8822, 'data_size': 10000}, 87.33873433299595)
INFO flwr 2024-04-07 13:09:55,816 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:09:55,816 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:10:04,111 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:10:05,480 | server.py:125 | fit progress: (9, 1.6353867053985596, {'accuracy': 0.8853, 'data_size': 10000}, 97.00394200900337)
INFO flwr 2024-04-07 13:10:05,481 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:10:05,481 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:10:13,686 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:10:15,066 | server.py:125 | fit progress: (10, 1.639601230621338, {'accuracy': 0.88, 'data_size': 10000}, 106.5890275700367)
INFO flwr 2024-04-07 13:10:15,066 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:10:15,066 | server.py:153 | FL finished in 106.58944302302552
INFO flwr 2024-04-07 13:10:15,066 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:10:15,066 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:10:15,066 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:10:15,066 | app.py:229 | app_fit: losses_centralized [(0, 2.3049814701080322), (1, 2.0909886360168457), (2, 1.8854132890701294), (3, 1.778102159500122), (4, 1.7227474451065063), (5, 1.6838568449020386), (6, 1.6742221117019653), (7, 1.6519759893417358), (8, 1.6435819864273071), (9, 1.6353867053985596), (10, 1.639601230621338)]
INFO flwr 2024-04-07 13:10:15,066 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0688), (1, 0.4722), (2, 0.7265), (3, 0.8066), (4, 0.8391), (5, 0.8644), (6, 0.8698), (7, 0.8801), (8, 0.8822), (9, 0.8853), (10, 0.88)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.88
wandb:     loss 1.6396
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_130807-gpvzlx84
wandb: Find logs at: ./wandb/offline-run-20240407_130807-gpvzlx84/logs
INFO flwr 2024-04-07 13:10:18,718 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:17:25,251 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1975999)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1975999)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 13:17:30,064	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:17:30,450	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:17:30,811	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_4e46c0e9db93f883.zip' (12.02MiB) to Ray cluster...
2024-04-07 13:17:30,836	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_4e46c0e9db93f883.zip'.
INFO flwr 2024-04-07 13:17:41,910 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 122644694426.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56847726182.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 13:17:41,910 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:17:41,910 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:17:41,927 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:17:41,928 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:17:41,928 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:17:41,928 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:17:44,319 | server.py:94 | initial parameters (loss, other metrics): 2.299114227294922, {'accuracy': 0.1118, 'data_size': 10000}
INFO flwr 2024-04-07 13:17:44,319 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:17:44,320 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1980418)[0m 2024-04-07 13:17:48.076025: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1980418)[0m 2024-04-07 13:17:48.171425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1980418)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1980317)[0m 2024-04-07 13:17:50.244945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1980419)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1980419)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1980312)[0m 2024-04-07 13:17:48.618008: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1980312)[0m 2024-04-07 13:17:48.716160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1980312)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1980312)[0m 2024-04-07 13:17:51.278273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1980312)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1980312)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:18:03,934 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:18:03,976 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:18:05,288 | server.py:125 | fit progress: (1, 2.012406587600708, {'accuracy': 0.5165, 'data_size': 10000}, 20.969035659974907)
INFO flwr 2024-04-07 13:18:05,289 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:05,289 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:18:14,341 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:18:15,691 | server.py:125 | fit progress: (2, 1.8062493801116943, {'accuracy': 0.7798, 'data_size': 10000}, 31.371537747967523)
INFO flwr 2024-04-07 13:18:15,691 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:15,691 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:18:23,653 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:18:25,066 | server.py:125 | fit progress: (3, 1.7061996459960938, {'accuracy': 0.8561, 'data_size': 10000}, 40.74686912598554)
INFO flwr 2024-04-07 13:18:25,067 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:25,067 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:18:33,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:18:34,921 | server.py:125 | fit progress: (4, 1.6639741659164429, {'accuracy': 0.8758, 'data_size': 10000}, 50.60134425695287)
INFO flwr 2024-04-07 13:18:34,921 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:34,921 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:18:43,027 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:18:44,204 | server.py:125 | fit progress: (5, 1.6365488767623901, {'accuracy': 0.8877, 'data_size': 10000}, 59.88410834496608)
INFO flwr 2024-04-07 13:18:44,204 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:44,204 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:18:52,798 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:18:54,255 | server.py:125 | fit progress: (6, 1.6302247047424316, {'accuracy': 0.8863, 'data_size': 10000}, 69.93548544700025)
INFO flwr 2024-04-07 13:18:54,255 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:18:54,255 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:19:02,620 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:19:03,902 | server.py:125 | fit progress: (7, 1.6324070692062378, {'accuracy': 0.8798, 'data_size': 10000}, 79.58250085898908)
INFO flwr 2024-04-07 13:19:03,902 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:19:03,902 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:19:12,616 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:19:13,914 | server.py:125 | fit progress: (8, 1.6189035177230835, {'accuracy': 0.8883, 'data_size': 10000}, 89.5948495399789)
INFO flwr 2024-04-07 13:19:13,915 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:19:13,915 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:19:22,758 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:19:24,298 | server.py:125 | fit progress: (9, 1.6185885667800903, {'accuracy': 0.8872, 'data_size': 10000}, 99.97838909196435)
INFO flwr 2024-04-07 13:19:24,298 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:19:24,298 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:19:32,822 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:19:34,416 | server.py:125 | fit progress: (10, 1.6243689060211182, {'accuracy': 0.8771, 'data_size': 10000}, 110.09706461400492)
INFO flwr 2024-04-07 13:19:34,417 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:19:34,417 | server.py:153 | FL finished in 110.09745937096886
INFO flwr 2024-04-07 13:19:34,417 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:19:34,417 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:19:34,417 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:19:34,417 | app.py:229 | app_fit: losses_centralized [(0, 2.299114227294922), (1, 2.012406587600708), (2, 1.8062493801116943), (3, 1.7061996459960938), (4, 1.6639741659164429), (5, 1.6365488767623901), (6, 1.6302247047424316), (7, 1.6324070692062378), (8, 1.6189035177230835), (9, 1.6185885667800903), (10, 1.6243689060211182)]
INFO flwr 2024-04-07 13:19:34,418 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1118), (1, 0.5165), (2, 0.7798), (3, 0.8561), (4, 0.8758), (5, 0.8877), (6, 0.8863), (7, 0.8798), (8, 0.8883), (9, 0.8872), (10, 0.8771)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8771
wandb:     loss 1.62437
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_131724-muwyvdma
wandb: Find logs at: ./wandb/offline-run-20240407_131724-muwyvdma/logs
INFO flwr 2024-04-07 13:19:38,016 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:26:42,959 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
2024-04-07 13:26:47,769	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:26:48,236	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:26:48,689	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9cb30b2a64bddb5e.zip' (12.03MiB) to Ray cluster...
2024-04-07 13:26:48,722	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9cb30b2a64bddb5e.zip'.
INFO flwr 2024-04-07 13:26:59,529 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 122764979200.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56899276800.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 13:26:59,529 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:26:59,529 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:26:59,542 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:26:59,543 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:26:59,543 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:26:59,543 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:27:02,594 | server.py:94 | initial parameters (loss, other metrics): 2.303743839263916, {'accuracy': 0.0869, 'data_size': 10000}
INFO flwr 2024-04-07 13:27:02,597 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:27:02,598 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1984078)[0m 2024-04-07 13:27:05.268068: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1984076)[0m 2024-04-07 13:27:05.434313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1984076)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1984076)[0m 2024-04-07 13:27:07.426879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1984070)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1984070)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1984073)[0m 2024-04-07 13:27:05.886652: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1984073)[0m 2024-04-07 13:27:05.978487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1984073)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1984073)[0m 2024-04-07 13:27:08.399740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:27:20,480 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:27:20,519 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:27:21,768 | server.py:125 | fit progress: (1, 1.9565130472183228, {'accuracy': 0.6168, 'data_size': 10000}, 19.170014302013442)
INFO flwr 2024-04-07 13:27:21,769 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:27:21,769 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:27:30,667 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:27:31,979 | server.py:125 | fit progress: (2, 1.7894935607910156, {'accuracy': 0.7279, 'data_size': 10000}, 29.380497960024513)
INFO flwr 2024-04-07 13:27:31,979 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:27:31,979 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:27:40,184 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:27:41,528 | server.py:125 | fit progress: (3, 1.7054234743118286, {'accuracy': 0.8279, 'data_size': 10000}, 38.92945512098959)
INFO flwr 2024-04-07 13:27:41,528 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:27:41,528 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:27:49,922 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:27:51,381 | server.py:125 | fit progress: (4, 1.6437695026397705, {'accuracy': 0.884, 'data_size': 10000}, 48.78297093202127)
INFO flwr 2024-04-07 13:27:51,382 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:27:51,382 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:27:59,874 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:01,270 | server.py:125 | fit progress: (5, 1.6585277318954468, {'accuracy': 0.8455, 'data_size': 10000}, 58.67179113801103)
INFO flwr 2024-04-07 13:28:01,270 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:28:01,271 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:28:09,266 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:10,480 | server.py:125 | fit progress: (6, 1.6253070831298828, {'accuracy': 0.8853, 'data_size': 10000}, 67.88154381403001)
INFO flwr 2024-04-07 13:28:10,480 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:28:10,480 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:28:18,670 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:19,938 | server.py:125 | fit progress: (7, 1.6143651008605957, {'accuracy': 0.8913, 'data_size': 10000}, 77.33974810602376)
INFO flwr 2024-04-07 13:28:19,938 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:28:19,939 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:28:28,041 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:29,346 | server.py:125 | fit progress: (8, 1.606798529624939, {'accuracy': 0.8928, 'data_size': 10000}, 86.74802236200776)
INFO flwr 2024-04-07 13:28:29,347 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:28:29,347 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:28:37,367 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:38,913 | server.py:125 | fit progress: (9, 1.6060521602630615, {'accuracy': 0.8946, 'data_size': 10000}, 96.31437904201448)
INFO flwr 2024-04-07 13:28:38,913 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:28:38,913 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:28:46,822 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:28:48,164 | server.py:125 | fit progress: (10, 1.5940916538238525, {'accuracy': 0.9018, 'data_size': 10000}, 105.56539682199946)
INFO flwr 2024-04-07 13:28:48,164 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:28:48,164 | server.py:153 | FL finished in 105.56608982302714
INFO flwr 2024-04-07 13:28:48,165 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:28:48,165 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:28:48,165 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:28:48,165 | app.py:229 | app_fit: losses_centralized [(0, 2.303743839263916), (1, 1.9565130472183228), (2, 1.7894935607910156), (3, 1.7054234743118286), (4, 1.6437695026397705), (5, 1.6585277318954468), (6, 1.6253070831298828), (7, 1.6143651008605957), (8, 1.606798529624939), (9, 1.6060521602630615), (10, 1.5940916538238525)]
INFO flwr 2024-04-07 13:28:48,165 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0869), (1, 0.6168), (2, 0.7279), (3, 0.8279), (4, 0.884), (5, 0.8455), (6, 0.8853), (7, 0.8913), (8, 0.8928), (9, 0.8946), (10, 0.9018)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9018
wandb:     loss 1.59409
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_132642-ynumrmmg
wandb: Find logs at: ./wandb/offline-run-20240407_132642-ynumrmmg/logs
INFO flwr 2024-04-07 13:28:51,680 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:35:57,058 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1984068)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1984068)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 13:36:01,804	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:36:02,185	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:36:02,527	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2e5c09936fd20e41.zip' (12.04MiB) to Ray cluster...
2024-04-07 13:36:02,561	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2e5c09936fd20e41.zip'.
INFO flwr 2024-04-07 13:36:13,363 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 56843629363.0, 'node:10.20.240.18': 1.0, 'memory': 122635135181.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 13:36:13,365 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:36:13,366 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:36:13,385 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:36:13,387 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:36:13,387 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:36:13,387 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:36:17,102 | server.py:94 | initial parameters (loss, other metrics): 2.304412364959717, {'accuracy': 0.1169, 'data_size': 10000}
INFO flwr 2024-04-07 13:36:17,103 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:36:17,103 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1988632)[0m 2024-04-07 13:36:18.691719: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1988632)[0m 2024-04-07 13:36:18.778228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1988632)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1988632)[0m 2024-04-07 13:36:20.995000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1988632)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1988632)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1988634)[0m 2024-04-07 13:36:19.996195: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1988634)[0m 2024-04-07 13:36:20.100397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1988634)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1988625)[0m 2024-04-07 13:36:22.255473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:36:34,462 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:36:34,503 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:36:35,812 | server.py:125 | fit progress: (1, 2.0089776515960693, {'accuracy': 0.4863, 'data_size': 10000}, 18.709014908992685)
INFO flwr 2024-04-07 13:36:35,812 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:36:35,812 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:36:44,563 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:36:45,930 | server.py:125 | fit progress: (2, 1.808763861656189, {'accuracy': 0.6861, 'data_size': 10000}, 28.826799594971817)
INFO flwr 2024-04-07 13:36:45,930 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:36:45,930 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:36:54,053 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:36:55,409 | server.py:125 | fit progress: (3, 1.6734517812728882, {'accuracy': 0.8602, 'data_size': 10000}, 38.30642827501288)
INFO flwr 2024-04-07 13:36:55,410 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:36:55,410 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:03,645 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:05,155 | server.py:125 | fit progress: (4, 1.6441086530685425, {'accuracy': 0.8657, 'data_size': 10000}, 48.052098987973295)
INFO flwr 2024-04-07 13:37:05,155 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:05,156 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:13,445 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:14,633 | server.py:125 | fit progress: (5, 1.6202467679977417, {'accuracy': 0.8911, 'data_size': 10000}, 57.52990501001477)
INFO flwr 2024-04-07 13:37:14,633 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:14,634 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:23,422 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:24,680 | server.py:125 | fit progress: (6, 1.617608904838562, {'accuracy': 0.8857, 'data_size': 10000}, 67.5767395409639)
INFO flwr 2024-04-07 13:37:24,680 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:24,680 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:33,936 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:35,196 | server.py:125 | fit progress: (7, 1.5981597900390625, {'accuracy': 0.9002, 'data_size': 10000}, 78.09318747796351)
INFO flwr 2024-04-07 13:37:35,196 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:35,197 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:43,889 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:45,473 | server.py:125 | fit progress: (8, 1.5921434164047241, {'accuracy': 0.9012, 'data_size': 10000}, 88.37044138001511)
INFO flwr 2024-04-07 13:37:45,474 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:45,474 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:37:54,437 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:37:55,828 | server.py:125 | fit progress: (9, 1.5922091007232666, {'accuracy': 0.9018, 'data_size': 10000}, 98.725123487995)
INFO flwr 2024-04-07 13:37:55,828 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:37:55,828 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:38:04,285 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:38:05,621 | server.py:125 | fit progress: (10, 1.5842481851577759, {'accuracy': 0.9061, 'data_size': 10000}, 108.51809411298018)
INFO flwr 2024-04-07 13:38:05,621 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:38:05,621 | server.py:153 | FL finished in 108.51851891499246
INFO flwr 2024-04-07 13:38:05,622 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:38:05,622 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:38:05,622 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:38:05,622 | app.py:229 | app_fit: losses_centralized [(0, 2.304412364959717), (1, 2.0089776515960693), (2, 1.808763861656189), (3, 1.6734517812728882), (4, 1.6441086530685425), (5, 1.6202467679977417), (6, 1.617608904838562), (7, 1.5981597900390625), (8, 1.5921434164047241), (9, 1.5922091007232666), (10, 1.5842481851577759)]
INFO flwr 2024-04-07 13:38:05,622 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1169), (1, 0.4863), (2, 0.6861), (3, 0.8602), (4, 0.8657), (5, 0.8911), (6, 0.8857), (7, 0.9002), (8, 0.9012), (9, 0.9018), (10, 0.9061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9061
wandb:     loss 1.58425
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_133556-0rr1xt34
wandb: Find logs at: ./wandb/offline-run-20240407_133556-0rr1xt34/logs
INFO flwr 2024-04-07 13:38:09,184 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:45:14,430 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1988623)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1988623)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 13:45:19,610	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:45:19,990	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:45:20,474	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_dd065176cbfca237.zip' (12.05MiB) to Ray cluster...
2024-04-07 13:45:20,508	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_dd065176cbfca237.zip'.
INFO flwr 2024-04-07 13:45:31,252 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 122522385408.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56795308032.0}
INFO flwr 2024-04-07 13:45:31,252 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:45:31,252 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:45:31,267 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:45:31,270 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:45:31,270 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:45:31,271 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:45:34,564 | server.py:94 | initial parameters (loss, other metrics): 2.3005549907684326, {'accuracy': 0.1038, 'data_size': 10000}
INFO flwr 2024-04-07 13:45:34,570 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:45:34,572 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1992938)[0m 2024-04-07 13:45:37.161795: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1992938)[0m 2024-04-07 13:45:37.256427: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1992938)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1992933)[0m 2024-04-07 13:45:39.338816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1992947)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1992947)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1992947)[0m 2024-04-07 13:45:37.434808: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1992947)[0m 2024-04-07 13:45:37.526159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1992947)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1992947)[0m 2024-04-07 13:45:39.757873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:45:52,321 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:45:52,356 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:45:53,642 | server.py:125 | fit progress: (1, 1.9402436017990112, {'accuracy': 0.5748, 'data_size': 10000}, 19.070959998993203)
INFO flwr 2024-04-07 13:45:53,643 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:45:53,643 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:02,467 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:03,793 | server.py:125 | fit progress: (2, 1.7756152153015137, {'accuracy': 0.7238, 'data_size': 10000}, 29.221268995956052)
INFO flwr 2024-04-07 13:46:03,793 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:03,793 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:11,750 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:12,888 | server.py:125 | fit progress: (3, 1.6442550420761108, {'accuracy': 0.8756, 'data_size': 10000}, 38.316143756965175)
INFO flwr 2024-04-07 13:46:12,888 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:12,888 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:20,495 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:21,873 | server.py:125 | fit progress: (4, 1.6293532848358154, {'accuracy': 0.878, 'data_size': 10000}, 47.301776259962935)
INFO flwr 2024-04-07 13:46:21,873 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:21,874 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:29,940 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:31,135 | server.py:125 | fit progress: (5, 1.6231251955032349, {'accuracy': 0.8763, 'data_size': 10000}, 56.56376948498655)
INFO flwr 2024-04-07 13:46:31,135 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:31,136 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:39,662 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:40,878 | server.py:125 | fit progress: (6, 1.6083717346191406, {'accuracy': 0.8901, 'data_size': 10000}, 66.30640626500826)
INFO flwr 2024-04-07 13:46:40,878 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:40,878 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:49,191 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:46:50,670 | server.py:125 | fit progress: (7, 1.593461275100708, {'accuracy': 0.901, 'data_size': 10000}, 76.09837196796434)
INFO flwr 2024-04-07 13:46:50,670 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:46:50,670 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:46:59,034 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:47:00,530 | server.py:125 | fit progress: (8, 1.6026800870895386, {'accuracy': 0.889, 'data_size': 10000}, 85.95813744200859)
INFO flwr 2024-04-07 13:47:00,530 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:47:00,530 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:47:08,753 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:47:10,277 | server.py:125 | fit progress: (9, 1.5828146934509277, {'accuracy': 0.9036, 'data_size': 10000}, 95.7053857629653)
INFO flwr 2024-04-07 13:47:10,277 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:47:10,277 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:47:18,770 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:47:20,208 | server.py:125 | fit progress: (10, 1.5841044187545776, {'accuracy': 0.9043, 'data_size': 10000}, 105.63663577398984)
INFO flwr 2024-04-07 13:47:20,208 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:47:20,208 | server.py:153 | FL finished in 105.6370754839736
INFO flwr 2024-04-07 13:47:20,209 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:47:20,209 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:47:20,209 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:47:20,209 | app.py:229 | app_fit: losses_centralized [(0, 2.3005549907684326), (1, 1.9402436017990112), (2, 1.7756152153015137), (3, 1.6442550420761108), (4, 1.6293532848358154), (5, 1.6231251955032349), (6, 1.6083717346191406), (7, 1.593461275100708), (8, 1.6026800870895386), (9, 1.5828146934509277), (10, 1.5841044187545776)]
INFO flwr 2024-04-07 13:47:20,209 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1038), (1, 0.5748), (2, 0.7238), (3, 0.8756), (4, 0.878), (5, 0.8763), (6, 0.8901), (7, 0.901), (8, 0.889), (9, 0.9036), (10, 0.9043)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9043
wandb:     loss 1.5841
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_134514-2x9pgw67
wandb: Find logs at: ./wandb/offline-run-20240407_134514-2x9pgw67/logs
INFO flwr 2024-04-07 13:47:23,743 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 13:54:28,834 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1992933)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1992933)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 13:54:34,723	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 13:54:35,047	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 13:54:35,401	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_74551ffd268c3196.zip' (12.06MiB) to Ray cluster...
2024-04-07 13:54:35,434	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_74551ffd268c3196.zip'.
INFO flwr 2024-04-07 13:54:46,209 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 122424702772.0, 'CPU': 64.0, 'object_store_memory': 56753444044.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 13:54:46,209 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 13:54:46,209 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 13:54:46,225 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 13:54:46,226 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 13:54:46,226 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 13:54:46,226 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 13:54:49,259 | server.py:94 | initial parameters (loss, other metrics): 2.3018124103546143, {'accuracy': 0.077, 'data_size': 10000}
INFO flwr 2024-04-07 13:54:49,259 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 13:54:49,260 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1997733)[0m 2024-04-07 13:54:51.778054: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1997738)[0m 2024-04-07 13:54:51.957242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1997738)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1997735)[0m 2024-04-07 13:54:54.207738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1997738)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1997738)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1997736)[0m 2024-04-07 13:54:52.778303: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1997736)[0m 2024-04-07 13:54:52.884237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1997736)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1997736)[0m 2024-04-07 13:54:55.024292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 13:55:06,782 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 13:55:06,838 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 13:55:08,116 | server.py:125 | fit progress: (1, 1.9757347106933594, {'accuracy': 0.5168, 'data_size': 10000}, 18.856488552992232)
INFO flwr 2024-04-07 13:55:08,116 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:08,116 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:55:17,065 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 13:55:18,180 | server.py:125 | fit progress: (2, 1.7716021537780762, {'accuracy': 0.704, 'data_size': 10000}, 28.920765732007567)
INFO flwr 2024-04-07 13:55:18,180 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:18,181 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:55:26,438 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 13:55:27,823 | server.py:125 | fit progress: (3, 1.6293516159057617, {'accuracy': 0.8912, 'data_size': 10000}, 38.56363861000864)
INFO flwr 2024-04-07 13:55:27,823 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:27,824 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:55:36,117 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 13:55:37,514 | server.py:125 | fit progress: (4, 1.6160837411880493, {'accuracy': 0.8926, 'data_size': 10000}, 48.25505525799235)
INFO flwr 2024-04-07 13:55:37,515 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:37,515 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:55:45,695 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 13:55:47,147 | server.py:125 | fit progress: (5, 1.5972322225570679, {'accuracy': 0.901, 'data_size': 10000}, 57.88741580903297)
INFO flwr 2024-04-07 13:55:47,147 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:47,147 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:55:55,508 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 13:55:56,722 | server.py:125 | fit progress: (6, 1.5950818061828613, {'accuracy': 0.8994, 'data_size': 10000}, 67.46276183601003)
INFO flwr 2024-04-07 13:55:56,722 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 13:55:56,723 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:56:05,393 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 13:56:07,143 | server.py:125 | fit progress: (7, 1.5878301858901978, {'accuracy': 0.9038, 'data_size': 10000}, 77.8837460600189)
INFO flwr 2024-04-07 13:56:07,143 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 13:56:07,144 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:56:15,218 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 13:56:16,508 | server.py:125 | fit progress: (8, 1.5829881429672241, {'accuracy': 0.9079, 'data_size': 10000}, 87.24834992102114)
INFO flwr 2024-04-07 13:56:16,508 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 13:56:16,508 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:56:25,345 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 13:56:26,684 | server.py:125 | fit progress: (9, 1.598920226097107, {'accuracy': 0.8891, 'data_size': 10000}, 97.42506204498932)
INFO flwr 2024-04-07 13:56:26,685 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 13:56:26,685 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 13:56:34,808 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 13:56:36,381 | server.py:125 | fit progress: (10, 1.5856215953826904, {'accuracy': 0.9017, 'data_size': 10000}, 107.1211817850126)
INFO flwr 2024-04-07 13:56:36,381 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 13:56:36,381 | server.py:153 | FL finished in 107.12166019598953
INFO flwr 2024-04-07 13:56:36,381 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 13:56:36,381 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 13:56:36,381 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 13:56:36,382 | app.py:229 | app_fit: losses_centralized [(0, 2.3018124103546143), (1, 1.9757347106933594), (2, 1.7716021537780762), (3, 1.6293516159057617), (4, 1.6160837411880493), (5, 1.5972322225570679), (6, 1.5950818061828613), (7, 1.5878301858901978), (8, 1.5829881429672241), (9, 1.598920226097107), (10, 1.5856215953826904)]
INFO flwr 2024-04-07 13:56:36,382 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.077), (1, 0.5168), (2, 0.704), (3, 0.8912), (4, 0.8926), (5, 0.901), (6, 0.8994), (7, 0.9038), (8, 0.9079), (9, 0.8891), (10, 0.9017)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9017
wandb:     loss 1.58562
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_135428-hvwyhebe
wandb: Find logs at: ./wandb/offline-run-20240407_135428-hvwyhebe/logs
INFO flwr 2024-04-07 13:56:39,960 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:03:45,267 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=1997729)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1997729)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:03:51,132	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:03:51,458	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:03:51,837	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_e223e9001fa7123f.zip' (12.08MiB) to Ray cluster...
2024-04-07 14:03:51,870	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_e223e9001fa7123f.zip'.
INFO flwr 2024-04-07 14:04:02,681 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 56684244172.0, 'node:10.20.240.18': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 122263236404.0}
INFO flwr 2024-04-07 14:04:02,681 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:04:02,681 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:04:02,698 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:04:02,699 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:04:02,700 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:04:02,700 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:04:06,697 | server.py:94 | initial parameters (loss, other metrics): 2.302661657333374, {'accuracy': 0.0757, 'data_size': 10000}
INFO flwr 2024-04-07 14:04:06,697 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:04:06,697 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2002311)[0m 2024-04-07 14:04:08.510352: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2002311)[0m 2024-04-07 14:04:08.610072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2002311)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2002311)[0m 2024-04-07 14:04:10.696383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2002313)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2002313)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2002318)[0m 2024-04-07 14:04:08.970990: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2002318)[0m 2024-04-07 14:04:09.071929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2002318)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2002318)[0m 2024-04-07 14:04:10.933098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 14:04:24,269 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:04:24,304 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:04:25,571 | server.py:125 | fit progress: (1, 2.292637825012207, {'accuracy': 0.1419, 'data_size': 10000}, 18.873376550036483)
INFO flwr 2024-04-07 14:04:25,571 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:04:25,571 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:04:34,964 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:04:36,086 | server.py:125 | fit progress: (2, 2.2784786224365234, {'accuracy': 0.2473, 'data_size': 10000}, 29.38836674002232)
INFO flwr 2024-04-07 14:04:36,086 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:04:36,086 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:04:44,497 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:04:45,629 | server.py:125 | fit progress: (3, 2.2559003829956055, {'accuracy': 0.2985, 'data_size': 10000}, 38.93199829000514)
INFO flwr 2024-04-07 14:04:45,630 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:04:45,630 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:04:54,285 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:04:55,488 | server.py:125 | fit progress: (4, 2.237041711807251, {'accuracy': 0.228, 'data_size': 10000}, 48.79089848703006)
INFO flwr 2024-04-07 14:04:55,489 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:04:55,489 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:04,287 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:05,679 | server.py:125 | fit progress: (5, 2.2184669971466064, {'accuracy': 0.247, 'data_size': 10000}, 58.98130929004401)
INFO flwr 2024-04-07 14:05:05,679 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:05:05,679 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:14,415 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:15,887 | server.py:125 | fit progress: (6, 2.1996519565582275, {'accuracy': 0.385, 'data_size': 10000}, 69.18993992800824)
INFO flwr 2024-04-07 14:05:15,887 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:05:15,888 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:25,088 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:26,339 | server.py:125 | fit progress: (7, 2.177086353302002, {'accuracy': 0.4513, 'data_size': 10000}, 79.64207807602361)
INFO flwr 2024-04-07 14:05:26,340 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:05:26,340 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:35,068 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:36,559 | server.py:125 | fit progress: (8, 2.154292106628418, {'accuracy': 0.5341, 'data_size': 10000}, 89.86155125900405)
INFO flwr 2024-04-07 14:05:36,559 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:05:36,559 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:45,740 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:47,312 | server.py:125 | fit progress: (9, 2.129409074783325, {'accuracy': 0.5809, 'data_size': 10000}, 100.61498144798679)
INFO flwr 2024-04-07 14:05:47,313 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:05:47,313 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:05:56,326 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:05:57,948 | server.py:125 | fit progress: (10, 2.108983278274536, {'accuracy': 0.6061, 'data_size': 10000}, 111.25024164898787)
INFO flwr 2024-04-07 14:05:57,948 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:05:57,948 | server.py:153 | FL finished in 111.25071594299516
INFO flwr 2024-04-07 14:05:57,948 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:05:57,948 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:05:57,948 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:05:57,949 | app.py:229 | app_fit: losses_centralized [(0, 2.302661657333374), (1, 2.292637825012207), (2, 2.2784786224365234), (3, 2.2559003829956055), (4, 2.237041711807251), (5, 2.2184669971466064), (6, 2.1996519565582275), (7, 2.177086353302002), (8, 2.154292106628418), (9, 2.129409074783325), (10, 2.108983278274536)]
INFO flwr 2024-04-07 14:05:57,949 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0757), (1, 0.1419), (2, 0.2473), (3, 0.2985), (4, 0.228), (5, 0.247), (6, 0.385), (7, 0.4513), (8, 0.5341), (9, 0.5809), (10, 0.6061)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6061
wandb:     loss 2.10898
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_140344-x6he8583
wandb: Find logs at: ./wandb/offline-run-20240407_140344-x6he8583/logs
INFO flwr 2024-04-07 14:06:01,482 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:13:07,612 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2002307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2002307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:13:13,490	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:13:13,805	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:13:14,120	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2916bf71e074a615.zip' (12.09MiB) to Ray cluster...
2024-04-07 14:13:14,153	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2916bf71e074a615.zip'.
INFO flwr 2024-04-07 14:13:25,158 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 56392335360.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 121582115840.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 14:13:25,158 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:13:25,158 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:13:25,171 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:13:25,173 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:13:25,173 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:13:25,174 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:13:27,535 | server.py:94 | initial parameters (loss, other metrics): 2.3003978729248047, {'accuracy': 0.1032, 'data_size': 10000}
INFO flwr 2024-04-07 14:13:27,535 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:13:27,536 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2006611)[0m 2024-04-07 14:13:30.998339: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2006605)[0m 2024-04-07 14:13:31.094194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2006605)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2006611)[0m 2024-04-07 14:13:33.138620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2006605)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2006605)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2006596)[0m 2024-04-07 14:13:31.535334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2006596)[0m 2024-04-07 14:13:31.629073: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2006596)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2006596)[0m 2024-04-07 14:13:33.816199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 14:13:46,510 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:13:46,542 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:13:47,853 | server.py:125 | fit progress: (1, 1.9861314296722412, {'accuracy': 0.6208, 'data_size': 10000}, 20.317658653017133)
INFO flwr 2024-04-07 14:13:47,853 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:13:47,854 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:13:57,469 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:13:58,782 | server.py:125 | fit progress: (2, 1.829479694366455, {'accuracy': 0.7131, 'data_size': 10000}, 31.246647508989554)
INFO flwr 2024-04-07 14:13:58,782 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:13:58,782 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:07,499 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:08,872 | server.py:125 | fit progress: (3, 1.702436923980713, {'accuracy': 0.8583, 'data_size': 10000}, 41.33665184903657)
INFO flwr 2024-04-07 14:14:08,872 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:08,872 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:17,591 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:18,734 | server.py:125 | fit progress: (4, 1.664141058921814, {'accuracy': 0.8719, 'data_size': 10000}, 51.198474075004924)
INFO flwr 2024-04-07 14:14:18,734 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:18,734 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:27,687 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:29,089 | server.py:125 | fit progress: (5, 1.6444240808486938, {'accuracy': 0.8809, 'data_size': 10000}, 61.553964034013916)
INFO flwr 2024-04-07 14:14:29,090 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:29,090 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:37,775 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:39,219 | server.py:125 | fit progress: (6, 1.6311568021774292, {'accuracy': 0.8844, 'data_size': 10000}, 71.68338559602853)
INFO flwr 2024-04-07 14:14:39,219 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:39,219 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:48,222 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:49,722 | server.py:125 | fit progress: (7, 1.6311336755752563, {'accuracy': 0.8828, 'data_size': 10000}, 82.18676196900196)
INFO flwr 2024-04-07 14:14:49,722 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:49,723 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:14:58,628 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:14:59,918 | server.py:125 | fit progress: (8, 1.6283478736877441, {'accuracy': 0.8786, 'data_size': 10000}, 92.38276976603083)
INFO flwr 2024-04-07 14:14:59,918 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:14:59,919 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:15:08,883 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:15:10,422 | server.py:125 | fit progress: (9, 1.6106475591659546, {'accuracy': 0.897, 'data_size': 10000}, 102.88622717099497)
INFO flwr 2024-04-07 14:15:10,422 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:15:10,422 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:15:19,495 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:15:21,058 | server.py:125 | fit progress: (10, 1.6046595573425293, {'accuracy': 0.8969, 'data_size': 10000}, 113.52255741902627)
INFO flwr 2024-04-07 14:15:21,058 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:15:21,058 | server.py:153 | FL finished in 113.52291556098498
INFO flwr 2024-04-07 14:15:21,058 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:15:21,058 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:15:21,059 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:15:21,059 | app.py:229 | app_fit: losses_centralized [(0, 2.3003978729248047), (1, 1.9861314296722412), (2, 1.829479694366455), (3, 1.702436923980713), (4, 1.664141058921814), (5, 1.6444240808486938), (6, 1.6311568021774292), (7, 1.6311336755752563), (8, 1.6283478736877441), (9, 1.6106475591659546), (10, 1.6046595573425293)]
INFO flwr 2024-04-07 14:15:21,059 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1032), (1, 0.6208), (2, 0.7131), (3, 0.8583), (4, 0.8719), (5, 0.8809), (6, 0.8844), (7, 0.8828), (8, 0.8786), (9, 0.897), (10, 0.8969)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8969
wandb:     loss 1.60466
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_141307-ml041srq
wandb: Find logs at: ./wandb/offline-run-20240407_141307-ml041srq/logs
INFO flwr 2024-04-07 14:15:24,608 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:22:29,734 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2006596)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2006596)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:22:35,569	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:22:35,914	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:22:36,309	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_aca6821871a45006.zip' (12.10MiB) to Ray cluster...
2024-04-07 14:22:36,356	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_aca6821871a45006.zip'.
INFO flwr 2024-04-07 14:22:47,357 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 56572913664.0, 'memory': 122003465216.0}
INFO flwr 2024-04-07 14:22:47,357 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:22:47,357 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:22:47,373 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:22:47,374 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:22:47,374 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:22:47,374 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:22:49,853 | server.py:94 | initial parameters (loss, other metrics): 2.301098585128784, {'accuracy': 0.1207, 'data_size': 10000}
INFO flwr 2024-04-07 14:22:49,854 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:22:49,854 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2010923)[0m 2024-04-07 14:22:53.579905: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2010929)[0m 2024-04-07 14:22:53.593148: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2010929)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2010930)[0m 2024-04-07 14:22:55.598049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2010931)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2010931)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2010931)[0m 2024-04-07 14:22:53.737837: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2010931)[0m 2024-04-07 14:22:53.838200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2010931)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2010924)[0m 2024-04-07 14:22:56.058601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2010924)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 3x across cluster][0m
[2m[36m(DefaultActor pid=2010924)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-04-07 14:23:11,066 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:23:11,105 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:23:12,390 | server.py:125 | fit progress: (1, 1.9590202569961548, {'accuracy': 0.6046, 'data_size': 10000}, 22.53608568996424)
INFO flwr 2024-04-07 14:23:12,390 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:23:12,391 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:23:21,729 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:23:22,862 | server.py:125 | fit progress: (2, 1.710903286933899, {'accuracy': 0.8427, 'data_size': 10000}, 33.00804876600159)
INFO flwr 2024-04-07 14:23:22,862 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:23:22,862 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:23:30,969 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:23:32,373 | server.py:125 | fit progress: (3, 1.685492992401123, {'accuracy': 0.835, 'data_size': 10000}, 42.518758495978545)
INFO flwr 2024-04-07 14:23:32,373 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:23:32,373 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:23:40,928 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:23:42,337 | server.py:125 | fit progress: (4, 1.635998249053955, {'accuracy': 0.8836, 'data_size': 10000}, 52.483560620981734)
INFO flwr 2024-04-07 14:23:42,338 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:23:42,338 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:23:50,937 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:23:52,328 | server.py:125 | fit progress: (5, 1.621456503868103, {'accuracy': 0.8876, 'data_size': 10000}, 62.47421771497466)
INFO flwr 2024-04-07 14:23:52,328 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:23:52,329 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:24:00,959 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:24:02,194 | server.py:125 | fit progress: (6, 1.6133477687835693, {'accuracy': 0.8902, 'data_size': 10000}, 72.33987695397809)
INFO flwr 2024-04-07 14:24:02,194 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:24:02,194 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:24:10,812 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:24:12,278 | server.py:125 | fit progress: (7, 1.619755744934082, {'accuracy': 0.8804, 'data_size': 10000}, 82.42387433996191)
INFO flwr 2024-04-07 14:24:12,278 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:24:12,278 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:24:20,526 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:24:22,082 | server.py:125 | fit progress: (8, 1.5976628065109253, {'accuracy': 0.8978, 'data_size': 10000}, 92.22775581094902)
INFO flwr 2024-04-07 14:24:22,082 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:24:22,082 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:24:30,849 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:24:32,545 | server.py:125 | fit progress: (9, 1.5950579643249512, {'accuracy': 0.8956, 'data_size': 10000}, 102.69123457197566)
INFO flwr 2024-04-07 14:24:32,545 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:24:32,546 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:24:41,158 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:24:42,517 | server.py:125 | fit progress: (10, 1.594605565071106, {'accuracy': 0.8952, 'data_size': 10000}, 112.66319021600066)
INFO flwr 2024-04-07 14:24:42,517 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:24:42,517 | server.py:153 | FL finished in 112.66362335899612
INFO flwr 2024-04-07 14:24:42,518 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:24:42,518 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:24:42,518 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:24:42,518 | app.py:229 | app_fit: losses_centralized [(0, 2.301098585128784), (1, 1.9590202569961548), (2, 1.710903286933899), (3, 1.685492992401123), (4, 1.635998249053955), (5, 1.621456503868103), (6, 1.6133477687835693), (7, 1.619755744934082), (8, 1.5976628065109253), (9, 1.5950579643249512), (10, 1.594605565071106)]
INFO flwr 2024-04-07 14:24:42,518 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1207), (1, 0.6046), (2, 0.8427), (3, 0.835), (4, 0.8836), (5, 0.8876), (6, 0.8902), (7, 0.8804), (8, 0.8978), (9, 0.8956), (10, 0.8952)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8952
wandb:     loss 1.59461
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_142229-1d3rglve
wandb: Find logs at: ./wandb/offline-run-20240407_142229-1d3rglve/logs
INFO flwr 2024-04-07 14:24:46,029 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:31:51,067 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2010923)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 4x across cluster][0m
[2m[36m(DefaultActor pid=2010923)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 4x across cluster][0m
2024-04-07 14:31:56,055	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:31:56,434	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:31:56,871	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_923a7b6548780c94.zip' (12.11MiB) to Ray cluster...
2024-04-07 14:31:56,907	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_923a7b6548780c94.zip'.
INFO flwr 2024-04-07 14:32:07,727 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.18': 1.0, 'object_store_memory': 56541684940.0, 'node:__internal_head__': 1.0, 'memory': 121930598196.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 14:32:07,728 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:32:07,728 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:32:07,744 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:32:07,745 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:32:07,746 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:32:07,746 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:32:11,583 | server.py:94 | initial parameters (loss, other metrics): 2.3031539916992188, {'accuracy': 0.0548, 'data_size': 10000}
INFO flwr 2024-04-07 14:32:11,583 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:32:11,584 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2014921)[0m 2024-04-07 14:32:13.442060: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2014921)[0m 2024-04-07 14:32:13.540388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2014921)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2014921)[0m 2024-04-07 14:32:15.628087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2014921)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2014921)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2014911)[0m 2024-04-07 14:32:14.077481: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2014911)[0m 2024-04-07 14:32:14.172154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2014911)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2014911)[0m 2024-04-07 14:32:16.504871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 14:32:30,006 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:32:30,048 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:32:31,346 | server.py:125 | fit progress: (1, 1.8727455139160156, {'accuracy': 0.6876, 'data_size': 10000}, 19.762657594052143)
INFO flwr 2024-04-07 14:32:31,347 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:32:31,347 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:32:40,907 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:32:42,307 | server.py:125 | fit progress: (2, 1.7246593236923218, {'accuracy': 0.7869, 'data_size': 10000}, 30.723168259020895)
INFO flwr 2024-04-07 14:32:42,307 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:32:42,307 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:32:51,362 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:32:52,894 | server.py:125 | fit progress: (3, 1.6383450031280518, {'accuracy': 0.8835, 'data_size': 10000}, 41.310231781040784)
INFO flwr 2024-04-07 14:32:52,894 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:32:52,895 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:01,333 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:02,735 | server.py:125 | fit progress: (4, 1.6143805980682373, {'accuracy': 0.8917, 'data_size': 10000}, 51.15090258803684)
INFO flwr 2024-04-07 14:33:02,735 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:02,735 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:11,360 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:12,544 | server.py:125 | fit progress: (5, 1.5991555452346802, {'accuracy': 0.8995, 'data_size': 10000}, 60.960094113019295)
INFO flwr 2024-04-07 14:33:12,544 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:12,544 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:21,073 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:22,516 | server.py:125 | fit progress: (6, 1.5951662063598633, {'accuracy': 0.8987, 'data_size': 10000}, 70.93175545299891)
INFO flwr 2024-04-07 14:33:22,516 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:22,516 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:31,719 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:32,976 | server.py:125 | fit progress: (7, 1.5915687084197998, {'accuracy': 0.9012, 'data_size': 10000}, 81.39264649001416)
INFO flwr 2024-04-07 14:33:32,977 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:32,977 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:41,680 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:42,981 | server.py:125 | fit progress: (8, 1.584362506866455, {'accuracy': 0.9061, 'data_size': 10000}, 91.39686286001233)
INFO flwr 2024-04-07 14:33:42,981 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:42,981 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:33:51,728 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:33:53,253 | server.py:125 | fit progress: (9, 1.580496907234192, {'accuracy': 0.9064, 'data_size': 10000}, 101.66942610504339)
INFO flwr 2024-04-07 14:33:53,253 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:33:53,254 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:34:02,501 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:34:04,059 | server.py:125 | fit progress: (10, 1.5853992700576782, {'accuracy': 0.9012, 'data_size': 10000}, 112.47501891403226)
INFO flwr 2024-04-07 14:34:04,059 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:34:04,059 | server.py:153 | FL finished in 112.47540108504472
INFO flwr 2024-04-07 14:34:04,059 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:34:04,059 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:34:04,060 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:34:04,060 | app.py:229 | app_fit: losses_centralized [(0, 2.3031539916992188), (1, 1.8727455139160156), (2, 1.7246593236923218), (3, 1.6383450031280518), (4, 1.6143805980682373), (5, 1.5991555452346802), (6, 1.5951662063598633), (7, 1.5915687084197998), (8, 1.584362506866455), (9, 1.580496907234192), (10, 1.5853992700576782)]
INFO flwr 2024-04-07 14:34:04,060 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0548), (1, 0.6876), (2, 0.7869), (3, 0.8835), (4, 0.8917), (5, 0.8995), (6, 0.8987), (7, 0.9012), (8, 0.9061), (9, 0.9064), (10, 0.9012)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9012
wandb:     loss 1.5854
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_143150-7ejnhp0c
wandb: Find logs at: ./wandb/offline-run-20240407_143150-7ejnhp0c/logs
INFO flwr 2024-04-07 14:34:07,628 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:41:12,651 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2014911)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2014911)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:41:17,370	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:41:17,844	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:41:18,248	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3387f705b27eb215.zip' (12.12MiB) to Ray cluster...
2024-04-07 14:41:18,283	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3387f705b27eb215.zip'.
INFO flwr 2024-04-07 14:41:28,704 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 135611227956.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 62404811980.0}
INFO flwr 2024-04-07 14:41:28,704 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:41:28,705 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:41:28,722 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:41:28,723 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:41:28,724 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:41:28,725 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:41:31,153 | server.py:94 | initial parameters (loss, other metrics): 2.301710844039917, {'accuracy': 0.1334, 'data_size': 10000}
INFO flwr 2024-04-07 14:41:31,154 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:41:31,154 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2019267)[0m 2024-04-07 14:41:34.390790: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2019267)[0m 2024-04-07 14:41:34.482522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2019267)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2019267)[0m 2024-04-07 14:41:36.421981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2019271)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2019271)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2019262)[0m 2024-04-07 14:41:34.949163: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2019262)[0m 2024-04-07 14:41:35.049785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2019262)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2019262)[0m 2024-04-07 14:41:37.146720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 14:41:50,762 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:41:50,801 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:41:51,834 | server.py:125 | fit progress: (1, 1.9080272912979126, {'accuracy': 0.5934, 'data_size': 10000}, 20.67982378596207)
INFO flwr 2024-04-07 14:41:51,834 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:41:51,834 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:01,214 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:02,406 | server.py:125 | fit progress: (2, 1.6769624948501587, {'accuracy': 0.8493, 'data_size': 10000}, 31.252327449969016)
INFO flwr 2024-04-07 14:42:02,407 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:02,407 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:12,449 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:13,617 | server.py:125 | fit progress: (3, 1.6241031885147095, {'accuracy': 0.8856, 'data_size': 10000}, 42.46320697595365)
INFO flwr 2024-04-07 14:42:13,617 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:13,618 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:22,490 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:23,861 | server.py:125 | fit progress: (4, 1.6027575731277466, {'accuracy': 0.8972, 'data_size': 10000}, 52.70659987197723)
INFO flwr 2024-04-07 14:42:23,861 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:23,861 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:32,298 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:33,723 | server.py:125 | fit progress: (5, 1.5963175296783447, {'accuracy': 0.8972, 'data_size': 10000}, 62.56881644099485)
INFO flwr 2024-04-07 14:42:33,723 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:33,723 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:43,133 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:44,387 | server.py:125 | fit progress: (6, 1.590725064277649, {'accuracy': 0.8993, 'data_size': 10000}, 73.23271130095236)
INFO flwr 2024-04-07 14:42:44,387 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:44,387 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:42:52,979 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:42:54,435 | server.py:125 | fit progress: (7, 1.5907047986984253, {'accuracy': 0.8982, 'data_size': 10000}, 83.2808384699747)
INFO flwr 2024-04-07 14:42:54,435 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:42:54,435 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:43:03,407 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:43:04,638 | server.py:125 | fit progress: (8, 1.5817193984985352, {'accuracy': 0.9049, 'data_size': 10000}, 93.48421206098283)
INFO flwr 2024-04-07 14:43:04,638 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:43:04,639 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:43:14,071 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:43:15,337 | server.py:125 | fit progress: (9, 1.5814224481582642, {'accuracy': 0.9022, 'data_size': 10000}, 104.18319122400135)
INFO flwr 2024-04-07 14:43:15,337 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:43:15,338 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:43:24,327 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:43:25,851 | server.py:125 | fit progress: (10, 1.587092638015747, {'accuracy': 0.8976, 'data_size': 10000}, 114.69730868696934)
INFO flwr 2024-04-07 14:43:25,851 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:43:25,852 | server.py:153 | FL finished in 114.69770165195223
INFO flwr 2024-04-07 14:43:25,852 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:43:25,852 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:43:25,852 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:43:25,852 | app.py:229 | app_fit: losses_centralized [(0, 2.301710844039917), (1, 1.9080272912979126), (2, 1.6769624948501587), (3, 1.6241031885147095), (4, 1.6027575731277466), (5, 1.5963175296783447), (6, 1.590725064277649), (7, 1.5907047986984253), (8, 1.5817193984985352), (9, 1.5814224481582642), (10, 1.587092638015747)]
INFO flwr 2024-04-07 14:43:25,852 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1334), (1, 0.5934), (2, 0.8493), (3, 0.8856), (4, 0.8972), (5, 0.8972), (6, 0.8993), (7, 0.8982), (8, 0.9049), (9, 0.9022), (10, 0.8976)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8976
wandb:     loss 1.58709
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_144112-bgp0szzr
wandb: Find logs at: ./wandb/offline-run-20240407_144112-bgp0szzr/logs
INFO flwr 2024-04-07 14:43:29,423 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:50:34,173 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2019254)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2019254)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:50:39,164	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:50:39,552	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:50:40,042	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_1e946a170e1f6d41.zip' (12.14MiB) to Ray cluster...
2024-04-07 14:50:40,067	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_1e946a170e1f6d41.zip'.
INFO flwr 2024-04-07 14:50:50,259 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 68240085811.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 149226866893.0}
INFO flwr 2024-04-07 14:50:50,259 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 14:50:50,259 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 14:50:50,275 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 14:50:50,275 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 14:50:50,276 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 14:50:50,276 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 14:50:52,378 | server.py:94 | initial parameters (loss, other metrics): 2.3027989864349365, {'accuracy': 0.0971, 'data_size': 10000}
INFO flwr 2024-04-07 14:50:52,379 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 14:50:52,379 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2023599)[0m 2024-04-07 14:50:55.971768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2023599)[0m 2024-04-07 14:50:56.060731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2023599)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2023606)[0m 2024-04-07 14:50:58.033162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2023606)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2023606)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2023605)[0m 2024-04-07 14:50:56.304777: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2023605)[0m 2024-04-07 14:50:56.456467: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2023605)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2023603)[0m 2024-04-07 14:50:58.338316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 14:51:11,571 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 14:51:11,613 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 14:51:12,650 | server.py:125 | fit progress: (1, 1.861063838005066, {'accuracy': 0.6689, 'data_size': 10000}, 20.270479052036535)
INFO flwr 2024-04-07 14:51:12,650 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 14:51:12,650 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:51:22,164 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 14:51:23,240 | server.py:125 | fit progress: (2, 1.7136523723602295, {'accuracy': 0.7681, 'data_size': 10000}, 30.860378840006888)
INFO flwr 2024-04-07 14:51:23,240 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 14:51:23,240 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:51:31,905 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 14:51:33,260 | server.py:125 | fit progress: (3, 1.6316547393798828, {'accuracy': 0.8658, 'data_size': 10000}, 40.88073269202141)
INFO flwr 2024-04-07 14:51:33,260 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 14:51:33,260 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:51:41,738 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 14:51:43,076 | server.py:125 | fit progress: (4, 1.5903486013412476, {'accuracy': 0.9032, 'data_size': 10000}, 50.69685649703024)
INFO flwr 2024-04-07 14:51:43,076 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 14:51:43,077 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:51:51,407 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 14:51:52,594 | server.py:125 | fit progress: (5, 1.5894495248794556, {'accuracy': 0.8984, 'data_size': 10000}, 60.2143722170149)
INFO flwr 2024-04-07 14:51:52,594 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 14:51:52,594 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:52:00,831 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 14:52:02,249 | server.py:125 | fit progress: (6, 1.5827267169952393, {'accuracy': 0.9014, 'data_size': 10000}, 69.86990849801805)
INFO flwr 2024-04-07 14:52:02,249 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 14:52:02,250 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:52:11,238 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 14:52:12,471 | server.py:125 | fit progress: (7, 1.5952658653259277, {'accuracy': 0.8901, 'data_size': 10000}, 80.0918780110078)
INFO flwr 2024-04-07 14:52:12,471 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 14:52:12,472 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:52:21,801 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 14:52:23,065 | server.py:125 | fit progress: (8, 1.6010643243789673, {'accuracy': 0.8793, 'data_size': 10000}, 90.68606043304317)
INFO flwr 2024-04-07 14:52:23,066 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 14:52:23,066 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:52:31,527 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 14:52:33,038 | server.py:125 | fit progress: (9, 1.5772790908813477, {'accuracy': 0.9034, 'data_size': 10000}, 100.65876858500997)
INFO flwr 2024-04-07 14:52:33,038 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 14:52:33,038 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 14:52:41,862 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 14:52:43,419 | server.py:125 | fit progress: (10, 1.573158860206604, {'accuracy': 0.9086, 'data_size': 10000}, 111.03968597401399)
INFO flwr 2024-04-07 14:52:43,419 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 14:52:43,419 | server.py:153 | FL finished in 111.04012291703839
INFO flwr 2024-04-07 14:52:43,419 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 14:52:43,420 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 14:52:43,420 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 14:52:43,420 | app.py:229 | app_fit: losses_centralized [(0, 2.3027989864349365), (1, 1.861063838005066), (2, 1.7136523723602295), (3, 1.6316547393798828), (4, 1.5903486013412476), (5, 1.5894495248794556), (6, 1.5827267169952393), (7, 1.5952658653259277), (8, 1.6010643243789673), (9, 1.5772790908813477), (10, 1.573158860206604)]
INFO flwr 2024-04-07 14:52:43,420 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0971), (1, 0.6689), (2, 0.7681), (3, 0.8658), (4, 0.9032), (5, 0.8984), (6, 0.9014), (7, 0.8901), (8, 0.8793), (9, 0.9034), (10, 0.9086)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9086
wandb:     loss 1.57316
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_145033-sr70g5zk
wandb: Find logs at: ./wandb/offline-run-20240407_145033-sr70g5zk/logs
INFO flwr 2024-04-07 14:52:46,940 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 16
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 14:59:52,009 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2023597)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2023597)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 14:59:57,123	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 14:59:57,557	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 14:59:58,043	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b8aad97d75107479.zip' (12.15MiB) to Ray cluster...
2024-04-07 14:59:58,075	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b8aad97d75107479.zip'.
INFO flwr 2024-04-07 15:00:08,362 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 149169178829.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68215362355.0}
INFO flwr 2024-04-07 15:00:08,363 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:00:08,363 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:00:08,381 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:00:08,382 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:00:08,382 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:00:08,382 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:00:10,584 | server.py:94 | initial parameters (loss, other metrics): 2.29967999458313, {'accuracy': 0.142, 'data_size': 10000}
INFO flwr 2024-04-07 15:00:10,585 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:00:10,586 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2028187)[0m 2024-04-07 15:00:14.110326: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2028187)[0m 2024-04-07 15:00:14.200392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2028187)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2028195)[0m 2024-04-07 15:00:16.080741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2028195)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2028195)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2028191)[0m 2024-04-07 15:00:14.268050: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2028191)[0m 2024-04-07 15:00:14.366349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2028191)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2028197)[0m 2024-04-07 15:00:16.441561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:00:29,494 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:00:29,536 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:00:30,590 | server.py:125 | fit progress: (1, 1.8255345821380615, {'accuracy': 0.7226, 'data_size': 10000}, 20.00506154599134)
INFO flwr 2024-04-07 15:00:30,590 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:00:30,591 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:00:39,780 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:00:40,966 | server.py:125 | fit progress: (2, 1.638602375984192, {'accuracy': 0.8768, 'data_size': 10000}, 30.38138285302557)
INFO flwr 2024-04-07 15:00:40,967 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:00:40,967 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:00:50,838 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:00:52,022 | server.py:125 | fit progress: (3, 1.6148430109024048, {'accuracy': 0.886, 'data_size': 10000}, 41.436663957021665)
INFO flwr 2024-04-07 15:00:52,022 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:00:52,022 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:00,129 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:01,552 | server.py:125 | fit progress: (4, 1.593827247619629, {'accuracy': 0.8993, 'data_size': 10000}, 50.96724354097387)
INFO flwr 2024-04-07 15:01:01,553 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:01,553 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:10,070 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:11,503 | server.py:125 | fit progress: (5, 1.587796926498413, {'accuracy': 0.8984, 'data_size': 10000}, 60.91745946701849)
INFO flwr 2024-04-07 15:01:11,503 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:11,503 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:20,094 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:21,321 | server.py:125 | fit progress: (6, 1.5830620527267456, {'accuracy': 0.9022, 'data_size': 10000}, 70.73617325798841)
INFO flwr 2024-04-07 15:01:21,321 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:21,322 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:30,648 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:32,160 | server.py:125 | fit progress: (7, 1.5814800262451172, {'accuracy': 0.9011, 'data_size': 10000}, 81.57515309401788)
INFO flwr 2024-04-07 15:01:32,160 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:32,161 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:40,369 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:41,919 | server.py:125 | fit progress: (8, 1.5905568599700928, {'accuracy': 0.8915, 'data_size': 10000}, 91.33415328100091)
INFO flwr 2024-04-07 15:01:41,920 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:41,920 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:01:50,625 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:01:52,166 | server.py:125 | fit progress: (9, 1.5751272439956665, {'accuracy': 0.9062, 'data_size': 10000}, 101.58050404902315)
INFO flwr 2024-04-07 15:01:52,166 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:01:52,172 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:02:01,124 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:02:02,491 | server.py:125 | fit progress: (10, 1.5746787786483765, {'accuracy': 0.9049, 'data_size': 10000}, 111.90592773997923)
INFO flwr 2024-04-07 15:02:02,491 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:02:02,491 | server.py:153 | FL finished in 111.90635067102266
INFO flwr 2024-04-07 15:02:02,491 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:02:02,492 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:02:02,492 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:02:02,492 | app.py:229 | app_fit: losses_centralized [(0, 2.29967999458313), (1, 1.8255345821380615), (2, 1.638602375984192), (3, 1.6148430109024048), (4, 1.593827247619629), (5, 1.587796926498413), (6, 1.5830620527267456), (7, 1.5814800262451172), (8, 1.5905568599700928), (9, 1.5751272439956665), (10, 1.5746787786483765)]
INFO flwr 2024-04-07 15:02:02,492 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.142), (1, 0.7226), (2, 0.8768), (3, 0.886), (4, 0.8993), (5, 0.8984), (6, 0.9022), (7, 0.9011), (8, 0.8915), (9, 0.9062), (10, 0.9049)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9049
wandb:     loss 1.57468
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_145951-ims7bb9m
wandb: Find logs at: ./wandb/offline-run-20240407_145951-ims7bb9m/logs
INFO flwr 2024-04-07 15:02:06,020 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:09:10,851 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2028185)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2028185)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:09:15,758	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:09:16,115	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:09:16,434	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5b8d53e1fdb27569.zip' (12.16MiB) to Ray cluster...
2024-04-07 15:09:16,464	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5b8d53e1fdb27569.zip'.
INFO flwr 2024-04-07 15:09:26,888 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 68206793932.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 149149185844.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 15:09:26,889 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:09:26,889 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:09:26,911 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:09:26,912 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:09:26,912 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:09:26,912 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:09:29,891 | server.py:94 | initial parameters (loss, other metrics): 2.303328514099121, {'accuracy': 0.0966, 'data_size': 10000}
INFO flwr 2024-04-07 15:09:29,891 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:09:29,891 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2032484)[0m 2024-04-07 15:09:32.675491: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2032484)[0m 2024-04-07 15:09:32.759900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2032484)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2032482)[0m 2024-04-07 15:09:34.512352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2032489)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2032489)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2032486)[0m 2024-04-07 15:09:32.915209: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2032486)[0m 2024-04-07 15:09:33.004305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2032486)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2032484)[0m 2024-04-07 15:09:35.374072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:09:46,373 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:09:46,411 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:09:47,804 | server.py:125 | fit progress: (1, 2.3031086921691895, {'accuracy': 0.0989, 'data_size': 10000}, 17.91281304397853)
INFO flwr 2024-04-07 15:09:47,804 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:09:47,804 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:09:56,421 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:09:57,539 | server.py:125 | fit progress: (2, 2.302879571914673, {'accuracy': 0.1011, 'data_size': 10000}, 27.647568891989067)
INFO flwr 2024-04-07 15:09:57,539 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:09:57,539 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:05,912 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:07,383 | server.py:125 | fit progress: (3, 2.302647352218628, {'accuracy': 0.1046, 'data_size': 10000}, 37.49213988898555)
INFO flwr 2024-04-07 15:10:07,384 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:07,384 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:15,653 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:17,163 | server.py:125 | fit progress: (4, 2.30236554145813, {'accuracy': 0.1078, 'data_size': 10000}, 47.27167232899228)
INFO flwr 2024-04-07 15:10:17,163 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:17,163 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:25,808 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:27,337 | server.py:125 | fit progress: (5, 2.302130937576294, {'accuracy': 0.1099, 'data_size': 10000}, 57.445951456960756)
INFO flwr 2024-04-07 15:10:27,337 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:27,338 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:35,973 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:37,258 | server.py:125 | fit progress: (6, 2.301873207092285, {'accuracy': 0.1141, 'data_size': 10000}, 67.36693746701349)
INFO flwr 2024-04-07 15:10:37,258 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:37,259 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:46,047 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:47,632 | server.py:125 | fit progress: (7, 2.3016393184661865, {'accuracy': 0.118, 'data_size': 10000}, 77.74062579800375)
INFO flwr 2024-04-07 15:10:47,632 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:47,632 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:10:56,242 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:10:57,601 | server.py:125 | fit progress: (8, 2.3014144897460938, {'accuracy': 0.1194, 'data_size': 10000}, 87.70942500297679)
INFO flwr 2024-04-07 15:10:57,601 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:10:57,601 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:11:06,547 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:11:07,902 | server.py:125 | fit progress: (9, 2.301151990890503, {'accuracy': 0.1211, 'data_size': 10000}, 98.01042036997387)
INFO flwr 2024-04-07 15:11:07,902 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:11:07,902 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:11:16,440 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:11:18,128 | server.py:125 | fit progress: (10, 2.3008811473846436, {'accuracy': 0.1258, 'data_size': 10000}, 108.23638644296443)
INFO flwr 2024-04-07 15:11:18,128 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:11:18,128 | server.py:153 | FL finished in 108.23689361097058
INFO flwr 2024-04-07 15:11:18,128 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:11:18,128 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:11:18,128 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:11:18,129 | app.py:229 | app_fit: losses_centralized [(0, 2.303328514099121), (1, 2.3031086921691895), (2, 2.302879571914673), (3, 2.302647352218628), (4, 2.30236554145813), (5, 2.302130937576294), (6, 2.301873207092285), (7, 2.3016393184661865), (8, 2.3014144897460938), (9, 2.301151990890503), (10, 2.3008811473846436)]
INFO flwr 2024-04-07 15:11:18,129 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0966), (1, 0.0989), (2, 0.1011), (3, 0.1046), (4, 0.1078), (5, 0.1099), (6, 0.1141), (7, 0.118), (8, 0.1194), (9, 0.1211), (10, 0.1258)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1258
wandb:     loss 2.30088
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_150910-19m8cv1d
wandb: Find logs at: ./wandb/offline-run-20240407_150910-19m8cv1d/logs
INFO flwr 2024-04-07 15:11:21,728 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:18:26,782 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2032481)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2032481)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:18:32,093	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:18:32,419	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:18:32,828	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0e1a94ae7cd597bb.zip' (12.17MiB) to Ray cluster...
2024-04-07 15:18:32,860	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0e1a94ae7cd597bb.zip'.
INFO flwr 2024-04-07 15:18:43,132 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 68170433740.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 149064345396.0}
INFO flwr 2024-04-07 15:18:43,132 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:18:43,132 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:18:43,157 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:18:43,159 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:18:43,160 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:18:43,160 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:18:45,890 | server.py:94 | initial parameters (loss, other metrics): 2.3014588356018066, {'accuracy': 0.1098, 'data_size': 10000}
INFO flwr 2024-04-07 15:18:45,891 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:18:45,892 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2036765)[0m 2024-04-07 15:18:48.826101: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2036765)[0m 2024-04-07 15:18:48.917527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2036765)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2036765)[0m 2024-04-07 15:18:50.880714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2036776)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2036776)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2036777)[0m 2024-04-07 15:18:49.314032: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2036777)[0m 2024-04-07 15:18:49.425566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2036777)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2036777)[0m 2024-04-07 15:18:51.586993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:19:02,367 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:19:02,413 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:19:03,704 | server.py:125 | fit progress: (1, 2.2846786975860596, {'accuracy': 0.1885, 'data_size': 10000}, 17.812252903007902)
INFO flwr 2024-04-07 15:19:03,704 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:03,704 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:12,050 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:13,157 | server.py:125 | fit progress: (2, 2.2680628299713135, {'accuracy': 0.251, 'data_size': 10000}, 27.265413835993968)
INFO flwr 2024-04-07 15:19:13,157 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:13,157 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:20,949 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:22,082 | server.py:125 | fit progress: (3, 2.2438721656799316, {'accuracy': 0.3794, 'data_size': 10000}, 36.19029066700023)
INFO flwr 2024-04-07 15:19:22,082 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:22,082 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:29,906 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:31,056 | server.py:125 | fit progress: (4, 2.2153658866882324, {'accuracy': 0.382, 'data_size': 10000}, 45.164896860020235)
INFO flwr 2024-04-07 15:19:31,057 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:31,057 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:38,416 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:39,810 | server.py:125 | fit progress: (5, 2.185638904571533, {'accuracy': 0.4652, 'data_size': 10000}, 53.91831669799285)
INFO flwr 2024-04-07 15:19:39,810 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:39,810 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:47,300 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:48,732 | server.py:125 | fit progress: (6, 2.1564505100250244, {'accuracy': 0.478, 'data_size': 10000}, 62.84017063502688)
INFO flwr 2024-04-07 15:19:48,732 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:48,732 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:19:55,876 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:19:57,126 | server.py:125 | fit progress: (7, 2.1167643070220947, {'accuracy': 0.5884, 'data_size': 10000}, 71.23431380302645)
INFO flwr 2024-04-07 15:19:57,126 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:19:57,126 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:20:04,446 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:20:05,929 | server.py:125 | fit progress: (8, 2.087610960006714, {'accuracy': 0.6594, 'data_size': 10000}, 80.03724293300183)
INFO flwr 2024-04-07 15:20:05,929 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:20:05,929 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:20:13,272 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:20:14,785 | server.py:125 | fit progress: (9, 2.055391550064087, {'accuracy': 0.6694, 'data_size': 10000}, 88.89365450502373)
INFO flwr 2024-04-07 15:20:14,785 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:20:14,785 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:20:21,875 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:20:23,472 | server.py:125 | fit progress: (10, 2.029815435409546, {'accuracy': 0.7045, 'data_size': 10000}, 97.58054127701325)
INFO flwr 2024-04-07 15:20:23,472 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:20:23,472 | server.py:153 | FL finished in 97.58103705599206
INFO flwr 2024-04-07 15:20:23,473 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:20:23,473 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:20:23,473 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:20:23,473 | app.py:229 | app_fit: losses_centralized [(0, 2.3014588356018066), (1, 2.2846786975860596), (2, 2.2680628299713135), (3, 2.2438721656799316), (4, 2.2153658866882324), (5, 2.185638904571533), (6, 2.1564505100250244), (7, 2.1167643070220947), (8, 2.087610960006714), (9, 2.055391550064087), (10, 2.029815435409546)]
INFO flwr 2024-04-07 15:20:23,473 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1098), (1, 0.1885), (2, 0.251), (3, 0.3794), (4, 0.382), (5, 0.4652), (6, 0.478), (7, 0.5884), (8, 0.6594), (9, 0.6694), (10, 0.7045)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7045
wandb:     loss 2.02982
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_151826-icwpkc0b
wandb: Find logs at: ./wandb/offline-run-20240407_151826-icwpkc0b/logs
INFO flwr 2024-04-07 15:20:27,048 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:27:31,743 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2036763)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2036763)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:27:37,770	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:27:38,203	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:27:38,600	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_23b2eb33ef34a781.zip' (12.18MiB) to Ray cluster...
2024-04-07 15:27:38,630	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_23b2eb33ef34a781.zip'.
INFO flwr 2024-04-07 15:27:48,953 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 149002448282.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68143906406.0}
INFO flwr 2024-04-07 15:27:48,954 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:27:48,954 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:27:48,975 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:27:48,976 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:27:48,976 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:27:48,976 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:27:51,838 | server.py:94 | initial parameters (loss, other metrics): 2.299891710281372, {'accuracy': 0.1434, 'data_size': 10000}
INFO flwr 2024-04-07 15:27:51,839 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:27:51,840 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2041072)[0m 2024-04-07 15:27:54.123590: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2041072)[0m 2024-04-07 15:27:54.214990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2041072)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2041072)[0m 2024-04-07 15:27:56.496896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2041073)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2041073)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2041073)[0m 2024-04-07 15:27:54.928874: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2041073)[0m 2024-04-07 15:27:54.992790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2041073)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2041071)[0m 2024-04-07 15:27:57.153866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:28:09,116 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:28:09,154 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:28:10,203 | server.py:125 | fit progress: (1, 2.265700578689575, {'accuracy': 0.3933, 'data_size': 10000}, 18.36381428601453)
INFO flwr 2024-04-07 15:28:10,204 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:10,204 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:28:18,296 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:28:19,661 | server.py:125 | fit progress: (2, 2.21988844871521, {'accuracy': 0.5126, 'data_size': 10000}, 27.821869336999953)
INFO flwr 2024-04-07 15:28:19,662 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:19,662 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:28:26,690 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:28:28,042 | server.py:125 | fit progress: (3, 2.1511337757110596, {'accuracy': 0.5931, 'data_size': 10000}, 36.202924542012624)
INFO flwr 2024-04-07 15:28:28,043 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:28,043 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:28:35,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:28:37,004 | server.py:125 | fit progress: (4, 2.093607187271118, {'accuracy': 0.6309, 'data_size': 10000}, 45.164900579024106)
INFO flwr 2024-04-07 15:28:37,005 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:37,005 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:28:44,579 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:28:45,784 | server.py:125 | fit progress: (5, 2.0290565490722656, {'accuracy': 0.6252, 'data_size': 10000}, 53.94469079404371)
INFO flwr 2024-04-07 15:28:45,784 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:45,784 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:28:53,023 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:28:54,470 | server.py:125 | fit progress: (6, 1.990350365638733, {'accuracy': 0.6413, 'data_size': 10000}, 62.630529539019335)
INFO flwr 2024-04-07 15:28:54,470 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:28:54,470 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:29:02,022 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:29:03,662 | server.py:125 | fit progress: (7, 1.9568853378295898, {'accuracy': 0.7043, 'data_size': 10000}, 71.82216511300066)
INFO flwr 2024-04-07 15:29:03,662 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:29:03,662 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:29:11,205 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:29:12,723 | server.py:125 | fit progress: (8, 1.9260591268539429, {'accuracy': 0.7024, 'data_size': 10000}, 80.88387063500704)
INFO flwr 2024-04-07 15:29:12,723 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:29:12,724 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:29:20,591 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:29:21,946 | server.py:125 | fit progress: (9, 1.9021402597427368, {'accuracy': 0.7298, 'data_size': 10000}, 90.1063422000152)
INFO flwr 2024-04-07 15:29:21,946 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:29:21,946 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:29:29,281 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:29:30,869 | server.py:125 | fit progress: (10, 1.8761918544769287, {'accuracy': 0.7639, 'data_size': 10000}, 99.02950177300954)
INFO flwr 2024-04-07 15:29:30,869 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:29:30,869 | server.py:153 | FL finished in 99.02991755702533
INFO flwr 2024-04-07 15:29:30,869 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:29:30,870 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:29:30,870 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:29:30,870 | app.py:229 | app_fit: losses_centralized [(0, 2.299891710281372), (1, 2.265700578689575), (2, 2.21988844871521), (3, 2.1511337757110596), (4, 2.093607187271118), (5, 2.0290565490722656), (6, 1.990350365638733), (7, 1.9568853378295898), (8, 1.9260591268539429), (9, 1.9021402597427368), (10, 1.8761918544769287)]
INFO flwr 2024-04-07 15:29:30,870 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1434), (1, 0.3933), (2, 0.5126), (3, 0.5931), (4, 0.6309), (5, 0.6252), (6, 0.6413), (7, 0.7043), (8, 0.7024), (9, 0.7298), (10, 0.7639)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.7639
wandb:     loss 1.87619
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_152731-l2qd4e2r
wandb: Find logs at: ./wandb/offline-run-20240407_152731-l2qd4e2r/logs
INFO flwr 2024-04-07 15:29:34,366 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:36:39,128 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2041070)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2041070)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:36:45,372	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:36:45,774	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:36:46,220	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_68d8e09cc0b7ea35.zip' (12.19MiB) to Ray cluster...
2024-04-07 15:36:46,252	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_68d8e09cc0b7ea35.zip'.
INFO flwr 2024-04-07 15:36:56,517 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 148954276455.0, 'object_store_memory': 68123261337.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 15:36:56,518 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:36:56,518 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:36:56,535 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:36:56,536 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:36:56,536 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:36:56,536 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:36:59,684 | server.py:94 | initial parameters (loss, other metrics): 2.3026182651519775, {'accuracy': 0.0746, 'data_size': 10000}
INFO flwr 2024-04-07 15:36:59,685 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:36:59,685 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2045069)[0m 2024-04-07 15:37:02.275904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2045069)[0m 2024-04-07 15:37:02.365712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2045069)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2045055)[0m 2024-04-07 15:37:04.611396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2045069)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2045069)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2045064)[0m 2024-04-07 15:37:02.453323: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2045064)[0m 2024-04-07 15:37:02.536097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2045064)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2045066)[0m 2024-04-07 15:37:04.593273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:37:16,105 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:37:16,143 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:37:17,150 | server.py:125 | fit progress: (1, 2.24694561958313, {'accuracy': 0.3414, 'data_size': 10000}, 17.465510635985993)
INFO flwr 2024-04-07 15:37:17,151 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:37:17,151 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:37:25,633 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:37:26,760 | server.py:125 | fit progress: (2, 2.1921045780181885, {'accuracy': 0.5823, 'data_size': 10000}, 27.07492101297248)
INFO flwr 2024-04-07 15:37:26,760 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:37:26,760 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:37:34,315 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:37:35,661 | server.py:125 | fit progress: (3, 2.1122615337371826, {'accuracy': 0.5491, 'data_size': 10000}, 35.976106956019066)
INFO flwr 2024-04-07 15:37:35,661 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:37:35,661 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:37:42,935 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:37:44,279 | server.py:125 | fit progress: (4, 2.0440030097961426, {'accuracy': 0.6196, 'data_size': 10000}, 44.59427250700537)
INFO flwr 2024-04-07 15:37:44,279 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:37:44,280 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:37:51,734 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:37:53,134 | server.py:125 | fit progress: (5, 1.9853146076202393, {'accuracy': 0.6921, 'data_size': 10000}, 53.44937237002887)
INFO flwr 2024-04-07 15:37:53,135 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:37:53,135 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:38:00,389 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:38:01,601 | server.py:125 | fit progress: (6, 1.9434185028076172, {'accuracy': 0.6941, 'data_size': 10000}, 61.91629827400902)
INFO flwr 2024-04-07 15:38:01,601 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:38:01,602 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:38:08,959 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:38:10,423 | server.py:125 | fit progress: (7, 1.883178472518921, {'accuracy': 0.7957, 'data_size': 10000}, 70.73788431298453)
INFO flwr 2024-04-07 15:38:10,423 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:38:10,423 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:38:17,771 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:38:19,271 | server.py:125 | fit progress: (8, 1.8495056629180908, {'accuracy': 0.8075, 'data_size': 10000}, 79.58580468699802)
INFO flwr 2024-04-07 15:38:19,271 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:38:19,271 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:38:27,198 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:38:28,748 | server.py:125 | fit progress: (9, 1.8446931838989258, {'accuracy': 0.7406, 'data_size': 10000}, 89.06327276298543)
INFO flwr 2024-04-07 15:38:28,748 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:38:28,749 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:38:36,054 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:38:37,410 | server.py:125 | fit progress: (10, 1.8062223196029663, {'accuracy': 0.8259, 'data_size': 10000}, 97.72502900997642)
INFO flwr 2024-04-07 15:38:37,410 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:38:37,410 | server.py:153 | FL finished in 97.72552783600986
INFO flwr 2024-04-07 15:38:37,411 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:38:37,411 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:38:37,411 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:38:37,411 | app.py:229 | app_fit: losses_centralized [(0, 2.3026182651519775), (1, 2.24694561958313), (2, 2.1921045780181885), (3, 2.1122615337371826), (4, 2.0440030097961426), (5, 1.9853146076202393), (6, 1.9434185028076172), (7, 1.883178472518921), (8, 1.8495056629180908), (9, 1.8446931838989258), (10, 1.8062223196029663)]
INFO flwr 2024-04-07 15:38:37,411 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0746), (1, 0.3414), (2, 0.5823), (3, 0.5491), (4, 0.6196), (5, 0.6921), (6, 0.6941), (7, 0.7957), (8, 0.8075), (9, 0.7406), (10, 0.8259)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8259
wandb:     loss 1.80622
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_153638-q9yted2t
wandb: Find logs at: ./wandb/offline-run-20240407_153638-q9yted2t/logs
INFO flwr 2024-04-07 15:38:40,975 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:45:45,937 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2045055)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2045055)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:45:51,843	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:45:52,236	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:45:52,613	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b396385fe88f9fab.zip' (12.21MiB) to Ray cluster...
2024-04-07 15:45:52,645	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b396385fe88f9fab.zip'.
INFO flwr 2024-04-07 15:46:02,862 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 148888912896.0, 'object_store_memory': 68095248384.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-04-07 15:46:02,863 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:46:02,863 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:46:02,879 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:46:02,880 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:46:02,880 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:46:02,880 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:46:05,467 | server.py:94 | initial parameters (loss, other metrics): 2.3034276962280273, {'accuracy': 0.1037, 'data_size': 10000}
INFO flwr 2024-04-07 15:46:05,468 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:46:05,468 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2049376)[0m 2024-04-07 15:46:08.670525: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2049376)[0m 2024-04-07 15:46:08.764482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2049376)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2049378)[0m 2024-04-07 15:46:10.731297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2049378)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2049378)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2049378)[0m 2024-04-07 15:46:08.914475: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2049378)[0m 2024-04-07 15:46:09.009212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2049378)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2049376)[0m 2024-04-07 15:46:10.872547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:46:21,967 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:46:22,003 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:46:23,266 | server.py:125 | fit progress: (1, 2.2493069171905518, {'accuracy': 0.4311, 'data_size': 10000}, 17.79838384897448)
INFO flwr 2024-04-07 15:46:23,267 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:46:23,267 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:46:32,012 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:46:33,454 | server.py:125 | fit progress: (2, 2.150219678878784, {'accuracy': 0.5347, 'data_size': 10000}, 27.98628120002104)
INFO flwr 2024-04-07 15:46:33,454 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:46:33,455 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:46:41,353 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:46:42,515 | server.py:125 | fit progress: (3, 2.0571491718292236, {'accuracy': 0.4869, 'data_size': 10000}, 37.04715060000308)
INFO flwr 2024-04-07 15:46:42,515 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:46:42,515 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:46:50,321 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:46:51,497 | server.py:125 | fit progress: (4, 1.9718941450119019, {'accuracy': 0.6531, 'data_size': 10000}, 46.02891940501286)
INFO flwr 2024-04-07 15:46:51,497 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:46:51,497 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:46:59,156 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:00,559 | server.py:125 | fit progress: (5, 1.9216210842132568, {'accuracy': 0.6718, 'data_size': 10000}, 55.090939891000744)
INFO flwr 2024-04-07 15:47:00,559 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:47:00,559 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:47:08,358 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:09,820 | server.py:125 | fit progress: (6, 1.876827597618103, {'accuracy': 0.7175, 'data_size': 10000}, 64.35243554698536)
INFO flwr 2024-04-07 15:47:09,821 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:47:09,821 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:47:17,474 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:18,974 | server.py:125 | fit progress: (7, 1.839369535446167, {'accuracy': 0.7749, 'data_size': 10000}, 73.50585682998644)
INFO flwr 2024-04-07 15:47:18,974 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:47:18,974 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:47:26,349 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:27,632 | server.py:125 | fit progress: (8, 1.8133256435394287, {'accuracy': 0.802, 'data_size': 10000}, 82.16421532200184)
INFO flwr 2024-04-07 15:47:27,632 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:47:27,633 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:47:35,307 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:36,833 | server.py:125 | fit progress: (9, 1.795637845993042, {'accuracy': 0.8236, 'data_size': 10000}, 91.36476414196659)
INFO flwr 2024-04-07 15:47:36,833 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:47:36,833 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:47:44,599 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:47:46,190 | server.py:125 | fit progress: (10, 1.7824245691299438, {'accuracy': 0.8264, 'data_size': 10000}, 100.72167413798161)
INFO flwr 2024-04-07 15:47:46,190 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:47:46,190 | server.py:153 | FL finished in 100.72208542498993
INFO flwr 2024-04-07 15:47:46,190 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:47:46,190 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:47:46,190 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:47:46,190 | app.py:229 | app_fit: losses_centralized [(0, 2.3034276962280273), (1, 2.2493069171905518), (2, 2.150219678878784), (3, 2.0571491718292236), (4, 1.9718941450119019), (5, 1.9216210842132568), (6, 1.876827597618103), (7, 1.839369535446167), (8, 1.8133256435394287), (9, 1.795637845993042), (10, 1.7824245691299438)]
INFO flwr 2024-04-07 15:47:46,190 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1037), (1, 0.4311), (2, 0.5347), (3, 0.4869), (4, 0.6531), (5, 0.6718), (6, 0.7175), (7, 0.7749), (8, 0.802), (9, 0.8236), (10, 0.8264)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8264
wandb:     loss 1.78242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_154545-5db8mifp
wandb: Find logs at: ./wandb/offline-run-20240407_154545-5db8mifp/logs
INFO flwr 2024-04-07 15:47:49,695 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 15:54:54,549 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2049370)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2049370)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 15:55:00,520	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 15:55:00,874	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 15:55:01,204	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f246ac4bed568ca8.zip' (12.22MiB) to Ray cluster...
2024-04-07 15:55:01,237	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f246ac4bed568ca8.zip'.
INFO flwr 2024-04-07 15:55:11,468 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68051672678.0, 'memory': 148787236250.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 15:55:11,469 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 15:55:11,469 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 15:55:11,484 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 15:55:11,485 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 15:55:11,485 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 15:55:11,485 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 15:55:14,040 | server.py:94 | initial parameters (loss, other metrics): 2.301147937774658, {'accuracy': 0.0948, 'data_size': 10000}
INFO flwr 2024-04-07 15:55:14,040 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 15:55:14,040 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2053659)[0m 2024-04-07 15:55:17.192258: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2053659)[0m 2024-04-07 15:55:17.287537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2053659)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2053663)[0m 2024-04-07 15:55:19.132451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2053661)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2053661)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2053668)[0m 2024-04-07 15:55:17.699486: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2053668)[0m 2024-04-07 15:55:17.803077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2053668)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2053668)[0m 2024-04-07 15:55:19.816751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 15:55:30,877 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 15:55:30,919 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 15:55:31,964 | server.py:125 | fit progress: (1, 2.2239630222320557, {'accuracy': 0.3799, 'data_size': 10000}, 17.923307744029444)
INFO flwr 2024-04-07 15:55:31,964 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 15:55:31,964 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:55:40,453 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 15:55:41,525 | server.py:125 | fit progress: (2, 2.081156015396118, {'accuracy': 0.5542, 'data_size': 10000}, 27.48428995604627)
INFO flwr 2024-04-07 15:55:41,525 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 15:55:41,525 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:55:49,526 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 15:55:50,703 | server.py:125 | fit progress: (3, 1.9961344003677368, {'accuracy': 0.6504, 'data_size': 10000}, 36.66227897501085)
INFO flwr 2024-04-07 15:55:50,703 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 15:55:50,704 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:55:58,485 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 15:55:59,999 | server.py:125 | fit progress: (4, 1.9312756061553955, {'accuracy': 0.663, 'data_size': 10000}, 45.958806106995326)
INFO flwr 2024-04-07 15:55:59,999 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:00,000 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:08,740 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:10,244 | server.py:125 | fit progress: (5, 1.8725779056549072, {'accuracy': 0.7124, 'data_size': 10000}, 56.20399933704175)
INFO flwr 2024-04-07 15:56:10,245 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:10,245 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:18,403 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:19,937 | server.py:125 | fit progress: (6, 1.827308177947998, {'accuracy': 0.7802, 'data_size': 10000}, 65.89673473499715)
INFO flwr 2024-04-07 15:56:19,937 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:19,938 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:28,002 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:29,253 | server.py:125 | fit progress: (7, 1.7984275817871094, {'accuracy': 0.8086, 'data_size': 10000}, 75.21239894599421)
INFO flwr 2024-04-07 15:56:29,253 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:29,253 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:37,214 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:38,815 | server.py:125 | fit progress: (8, 1.7698696851730347, {'accuracy': 0.8366, 'data_size': 10000}, 84.7749979950022)
INFO flwr 2024-04-07 15:56:38,816 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:38,816 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:46,830 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:48,382 | server.py:125 | fit progress: (9, 1.7532680034637451, {'accuracy': 0.8354, 'data_size': 10000}, 94.34172048902838)
INFO flwr 2024-04-07 15:56:48,382 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 15:56:48,383 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 15:56:56,493 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 15:56:57,860 | server.py:125 | fit progress: (10, 1.7392675876617432, {'accuracy': 0.8463, 'data_size': 10000}, 103.81996614503441)
INFO flwr 2024-04-07 15:56:57,861 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 15:56:57,861 | server.py:153 | FL finished in 103.82036030699965
INFO flwr 2024-04-07 15:56:57,861 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 15:56:57,861 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 15:56:57,861 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 15:56:57,861 | app.py:229 | app_fit: losses_centralized [(0, 2.301147937774658), (1, 2.2239630222320557), (2, 2.081156015396118), (3, 1.9961344003677368), (4, 1.9312756061553955), (5, 1.8725779056549072), (6, 1.827308177947998), (7, 1.7984275817871094), (8, 1.7698696851730347), (9, 1.7532680034637451), (10, 1.7392675876617432)]
INFO flwr 2024-04-07 15:56:57,861 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0948), (1, 0.3799), (2, 0.5542), (3, 0.6504), (4, 0.663), (5, 0.7124), (6, 0.7802), (7, 0.8086), (8, 0.8366), (9, 0.8354), (10, 0.8463)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8463
wandb:     loss 1.73927
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_155454-rap5mexh
wandb: Find logs at: ./wandb/offline-run-20240407_155454-rap5mexh/logs
INFO flwr 2024-04-07 15:57:01,387 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 1
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:04:06,180 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2053659)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2053659)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:04:11,096	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:04:11,410	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:04:11,753	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ab27ab5caebe81fe.zip' (12.23MiB) to Ray cluster...
2024-04-07 16:04:11,791	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ab27ab5caebe81fe.zip'.
INFO flwr 2024-04-07 16:04:22,112 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 148787508634.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'object_store_memory': 68051789414.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 16:04:22,112 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:04:22,112 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:04:22,130 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:04:22,130 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:04:22,131 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:04:22,131 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:04:25,074 | server.py:94 | initial parameters (loss, other metrics): 2.2998669147491455, {'accuracy': 0.1518, 'data_size': 10000}
INFO flwr 2024-04-07 16:04:25,075 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:04:25,075 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2058267)[0m 2024-04-07 16:04:27.822818: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2058267)[0m 2024-04-07 16:04:27.913663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2058267)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2058267)[0m 2024-04-07 16:04:29.987022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2058273)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2058273)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2058275)[0m 2024-04-07 16:04:28.018329: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2058275)[0m 2024-04-07 16:04:28.108884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2058275)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2058275)[0m 2024-04-07 16:04:30.144747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:04:41,452 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:04:41,483 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:04:42,790 | server.py:125 | fit progress: (1, 2.212937593460083, {'accuracy': 0.3369, 'data_size': 10000}, 17.715238045959268)
INFO flwr 2024-04-07 16:04:42,790 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:04:42,790 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:04:50,988 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:04:52,117 | server.py:125 | fit progress: (2, 2.0816268920898438, {'accuracy': 0.4925, 'data_size': 10000}, 27.0417406289489)
INFO flwr 2024-04-07 16:04:52,117 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:04:52,117 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:04:59,594 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:00,939 | server.py:125 | fit progress: (3, 1.9689998626708984, {'accuracy': 0.6369, 'data_size': 10000}, 35.864325718954206)
INFO flwr 2024-04-07 16:05:00,939 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:00,940 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:08,259 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:09,505 | server.py:125 | fit progress: (4, 1.8985241651535034, {'accuracy': 0.6998, 'data_size': 10000}, 44.430462575983256)
INFO flwr 2024-04-07 16:05:09,506 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:09,506 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:17,310 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:18,526 | server.py:125 | fit progress: (5, 1.8514961004257202, {'accuracy': 0.7421, 'data_size': 10000}, 53.45145013794536)
INFO flwr 2024-04-07 16:05:18,527 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:18,527 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:26,185 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:27,631 | server.py:125 | fit progress: (6, 1.8097015619277954, {'accuracy': 0.7809, 'data_size': 10000}, 62.556045839970466)
INFO flwr 2024-04-07 16:05:27,631 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:27,631 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:35,054 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:36,682 | server.py:125 | fit progress: (7, 1.7911109924316406, {'accuracy': 0.7839, 'data_size': 10000}, 71.60681553796167)
INFO flwr 2024-04-07 16:05:36,682 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:36,682 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:45,044 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:46,630 | server.py:125 | fit progress: (8, 1.779889702796936, {'accuracy': 0.783, 'data_size': 10000}, 81.55493698798819)
INFO flwr 2024-04-07 16:05:46,630 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:46,630 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:05:54,664 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:05:56,009 | server.py:125 | fit progress: (9, 1.7377898693084717, {'accuracy': 0.8475, 'data_size': 10000}, 90.93410322797718)
INFO flwr 2024-04-07 16:05:56,009 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:05:56,010 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:06:03,687 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:06:05,263 | server.py:125 | fit progress: (10, 1.729438066482544, {'accuracy': 0.8503, 'data_size': 10000}, 100.18845264898846)
INFO flwr 2024-04-07 16:06:05,264 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:06:05,264 | server.py:153 | FL finished in 100.18885841598967
INFO flwr 2024-04-07 16:06:05,264 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:06:05,264 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:06:05,264 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:06:05,264 | app.py:229 | app_fit: losses_centralized [(0, 2.2998669147491455), (1, 2.212937593460083), (2, 2.0816268920898438), (3, 1.9689998626708984), (4, 1.8985241651535034), (5, 1.8514961004257202), (6, 1.8097015619277954), (7, 1.7911109924316406), (8, 1.779889702796936), (9, 1.7377898693084717), (10, 1.729438066482544)]
INFO flwr 2024-04-07 16:06:05,264 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1518), (1, 0.3369), (2, 0.4925), (3, 0.6369), (4, 0.6998), (5, 0.7421), (6, 0.7809), (7, 0.7839), (8, 0.783), (9, 0.8475), (10, 0.8503)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8503
wandb:     loss 1.72944
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_160405-33ixkp2a
wandb: Find logs at: ./wandb/offline-run-20240407_160405-33ixkp2a/logs
INFO flwr 2024-04-07 16:06:08,830 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:13:13,921 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2058264)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2058264)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:13:21,141	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:13:21,515	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:13:21,893	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_56d93aa08fd1610f.zip' (12.24MiB) to Ray cluster...
2024-04-07 16:13:21,927	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_56d93aa08fd1610f.zip'.
INFO flwr 2024-04-07 16:13:32,224 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 67953630412.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0, 'memory': 148558470964.0}
INFO flwr 2024-04-07 16:13:32,224 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:13:32,224 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:13:32,239 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:13:32,240 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:13:32,241 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:13:32,241 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:13:36,435 | server.py:94 | initial parameters (loss, other metrics): 2.3028862476348877, {'accuracy': 0.0693, 'data_size': 10000}
INFO flwr 2024-04-07 16:13:36,435 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:13:36,435 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2062555)[0m 2024-04-07 16:13:37.740102: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2062555)[0m 2024-04-07 16:13:37.871575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2062555)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2062555)[0m 2024-04-07 16:13:39.883778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2062555)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2062555)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2062551)[0m 2024-04-07 16:13:37.991003: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2062551)[0m 2024-04-07 16:13:38.082195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2062551)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2062552)[0m 2024-04-07 16:13:39.935187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:13:51,619 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:13:51,658 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:13:52,682 | server.py:125 | fit progress: (1, 2.301607847213745, {'accuracy': 0.087, 'data_size': 10000}, 16.246612188988365)
INFO flwr 2024-04-07 16:13:52,682 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:13:52,682 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:01,074 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:02,193 | server.py:125 | fit progress: (2, 2.3001153469085693, {'accuracy': 0.0968, 'data_size': 10000}, 25.757963971991558)
INFO flwr 2024-04-07 16:14:02,194 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:02,194 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:09,882 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:11,266 | server.py:125 | fit progress: (3, 2.2986741065979004, {'accuracy': 0.1012, 'data_size': 10000}, 34.83052152895834)
INFO flwr 2024-04-07 16:14:11,266 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:11,267 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:19,004 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:20,391 | server.py:125 | fit progress: (4, 2.2972867488861084, {'accuracy': 0.1061, 'data_size': 10000}, 43.95547720999457)
INFO flwr 2024-04-07 16:14:20,391 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:20,391 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:27,779 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:29,186 | server.py:125 | fit progress: (5, 2.2960097789764404, {'accuracy': 0.1107, 'data_size': 10000}, 52.75043618696509)
INFO flwr 2024-04-07 16:14:29,186 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:29,186 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:36,932 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:38,146 | server.py:125 | fit progress: (6, 2.294266700744629, {'accuracy': 0.1113, 'data_size': 10000}, 61.7106691969675)
INFO flwr 2024-04-07 16:14:38,146 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:38,146 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:45,557 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:47,076 | server.py:125 | fit progress: (7, 2.2923855781555176, {'accuracy': 0.1137, 'data_size': 10000}, 70.64036629296606)
INFO flwr 2024-04-07 16:14:47,076 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:47,076 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:14:54,998 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:14:56,497 | server.py:125 | fit progress: (8, 2.290788412094116, {'accuracy': 0.1167, 'data_size': 10000}, 80.06203341897344)
INFO flwr 2024-04-07 16:14:56,498 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:14:56,498 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:15:04,238 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:15:05,841 | server.py:125 | fit progress: (9, 2.2888247966766357, {'accuracy': 0.124, 'data_size': 10000}, 89.40607351099607)
INFO flwr 2024-04-07 16:15:05,842 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:15:05,842 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:15:13,624 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:15:14,964 | server.py:125 | fit progress: (10, 2.286891222000122, {'accuracy': 0.1304, 'data_size': 10000}, 98.52825827494962)
INFO flwr 2024-04-07 16:15:14,964 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:15:14,964 | server.py:153 | FL finished in 98.52871435094858
INFO flwr 2024-04-07 16:15:14,964 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:15:14,964 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:15:14,964 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:15:14,965 | app.py:229 | app_fit: losses_centralized [(0, 2.3028862476348877), (1, 2.301607847213745), (2, 2.3001153469085693), (3, 2.2986741065979004), (4, 2.2972867488861084), (5, 2.2960097789764404), (6, 2.294266700744629), (7, 2.2923855781555176), (8, 2.290788412094116), (9, 2.2888247966766357), (10, 2.286891222000122)]
INFO flwr 2024-04-07 16:15:14,965 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0693), (1, 0.087), (2, 0.0968), (3, 0.1012), (4, 0.1061), (5, 0.1107), (6, 0.1113), (7, 0.1137), (8, 0.1167), (9, 0.124), (10, 0.1304)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.1304
wandb:     loss 2.28689
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_161313-h99u24es
wandb: Find logs at: ./wandb/offline-run-20240407_161313-h99u24es/logs
INFO flwr 2024-04-07 16:15:18,516 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:22:34,878 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2062547)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2062547)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:22:58,599	ERROR services.py:1207 -- Failed to start the dashboard 
2024-04-07 16:22:58,600	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-04-07 16:22:58,601	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/tmp/ray/session_2024-04-07_16-22-37_078470_1357177/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-04-07 16:22:58,602	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-04-07 16:22:58,914	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:23:08,980	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:23:09,340	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d7a958cf6bd661ff.zip' (12.25MiB) to Ray cluster...
2024-04-07 16:23:09,383	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d7a958cf6bd661ff.zip'.
INFO flwr 2024-04-07 16:23:19,869 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 152241882522.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69532235366.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 16:23:19,869 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:23:19,870 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:23:19,884 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:23:19,885 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:23:19,885 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:23:19,885 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:23:23,887 | server.py:94 | initial parameters (loss, other metrics): 2.3044800758361816, {'accuracy': 0.0651, 'data_size': 10000}
INFO flwr 2024-04-07 16:23:23,889 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:23:23,889 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2069592)[0m 2024-04-07 16:23:37.000692: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2069592)[0m 2024-04-07 16:23:37.115845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2069592)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2069580)[0m 2024-04-07 16:23:58.077685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=2069591)[0m 2024-04-07 16:23:37.170603: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2069591)[0m 2024-04-07 16:23:37.229412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2069591)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2069580)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2069580)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2069591)[0m 2024-04-07 16:23:58.077656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:25:33,023 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:25:33,057 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:25:34,342 | server.py:125 | fit progress: (1, 2.2228262424468994, {'accuracy': 0.4902, 'data_size': 10000}, 130.4534305499983)
INFO flwr 2024-04-07 16:25:34,343 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:25:34,343 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:25:42,884 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:25:44,013 | server.py:125 | fit progress: (2, 2.10951828956604, {'accuracy': 0.5838, 'data_size': 10000}, 140.12411068199435)
INFO flwr 2024-04-07 16:25:44,013 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:25:44,014 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:25:52,060 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:25:53,187 | server.py:125 | fit progress: (3, 2.0117783546447754, {'accuracy': 0.5797, 'data_size': 10000}, 149.29788217501482)
INFO flwr 2024-04-07 16:25:53,187 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:25:53,187 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:01,489 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:02,674 | server.py:125 | fit progress: (4, 1.952752709388733, {'accuracy': 0.5709, 'data_size': 10000}, 158.78477383899735)
INFO flwr 2024-04-07 16:26:02,674 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:02,674 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:10,695 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:12,096 | server.py:125 | fit progress: (5, 1.8974374532699585, {'accuracy': 0.6587, 'data_size': 10000}, 168.20724119700026)
INFO flwr 2024-04-07 16:26:12,097 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:12,097 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:19,863 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:21,328 | server.py:125 | fit progress: (6, 1.84141206741333, {'accuracy': 0.7864, 'data_size': 10000}, 177.4388659709948)
INFO flwr 2024-04-07 16:26:21,328 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:21,328 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:29,509 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:30,860 | server.py:125 | fit progress: (7, 1.8138700723648071, {'accuracy': 0.7998, 'data_size': 10000}, 186.97072673402727)
INFO flwr 2024-04-07 16:26:30,860 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:30,860 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:39,274 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:40,851 | server.py:125 | fit progress: (8, 1.7870090007781982, {'accuracy': 0.8173, 'data_size': 10000}, 196.96158884104807)
INFO flwr 2024-04-07 16:26:40,851 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:40,851 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:48,573 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:50,081 | server.py:125 | fit progress: (9, 1.798456072807312, {'accuracy': 0.7382, 'data_size': 10000}, 206.19175606203498)
INFO flwr 2024-04-07 16:26:50,081 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:26:50,081 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:26:57,944 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:26:59,577 | server.py:125 | fit progress: (10, 1.751420497894287, {'accuracy': 0.8315, 'data_size': 10000}, 215.6875868460047)
INFO flwr 2024-04-07 16:26:59,577 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:26:59,577 | server.py:153 | FL finished in 215.68796315405052
INFO flwr 2024-04-07 16:26:59,577 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:26:59,577 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:26:59,577 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:26:59,577 | app.py:229 | app_fit: losses_centralized [(0, 2.3044800758361816), (1, 2.2228262424468994), (2, 2.10951828956604), (3, 2.0117783546447754), (4, 1.952752709388733), (5, 1.8974374532699585), (6, 1.84141206741333), (7, 1.8138700723648071), (8, 1.7870090007781982), (9, 1.798456072807312), (10, 1.751420497894287)]
INFO flwr 2024-04-07 16:26:59,577 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0651), (1, 0.4902), (2, 0.5838), (3, 0.5797), (4, 0.5709), (5, 0.6587), (6, 0.7864), (7, 0.7998), (8, 0.8173), (9, 0.7382), (10, 0.8315)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8315
wandb:     loss 1.75142
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_162231-dpmvxr2x
wandb: Find logs at: ./wandb/offline-run-20240407_162231-dpmvxr2x/logs
INFO flwr 2024-04-07 16:27:03,094 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:34:07,611 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2069591)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2069591)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:34:13,208	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:34:13,646	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:34:14,033	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_285a39ef0fca4237.zip' (12.26MiB) to Ray cluster...
2024-04-07 16:34:14,058	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_285a39ef0fca4237.zip'.
INFO flwr 2024-04-07 16:34:24,275 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 152358987572.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69582423244.0}
INFO flwr 2024-04-07 16:34:24,276 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:34:24,276 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:34:24,289 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:34:24,290 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:34:24,290 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:34:24,290 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:34:26,455 | server.py:94 | initial parameters (loss, other metrics): 2.3046982288360596, {'accuracy': 0.0686, 'data_size': 10000}
INFO flwr 2024-04-07 16:34:26,455 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:34:26,456 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2077695)[0m 2024-04-07 16:34:30.059364: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2077695)[0m 2024-04-07 16:34:30.150535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2077695)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2077699)[0m 2024-04-07 16:34:31.868348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2077700)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2077700)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2077703)[0m 2024-04-07 16:34:30.202872: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2077701)[0m 2024-04-07 16:34:30.251116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2077701)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2077701)[0m 2024-04-07 16:34:32.467859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:34:43,854 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:34:43,884 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:34:44,931 | server.py:125 | fit progress: (1, 2.114565849304199, {'accuracy': 0.5008, 'data_size': 10000}, 18.475996485969517)
INFO flwr 2024-04-07 16:34:44,932 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:34:44,932 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:34:53,380 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:34:54,709 | server.py:125 | fit progress: (2, 1.9889007806777954, {'accuracy': 0.6058, 'data_size': 10000}, 28.253299228963442)
INFO flwr 2024-04-07 16:34:54,709 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:34:54,709 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:02,596 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:03,993 | server.py:125 | fit progress: (3, 1.8927955627441406, {'accuracy': 0.6912, 'data_size': 10000}, 37.537407348980196)
INFO flwr 2024-04-07 16:35:03,993 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:03,993 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:11,525 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:12,941 | server.py:125 | fit progress: (4, 1.8385205268859863, {'accuracy': 0.7329, 'data_size': 10000}, 46.48546127794543)
INFO flwr 2024-04-07 16:35:12,941 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:12,941 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:21,336 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:22,544 | server.py:125 | fit progress: (5, 1.791543960571289, {'accuracy': 0.7719, 'data_size': 10000}, 56.08844858495286)
INFO flwr 2024-04-07 16:35:22,544 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:22,544 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:30,540 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:31,992 | server.py:125 | fit progress: (6, 1.7578896284103394, {'accuracy': 0.7977, 'data_size': 10000}, 65.53697083395673)
INFO flwr 2024-04-07 16:35:31,993 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:31,993 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:39,804 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:41,287 | server.py:125 | fit progress: (7, 1.7214884757995605, {'accuracy': 0.8561, 'data_size': 10000}, 74.8317696689628)
INFO flwr 2024-04-07 16:35:41,287 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:41,288 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:48,738 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:50,252 | server.py:125 | fit progress: (8, 1.7076804637908936, {'accuracy': 0.8513, 'data_size': 10000}, 83.79659295897)
INFO flwr 2024-04-07 16:35:50,252 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:50,252 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:35:57,886 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:35:59,417 | server.py:125 | fit progress: (9, 1.6930943727493286, {'accuracy': 0.8603, 'data_size': 10000}, 92.96203102596337)
INFO flwr 2024-04-07 16:35:59,418 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:35:59,418 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:36:07,243 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:36:08,568 | server.py:125 | fit progress: (10, 1.6773974895477295, {'accuracy': 0.8705, 'data_size': 10000}, 102.11263673799112)
INFO flwr 2024-04-07 16:36:08,568 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:36:08,568 | server.py:153 | FL finished in 102.11305968294619
INFO flwr 2024-04-07 16:36:08,569 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:36:08,569 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:36:08,569 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:36:08,569 | app.py:229 | app_fit: losses_centralized [(0, 2.3046982288360596), (1, 2.114565849304199), (2, 1.9889007806777954), (3, 1.8927955627441406), (4, 1.8385205268859863), (5, 1.791543960571289), (6, 1.7578896284103394), (7, 1.7214884757995605), (8, 1.7076804637908936), (9, 1.6930943727493286), (10, 1.6773974895477295)]
INFO flwr 2024-04-07 16:36:08,569 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0686), (1, 0.5008), (2, 0.6058), (3, 0.6912), (4, 0.7329), (5, 0.7719), (6, 0.7977), (7, 0.8561), (8, 0.8513), (9, 0.8603), (10, 0.8705)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8705
wandb:     loss 1.6774
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_163407-pqdc97ft
wandb: Find logs at: ./wandb/offline-run-20240407_163407-pqdc97ft/logs
INFO flwr 2024-04-07 16:36:12,064 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:43:17,033 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2077694)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2077694)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:43:23,201	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:43:23,568	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:43:23,955	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ca4bc09cb907caee.zip' (12.28MiB) to Ray cluster...
2024-04-07 16:43:23,996	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ca4bc09cb907caee.zip'.
INFO flwr 2024-04-07 16:43:34,268 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 152189581927.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69509820825.0}
INFO flwr 2024-04-07 16:43:34,269 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:43:34,269 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:43:34,284 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:43:34,284 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:43:34,285 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:43:34,285 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:43:37,452 | server.py:94 | initial parameters (loss, other metrics): 2.3008759021759033, {'accuracy': 0.1018, 'data_size': 10000}
INFO flwr 2024-04-07 16:43:37,453 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:43:37,453 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2082010)[0m 2024-04-07 16:43:39.982384: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2082020)[0m 2024-04-07 16:43:40.106203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2082020)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2082015)[0m 2024-04-07 16:43:42.051549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2082015)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2082015)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2082016)[0m 2024-04-07 16:43:40.038557: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2082016)[0m 2024-04-07 16:43:40.132195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2082016)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2082018)[0m 2024-04-07 16:43:42.405469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:43:53,809 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:43:53,845 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:43:55,110 | server.py:125 | fit progress: (1, 2.093698024749756, {'accuracy': 0.5724, 'data_size': 10000}, 17.656192137976177)
INFO flwr 2024-04-07 16:43:55,110 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:43:55,110 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:03,041 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:04,158 | server.py:125 | fit progress: (2, 1.937874436378479, {'accuracy': 0.6335, 'data_size': 10000}, 26.704539339989424)
INFO flwr 2024-04-07 16:44:04,158 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:04,158 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:11,271 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:12,649 | server.py:125 | fit progress: (3, 1.8143558502197266, {'accuracy': 0.7772, 'data_size': 10000}, 35.195320636965334)
INFO flwr 2024-04-07 16:44:12,649 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:12,649 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:20,416 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:21,581 | server.py:125 | fit progress: (4, 1.7777491807937622, {'accuracy': 0.7639, 'data_size': 10000}, 44.12735020101536)
INFO flwr 2024-04-07 16:44:21,581 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:21,581 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:29,639 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:30,818 | server.py:125 | fit progress: (5, 1.7462577819824219, {'accuracy': 0.7927, 'data_size': 10000}, 53.36481472401647)
INFO flwr 2024-04-07 16:44:30,818 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:30,819 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:38,150 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:39,754 | server.py:125 | fit progress: (6, 1.6991498470306396, {'accuracy': 0.8572, 'data_size': 10000}, 62.30087461799849)
INFO flwr 2024-04-07 16:44:39,755 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:39,755 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:47,073 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:48,583 | server.py:125 | fit progress: (7, 1.6978720426559448, {'accuracy': 0.8391, 'data_size': 10000}, 71.1297932100133)
INFO flwr 2024-04-07 16:44:48,583 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:48,584 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:44:55,711 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:44:57,201 | server.py:125 | fit progress: (8, 1.6674165725708008, {'accuracy': 0.8778, 'data_size': 10000}, 79.74754961999133)
INFO flwr 2024-04-07 16:44:57,201 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:44:57,201 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:45:04,814 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:45:06,122 | server.py:125 | fit progress: (9, 1.656587839126587, {'accuracy': 0.8766, 'data_size': 10000}, 88.66830764699262)
INFO flwr 2024-04-07 16:45:06,122 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:45:06,122 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:45:13,440 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:45:14,979 | server.py:125 | fit progress: (10, 1.6569478511810303, {'accuracy': 0.8643, 'data_size': 10000}, 97.5260923189926)
INFO flwr 2024-04-07 16:45:14,980 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:45:14,980 | server.py:153 | FL finished in 97.5264952389989
INFO flwr 2024-04-07 16:45:14,980 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:45:14,980 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:45:14,980 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:45:14,980 | app.py:229 | app_fit: losses_centralized [(0, 2.3008759021759033), (1, 2.093698024749756), (2, 1.937874436378479), (3, 1.8143558502197266), (4, 1.7777491807937622), (5, 1.7462577819824219), (6, 1.6991498470306396), (7, 1.6978720426559448), (8, 1.6674165725708008), (9, 1.656587839126587), (10, 1.6569478511810303)]
INFO flwr 2024-04-07 16:45:14,980 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1018), (1, 0.5724), (2, 0.6335), (3, 0.7772), (4, 0.7639), (5, 0.7927), (6, 0.8572), (7, 0.8391), (8, 0.8778), (9, 0.8766), (10, 0.8643)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8643
wandb:     loss 1.65695
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_164316-j42pvi70
wandb: Find logs at: ./wandb/offline-run-20240407_164316-j42pvi70/logs
INFO flwr 2024-04-07 16:45:18,512 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 16:52:23,676 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2082010)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2082010)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 16:52:28,932	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 16:52:29,354	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 16:52:29,842	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_a1f1de8f35b6ea82.zip' (12.29MiB) to Ray cluster...
2024-04-07 16:52:29,878	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_a1f1de8f35b6ea82.zip'.
INFO flwr 2024-04-07 16:52:40,028 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 152132120372.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 69485194444.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 16:52:40,029 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 16:52:40,029 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 16:52:40,044 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 16:52:40,045 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 16:52:40,046 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 16:52:40,046 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 16:52:43,714 | server.py:94 | initial parameters (loss, other metrics): 2.305657386779785, {'accuracy': 0.0792, 'data_size': 10000}
INFO flwr 2024-04-07 16:52:43,715 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 16:52:43,715 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2085726)[0m 2024-04-07 16:52:45.699400: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2085726)[0m 2024-04-07 16:52:45.790853: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2085726)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2085726)[0m 2024-04-07 16:52:47.709618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2085726)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2085726)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2085727)[0m 2024-04-07 16:52:45.858393: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2085727)[0m 2024-04-07 16:52:45.961360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2085727)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2085727)[0m 2024-04-07 16:52:47.892693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 16:52:59,325 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 16:52:59,361 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 16:53:00,387 | server.py:125 | fit progress: (1, 2.086960792541504, {'accuracy': 0.6474, 'data_size': 10000}, 16.67203656298807)
INFO flwr 2024-04-07 16:53:00,387 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:00,388 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:08,917 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:10,028 | server.py:125 | fit progress: (2, 1.8833644390106201, {'accuracy': 0.6889, 'data_size': 10000}, 26.312696954002604)
INFO flwr 2024-04-07 16:53:10,028 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:10,028 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:17,975 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:19,316 | server.py:125 | fit progress: (3, 1.7672624588012695, {'accuracy': 0.8136, 'data_size': 10000}, 35.60130521899555)
INFO flwr 2024-04-07 16:53:19,317 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:19,317 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:27,081 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:28,495 | server.py:125 | fit progress: (4, 1.7122771739959717, {'accuracy': 0.8582, 'data_size': 10000}, 44.779812674969435)
INFO flwr 2024-04-07 16:53:28,495 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:28,495 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:36,336 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:37,708 | server.py:125 | fit progress: (5, 1.6946642398834229, {'accuracy': 0.8524, 'data_size': 10000}, 53.99301620700862)
INFO flwr 2024-04-07 16:53:37,708 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:37,709 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:45,137 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:46,299 | server.py:125 | fit progress: (6, 1.6728763580322266, {'accuracy': 0.8594, 'data_size': 10000}, 62.58363194297999)
INFO flwr 2024-04-07 16:53:46,299 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:46,299 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:53:54,088 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 16:53:55,499 | server.py:125 | fit progress: (7, 1.6582564115524292, {'accuracy': 0.8747, 'data_size': 10000}, 71.7836215099669)
INFO flwr 2024-04-07 16:53:55,499 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 16:53:55,499 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:54:03,205 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 16:54:04,669 | server.py:125 | fit progress: (8, 1.6490384340286255, {'accuracy': 0.8755, 'data_size': 10000}, 80.95421956200153)
INFO flwr 2024-04-07 16:54:04,670 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 16:54:04,670 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:54:12,910 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 16:54:14,402 | server.py:125 | fit progress: (9, 1.6380491256713867, {'accuracy': 0.88, 'data_size': 10000}, 90.68694234499708)
INFO flwr 2024-04-07 16:54:14,402 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 16:54:14,402 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 16:54:22,065 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 16:54:23,374 | server.py:125 | fit progress: (10, 1.6350271701812744, {'accuracy': 0.8815, 'data_size': 10000}, 99.65919479395961)
INFO flwr 2024-04-07 16:54:23,375 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 16:54:23,375 | server.py:153 | FL finished in 99.65961105795577
INFO flwr 2024-04-07 16:54:23,375 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 16:54:23,375 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 16:54:23,375 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 16:54:23,375 | app.py:229 | app_fit: losses_centralized [(0, 2.305657386779785), (1, 2.086960792541504), (2, 1.8833644390106201), (3, 1.7672624588012695), (4, 1.7122771739959717), (5, 1.6946642398834229), (6, 1.6728763580322266), (7, 1.6582564115524292), (8, 1.6490384340286255), (9, 1.6380491256713867), (10, 1.6350271701812744)]
INFO flwr 2024-04-07 16:54:23,375 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0792), (1, 0.6474), (2, 0.6889), (3, 0.8136), (4, 0.8582), (5, 0.8524), (6, 0.8594), (7, 0.8747), (8, 0.8755), (9, 0.88), (10, 0.8815)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8815
wandb:     loss 1.63503
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_165223-k4eruwfo
wandb: Find logs at: ./wandb/offline-run-20240407_165223-k4eruwfo/logs
INFO flwr 2024-04-07 16:54:26,887 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:01:31,802 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2085719)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2085719)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:01:37,707	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:01:38,019	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:01:38,440	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_00954fc5f9f7583a.zip' (12.30MiB) to Ray cluster...
2024-04-07 17:01:38,485	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_00954fc5f9f7583a.zip'.
INFO flwr 2024-04-07 17:01:48,707 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 152067803341.0, 'object_store_memory': 69457630003.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 17:01:48,707 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:01:48,707 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:01:48,723 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:01:48,724 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:01:48,724 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:01:48,724 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:01:51,146 | server.py:94 | initial parameters (loss, other metrics): 2.29984712600708, {'accuracy': 0.1492, 'data_size': 10000}
INFO flwr 2024-04-07 17:01:51,147 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:01:51,147 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2090304)[0m 2024-04-07 17:01:54.375125: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2090304)[0m 2024-04-07 17:01:54.490966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2090304)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2090297)[0m 2024-04-07 17:01:56.504580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2090304)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2090304)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2090300)[0m 2024-04-07 17:01:54.640043: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2090300)[0m 2024-04-07 17:01:54.734643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2090300)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2090300)[0m 2024-04-07 17:01:56.697939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:02:08,386 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:02:08,416 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:02:09,678 | server.py:125 | fit progress: (1, 2.029099225997925, {'accuracy': 0.5991, 'data_size': 10000}, 18.531342778005637)
INFO flwr 2024-04-07 17:02:09,679 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:09,679 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:02:18,183 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:02:19,284 | server.py:125 | fit progress: (2, 1.8334906101226807, {'accuracy': 0.7523, 'data_size': 10000}, 28.136666238016915)
INFO flwr 2024-04-07 17:02:19,284 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:19,284 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:02:26,992 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:02:28,314 | server.py:125 | fit progress: (3, 1.7428195476531982, {'accuracy': 0.8322, 'data_size': 10000}, 37.167347720009275)
INFO flwr 2024-04-07 17:02:28,315 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:28,315 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:02:35,745 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:02:37,100 | server.py:125 | fit progress: (4, 1.7110625505447388, {'accuracy': 0.8356, 'data_size': 10000}, 45.95305934600765)
INFO flwr 2024-04-07 17:02:37,100 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:37,101 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:02:44,443 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:02:45,838 | server.py:125 | fit progress: (5, 1.6767507791519165, {'accuracy': 0.8623, 'data_size': 10000}, 54.69102764001582)
INFO flwr 2024-04-07 17:02:45,838 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:45,839 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:02:53,288 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:02:54,694 | server.py:125 | fit progress: (6, 1.6591157913208008, {'accuracy': 0.8746, 'data_size': 10000}, 63.547324534039944)
INFO flwr 2024-04-07 17:02:54,695 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:02:54,695 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:03:02,238 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:03:03,463 | server.py:125 | fit progress: (7, 1.6457617282867432, {'accuracy': 0.8808, 'data_size': 10000}, 72.31623100303113)
INFO flwr 2024-04-07 17:03:03,464 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:03:03,464 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:03:11,451 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:03:12,720 | server.py:125 | fit progress: (8, 1.6354029178619385, {'accuracy': 0.8878, 'data_size': 10000}, 81.573209544993)
INFO flwr 2024-04-07 17:03:12,721 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:03:12,721 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:03:20,328 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:03:21,811 | server.py:125 | fit progress: (9, 1.6278769969940186, {'accuracy': 0.8894, 'data_size': 10000}, 90.66391898301663)
INFO flwr 2024-04-07 17:03:21,811 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:03:21,812 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:03:29,633 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:03:31,255 | server.py:125 | fit progress: (10, 1.628990888595581, {'accuracy': 0.8859, 'data_size': 10000}, 100.10804990603356)
INFO flwr 2024-04-07 17:03:31,255 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:03:31,255 | server.py:153 | FL finished in 100.10840892099077
INFO flwr 2024-04-07 17:03:31,256 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:03:31,256 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:03:31,256 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:03:31,256 | app.py:229 | app_fit: losses_centralized [(0, 2.29984712600708), (1, 2.029099225997925), (2, 1.8334906101226807), (3, 1.7428195476531982), (4, 1.7110625505447388), (5, 1.6767507791519165), (6, 1.6591157913208008), (7, 1.6457617282867432), (8, 1.6354029178619385), (9, 1.6278769969940186), (10, 1.628990888595581)]
INFO flwr 2024-04-07 17:03:31,256 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1492), (1, 0.5991), (2, 0.7523), (3, 0.8322), (4, 0.8356), (5, 0.8623), (6, 0.8746), (7, 0.8808), (8, 0.8878), (9, 0.8894), (10, 0.8859)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8859
wandb:     loss 1.62899
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_170131-4bdszsgj
wandb: Find logs at: ./wandb/offline-run-20240407_170131-4bdszsgj/logs
INFO flwr 2024-04-07 17:03:34,820 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 5
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:10:39,819 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2090297)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2090297)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:10:45,195	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:10:45,665	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:10:46,474	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_f2148727a7518cf9.zip' (12.31MiB) to Ray cluster...
2024-04-07 17:10:46,502	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_f2148727a7518cf9.zip'.
INFO flwr 2024-04-07 17:10:58,193 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 69381414912.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 151889968128.0}
INFO flwr 2024-04-07 17:10:58,193 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:10:58,194 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:10:58,211 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:10:58,212 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:10:58,213 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:10:58,213 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:11:00,897 | server.py:94 | initial parameters (loss, other metrics): 2.302729606628418, {'accuracy': 0.0953, 'data_size': 10000}
INFO flwr 2024-04-07 17:11:00,898 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:11:00,898 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2094598)[0m 2024-04-07 17:11:06.470293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2094597)[0m 2024-04-07 17:11:06.563264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2094597)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2094599)[0m 2024-04-07 17:11:09.549840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2094598)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2094598)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2094600)[0m 2024-04-07 17:11:06.633145: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2094596)[0m 2024-04-07 17:11:06.795201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2094596)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2094596)[0m 2024-04-07 17:11:09.716156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:11:22,574 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:11:22,666 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:11:23,885 | server.py:125 | fit progress: (1, 2.03281569480896, {'accuracy': 0.4733, 'data_size': 10000}, 22.987340732011944)
INFO flwr 2024-04-07 17:11:23,885 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:11:23,885 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:11:32,284 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:11:33,345 | server.py:125 | fit progress: (2, 1.8657418489456177, {'accuracy': 0.6493, 'data_size': 10000}, 32.447726761049125)
INFO flwr 2024-04-07 17:11:33,346 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:11:33,346 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:11:40,706 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:11:42,085 | server.py:125 | fit progress: (3, 1.7570013999938965, {'accuracy': 0.7959, 'data_size': 10000}, 41.18748126004357)
INFO flwr 2024-04-07 17:11:42,085 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:11:42,086 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:11:49,185 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:11:50,521 | server.py:125 | fit progress: (4, 1.6957857608795166, {'accuracy': 0.8622, 'data_size': 10000}, 49.62346220301697)
INFO flwr 2024-04-07 17:11:50,521 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:11:50,522 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:11:57,920 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:11:59,283 | server.py:125 | fit progress: (5, 1.6599767208099365, {'accuracy': 0.8757, 'data_size': 10000}, 58.385532219021115)
INFO flwr 2024-04-07 17:11:59,283 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:11:59,284 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:12:06,424 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:12:07,610 | server.py:125 | fit progress: (6, 1.6494009494781494, {'accuracy': 0.8805, 'data_size': 10000}, 66.71213217102922)
INFO flwr 2024-04-07 17:12:07,610 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:12:07,610 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:12:15,211 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:12:16,624 | server.py:125 | fit progress: (7, 1.6547209024429321, {'accuracy': 0.8654, 'data_size': 10000}, 75.72621286503272)
INFO flwr 2024-04-07 17:12:16,624 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:12:16,624 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:12:24,253 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:12:25,745 | server.py:125 | fit progress: (8, 1.6364182233810425, {'accuracy': 0.8787, 'data_size': 10000}, 84.84707940102089)
INFO flwr 2024-04-07 17:12:25,745 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:12:25,745 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:12:33,307 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:12:34,570 | server.py:125 | fit progress: (9, 1.6211552619934082, {'accuracy': 0.8906, 'data_size': 10000}, 93.67229165305616)
INFO flwr 2024-04-07 17:12:34,570 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:12:34,570 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:12:42,258 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:12:43,566 | server.py:125 | fit progress: (10, 1.6315897703170776, {'accuracy': 0.8801, 'data_size': 10000}, 102.66797915200004)
INFO flwr 2024-04-07 17:12:43,566 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:12:43,566 | server.py:153 | FL finished in 102.66837746201782
INFO flwr 2024-04-07 17:12:43,566 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:12:43,566 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:12:43,566 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:12:43,566 | app.py:229 | app_fit: losses_centralized [(0, 2.302729606628418), (1, 2.03281569480896), (2, 1.8657418489456177), (3, 1.7570013999938965), (4, 1.6957857608795166), (5, 1.6599767208099365), (6, 1.6494009494781494), (7, 1.6547209024429321), (8, 1.6364182233810425), (9, 1.6211552619934082), (10, 1.6315897703170776)]
INFO flwr 2024-04-07 17:12:43,567 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0953), (1, 0.4733), (2, 0.6493), (3, 0.7959), (4, 0.8622), (5, 0.8757), (6, 0.8805), (7, 0.8654), (8, 0.8787), (9, 0.8906), (10, 0.8801)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8801
wandb:     loss 1.63159
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_171039-gdm2eb39
wandb: Find logs at: ./wandb/offline-run-20240407_171039-gdm2eb39/logs
INFO flwr 2024-04-07 17:12:47,072 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:19:52,120 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2094600)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2094600)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:19:57,076	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:19:57,391	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:19:57,810	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_fdfafda385568665.zip' (12.32MiB) to Ray cluster...
2024-04-07 17:19:57,835	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_fdfafda385568665.zip'.
INFO flwr 2024-04-07 17:20:08,100 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 151821562471.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69352098201.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 17:20:08,100 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:20:08,100 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:20:08,116 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:20:08,117 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:20:08,117 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:20:08,117 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:20:11,142 | server.py:94 | initial parameters (loss, other metrics): 2.3007566928863525, {'accuracy': 0.1159, 'data_size': 10000}
INFO flwr 2024-04-07 17:20:11,143 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:20:11,143 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2098908)[0m 2024-04-07 17:20:13.787314: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2098908)[0m 2024-04-07 17:20:13.880804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2098908)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2098912)[0m 2024-04-07 17:20:15.888813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2098913)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2098913)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2098906)[0m 2024-04-07 17:20:14.120076: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2098906)[0m 2024-04-07 17:20:14.213120: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2098906)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2098913)[0m 2024-04-07 17:20:16.021489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:20:27,771 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:20:27,811 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:20:29,099 | server.py:125 | fit progress: (1, 2.2983670234680176, {'accuracy': 0.1459, 'data_size': 10000}, 17.955831439001486)
INFO flwr 2024-04-07 17:20:29,099 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:20:29,100 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:20:37,783 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:20:39,160 | server.py:125 | fit progress: (2, 2.2954742908477783, {'accuracy': 0.1874, 'data_size': 10000}, 28.017057566030417)
INFO flwr 2024-04-07 17:20:39,160 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:20:39,161 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:20:46,751 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:20:47,893 | server.py:125 | fit progress: (3, 2.2925381660461426, {'accuracy': 0.2268, 'data_size': 10000}, 36.75049841002328)
INFO flwr 2024-04-07 17:20:47,894 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:20:47,894 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:20:55,550 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:20:56,941 | server.py:125 | fit progress: (4, 2.289923906326294, {'accuracy': 0.252, 'data_size': 10000}, 45.79811344202608)
INFO flwr 2024-04-07 17:20:56,941 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:20:56,942 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:04,818 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:06,244 | server.py:125 | fit progress: (5, 2.286992073059082, {'accuracy': 0.2727, 'data_size': 10000}, 55.101060312998015)
INFO flwr 2024-04-07 17:21:06,244 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:21:06,245 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:13,558 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:14,988 | server.py:125 | fit progress: (6, 2.2839598655700684, {'accuracy': 0.3161, 'data_size': 10000}, 63.84482363500865)
INFO flwr 2024-04-07 17:21:14,988 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:21:14,988 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:23,090 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:24,682 | server.py:125 | fit progress: (7, 2.2793920040130615, {'accuracy': 0.3204, 'data_size': 10000}, 73.5388592230156)
INFO flwr 2024-04-07 17:21:24,682 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:21:24,682 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:32,684 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:33,965 | server.py:125 | fit progress: (8, 2.2746596336364746, {'accuracy': 0.3066, 'data_size': 10000}, 82.822061614017)
INFO flwr 2024-04-07 17:21:33,965 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:21:33,966 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:41,632 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:42,965 | server.py:125 | fit progress: (9, 2.2699434757232666, {'accuracy': 0.2999, 'data_size': 10000}, 91.82158592803171)
INFO flwr 2024-04-07 17:21:42,965 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:21:42,965 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:21:50,517 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:21:52,096 | server.py:125 | fit progress: (10, 2.265122413635254, {'accuracy': 0.3031, 'data_size': 10000}, 100.95255452004494)
INFO flwr 2024-04-07 17:21:52,096 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:21:52,096 | server.py:153 | FL finished in 100.9529047520482
INFO flwr 2024-04-07 17:21:52,096 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:21:52,096 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:21:52,096 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:21:52,096 | app.py:229 | app_fit: losses_centralized [(0, 2.3007566928863525), (1, 2.2983670234680176), (2, 2.2954742908477783), (3, 2.2925381660461426), (4, 2.289923906326294), (5, 2.286992073059082), (6, 2.2839598655700684), (7, 2.2793920040130615), (8, 2.2746596336364746), (9, 2.2699434757232666), (10, 2.265122413635254)]
INFO flwr 2024-04-07 17:21:52,096 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1159), (1, 0.1459), (2, 0.1874), (3, 0.2268), (4, 0.252), (5, 0.2727), (6, 0.3161), (7, 0.3204), (8, 0.3066), (9, 0.2999), (10, 0.3031)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.3031
wandb:     loss 2.26512
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_171951-ek6g3xfk
wandb: Find logs at: ./wandb/offline-run-20240407_171951-ek6g3xfk/logs
INFO flwr 2024-04-07 17:21:55,618 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:29:01,005 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2098906)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2098906)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:29:05,839	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:29:06,228	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:29:06,600	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c0fe26cfd35229a4.zip' (12.34MiB) to Ray cluster...
2024-04-07 17:29:06,644	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c0fe26cfd35229a4.zip'.
INFO flwr 2024-04-07 17:29:17,116 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151647056077.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69277309747.0, 'node:10.20.240.18': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-04-07 17:29:17,117 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:29:17,117 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:29:17,133 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:29:17,134 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:29:17,134 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:29:17,134 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:29:19,975 | server.py:94 | initial parameters (loss, other metrics): 2.304020404815674, {'accuracy': 0.0858, 'data_size': 10000}
INFO flwr 2024-04-07 17:29:19,976 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:29:19,976 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2103473)[0m 2024-04-07 17:29:22.919835: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2103473)[0m 2024-04-07 17:29:23.010975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2103473)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2103473)[0m 2024-04-07 17:29:24.949025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2103485)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2103485)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2103477)[0m 2024-04-07 17:29:23.252030: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2103477)[0m 2024-04-07 17:29:23.345512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2103477)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2103477)[0m 2024-04-07 17:29:25.366345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:29:36,927 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:29:36,956 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:29:38,184 | server.py:125 | fit progress: (1, 2.1645607948303223, {'accuracy': 0.476, 'data_size': 10000}, 18.2083284999826)
INFO flwr 2024-04-07 17:29:38,185 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:29:38,185 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:29:46,360 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:29:47,681 | server.py:125 | fit progress: (2, 1.992871880531311, {'accuracy': 0.6699, 'data_size': 10000}, 27.704460722976364)
INFO flwr 2024-04-07 17:29:47,681 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:29:47,681 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:29:55,155 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:29:56,259 | server.py:125 | fit progress: (3, 1.862673044204712, {'accuracy': 0.7831, 'data_size': 10000}, 36.28280750202248)
INFO flwr 2024-04-07 17:29:56,259 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:29:56,259 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:04,185 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:05,556 | server.py:125 | fit progress: (4, 1.8015587329864502, {'accuracy': 0.8052, 'data_size': 10000}, 45.580204715020955)
INFO flwr 2024-04-07 17:30:05,557 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:05,557 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:13,059 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:14,212 | server.py:125 | fit progress: (5, 1.7624542713165283, {'accuracy': 0.8114, 'data_size': 10000}, 54.23620500502875)
INFO flwr 2024-04-07 17:30:14,213 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:14,213 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:22,147 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:23,376 | server.py:125 | fit progress: (6, 1.7450443506240845, {'accuracy': 0.8233, 'data_size': 10000}, 63.39985406800406)
INFO flwr 2024-04-07 17:30:23,376 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:23,376 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:31,122 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:32,538 | server.py:125 | fit progress: (7, 1.7098801136016846, {'accuracy': 0.8526, 'data_size': 10000}, 72.56179309799336)
INFO flwr 2024-04-07 17:30:32,538 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:32,538 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:39,999 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:41,513 | server.py:125 | fit progress: (8, 1.6945916414260864, {'accuracy': 0.8576, 'data_size': 10000}, 81.5364614460268)
INFO flwr 2024-04-07 17:30:41,513 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:41,513 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:49,657 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:30:51,202 | server.py:125 | fit progress: (9, 1.688840389251709, {'accuracy': 0.8574, 'data_size': 10000}, 91.22575894702459)
INFO flwr 2024-04-07 17:30:51,202 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:30:51,202 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:30:58,690 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:31:00,034 | server.py:125 | fit progress: (10, 1.6762492656707764, {'accuracy': 0.8629, 'data_size': 10000}, 100.05805988097563)
INFO flwr 2024-04-07 17:31:00,034 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:31:00,035 | server.py:153 | FL finished in 100.05849333602237
INFO flwr 2024-04-07 17:31:00,035 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:31:00,035 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:31:00,035 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:31:00,035 | app.py:229 | app_fit: losses_centralized [(0, 2.304020404815674), (1, 2.1645607948303223), (2, 1.992871880531311), (3, 1.862673044204712), (4, 1.8015587329864502), (5, 1.7624542713165283), (6, 1.7450443506240845), (7, 1.7098801136016846), (8, 1.6945916414260864), (9, 1.688840389251709), (10, 1.6762492656707764)]
INFO flwr 2024-04-07 17:31:00,035 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0858), (1, 0.476), (2, 0.6699), (3, 0.7831), (4, 0.8052), (5, 0.8114), (6, 0.8233), (7, 0.8526), (8, 0.8576), (9, 0.8574), (10, 0.8629)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8629
wandb:     loss 1.67625
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_172900-dew97kl7
wandb: Find logs at: ./wandb/offline-run-20240407_172900-dew97kl7/logs
INFO flwr 2024-04-07 17:31:03,596 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:38:08,470 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2103473)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2103473)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:38:13,660	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:38:14,160	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:38:14,614	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_0a593aafb80d3038.zip' (12.35MiB) to Ray cluster...
2024-04-07 17:38:14,646	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_0a593aafb80d3038.zip'.
INFO flwr 2024-04-07 17:38:25,866 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 69328779264.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'memory': 151767151616.0}
INFO flwr 2024-04-07 17:38:25,866 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:38:25,866 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:38:25,882 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:38:25,884 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:38:25,885 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:38:25,885 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:38:28,217 | server.py:94 | initial parameters (loss, other metrics): 2.303539514541626, {'accuracy': 0.0995, 'data_size': 10000}
INFO flwr 2024-04-07 17:38:28,218 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:38:28,218 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2107189)[0m 2024-04-07 17:38:59.936055: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2107189)[0m 2024-04-07 17:39:00.029731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2107189)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2107196)[0m 2024-04-07 17:39:01.771208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2107189)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2107189)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2107183)[0m 2024-04-07 17:38:59.984018: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2107183)[0m 2024-04-07 17:39:00.077464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2107183)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2107183)[0m 2024-04-07 17:39:01.826609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:39:13,654 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:39:13,694 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:39:14,973 | server.py:125 | fit progress: (1, 2.0475215911865234, {'accuracy': 0.5398, 'data_size': 10000}, 46.75486683502095)
INFO flwr 2024-04-07 17:39:14,973 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:39:14,973 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:39:23,385 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:39:24,498 | server.py:125 | fit progress: (2, 1.8849517107009888, {'accuracy': 0.7165, 'data_size': 10000}, 56.28047127497848)
INFO flwr 2024-04-07 17:39:24,499 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:39:24,499 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:39:32,592 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:39:33,959 | server.py:125 | fit progress: (3, 1.808729887008667, {'accuracy': 0.7721, 'data_size': 10000}, 65.74152466800297)
INFO flwr 2024-04-07 17:39:33,960 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:39:33,960 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:39:41,933 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:39:43,119 | server.py:125 | fit progress: (4, 1.7230521440505981, {'accuracy': 0.8478, 'data_size': 10000}, 74.90088345500408)
INFO flwr 2024-04-07 17:39:43,119 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:39:43,119 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:39:51,035 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:39:52,232 | server.py:125 | fit progress: (5, 1.691683053970337, {'accuracy': 0.8626, 'data_size': 10000}, 84.01381162699545)
INFO flwr 2024-04-07 17:39:52,232 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:39:52,232 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:40:00,185 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:40:01,636 | server.py:125 | fit progress: (6, 1.6817400455474854, {'accuracy': 0.86, 'data_size': 10000}, 93.41775938001228)
INFO flwr 2024-04-07 17:40:01,636 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:40:01,636 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:40:09,398 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:40:10,982 | server.py:125 | fit progress: (7, 1.6633793115615845, {'accuracy': 0.869, 'data_size': 10000}, 102.76439677301096)
INFO flwr 2024-04-07 17:40:10,983 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:40:10,983 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:40:18,839 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:40:20,351 | server.py:125 | fit progress: (8, 1.655463695526123, {'accuracy': 0.8733, 'data_size': 10000}, 112.1332583699841)
INFO flwr 2024-04-07 17:40:20,351 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:40:20,352 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:40:28,542 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:40:29,883 | server.py:125 | fit progress: (9, 1.6466002464294434, {'accuracy': 0.878, 'data_size': 10000}, 121.66500555799576)
INFO flwr 2024-04-07 17:40:29,883 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:40:29,883 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:40:37,484 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:40:39,047 | server.py:125 | fit progress: (10, 1.6329783201217651, {'accuracy': 0.8881, 'data_size': 10000}, 130.82882670802064)
INFO flwr 2024-04-07 17:40:39,047 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:40:39,047 | server.py:153 | FL finished in 130.82922408601735
INFO flwr 2024-04-07 17:40:39,047 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:40:39,047 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:40:39,047 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:40:39,048 | app.py:229 | app_fit: losses_centralized [(0, 2.303539514541626), (1, 2.0475215911865234), (2, 1.8849517107009888), (3, 1.808729887008667), (4, 1.7230521440505981), (5, 1.691683053970337), (6, 1.6817400455474854), (7, 1.6633793115615845), (8, 1.655463695526123), (9, 1.6466002464294434), (10, 1.6329783201217651)]
INFO flwr 2024-04-07 17:40:39,048 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0995), (1, 0.5398), (2, 0.7165), (3, 0.7721), (4, 0.8478), (5, 0.8626), (6, 0.86), (7, 0.869), (8, 0.8733), (9, 0.878), (10, 0.8881)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8881
wandb:     loss 1.63298
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_173808-18xzo97k
wandb: Find logs at: ./wandb/offline-run-20240407_173808-18xzo97k/logs
INFO flwr 2024-04-07 17:40:42,612 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:47:47,430 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2107188)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2107188)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:47:52,684	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:47:53,119	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:47:53,457	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_c9fd55c8891cc8c9.zip' (12.36MiB) to Ray cluster...
2024-04-07 17:47:53,493	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_c9fd55c8891cc8c9.zip'.
INFO flwr 2024-04-07 17:48:03,843 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'object_store_memory': 69316056268.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 151737464628.0}
INFO flwr 2024-04-07 17:48:03,843 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:48:03,843 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:48:03,863 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:48:03,864 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:48:03,864 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:48:03,865 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:48:06,272 | server.py:94 | initial parameters (loss, other metrics): 2.3037075996398926, {'accuracy': 0.1014, 'data_size': 10000}
INFO flwr 2024-04-07 17:48:06,272 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:48:06,272 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2111560)[0m 2024-04-07 17:48:09.621270: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2111560)[0m 2024-04-07 17:48:09.713808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2111560)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2111560)[0m 2024-04-07 17:48:11.700415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2111566)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2111566)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2111561)[0m 2024-04-07 17:48:09.801813: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2111561)[0m 2024-04-07 17:48:09.896607: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2111561)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2111561)[0m 2024-04-07 17:48:11.918815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:48:23,408 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:48:23,443 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:48:24,521 | server.py:125 | fit progress: (1, 2.0492236614227295, {'accuracy': 0.5774, 'data_size': 10000}, 18.248793778999243)
INFO flwr 2024-04-07 17:48:24,521 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:48:24,522 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:48:33,272 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:48:34,363 | server.py:125 | fit progress: (2, 1.8640398979187012, {'accuracy': 0.7209, 'data_size': 10000}, 28.090616952977143)
INFO flwr 2024-04-07 17:48:34,363 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:48:34,363 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:48:42,619 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:48:44,017 | server.py:125 | fit progress: (3, 1.7255247831344604, {'accuracy': 0.8636, 'data_size': 10000}, 37.7451213549939)
INFO flwr 2024-04-07 17:48:44,018 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:48:44,018 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:48:51,281 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:48:52,635 | server.py:125 | fit progress: (4, 1.6807531118392944, {'accuracy': 0.8756, 'data_size': 10000}, 46.362271932011936)
INFO flwr 2024-04-07 17:48:52,635 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:48:52,635 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:00,460 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:01,896 | server.py:125 | fit progress: (5, 1.6731467247009277, {'accuracy': 0.8534, 'data_size': 10000}, 55.62332085496746)
INFO flwr 2024-04-07 17:49:01,896 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:49:01,896 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:09,618 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:10,825 | server.py:125 | fit progress: (6, 1.6466633081436157, {'accuracy': 0.8806, 'data_size': 10000}, 64.55305019399384)
INFO flwr 2024-04-07 17:49:10,826 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:49:10,826 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:18,685 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:20,193 | server.py:125 | fit progress: (7, 1.64151132106781, {'accuracy': 0.8751, 'data_size': 10000}, 73.92079790198477)
INFO flwr 2024-04-07 17:49:20,193 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:49:20,194 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:27,867 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:29,338 | server.py:125 | fit progress: (8, 1.6287343502044678, {'accuracy': 0.8874, 'data_size': 10000}, 83.06539796898142)
INFO flwr 2024-04-07 17:49:29,338 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:49:29,338 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:37,247 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:38,782 | server.py:125 | fit progress: (9, 1.6202448606491089, {'accuracy': 0.8939, 'data_size': 10000}, 92.50941841700114)
INFO flwr 2024-04-07 17:49:38,782 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:49:38,782 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:49:46,443 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:49:47,770 | server.py:125 | fit progress: (10, 1.6286382675170898, {'accuracy': 0.8792, 'data_size': 10000}, 101.4977125179721)
INFO flwr 2024-04-07 17:49:47,770 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:49:47,770 | server.py:153 | FL finished in 101.49810356699163
INFO flwr 2024-04-07 17:49:47,771 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:49:47,771 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:49:47,771 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:49:47,771 | app.py:229 | app_fit: losses_centralized [(0, 2.3037075996398926), (1, 2.0492236614227295), (2, 1.8640398979187012), (3, 1.7255247831344604), (4, 1.6807531118392944), (5, 1.6731467247009277), (6, 1.6466633081436157), (7, 1.64151132106781), (8, 1.6287343502044678), (9, 1.6202448606491089), (10, 1.6286382675170898)]
INFO flwr 2024-04-07 17:49:47,771 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1014), (1, 0.5774), (2, 0.7209), (3, 0.8636), (4, 0.8756), (5, 0.8534), (6, 0.8806), (7, 0.8751), (8, 0.8874), (9, 0.8939), (10, 0.8792)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8792
wandb:     loss 1.62864
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_174747-bxbgpd3t
wandb: Find logs at: ./wandb/offline-run-20240407_174747-bxbgpd3t/logs
INFO flwr 2024-04-07 17:49:51,276 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 17:56:56,048 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2111556)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2111556)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 17:57:02,236	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 17:57:02,586	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 17:57:02,906	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d158709a1b4e50e1.zip' (12.37MiB) to Ray cluster...
2024-04-07 17:57:02,940	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d158709a1b4e50e1.zip'.
INFO flwr 2024-04-07 17:57:13,226 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69304283136.0, 'CPU': 64.0, 'memory': 151709993984.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 17:57:13,226 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 17:57:13,226 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 17:57:13,246 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 17:57:13,247 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 17:57:13,247 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 17:57:13,247 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 17:57:15,696 | server.py:94 | initial parameters (loss, other metrics): 2.3023548126220703, {'accuracy': 0.0996, 'data_size': 10000}
INFO flwr 2024-04-07 17:57:15,696 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 17:57:15,697 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2116341)[0m 2024-04-07 17:57:18.991418: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2116337)[0m 2024-04-07 17:57:19.133117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2116337)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2116337)[0m 2024-04-07 17:57:21.047343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2116337)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2116337)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2116335)[0m 2024-04-07 17:57:19.515556: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2116335)[0m 2024-04-07 17:57:19.622470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2116335)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2116335)[0m 2024-04-07 17:57:21.656985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 17:57:32,835 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 17:57:32,866 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 17:57:34,145 | server.py:125 | fit progress: (1, 2.0400338172912598, {'accuracy': 0.5457, 'data_size': 10000}, 18.448332099011168)
INFO flwr 2024-04-07 17:57:34,145 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 17:57:34,145 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:57:42,709 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 17:57:44,028 | server.py:125 | fit progress: (2, 1.7932666540145874, {'accuracy': 0.7512, 'data_size': 10000}, 28.33165305998409)
INFO flwr 2024-04-07 17:57:44,028 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 17:57:44,029 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:57:51,763 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 17:57:52,910 | server.py:125 | fit progress: (3, 1.6938730478286743, {'accuracy': 0.851, 'data_size': 10000}, 37.213831252011005)
INFO flwr 2024-04-07 17:57:52,911 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 17:57:52,911 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:00,475 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:01,652 | server.py:125 | fit progress: (4, 1.6636213064193726, {'accuracy': 0.8705, 'data_size': 10000}, 45.95586989400908)
INFO flwr 2024-04-07 17:58:01,653 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:01,653 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:09,715 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:11,161 | server.py:125 | fit progress: (5, 1.6633156538009644, {'accuracy': 0.8521, 'data_size': 10000}, 55.46454424300464)
INFO flwr 2024-04-07 17:58:11,161 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:11,162 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:18,577 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:19,998 | server.py:125 | fit progress: (6, 1.6441421508789062, {'accuracy': 0.8734, 'data_size': 10000}, 64.3012474119896)
INFO flwr 2024-04-07 17:58:19,998 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:19,998 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:27,776 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:29,250 | server.py:125 | fit progress: (7, 1.6239556074142456, {'accuracy': 0.8835, 'data_size': 10000}, 73.5533415019745)
INFO flwr 2024-04-07 17:58:29,250 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:29,250 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:36,967 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:38,271 | server.py:125 | fit progress: (8, 1.6221859455108643, {'accuracy': 0.8881, 'data_size': 10000}, 82.57463151100092)
INFO flwr 2024-04-07 17:58:38,271 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:38,272 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:46,156 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:47,687 | server.py:125 | fit progress: (9, 1.6245886087417603, {'accuracy': 0.8801, 'data_size': 10000}, 91.99076154897921)
INFO flwr 2024-04-07 17:58:47,688 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 17:58:47,688 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 17:58:55,716 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 17:58:57,386 | server.py:125 | fit progress: (10, 1.6286152601242065, {'accuracy': 0.8751, 'data_size': 10000}, 101.68949552997947)
INFO flwr 2024-04-07 17:58:57,386 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 17:58:57,386 | server.py:153 | FL finished in 101.68988597398857
INFO flwr 2024-04-07 17:58:57,387 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 17:58:57,387 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 17:58:57,387 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 17:58:57,387 | app.py:229 | app_fit: losses_centralized [(0, 2.3023548126220703), (1, 2.0400338172912598), (2, 1.7932666540145874), (3, 1.6938730478286743), (4, 1.6636213064193726), (5, 1.6633156538009644), (6, 1.6441421508789062), (7, 1.6239556074142456), (8, 1.6221859455108643), (9, 1.6245886087417603), (10, 1.6286152601242065)]
INFO flwr 2024-04-07 17:58:57,387 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0996), (1, 0.5457), (2, 0.7512), (3, 0.851), (4, 0.8705), (5, 0.8521), (6, 0.8734), (7, 0.8835), (8, 0.8881), (9, 0.8801), (10, 0.8751)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8751
wandb:     loss 1.62862
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_175655-yttthzya
wandb: Find logs at: ./wandb/offline-run-20240407_175655-yttthzya/logs
INFO flwr 2024-04-07 17:59:00,934 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:06:05,831 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2116331)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2116331)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:06:11,719	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:06:12,066	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:06:12,435	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_185ff93b302909c9.zip' (12.38MiB) to Ray cluster...
2024-04-07 18:06:12,473	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_185ff93b302909c9.zip'.
INFO flwr 2024-04-07 18:06:22,698 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151606367642.0, 'node:__internal_head__': 1.0, 'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 69259871846.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 18:06:22,699 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:06:22,699 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:06:22,721 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:06:22,722 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:06:22,722 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:06:22,722 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:06:25,221 | server.py:94 | initial parameters (loss, other metrics): 2.3039133548736572, {'accuracy': 0.0816, 'data_size': 10000}
INFO flwr 2024-04-07 18:06:25,222 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:06:25,222 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2120923)[0m 2024-04-07 18:06:28.420726: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2120923)[0m 2024-04-07 18:06:28.513622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2120923)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2120923)[0m 2024-04-07 18:06:30.280000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2120927)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2120927)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2120921)[0m 2024-04-07 18:06:28.768629: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2120921)[0m 2024-04-07 18:06:28.866219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2120921)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2120930)[0m 2024-04-07 18:06:30.737536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:06:42,595 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:06:42,628 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:06:43,692 | server.py:125 | fit progress: (1, 2.001974582672119, {'accuracy': 0.6389, 'data_size': 10000}, 18.47025052900426)
INFO flwr 2024-04-07 18:06:43,692 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:06:43,693 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:06:53,081 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:06:54,544 | server.py:125 | fit progress: (2, 1.77316153049469, {'accuracy': 0.775, 'data_size': 10000}, 29.32173527200939)
INFO flwr 2024-04-07 18:06:54,544 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:06:54,544 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:03,520 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:04,959 | server.py:125 | fit progress: (3, 1.7021291255950928, {'accuracy': 0.8472, 'data_size': 10000}, 39.73729392403038)
INFO flwr 2024-04-07 18:07:04,960 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:04,960 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:13,723 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:15,221 | server.py:125 | fit progress: (4, 1.6790233850479126, {'accuracy': 0.8489, 'data_size': 10000}, 49.99922439700458)
INFO flwr 2024-04-07 18:07:15,221 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:15,222 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:23,880 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:25,412 | server.py:125 | fit progress: (5, 1.6355328559875488, {'accuracy': 0.8816, 'data_size': 10000}, 60.190135918033775)
INFO flwr 2024-04-07 18:07:25,412 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:25,413 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:34,318 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:35,542 | server.py:125 | fit progress: (6, 1.6272716522216797, {'accuracy': 0.8848, 'data_size': 10000}, 70.31998192501487)
INFO flwr 2024-04-07 18:07:35,542 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:35,542 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:44,566 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:45,849 | server.py:125 | fit progress: (7, 1.638631820678711, {'accuracy': 0.8703, 'data_size': 10000}, 80.62710230000084)
INFO flwr 2024-04-07 18:07:45,849 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:45,850 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:07:54,833 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:07:56,436 | server.py:125 | fit progress: (8, 1.6121841669082642, {'accuracy': 0.8945, 'data_size': 10000}, 91.21381183800986)
INFO flwr 2024-04-07 18:07:56,436 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:07:56,436 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:08:05,361 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:08:07,015 | server.py:125 | fit progress: (9, 1.6063024997711182, {'accuracy': 0.8992, 'data_size': 10000}, 101.7927390980185)
INFO flwr 2024-04-07 18:08:07,015 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:08:07,015 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:08:15,966 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:08:17,295 | server.py:125 | fit progress: (10, 1.6068789958953857, {'accuracy': 0.893, 'data_size': 10000}, 112.0728147700429)
INFO flwr 2024-04-07 18:08:17,295 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:08:17,295 | server.py:153 | FL finished in 112.07330128003377
INFO flwr 2024-04-07 18:08:17,295 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:08:17,295 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:08:17,296 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:08:17,296 | app.py:229 | app_fit: losses_centralized [(0, 2.3039133548736572), (1, 2.001974582672119), (2, 1.77316153049469), (3, 1.7021291255950928), (4, 1.6790233850479126), (5, 1.6355328559875488), (6, 1.6272716522216797), (7, 1.638631820678711), (8, 1.6121841669082642), (9, 1.6063024997711182), (10, 1.6068789958953857)]
INFO flwr 2024-04-07 18:08:17,296 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0816), (1, 0.6389), (2, 0.775), (3, 0.8472), (4, 0.8489), (5, 0.8816), (6, 0.8848), (7, 0.8703), (8, 0.8945), (9, 0.8992), (10, 0.893)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.893
wandb:     loss 1.60688
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_180605-8diq2avt
wandb: Find logs at: ./wandb/offline-run-20240407_180605-8diq2avt/logs
INFO flwr 2024-04-07 18:08:20,822 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 10
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:15:26,249 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2120921)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2120921)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:15:32,113	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:15:32,436	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:15:32,780	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_d242d1e0507839f1.zip' (12.39MiB) to Ray cluster...
2024-04-07 18:15:32,814	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_d242d1e0507839f1.zip'.
INFO flwr 2024-04-07 18:15:43,113 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 151669434573.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69286900531.0}
INFO flwr 2024-04-07 18:15:43,113 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:15:43,113 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:15:43,131 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:15:43,132 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:15:43,132 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:15:43,132 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:15:45,496 | server.py:94 | initial parameters (loss, other metrics): 2.3025012016296387, {'accuracy': 0.1302, 'data_size': 10000}
INFO flwr 2024-04-07 18:15:45,497 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:15:45,497 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2125215)[0m 2024-04-07 18:15:48.875634: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2125212)[0m 2024-04-07 18:15:48.996535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2125212)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2125212)[0m 2024-04-07 18:15:50.827059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2125214)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2125214)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2125216)[0m 2024-04-07 18:15:49.099051: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2125216)[0m 2024-04-07 18:15:49.188603: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2125216)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2125216)[0m 2024-04-07 18:15:51.431947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:16:03,037 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:16:03,068 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:16:04,388 | server.py:125 | fit progress: (1, 1.9709210395812988, {'accuracy': 0.6293, 'data_size': 10000}, 18.89159455097979)
INFO flwr 2024-04-07 18:16:04,389 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:04,389 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:13,117 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:16:14,450 | server.py:125 | fit progress: (2, 1.7888730764389038, {'accuracy': 0.7389, 'data_size': 10000}, 28.953527897014283)
INFO flwr 2024-04-07 18:16:14,451 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:14,451 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:22,562 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:16:23,958 | server.py:125 | fit progress: (3, 1.6799005270004272, {'accuracy': 0.8631, 'data_size': 10000}, 38.461487466993276)
INFO flwr 2024-04-07 18:16:23,959 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:23,959 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:31,586 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:16:32,767 | server.py:125 | fit progress: (4, 1.662642002105713, {'accuracy': 0.8485, 'data_size': 10000}, 47.27025587897515)
INFO flwr 2024-04-07 18:16:32,767 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:32,768 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:40,561 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:16:41,988 | server.py:125 | fit progress: (5, 1.6338520050048828, {'accuracy': 0.8825, 'data_size': 10000}, 56.49072962900391)
INFO flwr 2024-04-07 18:16:41,988 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:41,988 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:49,567 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:16:51,020 | server.py:125 | fit progress: (6, 1.630506992340088, {'accuracy': 0.8768, 'data_size': 10000}, 65.52301694802009)
INFO flwr 2024-04-07 18:16:51,020 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:16:51,020 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:16:58,691 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:17:00,206 | server.py:125 | fit progress: (7, 1.6204513311386108, {'accuracy': 0.8837, 'data_size': 10000}, 74.70953734702198)
INFO flwr 2024-04-07 18:17:00,207 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:17:00,207 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:17:08,267 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:17:09,561 | server.py:125 | fit progress: (8, 1.6125553846359253, {'accuracy': 0.8916, 'data_size': 10000}, 84.06398281396832)
INFO flwr 2024-04-07 18:17:09,561 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:17:09,561 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:17:17,659 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:17:19,211 | server.py:125 | fit progress: (9, 1.6026774644851685, {'accuracy': 0.8953, 'data_size': 10000}, 93.71410024299985)
INFO flwr 2024-04-07 18:17:19,211 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:17:19,211 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:17:27,297 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:17:28,940 | server.py:125 | fit progress: (10, 1.6002780199050903, {'accuracy': 0.8965, 'data_size': 10000}, 103.44367757398868)
INFO flwr 2024-04-07 18:17:28,941 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:17:28,941 | server.py:153 | FL finished in 103.44410073600011
INFO flwr 2024-04-07 18:17:28,941 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:17:28,941 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:17:28,941 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:17:28,941 | app.py:229 | app_fit: losses_centralized [(0, 2.3025012016296387), (1, 1.9709210395812988), (2, 1.7888730764389038), (3, 1.6799005270004272), (4, 1.662642002105713), (5, 1.6338520050048828), (6, 1.630506992340088), (7, 1.6204513311386108), (8, 1.6125553846359253), (9, 1.6026774644851685), (10, 1.6002780199050903)]
INFO flwr 2024-04-07 18:17:28,942 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1302), (1, 0.6293), (2, 0.7389), (3, 0.8631), (4, 0.8485), (5, 0.8825), (6, 0.8768), (7, 0.8837), (8, 0.8916), (9, 0.8953), (10, 0.8965)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8965
wandb:     loss 1.60028
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_181525-qc46dko5
wandb: Find logs at: ./wandb/offline-run-20240407_181525-qc46dko5/logs
INFO flwr 2024-04-07 18:17:32,504 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.001
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:24:37,550 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2125207)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2125207)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:24:42,691	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:24:43,175	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:24:43,610	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9781714f72b11169.zip' (12.41MiB) to Ray cluster...
2024-04-07 18:24:43,647	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9781714f72b11169.zip'.
INFO flwr 2024-04-07 18:24:53,823 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'memory': 151275200308.0, 'object_store_memory': 69117942988.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
INFO flwr 2024-04-07 18:24:53,824 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:24:53,824 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:24:53,844 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:24:53,848 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:24:53,848 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:24:53,848 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:24:55,787 | server.py:94 | initial parameters (loss, other metrics): 2.3011317253112793, {'accuracy': 0.1106, 'data_size': 10000}
INFO flwr 2024-04-07 18:24:55,787 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:24:55,788 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2129532)[0m 2024-04-07 18:24:59.589858: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2129531)[0m 2024-04-07 18:24:59.679951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2129531)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2129533)[0m 2024-04-07 18:25:01.616423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2129535)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2129535)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2129535)[0m 2024-04-07 18:24:59.839074: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2129535)[0m 2024-04-07 18:24:59.928565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2129535)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2129538)[0m 2024-04-07 18:25:01.755679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:25:14,291 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:25:14,330 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:25:15,369 | server.py:125 | fit progress: (1, 2.2942028045654297, {'accuracy': 0.186, 'data_size': 10000}, 19.581907249987125)
INFO flwr 2024-04-07 18:25:15,370 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:25:15,370 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:25:24,370 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:25:25,695 | server.py:125 | fit progress: (2, 2.285886764526367, {'accuracy': 0.1961, 'data_size': 10000}, 29.90732981596375)
INFO flwr 2024-04-07 18:25:25,695 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:25:25,695 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:25:33,855 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:25:35,197 | server.py:125 | fit progress: (3, 2.2788939476013184, {'accuracy': 0.234, 'data_size': 10000}, 39.409830043965485)
INFO flwr 2024-04-07 18:25:35,198 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:25:35,198 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:25:43,427 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:25:44,586 | server.py:125 | fit progress: (4, 2.270258903503418, {'accuracy': 0.2924, 'data_size': 10000}, 48.7981915359851)
INFO flwr 2024-04-07 18:25:44,586 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:25:44,586 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:25:52,633 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:25:54,034 | server.py:125 | fit progress: (5, 2.2595863342285156, {'accuracy': 0.3622, 'data_size': 10000}, 58.246272398973815)
INFO flwr 2024-04-07 18:25:54,034 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:25:54,034 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:26:02,174 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:26:03,641 | server.py:125 | fit progress: (6, 2.2493093013763428, {'accuracy': 0.3067, 'data_size': 10000}, 67.85382048098836)
INFO flwr 2024-04-07 18:26:03,642 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:26:03,642 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:26:11,855 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:26:13,125 | server.py:125 | fit progress: (7, 2.239177703857422, {'accuracy': 0.339, 'data_size': 10000}, 77.33737870695768)
INFO flwr 2024-04-07 18:26:13,125 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:26:13,125 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:26:21,863 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:26:23,370 | server.py:125 | fit progress: (8, 2.2273640632629395, {'accuracy': 0.3851, 'data_size': 10000}, 87.58213968097698)
INFO flwr 2024-04-07 18:26:23,370 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:26:23,370 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:26:31,423 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:26:32,953 | server.py:125 | fit progress: (9, 2.2159483432769775, {'accuracy': 0.4243, 'data_size': 10000}, 97.16523949400289)
INFO flwr 2024-04-07 18:26:32,953 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:26:32,953 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:26:41,372 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:26:42,726 | server.py:125 | fit progress: (10, 2.204991102218628, {'accuracy': 0.4594, 'data_size': 10000}, 106.93889364699135)
INFO flwr 2024-04-07 18:26:42,727 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:26:42,727 | server.py:153 | FL finished in 106.93927116598934
INFO flwr 2024-04-07 18:26:42,727 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:26:42,727 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:26:42,727 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:26:42,727 | app.py:229 | app_fit: losses_centralized [(0, 2.3011317253112793), (1, 2.2942028045654297), (2, 2.285886764526367), (3, 2.2788939476013184), (4, 2.270258903503418), (5, 2.2595863342285156), (6, 2.2493093013763428), (7, 2.239177703857422), (8, 2.2273640632629395), (9, 2.2159483432769775), (10, 2.204991102218628)]
INFO flwr 2024-04-07 18:26:42,727 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1106), (1, 0.186), (2, 0.1961), (3, 0.234), (4, 0.2924), (5, 0.3622), (6, 0.3067), (7, 0.339), (8, 0.3851), (9, 0.4243), (10, 0.4594)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.4594
wandb:     loss 2.20499
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_182437-ef667uxn
wandb: Find logs at: ./wandb/offline-run-20240407_182437-ef667uxn/logs
INFO flwr 2024-04-07 18:26:46,239 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.051000000000000004
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:33:51,323 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2129529)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2129529)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:33:55,949	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:33:56,327	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:33:56,661	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cd8b4ff41c2ba65d.zip' (12.42MiB) to Ray cluster...
2024-04-07 18:33:56,693	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cd8b4ff41c2ba65d.zip'.
INFO flwr 2024-04-07 18:34:06,897 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'memory': 151536442368.0, 'node:__internal_head__': 1.0, 'object_store_memory': 69229903872.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 18:34:06,897 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:34:06,897 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:34:06,912 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:34:06,913 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:34:06,913 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:34:06,913 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:34:08,881 | server.py:94 | initial parameters (loss, other metrics): 2.3034143447875977, {'accuracy': 0.0851, 'data_size': 10000}
INFO flwr 2024-04-07 18:34:08,882 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:34:08,882 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2133525)[0m 2024-04-07 18:34:12.685741: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2133517)[0m 2024-04-07 18:34:12.868896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2133517)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2133517)[0m 2024-04-07 18:34:14.770626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2133521)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2133521)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2133518)[0m 2024-04-07 18:34:12.895260: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2133519)[0m 2024-04-07 18:34:13.021947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2133519)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2133518)[0m 2024-04-07 18:34:14.922590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:34:26,766 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:34:26,805 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:34:28,091 | server.py:125 | fit progress: (1, 2.098356246948242, {'accuracy': 0.4661, 'data_size': 10000}, 19.208919972006697)
INFO flwr 2024-04-07 18:34:28,091 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:34:28,091 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:34:36,760 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:34:38,286 | server.py:125 | fit progress: (2, 1.8901257514953613, {'accuracy': 0.6679, 'data_size': 10000}, 29.404640163003933)
INFO flwr 2024-04-07 18:34:38,287 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:34:38,287 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:34:47,224 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:34:48,593 | server.py:125 | fit progress: (3, 1.8058310747146606, {'accuracy': 0.7444, 'data_size': 10000}, 39.71149745199364)
INFO flwr 2024-04-07 18:34:48,594 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:34:48,594 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:34:56,705 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:34:58,098 | server.py:125 | fit progress: (4, 1.7507082223892212, {'accuracy': 0.7931, 'data_size': 10000}, 49.21651922300225)
INFO flwr 2024-04-07 18:34:58,099 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:34:58,099 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:06,070 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:07,296 | server.py:125 | fit progress: (5, 1.7169320583343506, {'accuracy': 0.8184, 'data_size': 10000}, 58.41457027901197)
INFO flwr 2024-04-07 18:35:07,297 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:35:07,297 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:15,778 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:17,017 | server.py:125 | fit progress: (6, 1.6726174354553223, {'accuracy': 0.8729, 'data_size': 10000}, 68.13472417299636)
INFO flwr 2024-04-07 18:35:17,017 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:35:17,017 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:25,457 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:26,950 | server.py:125 | fit progress: (7, 1.6570115089416504, {'accuracy': 0.8809, 'data_size': 10000}, 78.06807130004745)
INFO flwr 2024-04-07 18:35:26,950 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:35:26,950 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:35,017 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:36,538 | server.py:125 | fit progress: (8, 1.6588261127471924, {'accuracy': 0.8696, 'data_size': 10000}, 87.65568014100427)
INFO flwr 2024-04-07 18:35:36,538 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:35:36,538 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:44,899 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:46,222 | server.py:125 | fit progress: (9, 1.63945472240448, {'accuracy': 0.8872, 'data_size': 10000}, 97.34012029803125)
INFO flwr 2024-04-07 18:35:46,222 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:35:46,223 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:35:54,653 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:35:56,231 | server.py:125 | fit progress: (10, 1.6317750215530396, {'accuracy': 0.89, 'data_size': 10000}, 107.34888469905127)
INFO flwr 2024-04-07 18:35:56,231 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:35:56,231 | server.py:153 | FL finished in 107.34939673903864
INFO flwr 2024-04-07 18:35:56,231 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:35:56,231 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:35:56,232 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:35:56,232 | app.py:229 | app_fit: losses_centralized [(0, 2.3034143447875977), (1, 2.098356246948242), (2, 1.8901257514953613), (3, 1.8058310747146606), (4, 1.7507082223892212), (5, 1.7169320583343506), (6, 1.6726174354553223), (7, 1.6570115089416504), (8, 1.6588261127471924), (9, 1.63945472240448), (10, 1.6317750215530396)]
INFO flwr 2024-04-07 18:35:56,232 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0851), (1, 0.4661), (2, 0.6679), (3, 0.7444), (4, 0.7931), (5, 0.8184), (6, 0.8729), (7, 0.8809), (8, 0.8696), (9, 0.8872), (10, 0.89)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.89
wandb:     loss 1.63178
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_183350-a0yvgofn
wandb: Find logs at: ./wandb/offline-run-20240407_183350-a0yvgofn/logs
INFO flwr 2024-04-07 18:35:59,748 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.101
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:43:04,716 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2133514)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2133514)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:43:09,537	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:43:09,970	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:43:10,418	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b5dae7e56024f666.zip' (12.43MiB) to Ray cluster...
2024-04-07 18:43:10,445	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b5dae7e56024f666.zip'.
INFO flwr 2024-04-07 18:43:20,648 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151300377191.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69128733081.0}
INFO flwr 2024-04-07 18:43:20,648 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:43:20,648 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:43:20,664 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:43:20,665 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:43:20,665 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:43:20,665 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:43:23,121 | server.py:94 | initial parameters (loss, other metrics): 2.301313638687134, {'accuracy': 0.1186, 'data_size': 10000}
INFO flwr 2024-04-07 18:43:23,121 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:43:23,121 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2137837)[0m 2024-04-07 18:43:26.356075: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2137837)[0m 2024-04-07 18:43:26.459968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2137837)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2137826)[0m 2024-04-07 18:43:28.299880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2137831)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2137831)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2137836)[0m 2024-04-07 18:43:26.548016: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2137836)[0m 2024-04-07 18:43:26.647751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2137836)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2137828)[0m 2024-04-07 18:43:28.880912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:43:40,794 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:43:40,822 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:43:42,087 | server.py:125 | fit progress: (1, 2.0093202590942383, {'accuracy': 0.536, 'data_size': 10000}, 18.966223736002576)
INFO flwr 2024-04-07 18:43:42,088 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:43:42,088 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:43:51,165 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:43:52,551 | server.py:125 | fit progress: (2, 1.7781755924224854, {'accuracy': 0.8111, 'data_size': 10000}, 29.43024798197439)
INFO flwr 2024-04-07 18:43:52,552 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:43:52,552 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:00,912 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:02,046 | server.py:125 | fit progress: (3, 1.6902722120285034, {'accuracy': 0.862, 'data_size': 10000}, 38.924708864011336)
INFO flwr 2024-04-07 18:44:02,046 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:02,046 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:10,254 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:11,642 | server.py:125 | fit progress: (4, 1.6650993824005127, {'accuracy': 0.8688, 'data_size': 10000}, 48.52098772599129)
INFO flwr 2024-04-07 18:44:11,642 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:11,642 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:19,739 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:20,919 | server.py:125 | fit progress: (5, 1.6524239778518677, {'accuracy': 0.8736, 'data_size': 10000}, 57.798254668014124)
INFO flwr 2024-04-07 18:44:20,920 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:20,920 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:29,153 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:30,384 | server.py:125 | fit progress: (6, 1.6322994232177734, {'accuracy': 0.8877, 'data_size': 10000}, 67.26287136500468)
INFO flwr 2024-04-07 18:44:30,384 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:30,384 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:38,774 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:40,228 | server.py:125 | fit progress: (7, 1.6233974695205688, {'accuracy': 0.8886, 'data_size': 10000}, 77.10679594200337)
INFO flwr 2024-04-07 18:44:40,228 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:40,228 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:48,119 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:49,614 | server.py:125 | fit progress: (8, 1.6248407363891602, {'accuracy': 0.8846, 'data_size': 10000}, 86.49335465399781)
INFO flwr 2024-04-07 18:44:49,615 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:49,615 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:44:57,827 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:44:59,375 | server.py:125 | fit progress: (9, 1.6096454858779907, {'accuracy': 0.896, 'data_size': 10000}, 96.25370813597692)
INFO flwr 2024-04-07 18:44:59,375 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:44:59,375 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:45:08,113 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:45:09,494 | server.py:125 | fit progress: (10, 1.6073681116104126, {'accuracy': 0.8956, 'data_size': 10000}, 106.37278264795896)
INFO flwr 2024-04-07 18:45:09,494 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:45:09,494 | server.py:153 | FL finished in 106.37319074897096
INFO flwr 2024-04-07 18:45:09,494 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:45:09,494 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:45:09,495 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:45:09,495 | app.py:229 | app_fit: losses_centralized [(0, 2.301313638687134), (1, 2.0093202590942383), (2, 1.7781755924224854), (3, 1.6902722120285034), (4, 1.6650993824005127), (5, 1.6524239778518677), (6, 1.6322994232177734), (7, 1.6233974695205688), (8, 1.6248407363891602), (9, 1.6096454858779907), (10, 1.6073681116104126)]
INFO flwr 2024-04-07 18:45:09,495 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.1186), (1, 0.536), (2, 0.8111), (3, 0.862), (4, 0.8688), (5, 0.8736), (6, 0.8877), (7, 0.8886), (8, 0.8846), (9, 0.896), (10, 0.8956)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8956
wandb:     loss 1.60737
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_184304-0xzfbjkx
wandb: Find logs at: ./wandb/offline-run-20240407_184304-0xzfbjkx/logs
INFO flwr 2024-04-07 18:45:13,014 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.15100000000000002
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 18:52:18,158 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2137823)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2137823)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 18:52:23,331	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 18:52:23,663	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 18:52:24,058	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_5954bcb372245bcd.zip' (12.44MiB) to Ray cluster...
2024-04-07 18:52:24,099	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_5954bcb372245bcd.zip'.
INFO flwr 2024-04-07 18:52:34,276 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 151279475303.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 69119775129.0}
INFO flwr 2024-04-07 18:52:34,277 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 18:52:34,277 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 18:52:34,294 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 18:52:34,296 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 18:52:34,296 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 18:52:34,296 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 18:52:37,375 | server.py:94 | initial parameters (loss, other metrics): 2.3041133880615234, {'accuracy': 0.0878, 'data_size': 10000}
INFO flwr 2024-04-07 18:52:37,381 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 18:52:37,385 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2142120)[0m 2024-04-07 18:52:39.954027: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2142120)[0m 2024-04-07 18:52:40.043595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2142120)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2142123)[0m 2024-04-07 18:52:41.906699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2142122)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2142122)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2142119)[0m 2024-04-07 18:52:40.285154: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2142119)[0m 2024-04-07 18:52:40.377271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2142119)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2142119)[0m 2024-04-07 18:52:42.358395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 18:52:54,193 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 18:52:54,232 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 18:52:55,522 | server.py:125 | fit progress: (1, 1.962980031967163, {'accuracy': 0.7369, 'data_size': 10000}, 18.137226648977958)
INFO flwr 2024-04-07 18:52:55,522 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 18:52:55,523 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:04,190 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:05,309 | server.py:125 | fit progress: (2, 1.763197422027588, {'accuracy': 0.7636, 'data_size': 10000}, 27.924093133944552)
INFO flwr 2024-04-07 18:53:05,309 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:05,309 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:13,491 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:14,850 | server.py:125 | fit progress: (3, 1.6860543489456177, {'accuracy': 0.8431, 'data_size': 10000}, 37.46513934497489)
INFO flwr 2024-04-07 18:53:14,850 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:14,850 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:22,839 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:24,240 | server.py:125 | fit progress: (4, 1.6393834352493286, {'accuracy': 0.8857, 'data_size': 10000}, 46.85503258096287)
INFO flwr 2024-04-07 18:53:24,240 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:24,240 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:32,255 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:33,687 | server.py:125 | fit progress: (5, 1.6411279439926147, {'accuracy': 0.8713, 'data_size': 10000}, 56.30251014296664)
INFO flwr 2024-04-07 18:53:33,688 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:33,688 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:41,662 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:42,915 | server.py:125 | fit progress: (6, 1.6277145147323608, {'accuracy': 0.8834, 'data_size': 10000}, 65.53004545194563)
INFO flwr 2024-04-07 18:53:42,915 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:42,915 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:53:50,915 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 18:53:52,359 | server.py:125 | fit progress: (7, 1.611653208732605, {'accuracy': 0.8934, 'data_size': 10000}, 74.97392168396618)
INFO flwr 2024-04-07 18:53:52,359 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 18:53:52,359 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:54:00,725 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 18:54:02,021 | server.py:125 | fit progress: (8, 1.60336434841156, {'accuracy': 0.8975, 'data_size': 10000}, 84.63623962295242)
INFO flwr 2024-04-07 18:54:02,022 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 18:54:02,022 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:54:10,328 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 18:54:11,644 | server.py:125 | fit progress: (9, 1.5983771085739136, {'accuracy': 0.9002, 'data_size': 10000}, 94.25880750099896)
INFO flwr 2024-04-07 18:54:11,644 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 18:54:11,644 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 18:54:19,597 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 18:54:21,258 | server.py:125 | fit progress: (10, 1.6020740270614624, {'accuracy': 0.8942, 'data_size': 10000}, 103.87350031995447)
INFO flwr 2024-04-07 18:54:21,259 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 18:54:21,259 | server.py:153 | FL finished in 103.87390265800059
INFO flwr 2024-04-07 18:54:21,259 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 18:54:21,259 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 18:54:21,259 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 18:54:21,259 | app.py:229 | app_fit: losses_centralized [(0, 2.3041133880615234), (1, 1.962980031967163), (2, 1.763197422027588), (3, 1.6860543489456177), (4, 1.6393834352493286), (5, 1.6411279439926147), (6, 1.6277145147323608), (7, 1.611653208732605), (8, 1.60336434841156), (9, 1.5983771085739136), (10, 1.6020740270614624)]
INFO flwr 2024-04-07 18:54:21,259 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0878), (1, 0.7369), (2, 0.7636), (3, 0.8431), (4, 0.8857), (5, 0.8713), (6, 0.8834), (7, 0.8934), (8, 0.8975), (9, 0.9002), (10, 0.8942)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8942
wandb:     loss 1.60207
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_185217-tdjstsdf
wandb: Find logs at: ./wandb/offline-run-20240407_185217-tdjstsdf/logs
INFO flwr 2024-04-07 18:54:24,836 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.201
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 19:01:29,729 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2142116)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2142116)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 19:01:35,207	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 19:01:35,630	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 19:01:36,113	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9d73728587bdc38f.zip' (12.45MiB) to Ray cluster...
2024-04-07 19:01:36,145	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9d73728587bdc38f.zip'.
INFO flwr 2024-04-07 19:01:46,329 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 151231455437.0, 'object_store_memory': 69099195187.0, 'node:10.20.240.18': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-04-07 19:01:46,330 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 19:01:46,330 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 19:01:46,349 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 19:01:46,350 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 19:01:46,350 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 19:01:46,350 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 19:01:49,399 | server.py:94 | initial parameters (loss, other metrics): 2.302921772003174, {'accuracy': 0.0861, 'data_size': 10000}
INFO flwr 2024-04-07 19:01:49,401 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 19:01:49,402 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2146699)[0m 2024-04-07 19:01:51.994751: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2146699)[0m 2024-04-07 19:01:52.085635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2146699)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2146699)[0m 2024-04-07 19:01:54.110974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2146705)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2146705)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2146694)[0m 2024-04-07 19:01:52.384962: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2146694)[0m 2024-04-07 19:01:52.474389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2146694)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2146694)[0m 2024-04-07 19:01:54.561215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 19:02:06,458 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 19:02:06,494 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 19:02:07,771 | server.py:125 | fit progress: (1, 1.8999868631362915, {'accuracy': 0.642, 'data_size': 10000}, 18.368311274040025)
INFO flwr 2024-04-07 19:02:07,771 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:07,771 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:02:17,023 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 19:02:18,149 | server.py:125 | fit progress: (2, 1.7183994054794312, {'accuracy': 0.7976, 'data_size': 10000}, 28.746281046012882)
INFO flwr 2024-04-07 19:02:18,149 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:18,149 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:02:26,768 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 19:02:28,148 | server.py:125 | fit progress: (3, 1.6547318696975708, {'accuracy': 0.8604, 'data_size': 10000}, 38.746075313014444)
INFO flwr 2024-04-07 19:02:28,149 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:28,149 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:02:36,514 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 19:02:37,911 | server.py:125 | fit progress: (4, 1.6347599029541016, {'accuracy': 0.875, 'data_size': 10000}, 48.50871466402896)
INFO flwr 2024-04-07 19:02:37,911 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:37,912 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:02:45,845 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 19:02:47,277 | server.py:125 | fit progress: (5, 1.614999532699585, {'accuracy': 0.89, 'data_size': 10000}, 57.87481193203712)
INFO flwr 2024-04-07 19:02:47,277 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:47,278 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:02:55,522 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 19:02:57,010 | server.py:125 | fit progress: (6, 1.605776309967041, {'accuracy': 0.896, 'data_size': 10000}, 67.60772569500841)
INFO flwr 2024-04-07 19:02:57,010 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 19:02:57,011 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:03:05,198 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 19:03:06,440 | server.py:125 | fit progress: (7, 1.613013505935669, {'accuracy': 0.8839, 'data_size': 10000}, 77.03727152803913)
INFO flwr 2024-04-07 19:03:06,440 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 19:03:06,440 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:03:15,155 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 19:03:16,422 | server.py:125 | fit progress: (8, 1.5928107500076294, {'accuracy': 0.9023, 'data_size': 10000}, 87.02006918704137)
INFO flwr 2024-04-07 19:03:16,423 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 19:03:16,423 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:03:24,761 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 19:03:26,416 | server.py:125 | fit progress: (9, 1.588423490524292, {'accuracy': 0.9037, 'data_size': 10000}, 97.01351235003676)
INFO flwr 2024-04-07 19:03:26,416 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 19:03:26,416 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:03:34,742 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 19:03:36,321 | server.py:125 | fit progress: (10, 1.5848000049591064, {'accuracy': 0.9071, 'data_size': 10000}, 106.91835844900925)
INFO flwr 2024-04-07 19:03:36,321 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 19:03:36,321 | server.py:153 | FL finished in 106.91874111705692
INFO flwr 2024-04-07 19:03:36,321 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 19:03:36,321 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 19:03:36,321 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 19:03:36,321 | app.py:229 | app_fit: losses_centralized [(0, 2.302921772003174), (1, 1.8999868631362915), (2, 1.7183994054794312), (3, 1.6547318696975708), (4, 1.6347599029541016), (5, 1.614999532699585), (6, 1.605776309967041), (7, 1.613013505935669), (8, 1.5928107500076294), (9, 1.588423490524292), (10, 1.5848000049591064)]
INFO flwr 2024-04-07 19:03:36,322 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0861), (1, 0.642), (2, 0.7976), (3, 0.8604), (4, 0.875), (5, 0.89), (6, 0.896), (7, 0.8839), (8, 0.9023), (9, 0.9037), (10, 0.9071)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.9071
wandb:     loss 1.5848
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_190129-v5gtk52h
wandb: Find logs at: ./wandb/offline-run-20240407_190129-v5gtk52h/logs
INFO flwr 2024-04-07 19:03:39,844 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.251
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 19:10:45,053 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2146694)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2146694)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 19:10:51,925	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 19:10:52,296	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 19:10:52,662	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_add4c9daea5a4eff.zip' (12.46MiB) to Ray cluster...
2024-04-07 19:10:52,687	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_add4c9daea5a4eff.zip'.
INFO flwr 2024-04-07 19:11:02,882 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 151112813568.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'object_store_memory': 69048348672.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-04-07 19:11:02,882 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 19:11:02,882 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 19:11:02,900 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 19:11:02,901 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 19:11:02,902 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 19:11:02,902 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 19:11:05,551 | server.py:94 | initial parameters (loss, other metrics): 2.3078019618988037, {'accuracy': 0.0571, 'data_size': 10000}
INFO flwr 2024-04-07 19:11:05,552 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 19:11:05,552 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2150983)[0m 2024-04-07 19:11:08.567035: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2150983)[0m 2024-04-07 19:11:08.655801: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2150983)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2150985)[0m 2024-04-07 19:11:10.558698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2150994)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2150994)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2150984)[0m 2024-04-07 19:11:09.015159: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2150984)[0m 2024-04-07 19:11:09.107135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2150984)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2150984)[0m 2024-04-07 19:11:11.195420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 19:11:23,138 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 19:11:23,165 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 19:11:24,446 | server.py:125 | fit progress: (1, 1.950438141822815, {'accuracy': 0.5815, 'data_size': 10000}, 18.894091964000836)
INFO flwr 2024-04-07 19:11:24,446 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 19:11:24,446 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:11:33,434 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 19:11:34,514 | server.py:125 | fit progress: (2, 1.7305195331573486, {'accuracy': 0.7935, 'data_size': 10000}, 28.962446330988314)
INFO flwr 2024-04-07 19:11:34,514 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 19:11:34,515 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:11:43,038 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 19:11:44,185 | server.py:125 | fit progress: (3, 1.663714051246643, {'accuracy': 0.8556, 'data_size': 10000}, 38.63295737199951)
INFO flwr 2024-04-07 19:11:44,185 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 19:11:44,185 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:11:52,599 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 19:11:53,986 | server.py:125 | fit progress: (4, 1.6362367868423462, {'accuracy': 0.8702, 'data_size': 10000}, 48.43377318402054)
INFO flwr 2024-04-07 19:11:53,986 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 19:11:53,986 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:02,083 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:03,493 | server.py:125 | fit progress: (5, 1.6052589416503906, {'accuracy': 0.8993, 'data_size': 10000}, 57.941606270032935)
INFO flwr 2024-04-07 19:12:03,494 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 19:12:03,494 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:11,844 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:13,070 | server.py:125 | fit progress: (6, 1.6158430576324463, {'accuracy': 0.8847, 'data_size': 10000}, 67.51828610402299)
INFO flwr 2024-04-07 19:12:13,070 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 19:12:13,071 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:21,120 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:22,611 | server.py:125 | fit progress: (7, 1.5953634977340698, {'accuracy': 0.8992, 'data_size': 10000}, 77.05874712503282)
INFO flwr 2024-04-07 19:12:22,611 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 19:12:22,611 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:30,728 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:32,244 | server.py:125 | fit progress: (8, 1.591084361076355, {'accuracy': 0.9014, 'data_size': 10000}, 86.69233431603061)
INFO flwr 2024-04-07 19:12:32,244 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 19:12:32,245 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:40,609 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:41,930 | server.py:125 | fit progress: (9, 1.5939042568206787, {'accuracy': 0.8955, 'data_size': 10000}, 96.37843471602537)
INFO flwr 2024-04-07 19:12:41,930 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 19:12:41,931 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:12:50,228 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 19:12:51,801 | server.py:125 | fit progress: (10, 1.5925325155258179, {'accuracy': 0.8977, 'data_size': 10000}, 106.24941392597975)
INFO flwr 2024-04-07 19:12:51,801 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 19:12:51,802 | server.py:153 | FL finished in 106.24980085500283
INFO flwr 2024-04-07 19:12:51,802 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 19:12:51,802 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 19:12:51,802 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 19:12:51,802 | app.py:229 | app_fit: losses_centralized [(0, 2.3078019618988037), (1, 1.950438141822815), (2, 1.7305195331573486), (3, 1.663714051246643), (4, 1.6362367868423462), (5, 1.6052589416503906), (6, 1.6158430576324463), (7, 1.5953634977340698), (8, 1.591084361076355), (9, 1.5939042568206787), (10, 1.5925325155258179)]
INFO flwr 2024-04-07 19:12:51,802 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0571), (1, 0.5815), (2, 0.7935), (3, 0.8556), (4, 0.8702), (5, 0.8993), (6, 0.8847), (7, 0.8992), (8, 0.9014), (9, 0.8955), (10, 0.8977)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8977
wandb:     loss 1.59253
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_191044-p3ddeppx
wandb: Find logs at: ./wandb/offline-run-20240407_191044-p3ddeppx/logs
INFO flwr 2024-04-07 19:12:55,376 | run_simulation.py:118 | 
Running with Config
	Simulation
		batch_size: 32
		client_count: 100
		fraction_fit: 0.1
		global_rounds: 10
		local_rounds: 20
		Optimizer: SGD
			lr: 0.301
	Dataset
		name: MNIST
		preprocess_fn:
			def preprocess_fn(element):
			  return {
			    "x": element["image"].reshape(784) / 255.,
			    "y": element["label"]
			  }
			
		splitter:
			alpha: 1.0
			percent_non_iid: 25.0
	Model
		name: Logistic Regression
		criterion: CrossEntropyLoss()
		layers:
			Net(
			  (layers): Sequential(
			    (0): Linear(in_features=784, out_features=10, bias=True)
			    (1): Softmax(dim=-1)
			  )
			)
Using the latest cached version of the dataset since mnist couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'mnist' at .cache/mnist/mnist/1.0.0/b06aab39e05f7bcd9635d18ed25d06eae523c574 (last modified on Tue Apr  2 18:09:57 2024).
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-04-07 19:20:00,915 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2m[36m(DefaultActor pid=2150984)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2150984)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
2024-04-07 19:20:06,896	INFO worker.py:1621 -- Started a local Ray instance.
2024-04-07 19:20:07,356	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-04-07 19:20:07,786	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_20a5ef750e992fd5.zip' (12.48MiB) to Ray cluster...
2024-04-07 19:20:07,820	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_20a5ef750e992fd5.zip'.
INFO flwr 2024-04-07 19:20:18,080 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 68940055756.0, 'node:__internal_head__': 1.0, 'memory': 150860130100.0}
INFO flwr 2024-04-07 19:20:18,080 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-04-07 19:20:18,080 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-04-07 19:20:18,097 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-04-07 19:20:18,098 | server.py:89 | Initializing global parameters
INFO flwr 2024-04-07 19:20:18,099 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-04-07 19:20:18,099 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-04-07 19:20:20,734 | server.py:94 | initial parameters (loss, other metrics): 2.3016276359558105, {'accuracy': 0.114, 'data_size': 10000}
INFO flwr 2024-04-07 19:20:20,734 | server.py:104 | FL starting
DEBUG flwr 2024-04-07 19:20:20,735 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=2155304)[0m 2024-04-07 19:20:23.819470: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2155304)[0m 2024-04-07 19:20:23.922413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2155304)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2155307)[0m 2024-04-07 19:20:25.910402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=2155307)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=2155307)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=2155302)[0m 2024-04-07 19:20:24.111548: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2155302)[0m 2024-04-07 19:20:24.200505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2155302)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=2155302)[0m 2024-04-07 19:20:26.257382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
DEBUG flwr 2024-04-07 19:20:38,376 | server.py:236 | fit_round 1 received 10 results and 0 failures
WARNING flwr 2024-04-07 19:20:38,412 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
INFO flwr 2024-04-07 19:20:39,682 | server.py:125 | fit progress: (1, 1.9484468698501587, {'accuracy': 0.5976, 'data_size': 10000}, 18.94757825799752)
INFO flwr 2024-04-07 19:20:39,683 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-04-07 19:20:39,683 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:20:48,363 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-04-07 19:20:49,698 | server.py:125 | fit progress: (2, 1.760921835899353, {'accuracy': 0.7328, 'data_size': 10000}, 28.963701447995845)
INFO flwr 2024-04-07 19:20:49,699 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-04-07 19:20:49,699 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:20:57,702 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-04-07 19:20:58,832 | server.py:125 | fit progress: (3, 1.669211506843567, {'accuracy': 0.8412, 'data_size': 10000}, 38.09702465002192)
INFO flwr 2024-04-07 19:20:58,832 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-04-07 19:20:58,832 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:06,988 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:08,370 | server.py:125 | fit progress: (4, 1.6185505390167236, {'accuracy': 0.8912, 'data_size': 10000}, 47.63491047901334)
INFO flwr 2024-04-07 19:21:08,370 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:08,370 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:16,342 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:17,767 | server.py:125 | fit progress: (5, 1.6422688961029053, {'accuracy': 0.8536, 'data_size': 10000}, 57.03199060104089)
INFO flwr 2024-04-07 19:21:17,767 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:17,767 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:25,620 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:27,052 | server.py:125 | fit progress: (6, 1.59763765335083, {'accuracy': 0.9004, 'data_size': 10000}, 66.31765899603488)
INFO flwr 2024-04-07 19:21:27,053 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:27,053 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:35,146 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:36,424 | server.py:125 | fit progress: (7, 1.590592384338379, {'accuracy': 0.9027, 'data_size': 10000}, 75.68958995700814)
INFO flwr 2024-04-07 19:21:36,424 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:36,425 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:44,431 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:45,925 | server.py:125 | fit progress: (8, 1.6026811599731445, {'accuracy': 0.8874, 'data_size': 10000}, 85.1903853790136)
INFO flwr 2024-04-07 19:21:45,925 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:45,925 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:21:54,256 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-04-07 19:21:55,578 | server.py:125 | fit progress: (9, 1.5855355262756348, {'accuracy': 0.9047, 'data_size': 10000}, 94.84350124699995)
INFO flwr 2024-04-07 19:21:55,578 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-04-07 19:21:55,579 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-04-07 19:22:04,302 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-04-07 19:22:05,644 | server.py:125 | fit progress: (10, 1.5925425291061401, {'accuracy': 0.8966, 'data_size': 10000}, 104.90954518504441)
INFO flwr 2024-04-07 19:22:05,644 | server.py:171 | evaluate_round 10: no clients selected, cancel
INFO flwr 2024-04-07 19:22:05,645 | server.py:153 | FL finished in 104.91001103702001
INFO flwr 2024-04-07 19:22:05,645 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-04-07 19:22:05,645 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-04-07 19:22:05,645 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-04-07 19:22:05,645 | app.py:229 | app_fit: losses_centralized [(0, 2.3016276359558105), (1, 1.9484468698501587), (2, 1.760921835899353), (3, 1.669211506843567), (4, 1.6185505390167236), (5, 1.6422688961029053), (6, 1.59763765335083), (7, 1.590592384338379), (8, 1.6026811599731445), (9, 1.5855355262756348), (10, 1.5925425291061401)]
INFO flwr 2024-04-07 19:22:05,645 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.114), (1, 0.5976), (2, 0.7328), (3, 0.8412), (4, 0.8912), (5, 0.8536), (6, 0.9004), (7, 0.9027), (8, 0.8874), (9, 0.9047), (10, 0.8966)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.8966
wandb:     loss 1.59254
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240407_192000-5wkg753g
wandb: Find logs at: ./wandb/offline-run-20240407_192000-5wkg753g/logs
[2m[36m(DefaultActor pid=2155301)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=2155301)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 7x across cluster][0m
