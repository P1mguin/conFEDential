ctit088
2024-06-11 11:20:37,818	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-06-11 11:20:37,820	INFO scripts.py:722 -- Local node IP: 10.20.240.18
2024-06-11 11:21:04,041	SUCC scripts.py:759 -- --------------------
2024-06-11 11:21:04,041	SUCC scripts.py:760 -- Ray runtime started.
2024-06-11 11:21:04,042	SUCC scripts.py:761 -- --------------------
2024-06-11 11:21:04,042	INFO scripts.py:763 -- Next steps
2024-06-11 11:21:04,042	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-06-11 11:21:04,042	INFO scripts.py:769 --   ray start --address='10.20.240.18:6379'
2024-06-11 11:21:04,042	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-06-11 11:21:04,042	INFO scripts.py:780 -- import ray
2024-06-11 11:21:04,042	INFO scripts.py:781 -- ray.init()
2024-06-11 11:21:04,042	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-06-11 11:21:04,042	INFO scripts.py:813 --   ray stop
2024-06-11 11:21:04,043	INFO scripts.py:816 -- To view the status of the cluster, use
2024-06-11 11:21:04,043	INFO scripts.py:817 --   ray status
2024-06-11 11:23:06.455162: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 11:23:12.505684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 11:23:45.960120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:25:48,337 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 11:25:48,463 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-11 11:25:55,616 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 11:25:55,617 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 11:26:08,747 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-06-11 11:26:08,750 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAdam
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 11:26:08,757 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 11:26:08,836 | Config.py:77 | Found previous federated learning simulation with hash 87ea45dfa4df676e5e81d7806ced3e9b4155d64372f671ff356da4564b06977b, continuing to attack simulation...
2024-06-11 11:26:38.880945: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 11:26:38.937107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 11:26:40.133125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:26:46,575 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 11:26:46,576 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-11 11:26:46,798 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 11:26:46,798 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 11:26:54,831 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-06-11 11:26:54,834 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAvg
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 11:26:54,841 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 11:26:54,874 | Config.py:72 | No previous federated learning simulation found with hash 3dda1d960d86602a827554c0fcb07d542640155a876087bf82ace2d6bfc2db5d, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 11:26:56,571 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 11:26:56,572 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 11:26:56,617	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 11:26:56,630	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 11:26:56,644 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 11:26:56,697	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 11:26:56,698 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 34359738368.0, 'node:__internal_head__': 2.0, 'CPU': 4.0, 'node:10.20.240.18': 2.0, 'object_store_memory': 22894210252.0, 'accelerator_type:G': 2.0, 'GPU': 2.0}
INFO flwr 2024-06-11 11:26:56,698 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 11:26:56,698 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 11:26:56,709 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 11:26:56,710 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 11:26:56,710 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 11:26:56,710 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 11:26:59,617 | server.py:94 | initial parameters (loss, other metrics): 2.5065833984375, {'accuracy': 0.0978, 'data_size': 10000}
INFO flwr 2024-06-11 11:26:59,617 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 11:26:59,618 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=97709)[0m 2024-06-11 11:27:56.094348: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=97709)[0m 2024-06-11 11:27:56.152904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=97709)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=97709)[0m 2024-06-11 11:27:57.660297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[33m(raylet)[0m [2024-06-11 11:28:04,053 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:28:04,053 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:59,382 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:59,384 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:59,383 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:59,393 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:29:04,054 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:29:04,054 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:12,542 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:12,542 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,965 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,966 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,965 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,966 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,966 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,967 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,967 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,968 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,968 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,968 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:29:18,965 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:29:18,969 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:29:18,989 | server.py:236 | fit_round 1 received 1 results and 9 failures
INFO flwr 2024-06-11 11:30:50,588 | server.py:125 | fit progress: (1, 2.188518359375, {'accuracy': 0.2129, 'data_size': 10000}, 230.97057619597763)
INFO flwr 2024-06-11 11:30:50,589 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 11:30:50,591 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 11:31:24,382 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:24,398 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:31,309 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:31,309 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:37,979 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:37,980 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:37,981 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:37,981 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:44,644 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:44,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:57,284 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:57,284 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:57,285 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:57,285 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:57,286 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:57,286 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:57,286 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 42cccf52deb5c54da31505e702000000, name=DefaultActor.__init__, pid=97710, memory used=1.42GB) was running was 238.95GB / 251.51GB (0.950058), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d3160b4e59563305b52546e19ad5fa8e5032526012b9aeeeaec0c2c6*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
97710	1.42	ray::DefaultActor.run
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:57,286 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:31:57,287 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:31:57,286 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: b782567f8b3607c3d2c99d8702000000, name=DefaultActor.__init__, pid=97709, memory used=2.38GB) was running was 239.02GB / 251.51GB (0.95033), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d87a36516e9fb56f3b745d6a3109df26c77eef12918a16665a3fdc5e*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	7.00	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	3.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
97709	2.38	ray::DefaultActor
91576	2.15	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.22	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
91577	0.21	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:31:57,354 | server.py:236 | fit_round 2 received 0 results and 10 failures
ERROR flwr 2024-06-11 11:31:57,389 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 11:31:57,581 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 11:31:57,582 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 11:31:57,583 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2129
wandb:     loss 2.18852
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_112656-n7xouu5s
wandb: Find logs at: ./wandb/offline-run-20240611_112656-n7xouu5s/logs
[2m[36m(pid=97710)[0m 2024-06-11 11:27:56.601791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=97710)[0m 2024-06-11 11:27:56.714966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=97710)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=97710)[0m 2024-06-11 11:27:58.863332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[1m[36m(autoscaler +32s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
2024-06-11 11:33:07.851511: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 11:33:08.434883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 11:33:10.164836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:33:19,947 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 11:33:19,948 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-11 11:33:20,197 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 11:33:20,198 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 11:33:27,370 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDNAG, running...
INFO flwr 2024-06-11 11:33:27,374 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedNag
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
				momentum: 0.9
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				lr: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 11:33:27,380 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 11:33:27,397 | Config.py:72 | No previous federated learning simulation found with hash 34a8be47a4ea137dd430b718a2caa64d7341558dc6b6a5bdf24494dc629b6290, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 11:33:29,327 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 11:33:29,327 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 11:33:29,372	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 11:33:29,401	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 11:33:29,437 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 11:33:29,486	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 11:33:29,487 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 4.0, 'node:__internal_head__': 2.0, 'accelerator_type:G': 2.0, 'GPU': 2.0, 'object_store_memory': 22894210252.0, 'node:10.20.240.18': 2.0, 'memory': 34359738368.0}
INFO flwr 2024-06-11 11:33:29,488 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 11:33:29,488 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 11:33:29,503 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 11:33:29,505 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 11:33:29,505 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 11:33:29,505 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 11:33:31,165 | server.py:94 | initial parameters (loss, other metrics): 2.5065833984375, {'accuracy': 0.0978, 'data_size': 10000}
INFO flwr 2024-06-11 11:33:31,166 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 11:33:31,166 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=106690)[0m 2024-06-11 11:33:31.300240: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=106690)[0m 2024-06-11 11:33:31.369662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=106690)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=106690)[0m 2024-06-11 11:33:32.792972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ERROR flwr 2024-06-11 11:34:13,444 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:13,455 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:34:19,673 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:19,684 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:34:33,260 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:33,261 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:34:41,119 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:41,172 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:34:41,120 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:41,173 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:34:58,799 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:34:58,800 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:35:04,062 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:35:04,066 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:35:18,432 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:35:18,432 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:35:18,434 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:35:18,434 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 68ccaceef01af1719c23506603000000, name=DefaultActor.__init__, pid=106690, memory used=2.10GB) was running was 239.04GB / 251.51GB (0.9504), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-014149fb0b886fa8aa02fa18de09dccc9ed54572e751485ac38f1743*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	
106690	2.10	ray::DefaultActor.run
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:35:18,435 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:35:18,435 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 160e1a23c82db273d55e6e3303000000, name=DefaultActor.__init__, pid=106689, memory used=2.47GB) was running was 239.02GB / 251.51GB (0.950337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4ec811699666b1186595234573f82f11137b924e253808ad04f0b6ce*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
105408	4.17	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
92062	3.18	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106689	2.47	ray::DefaultActor
91577	2.07	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106690	2.04	ray::DefaultActor.run
91576	1.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:35:18,477 | server.py:236 | fit_round 1 received 1 results and 9 failures
