ctit081
	Adding python 3.10.7 (ubuntu 20.04) to your environment
	Adding slurm utilities
	Adding compute-node cpu/gpu monitoring utilities
2024-05-14 16:09:09.810104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-14 16:09:11.717390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 16:09:21,756 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-14 16:09:21,758 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-14 16:09:24,874 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-14 16:09:24,875 | Simulation.py:242 | Found previously split dataloaders, loading them
INFO flwr 2024-05-14 16:09:26,433 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-05-14 16:09:26,433 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-14 16:09:26,435 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-14 16:09:30,635 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-14 16:09:30,636 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-14 16:09:30,638 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-14 16:09:38,505	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-14 16:09:38,695	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-14 16:09:38,883	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_01d5da628194c522.zip' (1.09MiB) to Ray cluster...
2024-05-14 16:09:38,889	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_01d5da628194c522.zip'.
INFO flwr 2024-05-14 16:09:52,351 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'node:10.20.240.11': 1.0, 'GPU': 1.0, 'object_store_memory': 61656631296.0, 'node:__internal_head__': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 133865473024.0}
INFO flwr 2024-05-14 16:09:52,351 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-14 16:09:52,351 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-14 16:09:52,371 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-14 16:09:52,372 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-14 16:09:52,372 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-14 16:09:52,372 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1833180)[0m 2024-05-14 16:10:08.334327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1833180)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1833180)[0m 2024-05-14 16:10:10.679389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 16:10:40,398 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-14 16:10:40,399 | server.py:104 | FL starting
DEBUG flwr 2024-05-14 16:10:40,399 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=1833180)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1833180)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1833181)[0m 2024-05-14 16:10:08.332180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1833181)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1833181)[0m 2024-05-14 16:10:10.680755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1833175)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 6x across cluster][0m
[2m[36m(DefaultActor pid=1833175)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 6x across cluster][0m
DEBUG flwr 2024-05-14 16:11:23,775 | server.py:236 | fit_round 1 received 10 results and 0 failures
ERROR flwr 2024-05-14 16:12:27,800 | app.py:313 | tuple index out of range
ERROR flwr 2024-05-14 16:12:27,809 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 81, in aggregate_fit
    self._capture_aggregates(
  File "/home/s2240084/conFEDential/src/training/Server.py", line 112, in _capture_aggregates
    expanded_matrix = np.zeros((shape[0] + 1, shape[1], *shape[2:]))
IndexError: tuple index out of range

ERROR flwr 2024-05-14 16:12:27,810 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-14 16:12:27,810 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0943
wandb:     loss 0.00025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240514_160929-y3hw9kpg
wandb: Find logs at: ./wandb/offline-run-20240514_160929-y3hw9kpg/logs
[2m[36m(DefaultActor pid=1833174)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1833174)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
2024-05-14 16:13:53.314978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-14 16:13:54.907718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 16:13:59,787 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-14 16:13:59,788 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-14 16:14:01,393 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-14 16:14:01,394 | Simulation.py:242 | Found previously split dataloaders, loading them
INFO flwr 2024-05-14 16:14:02,387 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-05-14 16:14:02,387 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-14 16:14:02,389 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-14 16:14:06,692 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-14 16:14:06,693 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-14 16:14:06,695 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-14 16:14:11,808	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-14 16:14:11,914	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-14 16:14:12,000	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_01d5da628194c522.zip' (1.09MiB) to Ray cluster...
2024-05-14 16:14:12,003	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_01d5da628194c522.zip'.
INFO flwr 2024-05-14 16:14:24,623 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.11': 1.0, 'GPU': 1.0, 'memory': 132888417280.0, 'accelerator_type:TITAN': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'object_store_memory': 61237893120.0}
INFO flwr 2024-05-14 16:14:24,623 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-14 16:14:24,623 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-14 16:14:24,643 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-14 16:14:24,644 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-14 16:14:24,644 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-14 16:14:24,644 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-14 16:14:26,375 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-14 16:14:26,376 | server.py:104 | FL starting
DEBUG flwr 2024-05-14 16:14:26,377 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1835350)[0m 2024-05-14 16:14:31.411186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1835350)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1835350)[0m 2024-05-14 16:14:33.764373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1835353)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1835353)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1835349)[0m 2024-05-14 16:14:31.847689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1835349)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1835349)[0m 2024-05-14 16:14:34.418947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1835349)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1835349)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
DEBUG flwr 2024-05-14 16:14:58,943 | server.py:236 | fit_round 1 received 10 results and 0 failures
ERROR flwr 2024-05-14 16:15:36,286 | app.py:313 | tuple index out of range
ERROR flwr 2024-05-14 16:15:36,382 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 81, in aggregate_fit
    self._capture_aggregates(
  File "/home/s2240084/conFEDential/src/training/Server.py", line 112, in _capture_aggregates
    expanded_matrix = np.zeros((shape[0] + 1, shape[1], *shape[2:]))
IndexError: tuple index out of range

ERROR flwr 2024-05-14 16:15:36,382 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-14 16:15:36,382 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0943
wandb:     loss 0.00025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240514_161405-u5xcx5rk
wandb: Find logs at: ./wandb/offline-run-20240514_161405-u5xcx5rk/logs
[2m[36m(DefaultActor pid=1835348)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1835348)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
2024-05-14 16:16:42.900171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-14 16:16:44.367504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-14 16:16:49,262 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-14 16:16:49,263 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-14 16:16:51,014 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-14 16:16:51,017 | Simulation.py:242 | Found previously split dataloaders, loading them
INFO flwr 2024-05-14 16:16:52,017 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDNAG, running...
INFO flwr 2024-05-14 16:16:52,017 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-14 16:16:52,018 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-14 16:16:54,110 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-14 16:16:54,111 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-14 16:16:54,113 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-14 16:16:56,914	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-14 16:16:57,007	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-14 16:16:57,091	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_01d5da628194c522.zip' (1.09MiB) to Ray cluster...
2024-05-14 16:16:57,095	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_01d5da628194c522.zip'.
INFO flwr 2024-05-14 16:17:09,890 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 64.0, 'object_store_memory': 61191201177.0, 'accelerator_type:TITAN': 1.0, 'memory': 132779469415.0, 'node:10.20.240.11': 1.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-05-14 16:17:09,891 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-14 16:17:09,891 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-14 16:17:09,912 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-14 16:17:09,913 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-14 16:17:09,913 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-14 16:17:09,913 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-14 16:17:12,038 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-14 16:17:12,039 | server.py:104 | FL starting
DEBUG flwr 2024-05-14 16:17:12,039 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1839239)[0m 2024-05-14 16:17:17.021105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1839239)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1839244)[0m 2024-05-14 16:17:20.290944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=1839245)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=1839245)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=1839242)[0m 2024-05-14 16:17:17.639006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=1839242)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=1839242)[0m 2024-05-14 16:17:20.528838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
[2m[36m(DefaultActor pid=1839238)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1839238)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 5x across cluster][0m
[2m[36m(DefaultActor pid=1839240)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)[32m [repeated 2x across cluster][0m
[2m[36m(DefaultActor pid=1839240)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)[32m [repeated 2x across cluster][0m
DEBUG flwr 2024-05-14 16:17:45,403 | server.py:236 | fit_round 1 received 10 results and 0 failures
ERROR flwr 2024-05-14 16:19:39,547 | app.py:313 | tuple index out of range
ERROR flwr 2024-05-14 16:19:39,564 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 81, in aggregate_fit
    self._capture_aggregates(
  File "/home/s2240084/conFEDential/src/training/Server.py", line 112, in _capture_aggregates
    expanded_matrix = np.zeros((shape[0] + 1, shape[1], *shape[2:]))
IndexError: tuple index out of range

ERROR flwr 2024-05-14 16:19:39,564 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-14 16:19:39,565 | Simulation.py:168 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0943
wandb:     loss 0.00025
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240514_161653-zdpbniv7
wandb: Find logs at: ./wandb/offline-run-20240514_161653-zdpbniv7/logs
