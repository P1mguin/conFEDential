ctit088
2024-05-15 13:19:56.372574: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 13:20:07.412512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 13:20:26.240477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-15 13:22:28,122 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-15 13:22:28,146 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-15 13:23:16,202 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-15 13:23:16,203 | Simulation.py:240 | Found previously split dataloaders, loading them
INFO flwr 2024-05-15 13:23:39,853 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-05-15 13:23:39,853 | main.py:72 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 1
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.1, 'betas': [0.9, 0.999], 'eps': 1e-09, 'weight_decay': 0.9999}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-15 13:23:39,853 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-15 13:23:39,867 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-15 13:23:50,418 | Simulation.py:365 | Created 4 clients with resources 16 CPUs and 0.25 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-15 13:23:50,419 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-15 13:23:50,421 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-15 13:24:02,702	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-15 13:24:05,516	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-15 13:24:05,581	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip' (0.16MiB) to Ray cluster...
2024-05-15 13:24:05,582	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip'.
INFO flwr 2024-05-15 13:24:17,313 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:__internal_head__': 1.0, 'memory': 126443892122.0, 'object_store_memory': 58475953766.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-05-15 13:24:17,313 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-15 13:24:17,313 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 16, 'num_gpus': 0.25}
INFO flwr 2024-05-15 13:24:17,333 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
INFO flwr 2024-05-15 13:24:17,334 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-15 13:24:17,334 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-15 13:24:17,334 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=3008889)[0m 2024-05-15 13:24:21.512384: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3008889)[0m 2024-05-15 13:24:21.568899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3008889)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3008889)[0m 2024-05-15 13:24:23.561475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-15 13:24:46,388 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-15 13:24:46,388 | server.py:104 | FL starting
DEBUG flwr 2024-05-15 13:24:46,388 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=3008892)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3008892)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3008892)[0m 2024-05-15 13:24:21.640227: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=3008892)[0m 2024-05-15 13:24:21.729969: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster][0m
[2m[36m(pid=3008892)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
[2m[36m(pid=3008890)[0m 2024-05-15 13:24:23.865306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-05-15 13:25:11,762 | server.py:236 | fit_round 1 received 10 results and 0 failures
srun: error: ctit088: task 0: Killed
srun: Force Terminated StepId=285797.0
2024-05-15 13:32:17.254003: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 13:32:18.039399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 13:32:20.424153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-15 13:32:29,051 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-15 13:32:29,052 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-15 13:32:32,041 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-15 13:32:32,042 | Simulation.py:240 | Found previously split dataloaders, loading them
INFO flwr 2024-05-15 13:32:33,584 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-05-15 13:32:33,584 | main.py:72 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 1
			local_rounds: 1
		Model:
			optimizer_name: FedAvg
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-15 13:32:33,584 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-15 13:32:33,586 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-15 13:32:35,642 | Simulation.py:365 | Created 4 clients with resources 16 CPUs and 0.25 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-15 13:32:35,643 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-15 13:32:35,645 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-15 13:32:37,876	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-15 13:32:37,989	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-15 13:32:38,063	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip' (0.16MiB) to Ray cluster...
2024-05-15 13:32:38,064	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip'.
INFO flwr 2024-05-15 13:32:48,304 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 63314412748.0, 'node:10.20.240.18': 1.0, 'node:__internal_head__': 1.0, 'memory': 137733629748.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-15 13:32:48,304 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-15 13:32:48,304 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 16, 'num_gpus': 0.25}
INFO flwr 2024-05-15 13:32:48,317 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
INFO flwr 2024-05-15 13:32:48,317 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-15 13:32:48,317 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-15 13:32:48,318 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-15 13:32:52,599 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-15 13:32:52,599 | server.py:104 | FL starting
DEBUG flwr 2024-05-15 13:32:52,600 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3013015)[0m 2024-05-15 13:32:54.871562: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3013015)[0m 2024-05-15 13:32:54.927375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3013015)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3013015)[0m 2024-05-15 13:32:56.293114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(DefaultActor pid=3013018)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3013018)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3013017)[0m 2024-05-15 13:32:54.995714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=3013017)[0m 2024-05-15 13:32:55.086866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster][0m
[2m[36m(pid=3013017)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
[2m[36m(pid=3013017)[0m 2024-05-15 13:32:56.625250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-05-15 13:33:15,494 | server.py:236 | fit_round 1 received 10 results and 0 failures
srun: error: ctit088: task 0: Killed
srun: Force Terminated StepId=285797.1
2024-05-15 13:38:31.320175: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 13:38:55.437824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 13:39:05.178635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-15 13:39:41,012 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-15 13:39:41,013 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-15 13:40:06,920 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-15 13:40:06,921 | Simulation.py:240 | Found previously split dataloaders, loading them
INFO flwr 2024-05-15 13:40:45,249 | main.py:70 | Loaded 1 configs with name CIFAR10-RESNET18-FEDNAG, running...
INFO flwr 2024-05-15 13:40:45,249 | main.py:72 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 1
			local_rounds: 1
		Model:
			optimizer_name: FedNag
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
				momentum: 0.9
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-15 13:40:45,249 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-15 13:40:45,435 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-15 13:40:47,624 | Simulation.py:365 | Created 4 clients with resources 16 CPUs and 0.25 GPUs for the total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-15 13:40:47,626 | Simulation.py:157 | Starting federated learning simulation
INFO flwr 2024-05-15 13:40:47,628 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-15 13:40:57,902	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-15 13:40:58,042	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-15 13:40:58,128	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip' (0.16MiB) to Ray cluster...
2024-05-15 13:40:58,130	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_cbafdf10a2a492f0.zip'.
INFO flwr 2024-05-15 13:41:08,402 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 142716593972.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 64.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 65449968844.0}
INFO flwr 2024-05-15 13:41:08,402 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-15 13:41:08,402 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 16, 'num_gpus': 0.25}
INFO flwr 2024-05-15 13:41:08,415 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
INFO flwr 2024-05-15 13:41:08,416 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-15 13:41:08,416 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-15 13:41:08,416 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=3016836)[0m 2024-05-15 13:41:29.079911: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3016836)[0m 2024-05-15 13:41:29.138001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3016836)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3016835)[0m 2024-05-15 13:41:34.105089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3016833)[0m 2024-05-15 13:41:29.512876: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=3016833)[0m 2024-05-15 13:41:29.572724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster][0m
[2m[36m(pid=3016833)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
INFO flwr 2024-05-15 13:42:05,575 | server.py:94 | initial parameters (loss, other metrics): 0.0002507773160934448, {'accuracy': 0.0943, 'data_size': 10000}
INFO flwr 2024-05-15 13:42:05,576 | server.py:104 | FL starting
DEBUG flwr 2024-05-15 13:42:05,576 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(DefaultActor pid=3016836)[0m /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
[2m[36m(DefaultActor pid=3016836)[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
[2m[36m(pid=3016836)[0m 2024-05-15 13:41:34.121898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 3x across cluster][0m
DEBUG flwr 2024-05-15 13:42:42,539 | server.py:236 | fit_round 1 received 10 results and 0 failures
srun: error: ctit088: task 0: Killed
srun: Force Terminated StepId=285797.2
