ctit088
2024-06-10 16:23:33,526	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-06-10 16:23:33,526	INFO scripts.py:722 -- Local node IP: 10.20.240.18
2024-06-10 16:23:56,539	SUCC scripts.py:759 -- --------------------
2024-06-10 16:23:56,539	SUCC scripts.py:760 -- Ray runtime started.
2024-06-10 16:23:56,539	SUCC scripts.py:761 -- --------------------
2024-06-10 16:23:56,539	INFO scripts.py:763 -- Next steps
2024-06-10 16:23:56,540	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-06-10 16:23:56,540	INFO scripts.py:769 --   ray start --address='10.20.240.18:6379'
2024-06-10 16:23:56,540	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-06-10 16:23:56,540	INFO scripts.py:780 -- import ray
2024-06-10 16:23:56,540	INFO scripts.py:781 -- ray.init()
2024-06-10 16:23:56,540	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-06-10 16:23:56,540	INFO scripts.py:813 --   ray stop
2024-06-10 16:23:56,541	INFO scripts.py:816 -- To view the status of the cluster, use
2024-06-10 16:23:56,541	INFO scripts.py:817 --   ray status
2024-06-10 16:25:19.131917: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-10 16:25:36.419287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-10 16:25:57.753860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-10 16:26:15,833 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-10 16:26:15,834 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-10 16:26:17,758 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-10 16:26:17,759 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-10 16:26:28,341 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-06-10 16:26:28,343 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAdam
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-10 16:26:28,351 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-10 16:26:28,354 | Config.py:72 | No previous federated learning simulation found with hash 87ea45dfa4df676e5e81d7806ced3e9b4155d64372f671ff356da4564b06977b, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-10 16:26:33,788 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-10 16:26:33,788 | Simulation.py:161 | Starting federated learning simulation
2024-06-10 16:26:33,834	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-10 16:26:33,846	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-10 16:26:33,908 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-10 16:26:33,996	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-10 16:26:33,997 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 34359738368.0, 'node:__internal_head__': 2.0, 'CPU': 4.0, 'accelerator_type:G': 2.0, 'GPU': 2.0, 'object_store_memory': 36316086681.0, 'node:10.20.240.18': 2.0}
INFO flwr 2024-06-10 16:26:33,998 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-10 16:26:33,998 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-10 16:26:34,014 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-10 16:26:34,015 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-10 16:26:34,016 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-10 16:26:34,016 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=2909509)[0m 2024-06-10 16:26:35.654767: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2909509)[0m 2024-06-10 16:26:35.763439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2909509)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2909509)[0m 2024-06-10 16:26:37.489425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-10 16:26:43,600 | server.py:94 | initial parameters (loss, other metrics): 2.5065833984375, {'accuracy': 0.0978, 'data_size': 10000}
INFO flwr 2024-06-10 16:26:43,601 | server.py:104 | FL starting
DEBUG flwr 2024-06-10 16:26:43,601 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:28:43,471 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-10 16:30:23,677 | server.py:125 | fit progress: (1, 2.729272265625, {'accuracy': 0.2494, 'data_size': 10000}, 220.07533362694085)
INFO flwr 2024-06-10 16:30:23,677 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-10 16:30:23,678 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:32:03,329 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-10 16:32:24,833 | server.py:125 | fit progress: (2, 4.2696359375, {'accuracy': 0.1788, 'data_size': 10000}, 341.2313460907899)
INFO flwr 2024-06-10 16:32:24,833 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-10 16:32:24,833 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:34:10,867 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-10 16:34:32,836 | server.py:125 | fit progress: (3, 4.712312109375, {'accuracy': 0.1396, 'data_size': 10000}, 469.2348729809746)
INFO flwr 2024-06-10 16:34:32,837 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-10 16:34:32,837 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:36:11,138 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-10 16:36:32,825 | server.py:125 | fit progress: (4, 3.6408109375, {'accuracy': 0.2247, 'data_size': 10000}, 589.2241413178854)
INFO flwr 2024-06-10 16:36:32,826 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-10 16:36:32,826 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
[2m[1m[36m(autoscaler +1m29s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +1m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 16:38:19,184 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-10 16:38:41,469 | server.py:125 | fit progress: (5, 3.7635578125, {'accuracy': 0.2438, 'data_size': 10000}, 717.867880705744)
INFO flwr 2024-06-10 16:38:41,470 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-10 16:38:41,470 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:40:20,957 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-10 16:40:42,545 | server.py:125 | fit progress: (6, 3.1335787109375, {'accuracy': 0.2182, 'data_size': 10000}, 838.9440664378926)
INFO flwr 2024-06-10 16:40:42,546 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-10 16:40:42,546 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:42:28,145 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-10 16:42:51,071 | server.py:125 | fit progress: (7, 2.2705513671875, {'accuracy': 0.3092, 'data_size': 10000}, 967.4695722586475)
INFO flwr 2024-06-10 16:42:51,071 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-10 16:42:51,072 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:44:30,527 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-10 16:44:52,116 | server.py:125 | fit progress: (8, 2.10251953125, {'accuracy': 0.2802, 'data_size': 10000}, 1088.5147668826394)
INFO flwr 2024-06-10 16:44:52,117 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-10 16:44:52,117 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:46:37,302 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-06-10 16:46:59,615 | server.py:125 | fit progress: (9, 1.9574943359375, {'accuracy': 0.294, 'data_size': 10000}, 1216.0139115797356)
INFO flwr 2024-06-10 16:46:59,616 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-10 16:46:59,616 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +12m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +13m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +13m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +15m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +15m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +18m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +18m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +20m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +20m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 16:48:38,895 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-06-10 16:49:00,447 | server.py:125 | fit progress: (10, 1.858292578125, {'accuracy': 0.3509, 'data_size': 10000}, 1336.8459059759043)
INFO flwr 2024-06-10 16:49:00,448 | server.py:171 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2024-06-10 16:49:00,448 | server.py:222 | fit_round 11: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:50:45,924 | server.py:236 | fit_round 11 received 10 results and 0 failures
INFO flwr 2024-06-10 16:51:06,782 | server.py:125 | fit progress: (11, 1.761594921875, {'accuracy': 0.359, 'data_size': 10000}, 1463.1801781589165)
INFO flwr 2024-06-10 16:51:06,782 | server.py:171 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2024-06-10 16:51:06,782 | server.py:222 | fit_round 12: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:52:44,784 | server.py:236 | fit_round 12 received 10 results and 0 failures
INFO flwr 2024-06-10 16:53:05,725 | server.py:125 | fit progress: (12, 1.61089013671875, {'accuracy': 0.4021, 'data_size': 10000}, 1582.123943392653)
INFO flwr 2024-06-10 16:53:05,726 | server.py:171 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2024-06-10 16:53:05,726 | server.py:222 | fit_round 13: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:54:52,105 | server.py:236 | fit_round 13 received 10 results and 0 failures
INFO flwr 2024-06-10 16:55:12,919 | server.py:125 | fit progress: (13, 1.62726328125, {'accuracy': 0.4147, 'data_size': 10000}, 1709.3172866678797)
INFO flwr 2024-06-10 16:55:12,919 | server.py:171 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2024-06-10 16:55:12,920 | server.py:222 | fit_round 14: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:56:51,154 | server.py:236 | fit_round 14 received 10 results and 0 failures
[2m[1m[33m(autoscaler +21m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +21m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +21m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +21m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +22m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +22m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +23m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +23m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +27m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +27m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 16:57:20,499 | server.py:125 | fit progress: (14, 1.56366298828125, {'accuracy': 0.4302, 'data_size': 10000}, 1836.897944978904)
INFO flwr 2024-06-10 16:57:20,500 | server.py:171 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2024-06-10 16:57:20,500 | server.py:222 | fit_round 15: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 16:59:08,005 | server.py:236 | fit_round 15 received 10 results and 0 failures
INFO flwr 2024-06-10 16:59:35,403 | server.py:125 | fit progress: (15, 1.53457548828125, {'accuracy': 0.4415, 'data_size': 10000}, 1971.8019974916242)
INFO flwr 2024-06-10 16:59:35,404 | server.py:171 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2024-06-10 16:59:35,404 | server.py:222 | fit_round 16: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:01:13,910 | server.py:236 | fit_round 16 received 10 results and 0 failures
INFO flwr 2024-06-10 17:01:39,492 | server.py:125 | fit progress: (16, 1.4834265625, {'accuracy': 0.4586, 'data_size': 10000}, 2095.8911433019675)
INFO flwr 2024-06-10 17:01:39,493 | server.py:171 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2024-06-10 17:01:39,493 | server.py:222 | fit_round 17: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:03:25,277 | server.py:236 | fit_round 17 received 10 results and 0 failures
INFO flwr 2024-06-10 17:03:54,137 | server.py:125 | fit progress: (17, 1.52485615234375, {'accuracy': 0.4411, 'data_size': 10000}, 2230.536196091678)
INFO flwr 2024-06-10 17:03:54,138 | server.py:171 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2024-06-10 17:03:54,138 | server.py:222 | fit_round 18: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:05:32,806 | server.py:236 | fit_round 18 received 10 results and 0 failures
INFO flwr 2024-06-10 17:06:04,842 | server.py:125 | fit progress: (18, 1.54135634765625, {'accuracy': 0.424, 'data_size': 10000}, 2361.2410185118206)
INFO flwr 2024-06-10 17:06:04,843 | server.py:171 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2024-06-10 17:06:04,843 | server.py:222 | fit_round 19: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:07:50,807 | server.py:236 | fit_round 19 received 10 results and 0 failures
INFO flwr 2024-06-10 17:08:15,783 | server.py:125 | fit progress: (19, 1.467231640625, {'accuracy': 0.4662, 'data_size': 10000}, 2492.182056202553)
INFO flwr 2024-06-10 17:08:15,784 | server.py:171 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2024-06-10 17:08:15,784 | server.py:222 | fit_round 20: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:09:54,035 | server.py:236 | fit_round 20 received 10 results and 0 failures
[2m[1m[33m(autoscaler +30m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +34m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +35m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +35m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +35m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +35m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +36m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +36m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +36m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +37m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +37m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +37m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +38m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +39m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +39m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +40m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +40m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +40m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +40m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +41m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +41m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +42m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +42m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 17:10:16,366 | server.py:125 | fit progress: (20, 1.4451220703125, {'accuracy': 0.478, 'data_size': 10000}, 2612.7647662898526)
INFO flwr 2024-06-10 17:10:16,367 | server.py:171 | evaluate_round 20: no clients selected, cancel
DEBUG flwr 2024-06-10 17:10:16,367 | server.py:222 | fit_round 21: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:12:02,783 | server.py:236 | fit_round 21 received 10 results and 0 failures
INFO flwr 2024-06-10 17:12:24,211 | server.py:125 | fit progress: (21, 1.44535107421875, {'accuracy': 0.4862, 'data_size': 10000}, 2740.6095439218916)
INFO flwr 2024-06-10 17:12:24,212 | server.py:171 | evaluate_round 21: no clients selected, cancel
DEBUG flwr 2024-06-10 17:12:24,212 | server.py:222 | fit_round 22: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:14:01,883 | server.py:236 | fit_round 22 received 10 results and 0 failures
INFO flwr 2024-06-10 17:14:27,201 | server.py:125 | fit progress: (22, 1.37228916015625, {'accuracy': 0.4996, 'data_size': 10000}, 2863.599536096677)
INFO flwr 2024-06-10 17:14:27,201 | server.py:171 | evaluate_round 22: no clients selected, cancel
DEBUG flwr 2024-06-10 17:14:27,202 | server.py:222 | fit_round 23: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:16:11,988 | server.py:236 | fit_round 23 received 10 results and 0 failures
INFO flwr 2024-06-10 17:16:47,742 | server.py:125 | fit progress: (23, 1.4262708984375, {'accuracy': 0.4802, 'data_size': 10000}, 3004.140302131884)
INFO flwr 2024-06-10 17:16:47,742 | server.py:171 | evaluate_round 23: no clients selected, cancel
DEBUG flwr 2024-06-10 17:16:47,742 | server.py:222 | fit_round 24: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:18:26,926 | server.py:236 | fit_round 24 received 10 results and 0 failures
INFO flwr 2024-06-10 17:19:00,740 | server.py:125 | fit progress: (24, 1.409988671875, {'accuracy': 0.4813, 'data_size': 10000}, 3137.1391087807715)
INFO flwr 2024-06-10 17:19:00,741 | server.py:171 | evaluate_round 24: no clients selected, cancel
DEBUG flwr 2024-06-10 17:19:00,741 | server.py:222 | fit_round 25: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:20:46,362 | server.py:236 | fit_round 25 received 10 results and 0 failures
[2m[1m[33m(autoscaler +43m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +43m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +44m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +44m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +45m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +45m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +45m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +45m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +47m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +48m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +48m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +48m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +49m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +49m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +49m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +49m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +50m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +50m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +50m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +51m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +51m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +51m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +52m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +52m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +53m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +53m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +54m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 17:21:17,503 | server.py:125 | fit progress: (25, 1.3367427734375, {'accuracy': 0.5113, 'data_size': 10000}, 3273.901594659779)
INFO flwr 2024-06-10 17:21:17,503 | server.py:171 | evaluate_round 25: no clients selected, cancel
DEBUG flwr 2024-06-10 17:21:17,504 | server.py:222 | fit_round 26: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:22:56,484 | server.py:236 | fit_round 26 received 10 results and 0 failures
INFO flwr 2024-06-10 17:23:30,172 | server.py:125 | fit progress: (26, 1.3317255859375, {'accuracy': 0.5169, 'data_size': 10000}, 3406.5708796279505)
INFO flwr 2024-06-10 17:23:30,172 | server.py:171 | evaluate_round 26: no clients selected, cancel
DEBUG flwr 2024-06-10 17:23:30,173 | server.py:222 | fit_round 27: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:25:16,847 | server.py:236 | fit_round 27 received 10 results and 0 failures
INFO flwr 2024-06-10 17:25:44,849 | server.py:125 | fit progress: (27, 1.28835283203125, {'accuracy': 0.5363, 'data_size': 10000}, 3541.24748297967)
INFO flwr 2024-06-10 17:25:44,849 | server.py:171 | evaluate_round 27: no clients selected, cancel
DEBUG flwr 2024-06-10 17:25:44,850 | server.py:222 | fit_round 28: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:27:23,666 | server.py:236 | fit_round 28 received 10 results and 0 failures
INFO flwr 2024-06-10 17:28:02,212 | server.py:125 | fit progress: (28, 1.28419970703125, {'accuracy': 0.538, 'data_size': 10000}, 3678.6111682755873)
INFO flwr 2024-06-10 17:28:02,213 | server.py:171 | evaluate_round 28: no clients selected, cancel
DEBUG flwr 2024-06-10 17:28:02,213 | server.py:222 | fit_round 29: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:29:48,391 | server.py:236 | fit_round 29 received 10 results and 0 failures
INFO flwr 2024-06-10 17:30:22,157 | server.py:125 | fit progress: (29, 1.2825861328125, {'accuracy': 0.5443, 'data_size': 10000}, 3818.555867882911)
INFO flwr 2024-06-10 17:30:22,158 | server.py:171 | evaluate_round 29: no clients selected, cancel
DEBUG flwr 2024-06-10 17:30:22,158 | server.py:222 | fit_round 30: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +54m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +55m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +55m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +55m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +56m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +56m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +56m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +57m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +57m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +57m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +58m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +58m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +58m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +59m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +59m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +59m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h1m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h1m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h1m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h1m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h2m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h2m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h2m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h3m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h3m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h4m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 17:32:02,144 | server.py:236 | fit_round 30 received 10 results and 0 failures
INFO flwr 2024-06-10 17:32:29,520 | server.py:125 | fit progress: (30, 1.28283583984375, {'accuracy': 0.5437, 'data_size': 10000}, 3945.9186644628644)
INFO flwr 2024-06-10 17:32:29,521 | server.py:171 | evaluate_round 30: no clients selected, cancel
DEBUG flwr 2024-06-10 17:32:29,521 | server.py:222 | fit_round 31: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:34:14,866 | server.py:236 | fit_round 31 received 10 results and 0 failures
INFO flwr 2024-06-10 17:34:42,117 | server.py:125 | fit progress: (31, 1.23257060546875, {'accuracy': 0.5594, 'data_size': 10000}, 4078.51593254786)
INFO flwr 2024-06-10 17:34:42,118 | server.py:171 | evaluate_round 31: no clients selected, cancel
DEBUG flwr 2024-06-10 17:34:42,118 | server.py:222 | fit_round 32: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:36:21,167 | server.py:236 | fit_round 32 received 10 results and 0 failures
INFO flwr 2024-06-10 17:36:44,484 | server.py:125 | fit progress: (32, 1.23311591796875, {'accuracy': 0.5556, 'data_size': 10000}, 4200.8829161645845)
INFO flwr 2024-06-10 17:36:44,485 | server.py:171 | evaluate_round 32: no clients selected, cancel
DEBUG flwr 2024-06-10 17:36:44,485 | server.py:222 | fit_round 33: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:38:30,019 | server.py:236 | fit_round 33 received 10 results and 0 failures
INFO flwr 2024-06-10 17:38:52,672 | server.py:125 | fit progress: (33, 1.244328515625, {'accuracy': 0.5523, 'data_size': 10000}, 4329.070815380663)
INFO flwr 2024-06-10 17:38:52,673 | server.py:171 | evaluate_round 33: no clients selected, cancel
DEBUG flwr 2024-06-10 17:38:52,673 | server.py:222 | fit_round 34: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h4m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h4m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h5m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h5m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h6m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h6m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h6m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h7m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h7m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h7m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h8m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h8m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h8m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h9m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h9m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h9m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h9m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h10m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h10m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h10m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h10m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h11m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h11m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h12m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h12m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h12m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h13m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 17:40:29,657 | server.py:236 | fit_round 34 received 10 results and 0 failures
INFO flwr 2024-06-10 17:40:51,815 | server.py:125 | fit progress: (34, 1.21694501953125, {'accuracy': 0.5646, 'data_size': 10000}, 4448.213533536997)
INFO flwr 2024-06-10 17:40:51,815 | server.py:171 | evaluate_round 34: no clients selected, cancel
DEBUG flwr 2024-06-10 17:40:51,816 | server.py:222 | fit_round 35: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:42:36,540 | server.py:236 | fit_round 35 received 10 results and 0 failures
INFO flwr 2024-06-10 17:42:59,109 | server.py:125 | fit progress: (35, 1.240291015625, {'accuracy': 0.5649, 'data_size': 10000}, 4575.507553267758)
INFO flwr 2024-06-10 17:42:59,109 | server.py:171 | evaluate_round 35: no clients selected, cancel
DEBUG flwr 2024-06-10 17:42:59,110 | server.py:222 | fit_round 36: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:44:38,320 | server.py:236 | fit_round 36 received 10 results and 0 failures
INFO flwr 2024-06-10 17:45:02,744 | server.py:125 | fit progress: (36, 1.22968095703125, {'accuracy': 0.5658, 'data_size': 10000}, 4699.142286981922)
INFO flwr 2024-06-10 17:45:02,744 | server.py:171 | evaluate_round 36: no clients selected, cancel
DEBUG flwr 2024-06-10 17:45:02,744 | server.py:222 | fit_round 37: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:46:51,325 | server.py:236 | fit_round 37 received 10 results and 0 failures
INFO flwr 2024-06-10 17:47:15,244 | server.py:125 | fit progress: (37, 1.20941298828125, {'accuracy': 0.5652, 'data_size': 10000}, 4831.6430276939645)
INFO flwr 2024-06-10 17:47:15,245 | server.py:171 | evaluate_round 37: no clients selected, cancel
DEBUG flwr 2024-06-10 17:47:15,265 | server.py:222 | fit_round 38: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:48:55,016 | server.py:236 | fit_round 38 received 10 results and 0 failures
INFO flwr 2024-06-10 17:49:18,609 | server.py:125 | fit progress: (38, 1.169208203125, {'accuracy': 0.5843, 'data_size': 10000}, 4955.007399748545)
INFO flwr 2024-06-10 17:49:18,609 | server.py:171 | evaluate_round 38: no clients selected, cancel
DEBUG flwr 2024-06-10 17:49:18,609 | server.py:222 | fit_round 39: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h13m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h14m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h14m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h15m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h15m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h16m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h16m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h16m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h17m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h17m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h17m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h18m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h18m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h18m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h19m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h19m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h20m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h20m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h20m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h20m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h21m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h22m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h22m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h23m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h23m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h23m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 17:51:04,409 | server.py:236 | fit_round 39 received 10 results and 0 failures
INFO flwr 2024-06-10 17:51:28,426 | server.py:125 | fit progress: (39, 1.1730560546875, {'accuracy': 0.585, 'data_size': 10000}, 5084.824297923595)
INFO flwr 2024-06-10 17:51:28,426 | server.py:171 | evaluate_round 39: no clients selected, cancel
DEBUG flwr 2024-06-10 17:51:28,426 | server.py:222 | fit_round 40: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:53:06,494 | server.py:236 | fit_round 40 received 10 results and 0 failures
INFO flwr 2024-06-10 17:53:32,959 | server.py:125 | fit progress: (40, 1.18461904296875, {'accuracy': 0.5816, 'data_size': 10000}, 5209.357719877735)
INFO flwr 2024-06-10 17:53:32,959 | server.py:171 | evaluate_round 40: no clients selected, cancel
DEBUG flwr 2024-06-10 17:53:32,960 | server.py:222 | fit_round 41: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:55:20,297 | server.py:236 | fit_round 41 received 10 results and 0 failures
INFO flwr 2024-06-10 17:55:57,066 | server.py:125 | fit progress: (41, 1.19386455078125, {'accuracy': 0.5752, 'data_size': 10000}, 5353.464748956729)
INFO flwr 2024-06-10 17:55:57,066 | server.py:171 | evaluate_round 41: no clients selected, cancel
DEBUG flwr 2024-06-10 17:55:57,067 | server.py:222 | fit_round 42: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 17:57:36,348 | server.py:236 | fit_round 42 received 10 results and 0 failures
INFO flwr 2024-06-10 17:58:28,933 | server.py:125 | fit progress: (42, 1.16239951171875, {'accuracy': 0.5863, 'data_size': 10000}, 5505.331657039002)
INFO flwr 2024-06-10 17:58:28,933 | server.py:171 | evaluate_round 42: no clients selected, cancel
DEBUG flwr 2024-06-10 17:58:28,934 | server.py:222 | fit_round 43: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h24m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h24m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h24m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h25m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h25m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h25m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h25m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h26m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h27m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h27m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h27m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h28m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h28m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h29m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h29m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h29m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h30m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h30m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h30m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h31m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h31m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h31m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h32m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h32m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h32m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h32m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:00:14,532 | server.py:236 | fit_round 43 received 10 results and 0 failures
INFO flwr 2024-06-10 18:00:39,087 | server.py:125 | fit progress: (43, 1.19597041015625, {'accuracy': 0.5791, 'data_size': 10000}, 5635.485559839755)
INFO flwr 2024-06-10 18:00:39,087 | server.py:171 | evaluate_round 43: no clients selected, cancel
DEBUG flwr 2024-06-10 18:00:39,088 | server.py:222 | fit_round 44: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:02:19,451 | server.py:236 | fit_round 44 received 10 results and 0 failures
INFO flwr 2024-06-10 18:02:44,123 | server.py:125 | fit progress: (44, 1.15940693359375, {'accuracy': 0.5912, 'data_size': 10000}, 5760.521830413956)
INFO flwr 2024-06-10 18:02:44,124 | server.py:171 | evaluate_round 44: no clients selected, cancel
DEBUG flwr 2024-06-10 18:02:44,124 | server.py:222 | fit_round 45: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:04:30,448 | server.py:236 | fit_round 45 received 10 results and 0 failures
INFO flwr 2024-06-10 18:05:12,774 | server.py:125 | fit progress: (45, 1.16660419921875, {'accuracy': 0.5915, 'data_size': 10000}, 5909.1725307968445)
INFO flwr 2024-06-10 18:05:12,774 | server.py:171 | evaluate_round 45: no clients selected, cancel
DEBUG flwr 2024-06-10 18:05:12,775 | server.py:222 | fit_round 46: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:06:52,348 | server.py:236 | fit_round 46 received 10 results and 0 failures
INFO flwr 2024-06-10 18:07:15,609 | server.py:125 | fit progress: (46, 1.14573203125, {'accuracy': 0.5956, 'data_size': 10000}, 6032.007739953697)
INFO flwr 2024-06-10 18:07:15,609 | server.py:171 | evaluate_round 46: no clients selected, cancel
DEBUG flwr 2024-06-10 18:07:15,610 | server.py:222 | fit_round 47: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h33m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h33m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h34m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h34m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h34m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h34m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h35m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h35m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h35m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h36m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h36m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h36m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h37m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h37m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h38m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h38m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h38m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h39m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h39m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h40m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h40m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h41m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h41m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h41m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h41m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h42m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:09:02,919 | server.py:236 | fit_round 47 received 10 results and 0 failures
INFO flwr 2024-06-10 18:09:35,341 | server.py:125 | fit progress: (47, 1.18052978515625, {'accuracy': 0.5905, 'data_size': 10000}, 6171.740036435891)
INFO flwr 2024-06-10 18:09:35,342 | server.py:171 | evaluate_round 47: no clients selected, cancel
DEBUG flwr 2024-06-10 18:09:35,342 | server.py:222 | fit_round 48: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:11:14,617 | server.py:236 | fit_round 48 received 10 results and 0 failures
INFO flwr 2024-06-10 18:11:43,997 | server.py:125 | fit progress: (48, 1.19252060546875, {'accuracy': 0.59, 'data_size': 10000}, 6300.395335392561)
INFO flwr 2024-06-10 18:11:43,997 | server.py:171 | evaluate_round 48: no clients selected, cancel
DEBUG flwr 2024-06-10 18:11:43,997 | server.py:222 | fit_round 49: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:13:29,832 | server.py:236 | fit_round 49 received 10 results and 0 failures
INFO flwr 2024-06-10 18:13:51,986 | server.py:125 | fit progress: (49, 1.1914056640625, {'accuracy': 0.5911, 'data_size': 10000}, 6428.384552652948)
INFO flwr 2024-06-10 18:13:51,986 | server.py:171 | evaluate_round 49: no clients selected, cancel
DEBUG flwr 2024-06-10 18:13:51,987 | server.py:222 | fit_round 50: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:15:30,825 | server.py:236 | fit_round 50 received 10 results and 0 failures
INFO flwr 2024-06-10 18:15:53,576 | server.py:125 | fit progress: (50, 1.1232736328125, {'accuracy': 0.6123, 'data_size': 10000}, 6549.975087828003)
INFO flwr 2024-06-10 18:15:53,577 | server.py:171 | evaluate_round 50: no clients selected, cancel
DEBUG flwr 2024-06-10 18:15:53,577 | server.py:222 | fit_round 51: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:17:40,935 | server.py:236 | fit_round 51 received 10 results and 0 failures
INFO flwr 2024-06-10 18:18:04,938 | server.py:125 | fit progress: (51, 1.15576337890625, {'accuracy': 0.6025, 'data_size': 10000}, 6681.336648785975)
INFO flwr 2024-06-10 18:18:04,938 | server.py:171 | evaluate_round 51: no clients selected, cancel
DEBUG flwr 2024-06-10 18:18:04,939 | server.py:222 | fit_round 52: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:19:44,314 | server.py:236 | fit_round 52 received 10 results and 0 failures
INFO flwr 2024-06-10 18:20:06,262 | server.py:125 | fit progress: (52, 1.20116796875, {'accuracy': 0.5962, 'data_size': 10000}, 6802.660654519685)
INFO flwr 2024-06-10 18:20:06,262 | server.py:171 | evaluate_round 52: no clients selected, cancel
DEBUG flwr 2024-06-10 18:20:06,263 | server.py:222 | fit_round 53: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h42m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h43m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h43m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h45m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h45m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h45m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h45m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h46m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h47m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h47m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h48m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h48m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h48m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h49m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h49m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h49m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h50m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h50m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h51m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h51m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h52m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h52m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h53m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h53m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h54m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h54m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:21:51,959 | server.py:236 | fit_round 53 received 10 results and 0 failures
INFO flwr 2024-06-10 18:22:16,192 | server.py:125 | fit progress: (53, 1.17725849609375, {'accuracy': 0.5986, 'data_size': 10000}, 6932.590591225773)
INFO flwr 2024-06-10 18:22:16,192 | server.py:171 | evaluate_round 53: no clients selected, cancel
DEBUG flwr 2024-06-10 18:22:16,439 | server.py:222 | fit_round 54: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:23:56,141 | server.py:236 | fit_round 54 received 10 results and 0 failures
INFO flwr 2024-06-10 18:24:21,737 | server.py:125 | fit progress: (54, 1.10302392578125, {'accuracy': 0.6135, 'data_size': 10000}, 7058.136229853611)
INFO flwr 2024-06-10 18:24:21,738 | server.py:171 | evaluate_round 54: no clients selected, cancel
DEBUG flwr 2024-06-10 18:24:21,738 | server.py:222 | fit_round 55: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:26:07,837 | server.py:236 | fit_round 55 received 10 results and 0 failures
INFO flwr 2024-06-10 18:26:35,150 | server.py:125 | fit progress: (55, 1.18910908203125, {'accuracy': 0.5901, 'data_size': 10000}, 7191.548658075742)
INFO flwr 2024-06-10 18:26:35,150 | server.py:171 | evaluate_round 55: no clients selected, cancel
DEBUG flwr 2024-06-10 18:26:35,151 | server.py:222 | fit_round 56: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:28:14,450 | server.py:236 | fit_round 56 received 10 results and 0 failures
INFO flwr 2024-06-10 18:28:35,894 | server.py:125 | fit progress: (56, 1.16947314453125, {'accuracy': 0.5957, 'data_size': 10000}, 7312.292300811969)
INFO flwr 2024-06-10 18:28:35,894 | server.py:171 | evaluate_round 56: no clients selected, cancel
DEBUG flwr 2024-06-10 18:28:35,894 | server.py:222 | fit_round 57: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:30:22,383 | server.py:236 | fit_round 57 received 10 results and 0 failures
INFO flwr 2024-06-10 18:30:45,560 | server.py:125 | fit progress: (57, 1.099759765625, {'accuracy': 0.6166, 'data_size': 10000}, 7441.958717416972)
INFO flwr 2024-06-10 18:30:45,561 | server.py:171 | evaluate_round 57: no clients selected, cancel
DEBUG flwr 2024-06-10 18:30:45,561 | server.py:222 | fit_round 58: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +1h54m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h55m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h55m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h55m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h56m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h56m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h56m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h57m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h57m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h57m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h58m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h58m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h59m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1h59m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h1m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h2m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h2m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h2m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h3m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h3m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h3m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h4m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h4m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h4m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:32:25,375 | server.py:236 | fit_round 58 received 10 results and 0 failures
INFO flwr 2024-06-10 18:32:46,721 | server.py:125 | fit progress: (58, 1.118721875, {'accuracy': 0.6152, 'data_size': 10000}, 7563.120054338593)
INFO flwr 2024-06-10 18:32:46,722 | server.py:171 | evaluate_round 58: no clients selected, cancel
DEBUG flwr 2024-06-10 18:32:46,722 | server.py:222 | fit_round 59: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:34:34,275 | server.py:236 | fit_round 59 received 10 results and 0 failures
INFO flwr 2024-06-10 18:34:59,056 | server.py:125 | fit progress: (59, 1.151608203125, {'accuracy': 0.6107, 'data_size': 10000}, 7695.454575856682)
INFO flwr 2024-06-10 18:34:59,056 | server.py:171 | evaluate_round 59: no clients selected, cancel
DEBUG flwr 2024-06-10 18:34:59,057 | server.py:222 | fit_round 60: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:36:39,010 | server.py:236 | fit_round 60 received 10 results and 0 failures
INFO flwr 2024-06-10 18:37:01,825 | server.py:125 | fit progress: (60, 1.1290822265625, {'accuracy': 0.6198, 'data_size': 10000}, 7818.223306088708)
INFO flwr 2024-06-10 18:37:01,825 | server.py:171 | evaluate_round 60: no clients selected, cancel
DEBUG flwr 2024-06-10 18:37:01,826 | server.py:222 | fit_round 61: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:38:48,007 | server.py:236 | fit_round 61 received 10 results and 0 failures
INFO flwr 2024-06-10 18:39:09,779 | server.py:125 | fit progress: (61, 1.09485048828125, {'accuracy': 0.6255, 'data_size': 10000}, 7946.177967575844)
INFO flwr 2024-06-10 18:39:09,780 | server.py:171 | evaluate_round 61: no clients selected, cancel
DEBUG flwr 2024-06-10 18:39:09,781 | server.py:222 | fit_round 62: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:40:48,729 | server.py:236 | fit_round 62 received 10 results and 0 failures
INFO flwr 2024-06-10 18:41:16,659 | server.py:125 | fit progress: (62, 1.15370419921875, {'accuracy': 0.6162, 'data_size': 10000}, 8073.057734587695)
INFO flwr 2024-06-10 18:41:16,659 | server.py:171 | evaluate_round 62: no clients selected, cancel
DEBUG flwr 2024-06-10 18:41:16,660 | server.py:222 | fit_round 63: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h5m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h6m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h6m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h6m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h7m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h7m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h7m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h8m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h8m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h8m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h9m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h9m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h9m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h9m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h10m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h10m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h11m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h11m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h12m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h12m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h13m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h13m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h13m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h13m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h14m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h14m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h14m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:43:04,076 | server.py:236 | fit_round 63 received 10 results and 0 failures
INFO flwr 2024-06-10 18:43:27,023 | server.py:125 | fit progress: (63, 1.13604609375, {'accuracy': 0.6166, 'data_size': 10000}, 8203.421978095546)
INFO flwr 2024-06-10 18:43:27,024 | server.py:171 | evaluate_round 63: no clients selected, cancel
DEBUG flwr 2024-06-10 18:43:27,024 | server.py:222 | fit_round 64: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:45:07,872 | server.py:236 | fit_round 64 received 10 results and 0 failures
INFO flwr 2024-06-10 18:45:32,027 | server.py:125 | fit progress: (64, 1.12811640625, {'accuracy': 0.6171, 'data_size': 10000}, 8328.42544281669)
INFO flwr 2024-06-10 18:45:32,027 | server.py:171 | evaluate_round 64: no clients selected, cancel
DEBUG flwr 2024-06-10 18:45:32,055 | server.py:222 | fit_round 65: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:47:18,652 | server.py:236 | fit_round 65 received 10 results and 0 failures
INFO flwr 2024-06-10 18:47:39,735 | server.py:125 | fit progress: (65, 1.119621875, {'accuracy': 0.6199, 'data_size': 10000}, 8456.13414193876)
INFO flwr 2024-06-10 18:47:39,736 | server.py:171 | evaluate_round 65: no clients selected, cancel
DEBUG flwr 2024-06-10 18:47:39,737 | server.py:222 | fit_round 66: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:49:17,845 | server.py:236 | fit_round 66 received 10 results and 0 failures
INFO flwr 2024-06-10 18:49:40,657 | server.py:125 | fit progress: (66, 1.14616806640625, {'accuracy': 0.6158, 'data_size': 10000}, 8577.055737661663)
INFO flwr 2024-06-10 18:49:40,657 | server.py:171 | evaluate_round 66: no clients selected, cancel
DEBUG flwr 2024-06-10 18:49:40,658 | server.py:222 | fit_round 67: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:51:27,532 | server.py:236 | fit_round 67 received 10 results and 0 failures
INFO flwr 2024-06-10 18:51:49,677 | server.py:125 | fit progress: (67, 1.1199734375, {'accuracy': 0.627, 'data_size': 10000}, 8706.075870171655)
INFO flwr 2024-06-10 18:51:49,678 | server.py:171 | evaluate_round 67: no clients selected, cancel
DEBUG flwr 2024-06-10 18:51:49,678 | server.py:222 | fit_round 68: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h15m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h15m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h15m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h16m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h16m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h16m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h17m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h17m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h17m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h18m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h18m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h18m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h19m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h19m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h19m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h20m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h21m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h21m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h21m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h21m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h22m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h22m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h23m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h23m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h25m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h25m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 18:53:32,409 | server.py:236 | fit_round 68 received 10 results and 0 failures
INFO flwr 2024-06-10 18:53:57,987 | server.py:125 | fit progress: (68, 1.16368505859375, {'accuracy': 0.6209, 'data_size': 10000}, 8834.385694105644)
INFO flwr 2024-06-10 18:53:57,988 | server.py:171 | evaluate_round 68: no clients selected, cancel
DEBUG flwr 2024-06-10 18:53:57,988 | server.py:222 | fit_round 69: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:55:46,024 | server.py:236 | fit_round 69 received 10 results and 0 failures
INFO flwr 2024-06-10 18:56:13,361 | server.py:125 | fit progress: (69, 1.13343173828125, {'accuracy': 0.6294, 'data_size': 10000}, 8969.759462258779)
INFO flwr 2024-06-10 18:56:13,361 | server.py:171 | evaluate_round 69: no clients selected, cancel
DEBUG flwr 2024-06-10 18:56:13,361 | server.py:222 | fit_round 70: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 18:57:55,325 | server.py:236 | fit_round 70 received 10 results and 0 failures
INFO flwr 2024-06-10 18:58:19,609 | server.py:125 | fit progress: (70, 1.137009765625, {'accuracy': 0.6321, 'data_size': 10000}, 9096.007421896793)
INFO flwr 2024-06-10 18:58:19,609 | server.py:171 | evaluate_round 70: no clients selected, cancel
DEBUG flwr 2024-06-10 18:58:19,610 | server.py:222 | fit_round 71: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:00:06,763 | server.py:236 | fit_round 71 received 10 results and 0 failures
INFO flwr 2024-06-10 19:00:29,694 | server.py:125 | fit progress: (71, 1.14374775390625, {'accuracy': 0.6303, 'data_size': 10000}, 9226.092488353606)
INFO flwr 2024-06-10 19:00:29,694 | server.py:171 | evaluate_round 71: no clients selected, cancel
DEBUG flwr 2024-06-10 19:00:29,694 | server.py:222 | fit_round 72: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h26m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h26m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h26m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h26m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h27m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h27m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h27m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h27m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h28m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h28m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h29m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h29m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h29m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h29m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h31m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h31m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h32m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h32m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h32m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h32m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h33m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h33m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h34m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h34m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h34m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h34m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 19:02:10,427 | server.py:236 | fit_round 72 received 10 results and 0 failures
INFO flwr 2024-06-10 19:02:32,266 | server.py:125 | fit progress: (72, 1.1694443359375, {'accuracy': 0.6246, 'data_size': 10000}, 9348.664787064772)
INFO flwr 2024-06-10 19:02:32,267 | server.py:171 | evaluate_round 72: no clients selected, cancel
DEBUG flwr 2024-06-10 19:02:32,267 | server.py:222 | fit_round 73: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:04:20,045 | server.py:236 | fit_round 73 received 10 results and 0 failures
INFO flwr 2024-06-10 19:04:42,320 | server.py:125 | fit progress: (73, 1.1428072265625, {'accuracy': 0.633, 'data_size': 10000}, 9478.718857185915)
INFO flwr 2024-06-10 19:04:42,321 | server.py:171 | evaluate_round 73: no clients selected, cancel
DEBUG flwr 2024-06-10 19:04:42,321 | server.py:222 | fit_round 74: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:06:22,375 | server.py:236 | fit_round 74 received 10 results and 0 failures
INFO flwr 2024-06-10 19:06:43,356 | server.py:125 | fit progress: (74, 1.154335546875, {'accuracy': 0.6346, 'data_size': 10000}, 9599.754658577964)
INFO flwr 2024-06-10 19:06:43,357 | server.py:171 | evaluate_round 74: no clients selected, cancel
DEBUG flwr 2024-06-10 19:06:43,357 | server.py:222 | fit_round 75: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:08:31,330 | server.py:236 | fit_round 75 received 10 results and 0 failures
INFO flwr 2024-06-10 19:08:52,785 | server.py:125 | fit progress: (75, 1.1628830078125, {'accuracy': 0.632, 'data_size': 10000}, 9729.183776741847)
INFO flwr 2024-06-10 19:08:52,786 | server.py:171 | evaluate_round 75: no clients selected, cancel
DEBUG flwr 2024-06-10 19:08:52,786 | server.py:222 | fit_round 76: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:10:33,245 | server.py:236 | fit_round 76 received 10 results and 0 failures
INFO flwr 2024-06-10 19:10:54,629 | server.py:125 | fit progress: (76, 1.1699841796875, {'accuracy': 0.6285, 'data_size': 10000}, 9851.027446271852)
INFO flwr 2024-06-10 19:10:54,629 | server.py:171 | evaluate_round 76: no clients selected, cancel
DEBUG flwr 2024-06-10 19:10:54,630 | server.py:222 | fit_round 77: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:12:40,570 | server.py:236 | fit_round 77 received 10 results and 0 failures
INFO flwr 2024-06-10 19:13:02,580 | server.py:125 | fit progress: (77, 1.1737794921875, {'accuracy': 0.6378, 'data_size': 10000}, 9978.978752249852)
INFO flwr 2024-06-10 19:13:02,580 | server.py:171 | evaluate_round 77: no clients selected, cancel
DEBUG flwr 2024-06-10 19:13:02,581 | server.py:222 | fit_round 78: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h35m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h35m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h36m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h36m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h37m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h38m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h38m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h39m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h39m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h40m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h40m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h40m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h40m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h41m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h42m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h42m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h43m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h43m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h44m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h44m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h45m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h45m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h45m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h46m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h46m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h46m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 19:14:43,687 | server.py:236 | fit_round 78 received 10 results and 0 failures
INFO flwr 2024-06-10 19:15:04,402 | server.py:125 | fit progress: (78, 1.18362607421875, {'accuracy': 0.6385, 'data_size': 10000}, 10100.800607193727)
INFO flwr 2024-06-10 19:15:04,402 | server.py:171 | evaluate_round 78: no clients selected, cancel
DEBUG flwr 2024-06-10 19:15:04,403 | server.py:222 | fit_round 79: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:16:52,438 | server.py:236 | fit_round 79 received 10 results and 0 failures
INFO flwr 2024-06-10 19:17:14,422 | server.py:125 | fit progress: (79, 1.1808314453125, {'accuracy': 0.6407, 'data_size': 10000}, 10230.820321529638)
INFO flwr 2024-06-10 19:17:14,422 | server.py:171 | evaluate_round 79: no clients selected, cancel
DEBUG flwr 2024-06-10 19:17:14,422 | server.py:222 | fit_round 80: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:18:54,802 | server.py:236 | fit_round 80 received 10 results and 0 failures
INFO flwr 2024-06-10 19:19:16,340 | server.py:125 | fit progress: (80, 1.22874580078125, {'accuracy': 0.6317, 'data_size': 10000}, 10352.73837855691)
INFO flwr 2024-06-10 19:19:16,340 | server.py:171 | evaluate_round 80: no clients selected, cancel
DEBUG flwr 2024-06-10 19:19:16,340 | server.py:222 | fit_round 81: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:21:02,830 | server.py:236 | fit_round 81 received 10 results and 0 failures
INFO flwr 2024-06-10 19:21:24,276 | server.py:125 | fit progress: (81, 1.2848076171875, {'accuracy': 0.6231, 'data_size': 10000}, 10480.674597499892)
INFO flwr 2024-06-10 19:21:24,277 | server.py:171 | evaluate_round 81: no clients selected, cancel
DEBUG flwr 2024-06-10 19:21:24,277 | server.py:222 | fit_round 82: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h47m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h47m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h48m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h48m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h48m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h49m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h50m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h50m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h50m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h50m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h51m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h51m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h52m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h52m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h52m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h52m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h53m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h53m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h53m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h53m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h54m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h54m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h55m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h55m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h55m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h55m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 19:23:04,282 | server.py:236 | fit_round 82 received 10 results and 0 failures
INFO flwr 2024-06-10 19:23:30,097 | server.py:125 | fit progress: (82, 1.26979072265625, {'accuracy': 0.637, 'data_size': 10000}, 10606.495743970852)
INFO flwr 2024-06-10 19:23:30,098 | server.py:171 | evaluate_round 82: no clients selected, cancel
DEBUG flwr 2024-06-10 19:23:30,098 | server.py:222 | fit_round 83: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:25:17,713 | server.py:236 | fit_round 83 received 10 results and 0 failures
INFO flwr 2024-06-10 19:25:41,227 | server.py:125 | fit progress: (83, 1.2676958984375, {'accuracy': 0.6444, 'data_size': 10000}, 10737.625791009981)
INFO flwr 2024-06-10 19:25:41,228 | server.py:171 | evaluate_round 83: no clients selected, cancel
DEBUG flwr 2024-06-10 19:25:41,228 | server.py:222 | fit_round 84: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:27:18,035 | server.py:236 | fit_round 84 received 10 results and 0 failures
INFO flwr 2024-06-10 19:27:41,278 | server.py:125 | fit progress: (84, 1.3049111328125, {'accuracy': 0.6394, 'data_size': 10000}, 10857.676939313766)
INFO flwr 2024-06-10 19:27:41,279 | server.py:171 | evaluate_round 84: no clients selected, cancel
DEBUG flwr 2024-06-10 19:27:41,279 | server.py:222 | fit_round 85: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:29:27,933 | server.py:236 | fit_round 85 received 10 results and 0 failures
INFO flwr 2024-06-10 19:29:51,098 | server.py:125 | fit progress: (85, 1.322943359375, {'accuracy': 0.6382, 'data_size': 10000}, 10987.496324853972)
INFO flwr 2024-06-10 19:29:51,098 | server.py:171 | evaluate_round 85: no clients selected, cancel
DEBUG flwr 2024-06-10 19:29:51,098 | server.py:222 | fit_round 86: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:31:31,073 | server.py:236 | fit_round 86 received 10 results and 0 failures
INFO flwr 2024-06-10 19:31:55,360 | server.py:125 | fit progress: (86, 1.3106509765625, {'accuracy': 0.6391, 'data_size': 10000}, 11111.758416021708)
INFO flwr 2024-06-10 19:31:55,360 | server.py:171 | evaluate_round 86: no clients selected, cancel
DEBUG flwr 2024-06-10 19:31:55,361 | server.py:222 | fit_round 87: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +2h56m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h56m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h57m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h57m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h57m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h58m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h58m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h59m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h59m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h59m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2h59m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h1m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h1m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h2m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h2m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h3m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h3m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h4m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h4m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h4m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h4m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h5m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h5m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h5m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h5m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 19:33:42,064 | server.py:236 | fit_round 87 received 10 results and 0 failures
INFO flwr 2024-06-10 19:34:04,043 | server.py:125 | fit progress: (87, 1.28587607421875, {'accuracy': 0.6319, 'data_size': 10000}, 11240.441859205719)
INFO flwr 2024-06-10 19:34:04,044 | server.py:171 | evaluate_round 87: no clients selected, cancel
DEBUG flwr 2024-06-10 19:34:04,044 | server.py:222 | fit_round 88: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:35:45,402 | server.py:236 | fit_round 88 received 10 results and 0 failures
INFO flwr 2024-06-10 19:36:10,465 | server.py:125 | fit progress: (88, 1.26599384765625, {'accuracy': 0.6375, 'data_size': 10000}, 11366.864023001865)
INFO flwr 2024-06-10 19:36:10,466 | server.py:171 | evaluate_round 88: no clients selected, cancel
DEBUG flwr 2024-06-10 19:36:10,466 | server.py:222 | fit_round 89: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:37:58,517 | server.py:236 | fit_round 89 received 10 results and 0 failures
INFO flwr 2024-06-10 19:38:22,444 | server.py:125 | fit progress: (89, 1.31313896484375, {'accuracy': 0.6322, 'data_size': 10000}, 11498.842873991933)
INFO flwr 2024-06-10 19:38:22,445 | server.py:171 | evaluate_round 89: no clients selected, cancel
DEBUG flwr 2024-06-10 19:38:22,446 | server.py:222 | fit_round 90: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:40:04,200 | server.py:236 | fit_round 90 received 10 results and 0 failures
INFO flwr 2024-06-10 19:40:28,917 | server.py:125 | fit progress: (90, 1.294200390625, {'accuracy': 0.6428, 'data_size': 10000}, 11625.31617376674)
INFO flwr 2024-06-10 19:40:28,918 | server.py:171 | evaluate_round 90: no clients selected, cancel
DEBUG flwr 2024-06-10 19:40:28,918 | server.py:222 | fit_round 91: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:42:15,650 | server.py:236 | fit_round 91 received 10 results and 0 failures
INFO flwr 2024-06-10 19:42:39,106 | server.py:125 | fit progress: (91, 1.31578544921875, {'accuracy': 0.6427, 'data_size': 10000}, 11755.504910002928)
INFO flwr 2024-06-10 19:42:39,107 | server.py:171 | evaluate_round 91: no clients selected, cancel
DEBUG flwr 2024-06-10 19:42:39,107 | server.py:222 | fit_round 92: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +3h6m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h6m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h7m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h7m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h7m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h7m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h8m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h8m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h9m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h9m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h10m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h10m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h11m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h11m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h13m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h13m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h14m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h14m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h15m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h15m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h15m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h15m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h16m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h16m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h16m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h16m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 19:44:18,325 | server.py:236 | fit_round 92 received 10 results and 0 failures
INFO flwr 2024-06-10 19:44:41,186 | server.py:125 | fit progress: (92, 1.36287900390625, {'accuracy': 0.6368, 'data_size': 10000}, 11877.584914337844)
INFO flwr 2024-06-10 19:44:41,187 | server.py:171 | evaluate_round 92: no clients selected, cancel
DEBUG flwr 2024-06-10 19:44:41,187 | server.py:222 | fit_round 93: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:46:29,356 | server.py:236 | fit_round 93 received 10 results and 0 failures
INFO flwr 2024-06-10 19:46:52,336 | server.py:125 | fit progress: (93, 1.32136240234375, {'accuracy': 0.6391, 'data_size': 10000}, 12008.735037670936)
INFO flwr 2024-06-10 19:46:52,337 | server.py:171 | evaluate_round 93: no clients selected, cancel
DEBUG flwr 2024-06-10 19:46:52,337 | server.py:222 | fit_round 94: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:48:32,625 | server.py:236 | fit_round 94 received 10 results and 0 failures
INFO flwr 2024-06-10 19:48:54,272 | server.py:125 | fit progress: (94, 1.3373755859375, {'accuracy': 0.6413, 'data_size': 10000}, 12130.670454569627)
INFO flwr 2024-06-10 19:48:54,272 | server.py:171 | evaluate_round 94: no clients selected, cancel
DEBUG flwr 2024-06-10 19:48:54,273 | server.py:222 | fit_round 95: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:50:39,682 | server.py:236 | fit_round 95 received 10 results and 0 failures
INFO flwr 2024-06-10 19:51:03,078 | server.py:125 | fit progress: (95, 1.38838916015625, {'accuracy': 0.6328, 'data_size': 10000}, 12259.476914657746)
INFO flwr 2024-06-10 19:51:03,079 | server.py:171 | evaluate_round 95: no clients selected, cancel
DEBUG flwr 2024-06-10 19:51:03,079 | server.py:222 | fit_round 96: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:52:42,033 | server.py:236 | fit_round 96 received 10 results and 0 failures
INFO flwr 2024-06-10 19:53:06,091 | server.py:125 | fit progress: (96, 1.398519140625, {'accuracy': 0.6337, 'data_size': 10000}, 12382.48925167881)
INFO flwr 2024-06-10 19:53:06,091 | server.py:171 | evaluate_round 96: no clients selected, cancel
DEBUG flwr 2024-06-10 19:53:06,113 | server.py:222 | fit_round 97: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:54:51,117 | server.py:236 | fit_round 97 received 10 results and 0 failures
INFO flwr 2024-06-10 19:55:23,955 | server.py:125 | fit progress: (97, 1.32692783203125, {'accuracy': 0.6444, 'data_size': 10000}, 12520.353292493615)
INFO flwr 2024-06-10 19:55:23,955 | server.py:171 | evaluate_round 97: no clients selected, cancel
DEBUG flwr 2024-06-10 19:55:23,955 | server.py:222 | fit_round 98: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:57:05,294 | server.py:236 | fit_round 98 received 10 results and 0 failures
INFO flwr 2024-06-10 19:57:28,306 | server.py:125 | fit progress: (98, 1.273144921875, {'accuracy': 0.6456, 'data_size': 10000}, 12644.704325706698)
INFO flwr 2024-06-10 19:57:28,306 | server.py:171 | evaluate_round 98: no clients selected, cancel
DEBUG flwr 2024-06-10 19:57:28,306 | server.py:222 | fit_round 99: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 19:59:17,809 | server.py:236 | fit_round 99 received 10 results and 0 failures
[2m[1m[33m(autoscaler +3h17m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h17m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h18m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h18m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h20m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h20m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h22m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h22m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h23m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h23m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h24m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h24m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h26m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h26m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h27m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h28m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h28m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h29m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h31m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h31m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h31m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h31m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h32m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h32m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h32m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h33m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 19:59:41,466 | server.py:125 | fit progress: (99, 1.33489423828125, {'accuracy': 0.6342, 'data_size': 10000}, 12777.865014627576)
INFO flwr 2024-06-10 19:59:41,467 | server.py:171 | evaluate_round 99: no clients selected, cancel
DEBUG flwr 2024-06-10 19:59:41,467 | server.py:222 | fit_round 100: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:01:20,476 | server.py:236 | fit_round 100 received 10 results and 0 failures
INFO flwr 2024-06-10 20:01:40,729 | server.py:125 | fit progress: (100, 1.44848330078125, {'accuracy': 0.6164, 'data_size': 10000}, 12897.127198253758)
INFO flwr 2024-06-10 20:01:40,729 | server.py:171 | evaluate_round 100: no clients selected, cancel
DEBUG flwr 2024-06-10 20:01:40,730 | server.py:222 | fit_round 101: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:03:25,895 | server.py:236 | fit_round 101 received 10 results and 0 failures
INFO flwr 2024-06-10 20:03:48,470 | server.py:125 | fit progress: (101, 1.3659994140625, {'accuracy': 0.6326, 'data_size': 10000}, 13024.868582430761)
INFO flwr 2024-06-10 20:03:48,470 | server.py:171 | evaluate_round 101: no clients selected, cancel
DEBUG flwr 2024-06-10 20:03:48,471 | server.py:222 | fit_round 102: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:05:29,090 | server.py:236 | fit_round 102 received 10 results and 0 failures
INFO flwr 2024-06-10 20:05:56,605 | server.py:125 | fit progress: (102, 1.36769150390625, {'accuracy': 0.6417, 'data_size': 10000}, 13153.004019288812)
INFO flwr 2024-06-10 20:05:56,606 | server.py:171 | evaluate_round 102: no clients selected, cancel
DEBUG flwr 2024-06-10 20:05:56,607 | server.py:222 | fit_round 103: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:07:45,223 | server.py:236 | fit_round 103 received 10 results and 0 failures
[2m[1m[33m(autoscaler +3h33m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h33m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h34m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h34m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h34m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h35m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h35m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h35m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h35m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h36m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h36m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h36m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h37m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h37m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h37m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h38m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h38m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h38m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h39m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h39m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h39m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h40m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h40m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h40m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h41m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h41m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 20:08:06,788 | server.py:125 | fit progress: (103, 1.39923671875, {'accuracy': 0.6459, 'data_size': 10000}, 13283.186262827832)
INFO flwr 2024-06-10 20:08:06,788 | server.py:171 | evaluate_round 103: no clients selected, cancel
DEBUG flwr 2024-06-10 20:08:06,788 | server.py:222 | fit_round 104: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:09:46,737 | server.py:236 | fit_round 104 received 10 results and 0 failures
INFO flwr 2024-06-10 20:10:09,582 | server.py:125 | fit progress: (104, 1.47105537109375, {'accuracy': 0.6338, 'data_size': 10000}, 13405.980604804587)
INFO flwr 2024-06-10 20:10:09,582 | server.py:171 | evaluate_round 104: no clients selected, cancel
DEBUG flwr 2024-06-10 20:10:09,583 | server.py:222 | fit_round 105: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:11:57,266 | server.py:236 | fit_round 105 received 10 results and 0 failures
INFO flwr 2024-06-10 20:12:20,430 | server.py:125 | fit progress: (105, 1.40454609375, {'accuracy': 0.6477, 'data_size': 10000}, 13536.82842305582)
INFO flwr 2024-06-10 20:12:20,430 | server.py:171 | evaluate_round 105: no clients selected, cancel
DEBUG flwr 2024-06-10 20:12:20,431 | server.py:222 | fit_round 106: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:14:01,935 | server.py:236 | fit_round 106 received 10 results and 0 failures
INFO flwr 2024-06-10 20:14:24,813 | server.py:125 | fit progress: (106, 1.37833349609375, {'accuracy': 0.6552, 'data_size': 10000}, 13661.211495728698)
INFO flwr 2024-06-10 20:14:24,813 | server.py:171 | evaluate_round 106: no clients selected, cancel
DEBUG flwr 2024-06-10 20:14:24,814 | server.py:222 | fit_round 107: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:16:12,301 | server.py:236 | fit_round 107 received 10 results and 0 failures
INFO flwr 2024-06-10 20:16:35,311 | server.py:125 | fit progress: (107, 1.46231484375, {'accuracy': 0.6419, 'data_size': 10000}, 13791.70929889055)
INFO flwr 2024-06-10 20:16:35,311 | server.py:171 | evaluate_round 107: no clients selected, cancel
DEBUG flwr 2024-06-10 20:16:35,311 | server.py:222 | fit_round 108: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +3h41m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h41m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h42m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h42m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h42m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h43m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h43m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h43m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h44m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h44m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h44m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h45m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h45m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h46m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h46m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h47m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h47m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h47m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h47m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h48m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h48m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h49m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h49m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h49m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h49m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h50m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 20:18:16,711 | server.py:236 | fit_round 108 received 10 results and 0 failures
INFO flwr 2024-06-10 20:18:37,918 | server.py:125 | fit progress: (108, 1.56998662109375, {'accuracy': 0.6272, 'data_size': 10000}, 13914.316425688565)
INFO flwr 2024-06-10 20:18:37,918 | server.py:171 | evaluate_round 108: no clients selected, cancel
DEBUG flwr 2024-06-10 20:18:37,918 | server.py:222 | fit_round 109: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:20:25,064 | server.py:236 | fit_round 109 received 10 results and 0 failures
INFO flwr 2024-06-10 20:20:46,975 | server.py:125 | fit progress: (109, 1.5395240234375, {'accuracy': 0.6393, 'data_size': 10000}, 14043.373911531642)
INFO flwr 2024-06-10 20:20:46,976 | server.py:171 | evaluate_round 109: no clients selected, cancel
DEBUG flwr 2024-06-10 20:20:46,976 | server.py:222 | fit_round 110: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:22:29,065 | server.py:236 | fit_round 110 received 10 results and 0 failures
INFO flwr 2024-06-10 20:22:51,320 | server.py:125 | fit progress: (110, 1.55040947265625, {'accuracy': 0.6446, 'data_size': 10000}, 14167.71902411757)
INFO flwr 2024-06-10 20:22:51,321 | server.py:171 | evaluate_round 110: no clients selected, cancel
DEBUG flwr 2024-06-10 20:22:51,321 | server.py:222 | fit_round 111: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:24:37,871 | server.py:236 | fit_round 111 received 10 results and 0 failures
INFO flwr 2024-06-10 20:25:02,325 | server.py:125 | fit progress: (111, 1.57582421875, {'accuracy': 0.648, 'data_size': 10000}, 14298.723881775979)
INFO flwr 2024-06-10 20:25:02,326 | server.py:171 | evaluate_round 111: no clients selected, cancel
DEBUG flwr 2024-06-10 20:25:02,326 | server.py:222 | fit_round 112: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:26:43,918 | server.py:236 | fit_round 112 received 10 results and 0 failures
INFO flwr 2024-06-10 20:27:06,623 | server.py:125 | fit progress: (112, 1.6772765625, {'accuracy': 0.6409, 'data_size': 10000}, 14423.021632475778)
INFO flwr 2024-06-10 20:27:06,624 | server.py:171 | evaluate_round 112: no clients selected, cancel
DEBUG flwr 2024-06-10 20:27:06,624 | server.py:222 | fit_round 113: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +3h50m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h50m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h51m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h51m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h52m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h52m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h52m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h52m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h53m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h53m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h53m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h54m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h54m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h54m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h55m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h55m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h55m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h56m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h56m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h56m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h58m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3h58m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h1m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h1m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 20:28:55,079 | server.py:236 | fit_round 113 received 10 results and 0 failures
INFO flwr 2024-06-10 20:29:16,078 | server.py:125 | fit progress: (113, 1.673266796875, {'accuracy': 0.6405, 'data_size': 10000}, 14552.47684509959)
INFO flwr 2024-06-10 20:29:16,079 | server.py:171 | evaluate_round 113: no clients selected, cancel
DEBUG flwr 2024-06-10 20:29:16,079 | server.py:222 | fit_round 114: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:30:55,421 | server.py:236 | fit_round 114 received 10 results and 0 failures
INFO flwr 2024-06-10 20:31:19,637 | server.py:125 | fit progress: (114, 1.62575, {'accuracy': 0.6384, 'data_size': 10000}, 14676.03561490262)
INFO flwr 2024-06-10 20:31:19,637 | server.py:171 | evaluate_round 114: no clients selected, cancel
DEBUG flwr 2024-06-10 20:31:19,638 | server.py:222 | fit_round 115: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:33:10,112 | server.py:236 | fit_round 115 received 10 results and 0 failures
INFO flwr 2024-06-10 20:33:32,868 | server.py:125 | fit progress: (115, 1.653718359375, {'accuracy': 0.6357, 'data_size': 10000}, 14809.266568739899)
INFO flwr 2024-06-10 20:33:32,868 | server.py:171 | evaluate_round 115: no clients selected, cancel
DEBUG flwr 2024-06-10 20:33:32,869 | server.py:222 | fit_round 116: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:35:13,241 | server.py:236 | fit_round 116 received 10 results and 0 failures
INFO flwr 2024-06-10 20:35:37,337 | server.py:125 | fit progress: (116, 1.6494529296875, {'accuracy': 0.6349, 'data_size': 10000}, 14933.735470503569)
INFO flwr 2024-06-10 20:35:37,337 | server.py:171 | evaluate_round 116: no clients selected, cancel
DEBUG flwr 2024-06-10 20:35:37,338 | server.py:222 | fit_round 117: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:37:23,886 | server.py:236 | fit_round 117 received 10 results and 0 failures
INFO flwr 2024-06-10 20:37:51,745 | server.py:125 | fit progress: (117, 1.658662890625, {'accuracy': 0.6297, 'data_size': 10000}, 15068.143473690841)
INFO flwr 2024-06-10 20:37:51,745 | server.py:171 | evaluate_round 117: no clients selected, cancel
DEBUG flwr 2024-06-10 20:37:51,746 | server.py:222 | fit_round 118: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:39:33,417 | server.py:236 | fit_round 118 received 10 results and 0 failures
[2m[1m[33m(autoscaler +4h1m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h2m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h2m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h2m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h3m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h3m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h3m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h4m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h4m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h5m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h5m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h7m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h7m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h9m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h9m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h9m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h9m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h10m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h10m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h11m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h11m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h11m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h11m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h12m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h12m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h12m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h13m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 20:40:00,984 | server.py:125 | fit progress: (118, 1.678629296875, {'accuracy': 0.6357, 'data_size': 10000}, 15197.382747816853)
INFO flwr 2024-06-10 20:40:00,985 | server.py:171 | evaluate_round 118: no clients selected, cancel
DEBUG flwr 2024-06-10 20:40:00,985 | server.py:222 | fit_round 119: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:41:48,853 | server.py:236 | fit_round 119 received 10 results and 0 failures
INFO flwr 2024-06-10 20:42:37,564 | server.py:125 | fit progress: (119, 1.6865439453125, {'accuracy': 0.6379, 'data_size': 10000}, 15353.962942130864)
INFO flwr 2024-06-10 20:42:37,565 | server.py:171 | evaluate_round 119: no clients selected, cancel
DEBUG flwr 2024-06-10 20:42:37,565 | server.py:222 | fit_round 120: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:44:18,162 | server.py:236 | fit_round 120 received 10 results and 0 failures
INFO flwr 2024-06-10 20:44:42,150 | server.py:125 | fit progress: (120, 1.7119427734375, {'accuracy': 0.6343, 'data_size': 10000}, 15478.548289900646)
INFO flwr 2024-06-10 20:44:42,150 | server.py:171 | evaluate_round 120: no clients selected, cancel
DEBUG flwr 2024-06-10 20:44:42,151 | server.py:222 | fit_round 121: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:46:31,279 | server.py:236 | fit_round 121 received 10 results and 0 failures
INFO flwr 2024-06-10 20:47:09,060 | server.py:125 | fit progress: (121, 1.6914203125, {'accuracy': 0.641, 'data_size': 10000}, 15625.458286247682)
INFO flwr 2024-06-10 20:47:09,060 | server.py:171 | evaluate_round 121: no clients selected, cancel
DEBUG flwr 2024-06-10 20:47:09,060 | server.py:222 | fit_round 122: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:48:49,496 | server.py:236 | fit_round 122 received 10 results and 0 failures
INFO flwr 2024-06-10 20:49:12,811 | server.py:125 | fit progress: (122, 1.6466013671875, {'accuracy': 0.6477, 'data_size': 10000}, 15749.209722786676)
INFO flwr 2024-06-10 20:49:12,812 | server.py:171 | evaluate_round 122: no clients selected, cancel
DEBUG flwr 2024-06-10 20:49:12,812 | server.py:222 | fit_round 123: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +4h13m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h13m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h14m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h14m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h15m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h15m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h16m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h16m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h16m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h17m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h17m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h17m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h18m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h18m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h19m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h19m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h20m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h20m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h21m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h21m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h21m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h21m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h22m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h22m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h22m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h23m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 20:51:01,881 | server.py:236 | fit_round 123 received 10 results and 0 failures
INFO flwr 2024-06-10 20:51:25,855 | server.py:125 | fit progress: (123, 1.605450390625, {'accuracy': 0.6535, 'data_size': 10000}, 15882.25378806796)
INFO flwr 2024-06-10 20:51:25,855 | server.py:171 | evaluate_round 123: no clients selected, cancel
DEBUG flwr 2024-06-10 20:51:25,856 | server.py:222 | fit_round 124: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:53:07,493 | server.py:236 | fit_round 124 received 10 results and 0 failures
INFO flwr 2024-06-10 20:53:31,472 | server.py:125 | fit progress: (124, 1.6535939453125, {'accuracy': 0.6414, 'data_size': 10000}, 16007.870734749828)
INFO flwr 2024-06-10 20:53:31,473 | server.py:171 | evaluate_round 124: no clients selected, cancel
DEBUG flwr 2024-06-10 20:53:31,473 | server.py:222 | fit_round 125: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:55:19,605 | server.py:236 | fit_round 125 received 10 results and 0 failures
INFO flwr 2024-06-10 20:55:43,595 | server.py:125 | fit progress: (125, 1.67950390625, {'accuracy': 0.6411, 'data_size': 10000}, 16139.994166011922)
INFO flwr 2024-06-10 20:55:43,596 | server.py:171 | evaluate_round 125: no clients selected, cancel
DEBUG flwr 2024-06-10 20:55:43,596 | server.py:222 | fit_round 126: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:57:24,618 | server.py:236 | fit_round 126 received 10 results and 0 failures
INFO flwr 2024-06-10 20:57:49,281 | server.py:125 | fit progress: (126, 1.7052255859375, {'accuracy': 0.6408, 'data_size': 10000}, 16265.67926049186)
INFO flwr 2024-06-10 20:57:49,281 | server.py:171 | evaluate_round 126: no clients selected, cancel
DEBUG flwr 2024-06-10 20:57:49,282 | server.py:222 | fit_round 127: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 20:59:37,807 | server.py:236 | fit_round 127 received 10 results and 0 failures
[2m[1m[33m(autoscaler +4h23m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h24m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h24m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h25m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h25m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h25m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h26m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h26m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h26m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h27m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h27m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h27m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h28m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h28m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h29m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h29m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h29m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h30m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h31m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h31m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h31m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h31m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h32m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h32m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h33m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h33m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 21:00:01,384 | server.py:125 | fit progress: (127, 1.72972421875, {'accuracy': 0.6501, 'data_size': 10000}, 16397.783100419678)
INFO flwr 2024-06-10 21:00:01,385 | server.py:171 | evaluate_round 127: no clients selected, cancel
DEBUG flwr 2024-06-10 21:00:01,385 | server.py:222 | fit_round 128: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:01:42,640 | server.py:236 | fit_round 128 received 10 results and 0 failures
INFO flwr 2024-06-10 21:02:04,241 | server.py:125 | fit progress: (128, 1.74816640625, {'accuracy': 0.6531, 'data_size': 10000}, 16520.639739541803)
INFO flwr 2024-06-10 21:02:04,242 | server.py:171 | evaluate_round 128: no clients selected, cancel
DEBUG flwr 2024-06-10 21:02:04,242 | server.py:222 | fit_round 129: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:03:51,021 | server.py:236 | fit_round 129 received 10 results and 0 failures
INFO flwr 2024-06-10 21:04:13,449 | server.py:125 | fit progress: (129, 1.7999490234375, {'accuracy': 0.6473, 'data_size': 10000}, 16649.84742926061)
INFO flwr 2024-06-10 21:04:13,449 | server.py:171 | evaluate_round 129: no clients selected, cancel
DEBUG flwr 2024-06-10 21:04:13,450 | server.py:222 | fit_round 130: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:05:54,537 | server.py:236 | fit_round 130 received 10 results and 0 failures
INFO flwr 2024-06-10 21:06:16,028 | server.py:125 | fit progress: (130, 1.8671890625, {'accuracy': 0.6417, 'data_size': 10000}, 16772.427088779863)
INFO flwr 2024-06-10 21:06:16,029 | server.py:171 | evaluate_round 130: no clients selected, cancel
DEBUG flwr 2024-06-10 21:06:16,029 | server.py:222 | fit_round 131: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:08:03,754 | server.py:236 | fit_round 131 received 10 results and 0 failures
INFO flwr 2024-06-10 21:08:26,944 | server.py:125 | fit progress: (131, 1.86601484375, {'accuracy': 0.645, 'data_size': 10000}, 16903.342558345757)
INFO flwr 2024-06-10 21:08:26,944 | server.py:171 | evaluate_round 131: no clients selected, cancel
DEBUG flwr 2024-06-10 21:08:26,945 | server.py:222 | fit_round 132: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +4h33m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h33m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h34m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h34m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h35m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h35m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h35m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h35m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h36m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h36m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h36m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h37m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h37m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h37m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h38m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h39m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h39m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h40m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h40m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h40m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h41m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h41m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h41m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h42m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h42m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h42m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 21:10:07,330 | server.py:236 | fit_round 132 received 10 results and 0 failures
INFO flwr 2024-06-10 21:10:28,554 | server.py:125 | fit progress: (132, 1.866074609375, {'accuracy': 0.647, 'data_size': 10000}, 17024.952241975814)
INFO flwr 2024-06-10 21:10:28,554 | server.py:171 | evaluate_round 132: no clients selected, cancel
DEBUG flwr 2024-06-10 21:10:28,554 | server.py:222 | fit_round 133: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:12:17,835 | server.py:236 | fit_round 133 received 10 results and 0 failures
INFO flwr 2024-06-10 21:12:46,216 | server.py:125 | fit progress: (133, 1.85076171875, {'accuracy': 0.6505, 'data_size': 10000}, 17162.61492087599)
INFO flwr 2024-06-10 21:12:46,217 | server.py:171 | evaluate_round 133: no clients selected, cancel
DEBUG flwr 2024-06-10 21:12:46,217 | server.py:222 | fit_round 134: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:14:28,270 | server.py:236 | fit_round 134 received 10 results and 0 failures
INFO flwr 2024-06-10 21:14:56,483 | server.py:125 | fit progress: (134, 1.8622533203125, {'accuracy': 0.6525, 'data_size': 10000}, 17292.88154553389)
INFO flwr 2024-06-10 21:14:56,484 | server.py:171 | evaluate_round 134: no clients selected, cancel
DEBUG flwr 2024-06-10 21:14:56,484 | server.py:222 | fit_round 135: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:16:44,348 | server.py:236 | fit_round 135 received 10 results and 0 failures
INFO flwr 2024-06-10 21:17:12,768 | server.py:125 | fit progress: (135, 1.94978125, {'accuracy': 0.6464, 'data_size': 10000}, 17429.166611864697)
INFO flwr 2024-06-10 21:17:12,768 | server.py:171 | evaluate_round 135: no clients selected, cancel
DEBUG flwr 2024-06-10 21:17:12,769 | server.py:222 | fit_round 136: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:18:54,207 | server.py:236 | fit_round 136 received 10 results and 0 failures
[2m[1m[33m(autoscaler +4h43m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h43m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h44m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h44m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h44m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h44m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h45m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h45m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h45m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h46m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h46m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h46m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h47m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h47m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h47m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h48m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h48m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h48m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h49m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h49m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h49m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h49m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h50m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h50m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h50m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h51m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 21:19:18,197 | server.py:125 | fit progress: (136, 1.94582421875, {'accuracy': 0.6486, 'data_size': 10000}, 17554.59543202864)
INFO flwr 2024-06-10 21:19:18,197 | server.py:171 | evaluate_round 136: no clients selected, cancel
DEBUG flwr 2024-06-10 21:19:18,197 | server.py:222 | fit_round 137: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:21:08,236 | server.py:236 | fit_round 137 received 10 results and 0 failures
INFO flwr 2024-06-10 21:21:34,252 | server.py:125 | fit progress: (137, 1.970516796875, {'accuracy': 0.6511, 'data_size': 10000}, 17690.650562204886)
INFO flwr 2024-06-10 21:21:34,252 | server.py:171 | evaluate_round 137: no clients selected, cancel
DEBUG flwr 2024-06-10 21:21:34,253 | server.py:222 | fit_round 138: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:23:17,030 | server.py:236 | fit_round 138 received 10 results and 0 failures
INFO flwr 2024-06-10 21:23:40,741 | server.py:125 | fit progress: (138, 1.978210546875, {'accuracy': 0.6521, 'data_size': 10000}, 17817.13987012487)
INFO flwr 2024-06-10 21:23:40,742 | server.py:171 | evaluate_round 138: no clients selected, cancel
DEBUG flwr 2024-06-10 21:23:40,742 | server.py:222 | fit_round 139: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:25:28,785 | server.py:236 | fit_round 139 received 10 results and 0 failures
INFO flwr 2024-06-10 21:25:51,478 | server.py:125 | fit progress: (139, 1.99235546875, {'accuracy': 0.6493, 'data_size': 10000}, 17947.876451724675)
INFO flwr 2024-06-10 21:25:51,478 | server.py:171 | evaluate_round 139: no clients selected, cancel
DEBUG flwr 2024-06-10 21:25:51,479 | server.py:222 | fit_round 140: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:27:33,520 | server.py:236 | fit_round 140 received 10 results and 0 failures
INFO flwr 2024-06-10 21:27:55,638 | server.py:125 | fit progress: (140, 2.0226279296875, {'accuracy': 0.6459, 'data_size': 10000}, 18072.03676047176)
INFO flwr 2024-06-10 21:27:55,639 | server.py:171 | evaluate_round 140: no clients selected, cancel
DEBUG flwr 2024-06-10 21:27:55,639 | server.py:222 | fit_round 141: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +4h52m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h52m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h53m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h53m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h53m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h54m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h54m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h54m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h55m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h55m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h56m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h56m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h56m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h57m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h57m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h57m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h58m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h58m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h59m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4h59m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h1m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h1m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h1m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h2m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 21:29:44,858 | server.py:236 | fit_round 141 received 10 results and 0 failures
INFO flwr 2024-06-10 21:30:08,591 | server.py:125 | fit progress: (141, 2.0180662109375, {'accuracy': 0.6563, 'data_size': 10000}, 18204.990051308647)
INFO flwr 2024-06-10 21:30:08,592 | server.py:171 | evaluate_round 141: no clients selected, cancel
DEBUG flwr 2024-06-10 21:30:08,592 | server.py:222 | fit_round 142: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:31:50,520 | server.py:236 | fit_round 142 received 10 results and 0 failures
INFO flwr 2024-06-10 21:32:14,811 | server.py:125 | fit progress: (142, 2.0095515625, {'accuracy': 0.6545, 'data_size': 10000}, 18331.209969257936)
INFO flwr 2024-06-10 21:32:14,812 | server.py:171 | evaluate_round 142: no clients selected, cancel
DEBUG flwr 2024-06-10 21:32:14,812 | server.py:222 | fit_round 143: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:34:02,607 | server.py:236 | fit_round 143 received 10 results and 0 failures
INFO flwr 2024-06-10 21:34:24,074 | server.py:125 | fit progress: (143, 2.02718984375, {'accuracy': 0.6494, 'data_size': 10000}, 18460.47228456987)
INFO flwr 2024-06-10 21:34:24,074 | server.py:171 | evaluate_round 143: no clients selected, cancel
DEBUG flwr 2024-06-10 21:34:24,074 | server.py:222 | fit_round 144: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:36:06,277 | server.py:236 | fit_round 144 received 10 results and 0 failures
INFO flwr 2024-06-10 21:36:29,821 | server.py:125 | fit progress: (144, 2.091676953125, {'accuracy': 0.6435, 'data_size': 10000}, 18586.219952127896)
INFO flwr 2024-06-10 21:36:29,822 | server.py:171 | evaluate_round 144: no clients selected, cancel
DEBUG flwr 2024-06-10 21:36:29,822 | server.py:222 | fit_round 145: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:38:16,099 | server.py:236 | fit_round 145 received 10 results and 0 failures
[2m[1m[33m(autoscaler +5h2m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h2m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h2m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h3m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h3m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h3m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h4m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h4m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h4m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h5m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h5m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h5m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h6m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h6m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h6m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h6m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h7m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h7m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h8m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h9m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h10m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h10m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h10m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h11m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h11m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h11m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h11m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 21:38:38,000 | server.py:125 | fit progress: (145, 2.043942578125, {'accuracy': 0.6504, 'data_size': 10000}, 18714.39920452377)
INFO flwr 2024-06-10 21:38:38,001 | server.py:171 | evaluate_round 145: no clients selected, cancel
DEBUG flwr 2024-06-10 21:38:38,001 | server.py:222 | fit_round 146: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:40:18,940 | server.py:236 | fit_round 146 received 10 results and 0 failures
INFO flwr 2024-06-10 21:40:44,417 | server.py:125 | fit progress: (146, 2.05336171875, {'accuracy': 0.6554, 'data_size': 10000}, 18840.81562682893)
INFO flwr 2024-06-10 21:40:44,417 | server.py:171 | evaluate_round 146: no clients selected, cancel
DEBUG flwr 2024-06-10 21:40:44,418 | server.py:222 | fit_round 147: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:42:33,759 | server.py:236 | fit_round 147 received 10 results and 0 failures
INFO flwr 2024-06-10 21:42:55,476 | server.py:125 | fit progress: (147, 2.0556724609375, {'accuracy': 0.6592, 'data_size': 10000}, 18971.874581672717)
INFO flwr 2024-06-10 21:42:55,476 | server.py:171 | evaluate_round 147: no clients selected, cancel
DEBUG flwr 2024-06-10 21:42:55,477 | server.py:222 | fit_round 148: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:44:37,022 | server.py:236 | fit_round 148 received 10 results and 0 failures
INFO flwr 2024-06-10 21:44:59,759 | server.py:125 | fit progress: (148, 2.100621875, {'accuracy': 0.6506, 'data_size': 10000}, 19096.15735805966)
INFO flwr 2024-06-10 21:44:59,759 | server.py:171 | evaluate_round 148: no clients selected, cancel
DEBUG flwr 2024-06-10 21:44:59,759 | server.py:222 | fit_round 149: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:46:46,712 | server.py:236 | fit_round 149 received 10 results and 0 failures
INFO flwr 2024-06-10 21:47:16,329 | server.py:125 | fit progress: (149, 2.1273169921875, {'accuracy': 0.6507, 'data_size': 10000}, 19232.727687968872)
INFO flwr 2024-06-10 21:47:16,329 | server.py:171 | evaluate_round 149: no clients selected, cancel
DEBUG flwr 2024-06-10 21:47:16,330 | server.py:222 | fit_round 150: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h12m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h12m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h12m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h13m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h13m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h14m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h14m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h14m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h15m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h15m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h15m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h16m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h16m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h17m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h18m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h18m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h18m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h18m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h19m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h19m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h19m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h20m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h20m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h20m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h21m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h21m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 21:48:58,244 | server.py:236 | fit_round 150 received 10 results and 0 failures
INFO flwr 2024-06-10 21:49:20,320 | server.py:125 | fit progress: (150, 2.0962333984375, {'accuracy': 0.6531, 'data_size': 10000}, 19356.718204692006)
INFO flwr 2024-06-10 21:49:20,320 | server.py:171 | evaluate_round 150: no clients selected, cancel
DEBUG flwr 2024-06-10 21:49:20,321 | server.py:222 | fit_round 151: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:51:08,401 | server.py:236 | fit_round 151 received 10 results and 0 failures
INFO flwr 2024-06-10 21:51:29,624 | server.py:125 | fit progress: (151, 2.105191015625, {'accuracy': 0.6506, 'data_size': 10000}, 19486.02270613564)
INFO flwr 2024-06-10 21:51:29,624 | server.py:171 | evaluate_round 151: no clients selected, cancel
DEBUG flwr 2024-06-10 21:51:29,625 | server.py:222 | fit_round 152: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:53:12,629 | server.py:236 | fit_round 152 received 10 results and 0 failures
INFO flwr 2024-06-10 21:53:36,246 | server.py:125 | fit progress: (152, 2.0868224609375, {'accuracy': 0.6571, 'data_size': 10000}, 19612.644312269986)
INFO flwr 2024-06-10 21:53:36,246 | server.py:171 | evaluate_round 152: no clients selected, cancel
DEBUG flwr 2024-06-10 21:53:36,246 | server.py:222 | fit_round 153: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:55:24,505 | server.py:236 | fit_round 153 received 10 results and 0 failures
INFO flwr 2024-06-10 21:55:45,280 | server.py:125 | fit progress: (153, 2.114711328125, {'accuracy': 0.6525, 'data_size': 10000}, 19741.678993608803)
INFO flwr 2024-06-10 21:55:45,281 | server.py:171 | evaluate_round 153: no clients selected, cancel
DEBUG flwr 2024-06-10 21:55:45,281 | server.py:222 | fit_round 154: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 21:57:27,350 | server.py:236 | fit_round 154 received 10 results and 0 failures
INFO flwr 2024-06-10 21:57:47,839 | server.py:125 | fit progress: (154, 2.1199068359375, {'accuracy': 0.6535, 'data_size': 10000}, 19864.237732613925)
INFO flwr 2024-06-10 21:57:47,839 | server.py:171 | evaluate_round 154: no clients selected, cancel
DEBUG flwr 2024-06-10 21:57:47,840 | server.py:222 | fit_round 155: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h21m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h22m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h22m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h23m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h23m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h23m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h23m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h25m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h25m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h25m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h25m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h26m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h26m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h26m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h27m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h27m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h27m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h28m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h28m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h28m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h29m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h29m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h30m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h31m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h31m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h31m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 21:59:38,422 | server.py:236 | fit_round 155 received 10 results and 0 failures
INFO flwr 2024-06-10 21:59:59,655 | server.py:125 | fit progress: (155, 2.1558296875, {'accuracy': 0.6514, 'data_size': 10000}, 19996.053546426818)
INFO flwr 2024-06-10 21:59:59,655 | server.py:171 | evaluate_round 155: no clients selected, cancel
DEBUG flwr 2024-06-10 21:59:59,656 | server.py:222 | fit_round 156: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:01:42,219 | server.py:236 | fit_round 156 received 10 results and 0 failures
INFO flwr 2024-06-10 22:02:04,511 | server.py:125 | fit progress: (156, 2.1498267578125, {'accuracy': 0.6505, 'data_size': 10000}, 20120.909695471637)
INFO flwr 2024-06-10 22:02:04,512 | server.py:171 | evaluate_round 156: no clients selected, cancel
DEBUG flwr 2024-06-10 22:02:04,512 | server.py:222 | fit_round 157: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:03:54,944 | server.py:236 | fit_round 157 received 10 results and 0 failures
INFO flwr 2024-06-10 22:04:17,224 | server.py:125 | fit progress: (157, 2.123957421875, {'accuracy': 0.6567, 'data_size': 10000}, 20253.622716554906)
INFO flwr 2024-06-10 22:04:17,224 | server.py:171 | evaluate_round 157: no clients selected, cancel
DEBUG flwr 2024-06-10 22:04:17,225 | server.py:222 | fit_round 158: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:05:58,816 | server.py:236 | fit_round 158 received 10 results and 0 failures
INFO flwr 2024-06-10 22:06:20,643 | server.py:125 | fit progress: (158, 2.12934453125, {'accuracy': 0.6562, 'data_size': 10000}, 20377.041517847683)
INFO flwr 2024-06-10 22:06:20,644 | server.py:171 | evaluate_round 158: no clients selected, cancel
DEBUG flwr 2024-06-10 22:06:20,644 | server.py:222 | fit_round 159: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h32m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h32m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h32m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h32m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h33m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h33m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h34m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h34m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h34m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h34m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h35m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h35m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h35m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h35m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h36m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h36m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h37m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h37m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h37m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h37m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h38m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h38m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h38m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h39m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h39m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h39m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 22:08:09,858 | server.py:236 | fit_round 159 received 10 results and 0 failures
INFO flwr 2024-06-10 22:08:32,459 | server.py:125 | fit progress: (159, 2.10553515625, {'accuracy': 0.6578, 'data_size': 10000}, 20508.857703794725)
INFO flwr 2024-06-10 22:08:32,459 | server.py:171 | evaluate_round 159: no clients selected, cancel
DEBUG flwr 2024-06-10 22:08:32,460 | server.py:222 | fit_round 160: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:10:13,581 | server.py:236 | fit_round 160 received 10 results and 0 failures
INFO flwr 2024-06-10 22:10:35,714 | server.py:125 | fit progress: (160, 2.09762578125, {'accuracy': 0.6622, 'data_size': 10000}, 20632.113201521803)
INFO flwr 2024-06-10 22:10:35,715 | server.py:171 | evaluate_round 160: no clients selected, cancel
DEBUG flwr 2024-06-10 22:10:35,715 | server.py:222 | fit_round 161: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:12:24,770 | server.py:236 | fit_round 161 received 10 results and 0 failures
INFO flwr 2024-06-10 22:12:45,966 | server.py:125 | fit progress: (161, 2.14917890625, {'accuracy': 0.658, 'data_size': 10000}, 20762.36518941261)
INFO flwr 2024-06-10 22:12:45,967 | server.py:171 | evaluate_round 161: no clients selected, cancel
DEBUG flwr 2024-06-10 22:12:45,968 | server.py:222 | fit_round 162: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:14:27,997 | server.py:236 | fit_round 162 received 10 results and 0 failures
INFO flwr 2024-06-10 22:14:48,875 | server.py:125 | fit progress: (162, 2.182381640625, {'accuracy': 0.6563, 'data_size': 10000}, 20885.273889322765)
INFO flwr 2024-06-10 22:14:48,876 | server.py:171 | evaluate_round 162: no clients selected, cancel
DEBUG flwr 2024-06-10 22:14:48,876 | server.py:222 | fit_round 163: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h40m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h40m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h41m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h42m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h42m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h42m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h43m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h43m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h44m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h44m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h44m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h44m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h45m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h45m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h45m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h46m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h46m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h46m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h47m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h47m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h47m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h48m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h48m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h48m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h49m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h49m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 22:16:37,502 | server.py:236 | fit_round 163 received 10 results and 0 failures
INFO flwr 2024-06-10 22:16:58,883 | server.py:125 | fit progress: (163, 2.220923828125, {'accuracy': 0.657, 'data_size': 10000}, 21015.282204386778)
INFO flwr 2024-06-10 22:16:58,884 | server.py:171 | evaluate_round 163: no clients selected, cancel
DEBUG flwr 2024-06-10 22:16:58,884 | server.py:222 | fit_round 164: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:18:41,234 | server.py:236 | fit_round 164 received 10 results and 0 failures
INFO flwr 2024-06-10 22:19:03,151 | server.py:125 | fit progress: (164, 2.2064275390625, {'accuracy': 0.6641, 'data_size': 10000}, 21139.549430031795)
INFO flwr 2024-06-10 22:19:03,151 | server.py:171 | evaluate_round 164: no clients selected, cancel
DEBUG flwr 2024-06-10 22:19:03,153 | server.py:222 | fit_round 165: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:20:50,268 | server.py:236 | fit_round 165 received 10 results and 0 failures
INFO flwr 2024-06-10 22:21:11,982 | server.py:125 | fit progress: (165, 2.2409744140625, {'accuracy': 0.6591, 'data_size': 10000}, 21268.381008743774)
INFO flwr 2024-06-10 22:21:11,983 | server.py:171 | evaluate_round 165: no clients selected, cancel
DEBUG flwr 2024-06-10 22:21:11,983 | server.py:222 | fit_round 166: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:22:55,414 | server.py:236 | fit_round 166 received 10 results and 0 failures
INFO flwr 2024-06-10 22:23:17,455 | server.py:125 | fit progress: (166, 2.2917984375, {'accuracy': 0.6601, 'data_size': 10000}, 21393.85332474066)
INFO flwr 2024-06-10 22:23:17,455 | server.py:171 | evaluate_round 166: no clients selected, cancel
DEBUG flwr 2024-06-10 22:23:17,455 | server.py:222 | fit_round 167: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:25:06,573 | server.py:236 | fit_round 167 received 10 results and 0 failures
INFO flwr 2024-06-10 22:25:28,505 | server.py:125 | fit progress: (167, 2.3417125, {'accuracy': 0.6602, 'data_size': 10000}, 21524.903582585976)
INFO flwr 2024-06-10 22:25:28,505 | server.py:171 | evaluate_round 167: no clients selected, cancel
DEBUG flwr 2024-06-10 22:25:28,506 | server.py:222 | fit_round 168: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h50m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h50m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h50m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h50m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h51m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h51m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h51m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h52m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h52m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h52m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h53m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h54m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h54m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h55m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h55m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h55m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h55m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h56m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h56m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h57m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h57m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h57m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h58m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h58m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h59m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h59m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 22:27:10,729 | server.py:236 | fit_round 168 received 10 results and 0 failures
INFO flwr 2024-06-10 22:27:32,024 | server.py:125 | fit progress: (168, 2.308609765625, {'accuracy': 0.6648, 'data_size': 10000}, 21648.422947780695)
INFO flwr 2024-06-10 22:27:32,025 | server.py:171 | evaluate_round 168: no clients selected, cancel
DEBUG flwr 2024-06-10 22:27:32,025 | server.py:222 | fit_round 169: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:29:20,845 | server.py:236 | fit_round 169 received 10 results and 0 failures
INFO flwr 2024-06-10 22:29:46,397 | server.py:125 | fit progress: (169, 2.288355859375, {'accuracy': 0.6655, 'data_size': 10000}, 21782.796199426986)
INFO flwr 2024-06-10 22:29:46,742 | server.py:171 | evaluate_round 169: no clients selected, cancel
DEBUG flwr 2024-06-10 22:29:46,742 | server.py:222 | fit_round 170: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:31:27,771 | server.py:236 | fit_round 170 received 10 results and 0 failures
INFO flwr 2024-06-10 22:31:49,101 | server.py:125 | fit progress: (170, 2.339387890625, {'accuracy': 0.6607, 'data_size': 10000}, 21905.499469740782)
INFO flwr 2024-06-10 22:31:49,101 | server.py:171 | evaluate_round 170: no clients selected, cancel
DEBUG flwr 2024-06-10 22:31:49,102 | server.py:222 | fit_round 171: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:33:37,075 | server.py:236 | fit_round 171 received 10 results and 0 failures
INFO flwr 2024-06-10 22:33:58,873 | server.py:125 | fit progress: (171, 2.353440234375, {'accuracy': 0.6558, 'data_size': 10000}, 22035.272138381843)
INFO flwr 2024-06-10 22:33:58,874 | server.py:171 | evaluate_round 171: no clients selected, cancel
DEBUG flwr 2024-06-10 22:33:58,874 | server.py:222 | fit_round 172: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:35:40,882 | server.py:236 | fit_round 172 received 10 results and 0 failures
INFO flwr 2024-06-10 22:36:01,928 | server.py:125 | fit progress: (172, 2.4246369140625, {'accuracy': 0.6526, 'data_size': 10000}, 22158.326707573608)
INFO flwr 2024-06-10 22:36:01,929 | server.py:171 | evaluate_round 172: no clients selected, cancel
DEBUG flwr 2024-06-10 22:36:01,929 | server.py:222 | fit_round 173: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +5h59m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5h59m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h1m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h1m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h3m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h3m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h4m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h4m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h5m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h5m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h5m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h5m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h6m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h6m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h7m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h7m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h7m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h7m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h8m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h8m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h8m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h9m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h9m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h9m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h10m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 22:37:50,059 | server.py:236 | fit_round 173 received 10 results and 0 failures
INFO flwr 2024-06-10 22:38:12,501 | server.py:125 | fit progress: (173, 2.39999765625, {'accuracy': 0.6585, 'data_size': 10000}, 22288.90015159268)
INFO flwr 2024-06-10 22:38:12,502 | server.py:171 | evaluate_round 173: no clients selected, cancel
DEBUG flwr 2024-06-10 22:38:12,502 | server.py:222 | fit_round 174: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:39:55,016 | server.py:236 | fit_round 174 received 10 results and 0 failures
INFO flwr 2024-06-10 22:40:16,509 | server.py:125 | fit progress: (174, 2.33826015625, {'accuracy': 0.6649, 'data_size': 10000}, 22412.90734495269)
INFO flwr 2024-06-10 22:40:16,509 | server.py:171 | evaluate_round 174: no clients selected, cancel
DEBUG flwr 2024-06-10 22:40:16,509 | server.py:222 | fit_round 175: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:42:06,051 | server.py:236 | fit_round 175 received 10 results and 0 failures
INFO flwr 2024-06-10 22:42:27,819 | server.py:125 | fit progress: (175, 2.3415013671875, {'accuracy': 0.6637, 'data_size': 10000}, 22544.217860264704)
INFO flwr 2024-06-10 22:42:27,820 | server.py:171 | evaluate_round 175: no clients selected, cancel
DEBUG flwr 2024-06-10 22:42:27,820 | server.py:222 | fit_round 176: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:44:09,868 | server.py:236 | fit_round 176 received 10 results and 0 failures
INFO flwr 2024-06-10 22:44:30,601 | server.py:125 | fit progress: (176, 2.409753515625, {'accuracy': 0.6577, 'data_size': 10000}, 22666.999672374688)
INFO flwr 2024-06-10 22:44:30,601 | server.py:171 | evaluate_round 176: no clients selected, cancel
DEBUG flwr 2024-06-10 22:44:30,602 | server.py:222 | fit_round 177: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:46:17,662 | server.py:236 | fit_round 177 received 10 results and 0 failures
[2m[1m[33m(autoscaler +6h10m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h11m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h11m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h12m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h12m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h12m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h13m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h13m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h14m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h14m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h14m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h15m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h15m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h15m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h16m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h16m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h16m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h16m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h17m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h17m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h18m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h18m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h18m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h18m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h19m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h19m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 22:46:41,165 | server.py:125 | fit progress: (177, 2.4292318359375, {'accuracy': 0.6586, 'data_size': 10000}, 22797.563698114827)
INFO flwr 2024-06-10 22:46:41,165 | server.py:171 | evaluate_round 177: no clients selected, cancel
DEBUG flwr 2024-06-10 22:46:41,166 | server.py:222 | fit_round 178: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:48:21,696 | server.py:236 | fit_round 178 received 10 results and 0 failures
INFO flwr 2024-06-10 22:48:43,129 | server.py:125 | fit progress: (178, 2.446654296875, {'accuracy': 0.6571, 'data_size': 10000}, 22919.52738331398)
INFO flwr 2024-06-10 22:48:43,129 | server.py:171 | evaluate_round 178: no clients selected, cancel
DEBUG flwr 2024-06-10 22:48:43,130 | server.py:222 | fit_round 179: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:50:31,885 | server.py:236 | fit_round 179 received 10 results and 0 failures
INFO flwr 2024-06-10 22:50:54,921 | server.py:125 | fit progress: (179, 2.4758572265625, {'accuracy': 0.6573, 'data_size': 10000}, 23051.320116874762)
INFO flwr 2024-06-10 22:50:54,922 | server.py:171 | evaluate_round 179: no clients selected, cancel
DEBUG flwr 2024-06-10 22:50:54,922 | server.py:222 | fit_round 180: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:52:36,566 | server.py:236 | fit_round 180 received 10 results and 0 failures
INFO flwr 2024-06-10 22:52:57,670 | server.py:125 | fit progress: (180, 2.4809462890625, {'accuracy': 0.6604, 'data_size': 10000}, 23174.06831232598)
INFO flwr 2024-06-10 22:52:57,670 | server.py:171 | evaluate_round 180: no clients selected, cancel
DEBUG flwr 2024-06-10 22:52:57,670 | server.py:222 | fit_round 181: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +6h20m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h20m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h20m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h20m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h21m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h21m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h21m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h21m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h22m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h22m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h23m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h23m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h23m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h23m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h24m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h24m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h25m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h25m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h25m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h25m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h26m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h26m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h27m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h27m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h27m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h27m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 22:54:47,657 | server.py:236 | fit_round 181 received 10 results and 0 failures
INFO flwr 2024-06-10 22:55:10,033 | server.py:125 | fit progress: (181, 2.4459544921875, {'accuracy': 0.6657, 'data_size': 10000}, 23306.431624308694)
INFO flwr 2024-06-10 22:55:10,033 | server.py:171 | evaluate_round 181: no clients selected, cancel
DEBUG flwr 2024-06-10 22:55:10,034 | server.py:222 | fit_round 182: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:56:51,862 | server.py:236 | fit_round 182 received 10 results and 0 failures
INFO flwr 2024-06-10 22:57:12,828 | server.py:125 | fit progress: (182, 2.4498216796875, {'accuracy': 0.6667, 'data_size': 10000}, 23429.226391315926)
INFO flwr 2024-06-10 22:57:12,828 | server.py:171 | evaluate_round 182: no clients selected, cancel
DEBUG flwr 2024-06-10 22:57:12,829 | server.py:222 | fit_round 183: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 22:59:00,998 | server.py:236 | fit_round 183 received 10 results and 0 failures
INFO flwr 2024-06-10 22:59:24,541 | server.py:125 | fit progress: (183, 2.445953515625, {'accuracy': 0.6683, 'data_size': 10000}, 23560.940006439574)
INFO flwr 2024-06-10 22:59:24,542 | server.py:171 | evaluate_round 183: no clients selected, cancel
DEBUG flwr 2024-06-10 22:59:24,543 | server.py:222 | fit_round 184: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:01:06,198 | server.py:236 | fit_round 184 received 10 results and 0 failures
INFO flwr 2024-06-10 23:01:31,320 | server.py:125 | fit progress: (184, 2.4514607421875, {'accuracy': 0.6665, 'data_size': 10000}, 23687.718584219925)
INFO flwr 2024-06-10 23:01:31,320 | server.py:171 | evaluate_round 184: no clients selected, cancel
DEBUG flwr 2024-06-10 23:01:31,321 | server.py:222 | fit_round 185: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:03:20,203 | server.py:236 | fit_round 185 received 10 results and 0 failures
INFO flwr 2024-06-10 23:03:41,392 | server.py:125 | fit progress: (185, 2.4895552734375, {'accuracy': 0.664, 'data_size': 10000}, 23817.790962521918)
INFO flwr 2024-06-10 23:03:41,393 | server.py:171 | evaluate_round 185: no clients selected, cancel
DEBUG flwr 2024-06-10 23:03:41,394 | server.py:222 | fit_round 186: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:05:25,423 | server.py:236 | fit_round 186 received 10 results and 0 failures
[2m[1m[33m(autoscaler +6h28m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h28m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h28m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h28m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h29m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h29m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h30m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h30m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h30m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h30m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h31m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h31m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h32m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h32m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h33m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h33m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h33m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h33m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h34m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h34m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h35m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h35m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h37m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h37m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h37m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h37m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 23:05:48,243 | server.py:125 | fit progress: (186, 2.542851953125, {'accuracy': 0.6588, 'data_size': 10000}, 23944.64168989286)
INFO flwr 2024-06-10 23:05:48,244 | server.py:171 | evaluate_round 186: no clients selected, cancel
DEBUG flwr 2024-06-10 23:05:48,244 | server.py:222 | fit_round 187: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:07:38,055 | server.py:236 | fit_round 187 received 10 results and 0 failures
INFO flwr 2024-06-10 23:07:59,718 | server.py:125 | fit progress: (187, 2.5870095703125, {'accuracy': 0.6559, 'data_size': 10000}, 24076.11646235967)
INFO flwr 2024-06-10 23:07:59,718 | server.py:171 | evaluate_round 187: no clients selected, cancel
DEBUG flwr 2024-06-10 23:07:59,719 | server.py:222 | fit_round 188: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:09:41,928 | server.py:236 | fit_round 188 received 10 results and 0 failures
INFO flwr 2024-06-10 23:10:04,015 | server.py:125 | fit progress: (188, 2.6006931640625, {'accuracy': 0.6592, 'data_size': 10000}, 24200.413231635)
INFO flwr 2024-06-10 23:10:04,015 | server.py:171 | evaluate_round 188: no clients selected, cancel
DEBUG flwr 2024-06-10 23:10:04,016 | server.py:222 | fit_round 189: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:11:52,440 | server.py:236 | fit_round 189 received 10 results and 0 failures
INFO flwr 2024-06-10 23:12:13,158 | server.py:125 | fit progress: (189, 2.588540234375, {'accuracy': 0.6607, 'data_size': 10000}, 24329.556927599944)
INFO flwr 2024-06-10 23:12:13,159 | server.py:171 | evaluate_round 189: no clients selected, cancel
DEBUG flwr 2024-06-10 23:12:13,160 | server.py:222 | fit_round 190: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:13:56,290 | server.py:236 | fit_round 190 received 10 results and 0 failures
INFO flwr 2024-06-10 23:14:19,511 | server.py:125 | fit progress: (190, 2.5161853515625, {'accuracy': 0.6666, 'data_size': 10000}, 24455.910170525778)
INFO flwr 2024-06-10 23:14:19,512 | server.py:171 | evaluate_round 190: no clients selected, cancel
DEBUG flwr 2024-06-10 23:14:19,512 | server.py:222 | fit_round 191: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:16:08,028 | server.py:236 | fit_round 191 received 10 results and 0 failures
[2m[1m[33m(autoscaler +6h39m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h39m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h39m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h39m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h40m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h40m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h41m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h41m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h41m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h41m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h42m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h42m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h43m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h43m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h43m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h43m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h44m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h44m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h45m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h45m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h45m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h46m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h47m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h47m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h48m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h48m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 23:16:31,492 | server.py:125 | fit progress: (191, 2.5167974609375, {'accuracy': 0.6718, 'data_size': 10000}, 24587.89072370762)
INFO flwr 2024-06-10 23:16:31,492 | server.py:171 | evaluate_round 191: no clients selected, cancel
DEBUG flwr 2024-06-10 23:16:31,494 | server.py:222 | fit_round 192: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:18:13,162 | server.py:236 | fit_round 192 received 10 results and 0 failures
INFO flwr 2024-06-10 23:18:42,139 | server.py:125 | fit progress: (192, 2.5254771484375, {'accuracy': 0.6685, 'data_size': 10000}, 24718.538198986556)
INFO flwr 2024-06-10 23:18:42,140 | server.py:171 | evaluate_round 192: no clients selected, cancel
DEBUG flwr 2024-06-10 23:18:42,140 | server.py:222 | fit_round 193: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:20:29,942 | server.py:236 | fit_round 193 received 10 results and 0 failures
INFO flwr 2024-06-10 23:20:51,022 | server.py:125 | fit progress: (193, 2.5643140625, {'accuracy': 0.6657, 'data_size': 10000}, 24847.420551336836)
INFO flwr 2024-06-10 23:20:51,022 | server.py:171 | evaluate_round 193: no clients selected, cancel
DEBUG flwr 2024-06-10 23:20:51,023 | server.py:222 | fit_round 194: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:22:33,191 | server.py:236 | fit_round 194 received 10 results and 0 failures
INFO flwr 2024-06-10 23:22:54,100 | server.py:125 | fit progress: (194, 2.605394921875, {'accuracy': 0.6638, 'data_size': 10000}, 24970.49884861894)
INFO flwr 2024-06-10 23:22:54,101 | server.py:171 | evaluate_round 194: no clients selected, cancel
DEBUG flwr 2024-06-10 23:22:54,101 | server.py:222 | fit_round 195: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:24:48,774 | server.py:236 | fit_round 195 received 10 results and 0 failures
INFO flwr 2024-06-10 23:26:05,514 | server.py:125 | fit progress: (195, 2.641516796875, {'accuracy': 0.6619, 'data_size': 10000}, 25161.912342267577)
INFO flwr 2024-06-10 23:26:05,514 | server.py:171 | evaluate_round 195: no clients selected, cancel
DEBUG flwr 2024-06-10 23:26:05,514 | server.py:222 | fit_round 196: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +6h50m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h50m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h50m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h50m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h51m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h51m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h52m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h52m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h54m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h54m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h55m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h55m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h55m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h56m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h56m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h56m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h57m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h57m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h57m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h58m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h58m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h58m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h58m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h59m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6h59m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 23:27:51,849 | server.py:236 | fit_round 196 received 10 results and 0 failures
INFO flwr 2024-06-10 23:33:30,545 | server.py:125 | fit progress: (196, 2.6397296875, {'accuracy': 0.663, 'data_size': 10000}, 25606.94384952262)
INFO flwr 2024-06-10 23:33:30,546 | server.py:171 | evaluate_round 196: no clients selected, cancel
DEBUG flwr 2024-06-10 23:33:30,546 | server.py:222 | fit_round 197: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h1m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h1m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h1m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h2m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h2m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h2m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h3m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h3m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h3m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h3m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h4m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h4m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h4m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h4m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h5m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h5m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h5m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h6m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h6m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h6m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h7m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h7m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h7m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h8m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 23:35:24,040 | server.py:236 | fit_round 197 received 10 results and 0 failures
INFO flwr 2024-06-10 23:42:20,773 | server.py:125 | fit progress: (197, 2.6307205078125, {'accuracy': 0.6652, 'data_size': 10000}, 26137.17203996377)
INFO flwr 2024-06-10 23:42:20,774 | server.py:171 | evaluate_round 197: no clients selected, cancel
DEBUG flwr 2024-06-10 23:42:20,774 | server.py:222 | fit_round 198: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h8m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h9m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h9m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h9m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h9m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h10m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h10m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h10m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h10m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h11m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h11m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h12m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h12m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h12m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h12m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h13m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h13m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h13m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h13m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h14m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h14m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h14m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h14m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h15m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h15m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h16m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-10 23:44:06,244 | server.py:236 | fit_round 198 received 10 results and 0 failures
INFO flwr 2024-06-10 23:47:56,704 | server.py:125 | fit progress: (198, 2.6419109375, {'accuracy': 0.6663, 'data_size': 10000}, 26473.102967455983)
INFO flwr 2024-06-10 23:47:56,705 | server.py:171 | evaluate_round 198: no clients selected, cancel
DEBUG flwr 2024-06-10 23:47:56,705 | server.py:222 | fit_round 199: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:49:50,530 | server.py:236 | fit_round 199 received 10 results and 0 failures
[2m[1m[33m(autoscaler +7h16m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h16m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h16m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h17m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h17m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h18m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h18m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h18m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h18m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h19m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h19m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h19m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h19m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h20m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h20m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h21m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h21m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h21m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h21m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h22m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h22m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h22m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h22m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h23m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h23m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h24m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-10 23:51:41,414 | server.py:125 | fit progress: (199, 2.6576373046875, {'accuracy': 0.6664, 'data_size': 10000}, 26697.812606392894)
INFO flwr 2024-06-10 23:51:41,414 | server.py:171 | evaluate_round 199: no clients selected, cancel
DEBUG flwr 2024-06-10 23:51:41,415 | server.py:222 | fit_round 200: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:53:25,870 | server.py:236 | fit_round 200 received 10 results and 0 failures
INFO flwr 2024-06-10 23:53:53,920 | server.py:125 | fit progress: (200, 2.6763115234375, {'accuracy': 0.6688, 'data_size': 10000}, 26830.3189106388)
INFO flwr 2024-06-10 23:53:53,921 | server.py:171 | evaluate_round 200: no clients selected, cancel
DEBUG flwr 2024-06-10 23:53:53,921 | server.py:222 | fit_round 201: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:55:42,728 | server.py:236 | fit_round 201 received 10 results and 0 failures
INFO flwr 2024-06-10 23:56:07,943 | server.py:125 | fit progress: (201, 2.693650390625, {'accuracy': 0.6696, 'data_size': 10000}, 26964.34146498982)
INFO flwr 2024-06-10 23:56:07,943 | server.py:171 | evaluate_round 201: no clients selected, cancel
DEBUG flwr 2024-06-10 23:56:07,944 | server.py:222 | fit_round 202: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-10 23:57:50,310 | server.py:236 | fit_round 202 received 10 results and 0 failures
INFO flwr 2024-06-10 23:58:17,513 | server.py:125 | fit progress: (202, 2.7209400390625, {'accuracy': 0.6684, 'data_size': 10000}, 27093.911800715607)
INFO flwr 2024-06-10 23:58:17,514 | server.py:171 | evaluate_round 202: no clients selected, cancel
DEBUG flwr 2024-06-10 23:58:17,514 | server.py:222 | fit_round 203: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h24m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h25m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h25m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h25m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h25m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h26m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h26m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h26m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h26m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h27m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h27m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h28m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h28m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h28m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h29m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h29m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h29m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h29m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h30m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h30m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h31m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h31m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h31m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h31m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h32m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h32m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:00:06,230 | server.py:236 | fit_round 203 received 10 results and 0 failures
INFO flwr 2024-06-11 00:00:30,997 | server.py:125 | fit progress: (203, 2.7368029296875, {'accuracy': 0.6674, 'data_size': 10000}, 27227.395749666728)
INFO flwr 2024-06-11 00:00:30,998 | server.py:171 | evaluate_round 203: no clients selected, cancel
DEBUG flwr 2024-06-11 00:00:30,999 | server.py:222 | fit_round 204: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:02:12,769 | server.py:236 | fit_round 204 received 10 results and 0 failures
INFO flwr 2024-06-11 00:02:35,403 | server.py:125 | fit progress: (204, 2.746566015625, {'accuracy': 0.6653, 'data_size': 10000}, 27351.80192742776)
INFO flwr 2024-06-11 00:02:35,404 | server.py:171 | evaluate_round 204: no clients selected, cancel
DEBUG flwr 2024-06-11 00:02:35,404 | server.py:222 | fit_round 205: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:04:24,595 | server.py:236 | fit_round 205 received 10 results and 0 failures
INFO flwr 2024-06-11 00:04:46,904 | server.py:125 | fit progress: (205, 2.7591244140625, {'accuracy': 0.665, 'data_size': 10000}, 27483.302914755885)
INFO flwr 2024-06-11 00:04:46,905 | server.py:171 | evaluate_round 205: no clients selected, cancel
DEBUG flwr 2024-06-11 00:04:46,905 | server.py:222 | fit_round 206: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:06:29,359 | server.py:236 | fit_round 206 received 10 results and 0 failures
INFO flwr 2024-06-11 00:06:51,176 | server.py:125 | fit progress: (206, 2.772296484375, {'accuracy': 0.6662, 'data_size': 10000}, 27607.57479680283)
INFO flwr 2024-06-11 00:06:51,177 | server.py:171 | evaluate_round 206: no clients selected, cancel
DEBUG flwr 2024-06-11 00:06:51,177 | server.py:222 | fit_round 207: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h33m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h33m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h33m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h33m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h34m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h34m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h35m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h35m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h35m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h35m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h36m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h36m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h36m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h37m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h37m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h37m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h37m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h38m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h38m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h39m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h39m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h39m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h39m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h40m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h40m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h41m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:08:40,971 | server.py:236 | fit_round 207 received 10 results and 0 failures
INFO flwr 2024-06-11 00:09:02,909 | server.py:125 | fit progress: (207, 2.784410546875, {'accuracy': 0.6662, 'data_size': 10000}, 27739.307412475813)
INFO flwr 2024-06-11 00:09:02,909 | server.py:171 | evaluate_round 207: no clients selected, cancel
DEBUG flwr 2024-06-11 00:09:02,910 | server.py:222 | fit_round 208: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:10:46,009 | server.py:236 | fit_round 208 received 10 results and 0 failures
INFO flwr 2024-06-11 00:11:07,485 | server.py:125 | fit progress: (208, 2.7849826171875, {'accuracy': 0.6662, 'data_size': 10000}, 27863.8836153429)
INFO flwr 2024-06-11 00:11:07,485 | server.py:171 | evaluate_round 208: no clients selected, cancel
DEBUG flwr 2024-06-11 00:11:07,487 | server.py:222 | fit_round 209: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:12:56,838 | server.py:236 | fit_round 209 received 10 results and 0 failures
INFO flwr 2024-06-11 00:13:17,737 | server.py:125 | fit progress: (209, 2.794425390625, {'accuracy': 0.6663, 'data_size': 10000}, 27994.135787065607)
INFO flwr 2024-06-11 00:13:17,737 | server.py:171 | evaluate_round 209: no clients selected, cancel
DEBUG flwr 2024-06-11 00:13:17,738 | server.py:222 | fit_round 210: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:15:01,523 | server.py:236 | fit_round 210 received 10 results and 0 failures
INFO flwr 2024-06-11 00:15:23,111 | server.py:125 | fit progress: (210, 2.798658984375, {'accuracy': 0.6653, 'data_size': 10000}, 28119.50980397081)
INFO flwr 2024-06-11 00:15:23,112 | server.py:171 | evaluate_round 210: no clients selected, cancel
DEBUG flwr 2024-06-11 00:15:23,113 | server.py:222 | fit_round 211: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h41m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h41m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h41m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h42m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h42m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h43m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h43m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h43m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h43m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h44m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h44m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h45m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h45m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h45m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h45m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h46m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h46m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h46m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h46m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h47m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h47m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h48m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h48m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h48m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h48m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h49m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:17:13,235 | server.py:236 | fit_round 211 received 10 results and 0 failures
INFO flwr 2024-06-11 00:17:35,738 | server.py:125 | fit progress: (211, 2.795673828125, {'accuracy': 0.6676, 'data_size': 10000}, 28252.136624728795)
INFO flwr 2024-06-11 00:17:35,738 | server.py:171 | evaluate_round 211: no clients selected, cancel
DEBUG flwr 2024-06-11 00:17:35,739 | server.py:222 | fit_round 212: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:19:19,949 | server.py:236 | fit_round 212 received 10 results and 0 failures
INFO flwr 2024-06-11 00:19:40,976 | server.py:125 | fit progress: (212, 2.7837201171875, {'accuracy': 0.6681, 'data_size': 10000}, 28377.37464010669)
INFO flwr 2024-06-11 00:19:40,976 | server.py:171 | evaluate_round 212: no clients selected, cancel
DEBUG flwr 2024-06-11 00:19:40,977 | server.py:222 | fit_round 213: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:21:28,915 | server.py:236 | fit_round 213 received 10 results and 0 failures
INFO flwr 2024-06-11 00:21:51,663 | server.py:125 | fit progress: (213, 2.7879001953125, {'accuracy': 0.668, 'data_size': 10000}, 28508.062063654885)
INFO flwr 2024-06-11 00:21:51,664 | server.py:171 | evaluate_round 213: no clients selected, cancel
DEBUG flwr 2024-06-11 00:21:51,664 | server.py:222 | fit_round 214: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:23:34,670 | server.py:236 | fit_round 214 received 10 results and 0 failures
INFO flwr 2024-06-11 00:23:56,658 | server.py:125 | fit progress: (214, 2.7989365234375, {'accuracy': 0.6683, 'data_size': 10000}, 28633.056403013878)
INFO flwr 2024-06-11 00:23:56,658 | server.py:171 | evaluate_round 214: no clients selected, cancel
DEBUG flwr 2024-06-11 00:23:56,659 | server.py:222 | fit_round 215: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h49m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h50m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h50m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h50m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h50m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h51m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h51m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h51m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h51m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h52m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h52m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h53m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h53m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h53m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h53m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h54m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h54m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h55m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h55m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h56m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h56m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h56m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h56m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h57m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h57m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h57m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:25:44,418 | server.py:236 | fit_round 215 received 10 results and 0 failures
INFO flwr 2024-06-11 00:26:05,750 | server.py:125 | fit progress: (215, 2.8134685546875, {'accuracy': 0.6678, 'data_size': 10000}, 28762.14840566879)
INFO flwr 2024-06-11 00:26:05,750 | server.py:171 | evaluate_round 215: no clients selected, cancel
DEBUG flwr 2024-06-11 00:26:05,751 | server.py:222 | fit_round 216: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:27:47,891 | server.py:236 | fit_round 216 received 10 results and 0 failures
INFO flwr 2024-06-11 00:28:09,289 | server.py:125 | fit progress: (216, 2.8077984375, {'accuracy': 0.6698, 'data_size': 10000}, 28885.6881944309)
INFO flwr 2024-06-11 00:28:09,290 | server.py:171 | evaluate_round 216: no clients selected, cancel
DEBUG flwr 2024-06-11 00:28:09,290 | server.py:222 | fit_round 217: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:29:57,478 | server.py:236 | fit_round 217 received 10 results and 0 failures
INFO flwr 2024-06-11 00:30:19,250 | server.py:125 | fit progress: (217, 2.8054734375, {'accuracy': 0.6729, 'data_size': 10000}, 29015.64879423566)
INFO flwr 2024-06-11 00:30:19,251 | server.py:171 | evaluate_round 217: no clients selected, cancel
DEBUG flwr 2024-06-11 00:30:19,252 | server.py:222 | fit_round 218: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:32:02,586 | server.py:236 | fit_round 218 received 10 results and 0 failures
INFO flwr 2024-06-11 00:32:24,227 | server.py:125 | fit progress: (218, 2.8173556640625, {'accuracy': 0.6704, 'data_size': 10000}, 29140.625728389714)
INFO flwr 2024-06-11 00:32:24,227 | server.py:171 | evaluate_round 218: no clients selected, cancel
DEBUG flwr 2024-06-11 00:32:24,228 | server.py:222 | fit_round 219: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +7h58m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h58m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h58m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h59m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h59m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h59m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7h59m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h1m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h1m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h2m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h2m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h2m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h2m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h3m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h3m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h4m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h4m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h4m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h4m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h5m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h5m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h5m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h6m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h6m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h6m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h7m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:34:12,039 | server.py:236 | fit_round 219 received 10 results and 0 failures
INFO flwr 2024-06-11 00:34:32,835 | server.py:125 | fit progress: (219, 2.850865234375, {'accuracy': 0.6676, 'data_size': 10000}, 29269.233226961922)
INFO flwr 2024-06-11 00:34:32,835 | server.py:171 | evaluate_round 219: no clients selected, cancel
DEBUG flwr 2024-06-11 00:34:32,836 | server.py:222 | fit_round 220: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:36:15,301 | server.py:236 | fit_round 220 received 10 results and 0 failures
INFO flwr 2024-06-11 00:36:36,899 | server.py:125 | fit progress: (220, 2.8850380859375, {'accuracy': 0.6652, 'data_size': 10000}, 29393.297759815585)
INFO flwr 2024-06-11 00:36:36,899 | server.py:171 | evaluate_round 220: no clients selected, cancel
DEBUG flwr 2024-06-11 00:36:36,900 | server.py:222 | fit_round 221: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:38:26,697 | server.py:236 | fit_round 221 received 10 results and 0 failures
INFO flwr 2024-06-11 00:38:47,259 | server.py:125 | fit progress: (221, 2.9056185546875, {'accuracy': 0.6647, 'data_size': 10000}, 29523.65746905282)
INFO flwr 2024-06-11 00:38:47,259 | server.py:171 | evaluate_round 221: no clients selected, cancel
DEBUG flwr 2024-06-11 00:38:47,260 | server.py:222 | fit_round 222: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:40:31,175 | server.py:236 | fit_round 222 received 10 results and 0 failures
INFO flwr 2024-06-11 00:40:52,053 | server.py:125 | fit progress: (222, 2.8955625, {'accuracy': 0.665, 'data_size': 10000}, 29648.451388640795)
INFO flwr 2024-06-11 00:40:52,053 | server.py:171 | evaluate_round 222: no clients selected, cancel
DEBUG flwr 2024-06-11 00:40:52,053 | server.py:222 | fit_round 223: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +8h7m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h7m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h8m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h8m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h8m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h9m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h9m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h9m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h10m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h10m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h10m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h10m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h11m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h11m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h12m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h12m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h12m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h13m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h13m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h14m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h14m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h14m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h14m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h15m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h15m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h15m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 00:42:42,136 | server.py:236 | fit_round 223 received 10 results and 0 failures
INFO flwr 2024-06-11 00:43:04,284 | server.py:125 | fit progress: (223, 2.88544453125, {'accuracy': 0.6673, 'data_size': 10000}, 29780.682933230884)
INFO flwr 2024-06-11 00:43:04,285 | server.py:171 | evaluate_round 223: no clients selected, cancel
DEBUG flwr 2024-06-11 00:43:04,285 | server.py:222 | fit_round 224: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:44:46,658 | server.py:236 | fit_round 224 received 10 results and 0 failures
INFO flwr 2024-06-11 00:45:08,050 | server.py:125 | fit progress: (224, 2.892315234375, {'accuracy': 0.6686, 'data_size': 10000}, 29904.44911847962)
INFO flwr 2024-06-11 00:45:08,051 | server.py:171 | evaluate_round 224: no clients selected, cancel
DEBUG flwr 2024-06-11 00:45:08,051 | server.py:222 | fit_round 225: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:46:56,760 | server.py:236 | fit_round 225 received 10 results and 0 failures
INFO flwr 2024-06-11 00:47:18,055 | server.py:125 | fit progress: (225, 2.9090490234375, {'accuracy': 0.669, 'data_size': 10000}, 30034.4536768198)
INFO flwr 2024-06-11 00:47:18,055 | server.py:171 | evaluate_round 225: no clients selected, cancel
DEBUG flwr 2024-06-11 00:47:18,056 | server.py:222 | fit_round 226: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:49:02,451 | server.py:236 | fit_round 226 received 10 results and 0 failures
INFO flwr 2024-06-11 00:49:23,704 | server.py:125 | fit progress: (226, 2.9392232421875, {'accuracy': 0.6648, 'data_size': 10000}, 30160.102610315662)
INFO flwr 2024-06-11 00:49:23,704 | server.py:171 | evaluate_round 226: no clients selected, cancel
DEBUG flwr 2024-06-11 00:49:23,705 | server.py:222 | fit_round 227: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:51:11,492 | server.py:236 | fit_round 227 received 10 results and 0 failures
[2m[1m[33m(autoscaler +8h16m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h16m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h16m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h17m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h17m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h18m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h18m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h18m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h19m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h19m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h19m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h20m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h20m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h20m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h20m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h21m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h21m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h22m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h22m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h22m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h23m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h23m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h23m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h23m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h24m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h25m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 00:51:38,957 | server.py:125 | fit progress: (227, 2.98654296875, {'accuracy': 0.6629, 'data_size': 10000}, 30295.355376531836)
INFO flwr 2024-06-11 00:51:38,957 | server.py:171 | evaluate_round 227: no clients selected, cancel
DEBUG flwr 2024-06-11 00:51:38,957 | server.py:222 | fit_round 228: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:53:21,611 | server.py:236 | fit_round 228 received 10 results and 0 failures
INFO flwr 2024-06-11 00:53:43,645 | server.py:125 | fit progress: (228, 3.00087421875, {'accuracy': 0.6635, 'data_size': 10000}, 30420.04367819894)
INFO flwr 2024-06-11 00:53:43,645 | server.py:171 | evaluate_round 228: no clients selected, cancel
DEBUG flwr 2024-06-11 00:53:43,646 | server.py:222 | fit_round 229: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:55:33,275 | server.py:236 | fit_round 229 received 10 results and 0 failures
INFO flwr 2024-06-11 00:55:53,863 | server.py:125 | fit progress: (229, 3.005280859375, {'accuracy': 0.6629, 'data_size': 10000}, 30550.26205644384)
INFO flwr 2024-06-11 00:55:53,864 | server.py:171 | evaluate_round 229: no clients selected, cancel
DEBUG flwr 2024-06-11 00:55:53,864 | server.py:222 | fit_round 230: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:57:38,036 | server.py:236 | fit_round 230 received 10 results and 0 failures
INFO flwr 2024-06-11 00:57:59,786 | server.py:125 | fit progress: (230, 2.990633984375, {'accuracy': 0.664, 'data_size': 10000}, 30676.184917977545)
INFO flwr 2024-06-11 00:57:59,787 | server.py:171 | evaluate_round 230: no clients selected, cancel
DEBUG flwr 2024-06-11 00:57:59,787 | server.py:222 | fit_round 231: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 00:59:50,762 | server.py:236 | fit_round 231 received 10 results and 0 failures
INFO flwr 2024-06-11 01:00:11,601 | server.py:125 | fit progress: (231, 2.982157421875, {'accuracy': 0.664, 'data_size': 10000}, 30807.99995645974)
INFO flwr 2024-06-11 01:00:11,602 | server.py:171 | evaluate_round 231: no clients selected, cancel
DEBUG flwr 2024-06-11 01:00:11,603 | server.py:222 | fit_round 232: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:01:52,198 | server.py:236 | fit_round 232 received 10 results and 0 failures
[2m[1m[33m(autoscaler +8h25m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h25m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h25m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h26m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h26m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h27m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h27m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h27m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h28m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h28m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h29m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h29m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h29m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h30m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h31m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h31m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h32m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h32m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h33m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h33m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h34m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h34m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h34m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h35m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h35m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h35m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 01:02:14,000 | server.py:125 | fit progress: (232, 2.990607421875, {'accuracy': 0.6631, 'data_size': 10000}, 30930.398353684694)
INFO flwr 2024-06-11 01:02:14,000 | server.py:171 | evaluate_round 232: no clients selected, cancel
DEBUG flwr 2024-06-11 01:02:14,000 | server.py:222 | fit_round 233: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:04:04,730 | server.py:236 | fit_round 233 received 10 results and 0 failures
INFO flwr 2024-06-11 01:04:27,151 | server.py:125 | fit progress: (233, 3.0022744140625, {'accuracy': 0.6662, 'data_size': 10000}, 31063.550022893585)
INFO flwr 2024-06-11 01:04:27,152 | server.py:171 | evaluate_round 233: no clients selected, cancel
DEBUG flwr 2024-06-11 01:04:27,152 | server.py:222 | fit_round 234: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:06:08,482 | server.py:236 | fit_round 234 received 10 results and 0 failures
INFO flwr 2024-06-11 01:06:33,419 | server.py:125 | fit progress: (234, 3.0124630859375, {'accuracy': 0.6648, 'data_size': 10000}, 31189.8174182456)
INFO flwr 2024-06-11 01:06:33,419 | server.py:171 | evaluate_round 234: no clients selected, cancel
DEBUG flwr 2024-06-11 01:06:33,420 | server.py:222 | fit_round 235: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:08:24,511 | server.py:236 | fit_round 235 received 10 results and 0 failures
INFO flwr 2024-06-11 01:08:49,220 | server.py:125 | fit progress: (235, 3.0435810546875, {'accuracy': 0.6636, 'data_size': 10000}, 31325.61880304385)
INFO flwr 2024-06-11 01:08:49,221 | server.py:171 | evaluate_round 235: no clients selected, cancel
DEBUG flwr 2024-06-11 01:08:49,221 | server.py:222 | fit_round 236: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:10:32,236 | server.py:236 | fit_round 236 received 10 results and 0 failures
[2m[1m[33m(autoscaler +8h35m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h36m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h36m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h37m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h37m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h37m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h37m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h38m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h38m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h38m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h39m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h39m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h40m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h40m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h40m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h41m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h41m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h41m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h41m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h42m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h42m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h43m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h43m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h43m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h43m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h44m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 01:10:56,787 | server.py:125 | fit progress: (236, 3.065095703125, {'accuracy': 0.6631, 'data_size': 10000}, 31453.185356399976)
INFO flwr 2024-06-11 01:10:56,787 | server.py:171 | evaluate_round 236: no clients selected, cancel
DEBUG flwr 2024-06-11 01:10:56,787 | server.py:222 | fit_round 237: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:12:48,266 | server.py:236 | fit_round 237 received 10 results and 0 failures
INFO flwr 2024-06-11 01:13:14,853 | server.py:125 | fit progress: (237, 3.045655078125, {'accuracy': 0.6653, 'data_size': 10000}, 31591.251503170934)
INFO flwr 2024-06-11 01:13:14,853 | server.py:171 | evaluate_round 237: no clients selected, cancel
DEBUG flwr 2024-06-11 01:13:14,854 | server.py:222 | fit_round 238: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:14:57,178 | server.py:236 | fit_round 238 received 10 results and 0 failures
INFO flwr 2024-06-11 01:15:21,893 | server.py:125 | fit progress: (238, 3.047577734375, {'accuracy': 0.6614, 'data_size': 10000}, 31718.291419136804)
INFO flwr 2024-06-11 01:15:21,893 | server.py:171 | evaluate_round 238: no clients selected, cancel
DEBUG flwr 2024-06-11 01:15:21,894 | server.py:222 | fit_round 239: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:17:10,541 | server.py:236 | fit_round 239 received 10 results and 0 failures
INFO flwr 2024-06-11 01:17:31,587 | server.py:125 | fit progress: (239, 3.0499900390625, {'accuracy': 0.6625, 'data_size': 10000}, 31847.986151694786)
INFO flwr 2024-06-11 01:17:31,588 | server.py:171 | evaluate_round 239: no clients selected, cancel
DEBUG flwr 2024-06-11 01:17:31,589 | server.py:222 | fit_round 240: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:19:15,001 | server.py:236 | fit_round 240 received 10 results and 0 failures
INFO flwr 2024-06-11 01:19:36,086 | server.py:125 | fit progress: (240, 3.04222421875, {'accuracy': 0.6656, 'data_size': 10000}, 31972.485059219878)
INFO flwr 2024-06-11 01:19:36,087 | server.py:171 | evaluate_round 240: no clients selected, cancel
DEBUG flwr 2024-06-11 01:19:36,087 | server.py:222 | fit_round 241: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +8h44m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h44m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h45m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h45m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h45m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h46m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h46m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h46m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h47m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h47m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h48m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h48m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h48m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h49m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h49m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h50m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h51m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h51m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h51m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h51m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h52m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h52m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h52m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h53m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h53m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h54m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 01:21:23,045 | server.py:236 | fit_round 241 received 10 results and 0 failures
INFO flwr 2024-06-11 01:21:44,089 | server.py:125 | fit progress: (241, 3.0541650390625, {'accuracy': 0.6669, 'data_size': 10000}, 32100.487677046563)
INFO flwr 2024-06-11 01:21:44,089 | server.py:171 | evaluate_round 241: no clients selected, cancel
DEBUG flwr 2024-06-11 01:21:44,090 | server.py:222 | fit_round 242: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:23:26,577 | server.py:236 | fit_round 242 received 10 results and 0 failures
INFO flwr 2024-06-11 01:23:48,058 | server.py:125 | fit progress: (242, 3.0690052734375, {'accuracy': 0.6671, 'data_size': 10000}, 32224.456935512833)
INFO flwr 2024-06-11 01:23:48,059 | server.py:171 | evaluate_round 242: no clients selected, cancel
DEBUG flwr 2024-06-11 01:23:48,060 | server.py:222 | fit_round 243: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:25:37,757 | server.py:236 | fit_round 243 received 10 results and 0 failures
INFO flwr 2024-06-11 01:25:58,690 | server.py:125 | fit progress: (243, 3.0360060546875, {'accuracy': 0.6725, 'data_size': 10000}, 32355.08891150076)
INFO flwr 2024-06-11 01:25:58,691 | server.py:171 | evaluate_round 243: no clients selected, cancel
DEBUG flwr 2024-06-11 01:25:58,691 | server.py:222 | fit_round 244: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:27:39,675 | server.py:236 | fit_round 244 received 10 results and 0 failures
INFO flwr 2024-06-11 01:28:00,859 | server.py:125 | fit progress: (244, 3.05456953125, {'accuracy': 0.6707, 'data_size': 10000}, 32477.25775840087)
INFO flwr 2024-06-11 01:28:00,860 | server.py:171 | evaluate_round 244: no clients selected, cancel
DEBUG flwr 2024-06-11 01:28:00,860 | server.py:222 | fit_round 245: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:29:50,763 | server.py:236 | fit_round 245 received 10 results and 0 failures
[2m[1m[33m(autoscaler +8h54m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h54m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h54m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h55m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h55m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h55m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h56m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h57m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h57m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h57m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h57m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h58m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h58m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h59m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h59m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h59m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8h59m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h1m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h1m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h1m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h2m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h2m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h2m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h3m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 01:30:11,680 | server.py:125 | fit progress: (245, 3.0803974609375, {'accuracy': 0.6706, 'data_size': 10000}, 32608.078698178753)
INFO flwr 2024-06-11 01:30:11,680 | server.py:171 | evaluate_round 245: no clients selected, cancel
DEBUG flwr 2024-06-11 01:30:11,681 | server.py:222 | fit_round 246: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:31:54,634 | server.py:236 | fit_round 246 received 10 results and 0 failures
INFO flwr 2024-06-11 01:32:16,319 | server.py:125 | fit progress: (246, 3.0965240234375, {'accuracy': 0.6704, 'data_size': 10000}, 32732.717435811646)
INFO flwr 2024-06-11 01:32:16,319 | server.py:171 | evaluate_round 246: no clients selected, cancel
DEBUG flwr 2024-06-11 01:32:16,320 | server.py:222 | fit_round 247: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:34:04,702 | server.py:236 | fit_round 247 received 10 results and 0 failures
INFO flwr 2024-06-11 01:34:25,818 | server.py:125 | fit progress: (247, 3.1453443359375, {'accuracy': 0.6675, 'data_size': 10000}, 32862.2163260486)
INFO flwr 2024-06-11 01:34:25,818 | server.py:171 | evaluate_round 247: no clients selected, cancel
DEBUG flwr 2024-06-11 01:34:25,818 | server.py:222 | fit_round 248: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:36:07,432 | server.py:236 | fit_round 248 received 10 results and 0 failures
INFO flwr 2024-06-11 01:36:28,664 | server.py:125 | fit progress: (248, 3.1736984375, {'accuracy': 0.6671, 'data_size': 10000}, 32985.06291768793)
INFO flwr 2024-06-11 01:36:28,665 | server.py:171 | evaluate_round 248: no clients selected, cancel
DEBUG flwr 2024-06-11 01:36:28,665 | server.py:222 | fit_round 249: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:38:17,574 | server.py:236 | fit_round 249 received 10 results and 0 failures
INFO flwr 2024-06-11 01:38:38,605 | server.py:125 | fit progress: (249, 3.1606009765625, {'accuracy': 0.6667, 'data_size': 10000}, 33115.00324007962)
INFO flwr 2024-06-11 01:38:38,605 | server.py:171 | evaluate_round 249: no clients selected, cancel
DEBUG flwr 2024-06-11 01:38:38,605 | server.py:222 | fit_round 250: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h3m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h4m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h4m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h4m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h5m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h5m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h5m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h5m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h6m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h6m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h7m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h7m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h7m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h7m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h8m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h8m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h9m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h9m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h10m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h10m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h10m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h11m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h11m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h11m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h11m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h12m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h12m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 01:40:20,402 | server.py:236 | fit_round 250 received 10 results and 0 failures
INFO flwr 2024-06-11 01:40:42,095 | server.py:125 | fit progress: (250, 3.154798828125, {'accuracy': 0.6677, 'data_size': 10000}, 33238.49348992156)
INFO flwr 2024-06-11 01:40:42,096 | server.py:171 | evaluate_round 250: no clients selected, cancel
DEBUG flwr 2024-06-11 01:40:42,096 | server.py:222 | fit_round 251: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:42:29,353 | server.py:236 | fit_round 251 received 10 results and 0 failures
INFO flwr 2024-06-11 01:42:49,326 | server.py:125 | fit progress: (251, 3.151919140625, {'accuracy': 0.667, 'data_size': 10000}, 33365.72490978893)
INFO flwr 2024-06-11 01:42:49,327 | server.py:171 | evaluate_round 251: no clients selected, cancel
DEBUG flwr 2024-06-11 01:42:49,328 | server.py:222 | fit_round 252: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:44:31,030 | server.py:236 | fit_round 252 received 10 results and 0 failures
INFO flwr 2024-06-11 01:44:51,884 | server.py:125 | fit progress: (252, 3.1299419921875, {'accuracy': 0.6688, 'data_size': 10000}, 33488.28301426582)
INFO flwr 2024-06-11 01:44:51,885 | server.py:171 | evaluate_round 252: no clients selected, cancel
DEBUG flwr 2024-06-11 01:44:51,885 | server.py:222 | fit_round 253: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:46:39,697 | server.py:236 | fit_round 253 received 10 results and 0 failures
INFO flwr 2024-06-11 01:47:00,093 | server.py:125 | fit progress: (253, 3.124659765625, {'accuracy': 0.6707, 'data_size': 10000}, 33616.49194013467)
INFO flwr 2024-06-11 01:47:00,094 | server.py:171 | evaluate_round 253: no clients selected, cancel
DEBUG flwr 2024-06-11 01:47:00,094 | server.py:222 | fit_round 254: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:48:41,043 | server.py:236 | fit_round 254 received 10 results and 0 failures
INFO flwr 2024-06-11 01:49:02,187 | server.py:125 | fit progress: (254, 3.143968359375, {'accuracy': 0.6682, 'data_size': 10000}, 33738.58537297556)
INFO flwr 2024-06-11 01:49:02,187 | server.py:171 | evaluate_round 254: no clients selected, cancel
DEBUG flwr 2024-06-11 01:49:02,188 | server.py:222 | fit_round 255: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:50:52,086 | server.py:236 | fit_round 255 received 10 results and 0 failures
INFO flwr 2024-06-11 01:51:12,826 | server.py:125 | fit progress: (255, 3.1658982421875, {'accuracy': 0.6656, 'data_size': 10000}, 33869.22448671982)
INFO flwr 2024-06-11 01:51:12,827 | server.py:171 | evaluate_round 255: no clients selected, cancel
DEBUG flwr 2024-06-11 01:51:12,827 | server.py:222 | fit_round 256: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h13m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h13m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h13m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h13m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h14m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h14m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h16m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h16m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h17m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h17m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h17m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h18m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h18m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h18m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h20m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h20m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h21m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h21m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h22m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h22m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h22m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h22m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h24m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h24m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h25m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h25m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 01:52:55,981 | server.py:236 | fit_round 256 received 10 results and 0 failures
INFO flwr 2024-06-11 01:53:16,756 | server.py:125 | fit progress: (256, 3.152991796875, {'accuracy': 0.666, 'data_size': 10000}, 33993.1551184468)
INFO flwr 2024-06-11 01:53:16,757 | server.py:171 | evaluate_round 256: no clients selected, cancel
DEBUG flwr 2024-06-11 01:53:16,757 | server.py:222 | fit_round 257: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:55:05,086 | server.py:236 | fit_round 257 received 10 results and 0 failures
INFO flwr 2024-06-11 01:55:25,871 | server.py:125 | fit progress: (257, 3.1566736328125, {'accuracy': 0.6658, 'data_size': 10000}, 34122.26943599386)
INFO flwr 2024-06-11 01:55:25,871 | server.py:171 | evaluate_round 257: no clients selected, cancel
DEBUG flwr 2024-06-11 01:55:25,871 | server.py:222 | fit_round 258: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:57:08,163 | server.py:236 | fit_round 258 received 10 results and 0 failures
INFO flwr 2024-06-11 01:57:29,578 | server.py:125 | fit progress: (258, 3.1532095703125, {'accuracy': 0.6642, 'data_size': 10000}, 34245.97668759059)
INFO flwr 2024-06-11 01:57:29,579 | server.py:171 | evaluate_round 258: no clients selected, cancel
DEBUG flwr 2024-06-11 01:57:29,579 | server.py:222 | fit_round 259: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 01:59:20,003 | server.py:236 | fit_round 259 received 10 results and 0 failures
INFO flwr 2024-06-11 01:59:40,229 | server.py:125 | fit progress: (259, 3.135646484375, {'accuracy': 0.668, 'data_size': 10000}, 34376.62803443195)
INFO flwr 2024-06-11 01:59:40,230 | server.py:171 | evaluate_round 259: no clients selected, cancel
DEBUG flwr 2024-06-11 01:59:40,230 | server.py:222 | fit_round 260: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:01:21,516 | server.py:236 | fit_round 260 received 10 results and 0 failures
INFO flwr 2024-06-11 02:01:41,709 | server.py:125 | fit progress: (260, 3.11133046875, {'accuracy': 0.67, 'data_size': 10000}, 34498.107509946916)
INFO flwr 2024-06-11 02:01:41,710 | server.py:171 | evaluate_round 260: no clients selected, cancel
DEBUG flwr 2024-06-11 02:01:41,710 | server.py:222 | fit_round 261: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h26m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h26m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h26m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h27m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h27m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h28m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h28m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h28m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h28m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h29m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h29m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h30m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h31m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h31m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h31m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h31m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h32m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h33m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h33m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h33m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h34m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h34m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h34m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h35m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h35m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h35m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 02:03:30,758 | server.py:236 | fit_round 261 received 10 results and 0 failures
INFO flwr 2024-06-11 02:03:50,508 | server.py:125 | fit progress: (261, 3.115894140625, {'accuracy': 0.6716, 'data_size': 10000}, 34626.90639272798)
INFO flwr 2024-06-11 02:03:50,508 | server.py:171 | evaluate_round 261: no clients selected, cancel
DEBUG flwr 2024-06-11 02:03:50,509 | server.py:222 | fit_round 262: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:05:32,298 | server.py:236 | fit_round 262 received 10 results and 0 failures
INFO flwr 2024-06-11 02:05:52,404 | server.py:125 | fit progress: (262, 3.1374978515625, {'accuracy': 0.6695, 'data_size': 10000}, 34748.80301280785)
INFO flwr 2024-06-11 02:05:52,405 | server.py:171 | evaluate_round 262: no clients selected, cancel
DEBUG flwr 2024-06-11 02:05:52,405 | server.py:222 | fit_round 263: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:07:41,852 | server.py:236 | fit_round 263 received 10 results and 0 failures
INFO flwr 2024-06-11 02:08:02,523 | server.py:125 | fit progress: (263, 3.15601875, {'accuracy': 0.6674, 'data_size': 10000}, 34878.92141717393)
INFO flwr 2024-06-11 02:08:02,523 | server.py:171 | evaluate_round 263: no clients selected, cancel
DEBUG flwr 2024-06-11 02:08:02,523 | server.py:222 | fit_round 264: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:09:45,595 | server.py:236 | fit_round 264 received 10 results and 0 failures
INFO flwr 2024-06-11 02:10:05,367 | server.py:125 | fit progress: (264, 3.1520734375, {'accuracy': 0.6691, 'data_size': 10000}, 35001.765663838945)
INFO flwr 2024-06-11 02:10:05,368 | server.py:171 | evaluate_round 264: no clients selected, cancel
DEBUG flwr 2024-06-11 02:10:05,368 | server.py:222 | fit_round 265: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h36m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h36m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h36m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h37m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h37m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h37m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h38m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h38m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h38m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h38m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h39m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h39m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h39m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h40m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h40m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h40m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h41m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h41m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h42m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h42m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h43m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h43m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h44m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h44m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h44m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h44m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 02:11:53,711 | server.py:236 | fit_round 265 received 10 results and 0 failures
INFO flwr 2024-06-11 02:12:13,311 | server.py:125 | fit progress: (265, 3.1493955078125, {'accuracy': 0.6682, 'data_size': 10000}, 35129.70956182666)
INFO flwr 2024-06-11 02:12:13,311 | server.py:171 | evaluate_round 265: no clients selected, cancel
DEBUG flwr 2024-06-11 02:12:13,312 | server.py:222 | fit_round 266: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:13:56,736 | server.py:236 | fit_round 266 received 10 results and 0 failures
INFO flwr 2024-06-11 02:14:16,818 | server.py:125 | fit progress: (266, 3.1524646484375, {'accuracy': 0.671, 'data_size': 10000}, 35253.21650982974)
INFO flwr 2024-06-11 02:14:16,818 | server.py:171 | evaluate_round 266: no clients selected, cancel
DEBUG flwr 2024-06-11 02:14:16,818 | server.py:222 | fit_round 267: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:16:05,916 | server.py:236 | fit_round 267 received 10 results and 0 failures
INFO flwr 2024-06-11 02:16:25,825 | server.py:125 | fit progress: (267, 3.16171640625, {'accuracy': 0.6721, 'data_size': 10000}, 35382.22333759675)
INFO flwr 2024-06-11 02:16:25,825 | server.py:171 | evaluate_round 267: no clients selected, cancel
DEBUG flwr 2024-06-11 02:16:25,826 | server.py:222 | fit_round 268: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:18:08,267 | server.py:236 | fit_round 268 received 10 results and 0 failures
INFO flwr 2024-06-11 02:18:28,877 | server.py:125 | fit progress: (268, 3.1708646484375, {'accuracy': 0.671, 'data_size': 10000}, 35505.275451563764)
INFO flwr 2024-06-11 02:18:28,877 | server.py:171 | evaluate_round 268: no clients selected, cancel
DEBUG flwr 2024-06-11 02:18:28,878 | server.py:222 | fit_round 269: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:20:17,089 | server.py:236 | fit_round 269 received 10 results and 0 failures
INFO flwr 2024-06-11 02:20:37,044 | server.py:125 | fit progress: (269, 3.18993359375, {'accuracy': 0.6694, 'data_size': 10000}, 35633.44233157486)
INFO flwr 2024-06-11 02:20:37,044 | server.py:171 | evaluate_round 269: no clients selected, cancel
DEBUG flwr 2024-06-11 02:20:37,044 | server.py:222 | fit_round 270: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h45m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h45m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h46m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h46m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h47m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h47m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h48m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h48m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h48m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h49m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h49m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h49m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h49m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h50m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h50m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h51m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h51m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h52m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h52m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h52m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h52m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h53m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h53m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h53m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h53m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h54m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 02:22:18,238 | server.py:236 | fit_round 270 received 10 results and 0 failures
INFO flwr 2024-06-11 02:22:38,472 | server.py:125 | fit progress: (270, 3.2142533203125, {'accuracy': 0.6662, 'data_size': 10000}, 35754.87116702879)
INFO flwr 2024-06-11 02:22:38,473 | server.py:171 | evaluate_round 270: no clients selected, cancel
DEBUG flwr 2024-06-11 02:22:38,473 | server.py:222 | fit_round 271: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:24:27,241 | server.py:236 | fit_round 271 received 10 results and 0 failures
INFO flwr 2024-06-11 02:24:48,542 | server.py:125 | fit progress: (271, 3.2193873046875, {'accuracy': 0.6658, 'data_size': 10000}, 35884.940593943)
INFO flwr 2024-06-11 02:24:48,542 | server.py:171 | evaluate_round 271: no clients selected, cancel
DEBUG flwr 2024-06-11 02:24:48,543 | server.py:222 | fit_round 272: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:26:29,805 | server.py:236 | fit_round 272 received 10 results and 0 failures
INFO flwr 2024-06-11 02:26:50,427 | server.py:125 | fit progress: (272, 3.2262125, {'accuracy': 0.6665, 'data_size': 10000}, 36006.82598480396)
INFO flwr 2024-06-11 02:26:50,428 | server.py:171 | evaluate_round 272: no clients selected, cancel
DEBUG flwr 2024-06-11 02:26:50,428 | server.py:222 | fit_round 273: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:28:40,252 | server.py:236 | fit_round 273 received 10 results and 0 failures
INFO flwr 2024-06-11 02:29:02,385 | server.py:125 | fit progress: (273, 3.221608984375, {'accuracy': 0.6665, 'data_size': 10000}, 36138.78407610394)
INFO flwr 2024-06-11 02:29:02,386 | server.py:171 | evaluate_round 273: no clients selected, cancel
DEBUG flwr 2024-06-11 02:29:02,386 | server.py:222 | fit_round 274: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +9h54m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h55m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h55m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h56m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h56m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h56m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h56m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h57m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h57m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h58m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h58m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h58m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h58m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h59m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h59m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9h59m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h1m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h1m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h1m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h1m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h2m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h2m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h3m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 02:30:43,614 | server.py:236 | fit_round 274 received 10 results and 0 failures
INFO flwr 2024-06-11 02:31:06,612 | server.py:125 | fit progress: (274, 3.221187890625, {'accuracy': 0.6671, 'data_size': 10000}, 36263.01115596481)
INFO flwr 2024-06-11 02:31:06,613 | server.py:171 | evaluate_round 274: no clients selected, cancel
DEBUG flwr 2024-06-11 02:31:06,613 | server.py:222 | fit_round 275: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:32:56,795 | server.py:236 | fit_round 275 received 10 results and 0 failures
INFO flwr 2024-06-11 02:33:16,776 | server.py:125 | fit progress: (275, 3.2209150390625, {'accuracy': 0.6662, 'data_size': 10000}, 36393.17489747889)
INFO flwr 2024-06-11 02:33:16,777 | server.py:171 | evaluate_round 275: no clients selected, cancel
DEBUG flwr 2024-06-11 02:33:16,777 | server.py:222 | fit_round 276: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:34:59,729 | server.py:236 | fit_round 276 received 10 results and 0 failures
INFO flwr 2024-06-11 02:35:20,543 | server.py:125 | fit progress: (276, 3.221718359375, {'accuracy': 0.6669, 'data_size': 10000}, 36516.94194259681)
INFO flwr 2024-06-11 02:35:20,544 | server.py:171 | evaluate_round 276: no clients selected, cancel
DEBUG flwr 2024-06-11 02:35:20,545 | server.py:222 | fit_round 277: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:37:08,139 | server.py:236 | fit_round 277 received 10 results and 0 failures
INFO flwr 2024-06-11 02:37:28,922 | server.py:125 | fit progress: (277, 3.228676953125, {'accuracy': 0.6662, 'data_size': 10000}, 36645.321124416776)
INFO flwr 2024-06-11 02:37:28,923 | server.py:171 | evaluate_round 277: no clients selected, cancel
DEBUG flwr 2024-06-11 02:37:28,923 | server.py:222 | fit_round 278: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:39:11,897 | server.py:236 | fit_round 278 received 10 results and 0 failures
INFO flwr 2024-06-11 02:39:33,016 | server.py:125 | fit progress: (278, 3.2391546875, {'accuracy': 0.6655, 'data_size': 10000}, 36769.41464866884)
INFO flwr 2024-06-11 02:39:33,016 | server.py:171 | evaluate_round 278: no clients selected, cancel
DEBUG flwr 2024-06-11 02:39:33,017 | server.py:222 | fit_round 279: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:41:22,396 | server.py:236 | fit_round 279 received 10 results and 0 failures
[2m[1m[33m(autoscaler +10h3m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h3m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h3m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h4m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h4m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h5m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h5m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h5m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h5m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h6m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h6m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h7m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h7m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h9m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h9m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h9m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h9m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h10m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h10m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h11m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h11m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h12m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h12m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h13m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h13m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h15m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 02:41:43,175 | server.py:125 | fit progress: (279, 3.252134765625, {'accuracy': 0.6651, 'data_size': 10000}, 36899.573279268574)
INFO flwr 2024-06-11 02:41:43,175 | server.py:171 | evaluate_round 279: no clients selected, cancel
DEBUG flwr 2024-06-11 02:41:43,176 | server.py:222 | fit_round 280: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:43:24,620 | server.py:236 | fit_round 280 received 10 results and 0 failures
INFO flwr 2024-06-11 02:43:45,067 | server.py:125 | fit progress: (280, 3.2642341796875, {'accuracy': 0.6664, 'data_size': 10000}, 37021.46597276768)
INFO flwr 2024-06-11 02:43:45,068 | server.py:171 | evaluate_round 280: no clients selected, cancel
DEBUG flwr 2024-06-11 02:43:45,068 | server.py:222 | fit_round 281: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:45:34,330 | server.py:236 | fit_round 281 received 10 results and 0 failures
INFO flwr 2024-06-11 02:45:55,562 | server.py:125 | fit progress: (281, 3.2704228515625, {'accuracy': 0.6666, 'data_size': 10000}, 37151.96100099059)
INFO flwr 2024-06-11 02:45:55,563 | server.py:171 | evaluate_round 281: no clients selected, cancel
DEBUG flwr 2024-06-11 02:45:55,563 | server.py:222 | fit_round 282: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:47:37,859 | server.py:236 | fit_round 282 received 10 results and 0 failures
INFO flwr 2024-06-11 02:47:59,195 | server.py:125 | fit progress: (282, 3.2755380859375, {'accuracy': 0.6665, 'data_size': 10000}, 37275.59337353567)
INFO flwr 2024-06-11 02:47:59,195 | server.py:171 | evaluate_round 282: no clients selected, cancel
DEBUG flwr 2024-06-11 02:47:59,196 | server.py:222 | fit_round 283: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:49:49,025 | server.py:236 | fit_round 283 received 10 results and 0 failures
[2m[1m[33m(autoscaler +10h15m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h15m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h16m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h16m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h16m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h17m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h17m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h17m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h17m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h18m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h18m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h19m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h19m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h19m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h19m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h20m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h20m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h20m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h21m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h21m33s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h21m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h22m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h22m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h22m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h23m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h23m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
INFO flwr 2024-06-11 02:50:09,435 | server.py:125 | fit progress: (283, 3.278362109375, {'accuracy': 0.666, 'data_size': 10000}, 37405.83402512176)
INFO flwr 2024-06-11 02:50:09,436 | server.py:171 | evaluate_round 283: no clients selected, cancel
DEBUG flwr 2024-06-11 02:50:09,436 | server.py:222 | fit_round 284: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:51:50,705 | server.py:236 | fit_round 284 received 10 results and 0 failures
INFO flwr 2024-06-11 02:52:10,798 | server.py:125 | fit progress: (284, 3.28126640625, {'accuracy': 0.6666, 'data_size': 10000}, 37527.19633240858)
INFO flwr 2024-06-11 02:52:10,798 | server.py:171 | evaluate_round 284: no clients selected, cancel
DEBUG flwr 2024-06-11 02:52:10,798 | server.py:222 | fit_round 285: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:53:58,765 | server.py:236 | fit_round 285 received 10 results and 0 failures
INFO flwr 2024-06-11 02:54:18,160 | server.py:125 | fit progress: (285, 3.284703125, {'accuracy': 0.6671, 'data_size': 10000}, 37654.55861167656)
INFO flwr 2024-06-11 02:54:18,161 | server.py:171 | evaluate_round 285: no clients selected, cancel
DEBUG flwr 2024-06-11 02:54:18,162 | server.py:222 | fit_round 286: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:56:00,838 | server.py:236 | fit_round 286 received 10 results and 0 failures
INFO flwr 2024-06-11 02:56:21,659 | server.py:125 | fit progress: (286, 3.28576640625, {'accuracy': 0.6672, 'data_size': 10000}, 37778.05815525167)
INFO flwr 2024-06-11 02:56:21,660 | server.py:171 | evaluate_round 286: no clients selected, cancel
DEBUG flwr 2024-06-11 02:56:21,661 | server.py:222 | fit_round 287: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 02:58:09,729 | server.py:236 | fit_round 287 received 10 results and 0 failures
INFO flwr 2024-06-11 02:58:29,345 | server.py:125 | fit progress: (287, 3.285358203125, {'accuracy': 0.6694, 'data_size': 10000}, 37905.74418906076)
INFO flwr 2024-06-11 02:58:29,346 | server.py:171 | evaluate_round 287: no clients selected, cancel
DEBUG flwr 2024-06-11 02:58:29,346 | server.py:222 | fit_round 288: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +10h23m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h23m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h24m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h24m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h24m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h25m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h25m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h25m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h26m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h26m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h26m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h27m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h27m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h27m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h28m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h28m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h28m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h29m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h29m19s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h29m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h29m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h30m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h31m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h31m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h32m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h32m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 03:00:10,079 | server.py:236 | fit_round 288 received 10 results and 0 failures
INFO flwr 2024-06-11 03:00:30,429 | server.py:125 | fit progress: (288, 3.285437109375, {'accuracy': 0.6699, 'data_size': 10000}, 38026.82727968274)
INFO flwr 2024-06-11 03:00:30,429 | server.py:171 | evaluate_round 288: no clients selected, cancel
DEBUG flwr 2024-06-11 03:00:30,430 | server.py:222 | fit_round 289: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:02:19,473 | server.py:236 | fit_round 289 received 10 results and 0 failures
INFO flwr 2024-06-11 03:02:39,798 | server.py:125 | fit progress: (289, 3.28374765625, {'accuracy': 0.6694, 'data_size': 10000}, 38156.19657859579)
INFO flwr 2024-06-11 03:02:39,798 | server.py:171 | evaluate_round 289: no clients selected, cancel
DEBUG flwr 2024-06-11 03:02:39,799 | server.py:222 | fit_round 290: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:04:21,020 | server.py:236 | fit_round 290 received 10 results and 0 failures
INFO flwr 2024-06-11 03:04:43,924 | server.py:125 | fit progress: (290, 3.281148046875, {'accuracy': 0.6694, 'data_size': 10000}, 38280.32291953685)
INFO flwr 2024-06-11 03:04:43,925 | server.py:171 | evaluate_round 290: no clients selected, cancel
DEBUG flwr 2024-06-11 03:04:43,925 | server.py:222 | fit_round 291: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:06:33,741 | server.py:236 | fit_round 291 received 10 results and 0 failures
INFO flwr 2024-06-11 03:06:54,125 | server.py:125 | fit progress: (291, 3.27919609375, {'accuracy': 0.6699, 'data_size': 10000}, 38410.52385125868)
INFO flwr 2024-06-11 03:06:54,126 | server.py:171 | evaluate_round 291: no clients selected, cancel
DEBUG flwr 2024-06-11 03:06:54,127 | server.py:222 | fit_round 292: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:08:34,950 | server.py:236 | fit_round 292 received 10 results and 0 failures
INFO flwr 2024-06-11 03:08:55,766 | server.py:125 | fit progress: (292, 3.2813890625, {'accuracy': 0.67, 'data_size': 10000}, 38532.164801266976)
INFO flwr 2024-06-11 03:08:55,767 | server.py:171 | evaluate_round 292: no clients selected, cancel
DEBUG flwr 2024-06-11 03:08:55,767 | server.py:222 | fit_round 293: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +10h32m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h33m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h33m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h33m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h34m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h34m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h34m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h35m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h36m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h36m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h36m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h36m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h37m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h37m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h38m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h38m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h39m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h39m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h39m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h39m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h40m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h40m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h41m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h41m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h42m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h42m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 03:10:44,865 | server.py:236 | fit_round 293 received 10 results and 0 failures
INFO flwr 2024-06-11 03:11:06,551 | server.py:125 | fit progress: (293, 3.28429921875, {'accuracy': 0.6702, 'data_size': 10000}, 38662.94973623892)
INFO flwr 2024-06-11 03:11:06,551 | server.py:171 | evaluate_round 293: no clients selected, cancel
DEBUG flwr 2024-06-11 03:11:06,552 | server.py:222 | fit_round 294: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:12:50,189 | server.py:236 | fit_round 294 received 10 results and 0 failures
INFO flwr 2024-06-11 03:13:10,769 | server.py:125 | fit progress: (294, 3.288938671875, {'accuracy': 0.6708, 'data_size': 10000}, 38787.16754006175)
INFO flwr 2024-06-11 03:13:10,769 | server.py:171 | evaluate_round 294: no clients selected, cancel
DEBUG flwr 2024-06-11 03:13:10,771 | server.py:222 | fit_round 295: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:15:00,420 | server.py:236 | fit_round 295 received 10 results and 0 failures
INFO flwr 2024-06-11 03:15:19,962 | server.py:125 | fit progress: (295, 3.292806640625, {'accuracy': 0.6701, 'data_size': 10000}, 38916.360454255715)
INFO flwr 2024-06-11 03:15:19,962 | server.py:171 | evaluate_round 295: no clients selected, cancel
DEBUG flwr 2024-06-11 03:15:19,962 | server.py:222 | fit_round 296: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:17:02,704 | server.py:236 | fit_round 296 received 10 results and 0 failures
INFO flwr 2024-06-11 03:17:22,135 | server.py:125 | fit progress: (296, 3.2968734375, {'accuracy': 0.6704, 'data_size': 10000}, 39038.53398045199)
INFO flwr 2024-06-11 03:17:22,136 | server.py:171 | evaluate_round 296: no clients selected, cancel
DEBUG flwr 2024-06-11 03:17:22,136 | server.py:222 | fit_round 297: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:19:11,189 | server.py:236 | fit_round 297 received 10 results and 0 failures
INFO flwr 2024-06-11 03:19:32,948 | server.py:125 | fit progress: (297, 3.296112890625, {'accuracy': 0.6709, 'data_size': 10000}, 39169.34692728799)
INFO flwr 2024-06-11 03:19:32,949 | server.py:171 | evaluate_round 297: no clients selected, cancel
DEBUG flwr 2024-06-11 03:19:32,950 | server.py:222 | fit_round 298: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +10h43m0s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h43m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h43m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h43m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h44m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h44m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h44m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h44m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h45m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h45m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h46m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h46m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h46m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h46m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h47m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h47m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h49m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h49m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h49m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h49m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h50m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h50m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h51m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h51m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h53m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h53m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
DEBUG flwr 2024-06-11 03:21:15,391 | server.py:236 | fit_round 298 received 10 results and 0 failures
INFO flwr 2024-06-11 03:21:36,162 | server.py:125 | fit progress: (298, 3.295468359375, {'accuracy': 0.6707, 'data_size': 10000}, 39292.56091326568)
INFO flwr 2024-06-11 03:21:36,163 | server.py:171 | evaluate_round 298: no clients selected, cancel
DEBUG flwr 2024-06-11 03:21:36,163 | server.py:222 | fit_round 299: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:23:24,964 | server.py:236 | fit_round 299 received 10 results and 0 failures
INFO flwr 2024-06-11 03:23:44,987 | server.py:125 | fit progress: (299, 3.295925, {'accuracy': 0.6702, 'data_size': 10000}, 39421.385979346)
INFO flwr 2024-06-11 03:23:44,988 | server.py:171 | evaluate_round 299: no clients selected, cancel
DEBUG flwr 2024-06-11 03:23:44,988 | server.py:222 | fit_round 300: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 03:25:25,634 | server.py:236 | fit_round 300 received 10 results and 0 failures
INFO flwr 2024-06-11 03:25:45,703 | server.py:125 | fit progress: (300, 3.297619140625, {'accuracy': 0.6712, 'data_size': 10000}, 39542.10125132883)
INFO flwr 2024-06-11 03:25:45,703 | server.py:171 | evaluate_round 300: no clients selected, cancel
INFO flwr 2024-06-11 03:25:45,703 | server.py:153 | FL finished in 39542.10210135579
INFO flwr 2024-06-11 03:25:45,711 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-06-11 03:25:45,711 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-06-11 03:25:45,712 | app.py:229 | app_fit: losses_centralized [(0, 2.5065833984375), (1, 2.729272265625), (2, 4.2696359375), (3, 4.712312109375), (4, 3.6408109375), (5, 3.7635578125), (6, 3.1335787109375), (7, 2.2705513671875), (8, 2.10251953125), (9, 1.9574943359375), (10, 1.858292578125), (11, 1.761594921875), (12, 1.61089013671875), (13, 1.62726328125), (14, 1.56366298828125), (15, 1.53457548828125), (16, 1.4834265625), (17, 1.52485615234375), (18, 1.54135634765625), (19, 1.467231640625), (20, 1.4451220703125), (21, 1.44535107421875), (22, 1.37228916015625), (23, 1.4262708984375), (24, 1.409988671875), (25, 1.3367427734375), (26, 1.3317255859375), (27, 1.28835283203125), (28, 1.28419970703125), (29, 1.2825861328125), (30, 1.28283583984375), (31, 1.23257060546875), (32, 1.23311591796875), (33, 1.244328515625), (34, 1.21694501953125), (35, 1.240291015625), (36, 1.22968095703125), (37, 1.20941298828125), (38, 1.169208203125), (39, 1.1730560546875), (40, 1.18461904296875), (41, 1.19386455078125), (42, 1.16239951171875), (43, 1.19597041015625), (44, 1.15940693359375), (45, 1.16660419921875), (46, 1.14573203125), (47, 1.18052978515625), (48, 1.19252060546875), (49, 1.1914056640625), (50, 1.1232736328125), (51, 1.15576337890625), (52, 1.20116796875), (53, 1.17725849609375), (54, 1.10302392578125), (55, 1.18910908203125), (56, 1.16947314453125), (57, 1.099759765625), (58, 1.118721875), (59, 1.151608203125), (60, 1.1290822265625), (61, 1.09485048828125), (62, 1.15370419921875), (63, 1.13604609375), (64, 1.12811640625), (65, 1.119621875), (66, 1.14616806640625), (67, 1.1199734375), (68, 1.16368505859375), (69, 1.13343173828125), (70, 1.137009765625), (71, 1.14374775390625), (72, 1.1694443359375), (73, 1.1428072265625), (74, 1.154335546875), (75, 1.1628830078125), (76, 1.1699841796875), (77, 1.1737794921875), (78, 1.18362607421875), (79, 1.1808314453125), (80, 1.22874580078125), (81, 1.2848076171875), (82, 1.26979072265625), (83, 1.2676958984375), (84, 1.3049111328125), (85, 1.322943359375), (86, 1.3106509765625), (87, 1.28587607421875), (88, 1.26599384765625), (89, 1.31313896484375), (90, 1.294200390625), (91, 1.31578544921875), (92, 1.36287900390625), (93, 1.32136240234375), (94, 1.3373755859375), (95, 1.38838916015625), (96, 1.398519140625), (97, 1.32692783203125), (98, 1.273144921875), (99, 1.33489423828125), (100, 1.44848330078125), (101, 1.3659994140625), (102, 1.36769150390625), (103, 1.39923671875), (104, 1.47105537109375), (105, 1.40454609375), (106, 1.37833349609375), (107, 1.46231484375), (108, 1.56998662109375), (109, 1.5395240234375), (110, 1.55040947265625), (111, 1.57582421875), (112, 1.6772765625), (113, 1.673266796875), (114, 1.62575), (115, 1.653718359375), (116, 1.6494529296875), (117, 1.658662890625), (118, 1.678629296875), (119, 1.6865439453125), (120, 1.7119427734375), (121, 1.6914203125), (122, 1.6466013671875), (123, 1.605450390625), (124, 1.6535939453125), (125, 1.67950390625), (126, 1.7052255859375), (127, 1.72972421875), (128, 1.74816640625), (129, 1.7999490234375), (130, 1.8671890625), (131, 1.86601484375), (132, 1.866074609375), (133, 1.85076171875), (134, 1.8622533203125), (135, 1.94978125), (136, 1.94582421875), (137, 1.970516796875), (138, 1.978210546875), (139, 1.99235546875), (140, 2.0226279296875), (141, 2.0180662109375), (142, 2.0095515625), (143, 2.02718984375), (144, 2.091676953125), (145, 2.043942578125), (146, 2.05336171875), (147, 2.0556724609375), (148, 2.100621875), (149, 2.1273169921875), (150, 2.0962333984375), (151, 2.105191015625), (152, 2.0868224609375), (153, 2.114711328125), (154, 2.1199068359375), (155, 2.1558296875), (156, 2.1498267578125), (157, 2.123957421875), (158, 2.12934453125), (159, 2.10553515625), (160, 2.09762578125), (161, 2.14917890625), (162, 2.182381640625), (163, 2.220923828125), (164, 2.2064275390625), (165, 2.2409744140625), (166, 2.2917984375), (167, 2.3417125), (168, 2.308609765625), (169, 2.288355859375), (170, 2.339387890625), (171, 2.353440234375), (172, 2.4246369140625), (173, 2.39999765625), (174, 2.33826015625), (175, 2.3415013671875), (176, 2.409753515625), (177, 2.4292318359375), (178, 2.446654296875), (179, 2.4758572265625), (180, 2.4809462890625), (181, 2.4459544921875), (182, 2.4498216796875), (183, 2.445953515625), (184, 2.4514607421875), (185, 2.4895552734375), (186, 2.542851953125), (187, 2.5870095703125), (188, 2.6006931640625), (189, 2.588540234375), (190, 2.5161853515625), (191, 2.5167974609375), (192, 2.5254771484375), (193, 2.5643140625), (194, 2.605394921875), (195, 2.641516796875), (196, 2.6397296875), (197, 2.6307205078125), (198, 2.6419109375), (199, 2.6576373046875), (200, 2.6763115234375), (201, 2.693650390625), (202, 2.7209400390625), (203, 2.7368029296875), (204, 2.746566015625), (205, 2.7591244140625), (206, 2.772296484375), (207, 2.784410546875), (208, 2.7849826171875), (209, 2.794425390625), (210, 2.798658984375), (211, 2.795673828125), (212, 2.7837201171875), (213, 2.7879001953125), (214, 2.7989365234375), (215, 2.8134685546875), (216, 2.8077984375), (217, 2.8054734375), (218, 2.8173556640625), (219, 2.850865234375), (220, 2.8850380859375), (221, 2.9056185546875), (222, 2.8955625), (223, 2.88544453125), (224, 2.892315234375), (225, 2.9090490234375), (226, 2.9392232421875), (227, 2.98654296875), (228, 3.00087421875), (229, 3.005280859375), (230, 2.990633984375), (231, 2.982157421875), (232, 2.990607421875), (233, 3.0022744140625), (234, 3.0124630859375), (235, 3.0435810546875), (236, 3.065095703125), (237, 3.045655078125), (238, 3.047577734375), (239, 3.0499900390625), (240, 3.04222421875), (241, 3.0541650390625), (242, 3.0690052734375), (243, 3.0360060546875), (244, 3.05456953125), (245, 3.0803974609375), (246, 3.0965240234375), (247, 3.1453443359375), (248, 3.1736984375), (249, 3.1606009765625), (250, 3.154798828125), (251, 3.151919140625), (252, 3.1299419921875), (253, 3.124659765625), (254, 3.143968359375), (255, 3.1658982421875), (256, 3.152991796875), (257, 3.1566736328125), (258, 3.1532095703125), (259, 3.135646484375), (260, 3.11133046875), (261, 3.115894140625), (262, 3.1374978515625), (263, 3.15601875), (264, 3.1520734375), (265, 3.1493955078125), (266, 3.1524646484375), (267, 3.16171640625), (268, 3.1708646484375), (269, 3.18993359375), (270, 3.2142533203125), (271, 3.2193873046875), (272, 3.2262125), (273, 3.221608984375), (274, 3.221187890625), (275, 3.2209150390625), (276, 3.221718359375), (277, 3.228676953125), (278, 3.2391546875), (279, 3.252134765625), (280, 3.2642341796875), (281, 3.2704228515625), (282, 3.2755380859375), (283, 3.278362109375), (284, 3.28126640625), (285, 3.284703125), (286, 3.28576640625), (287, 3.285358203125), (288, 3.285437109375), (289, 3.28374765625), (290, 3.281148046875), (291, 3.27919609375), (292, 3.2813890625), (293, 3.28429921875), (294, 3.288938671875), (295, 3.292806640625), (296, 3.2968734375), (297, 3.296112890625), (298, 3.295468359375), (299, 3.295925), (300, 3.297619140625)]
INFO flwr 2024-06-11 03:25:45,713 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0978), (1, 0.2494), (2, 0.1788), (3, 0.1396), (4, 0.2247), (5, 0.2438), (6, 0.2182), (7, 0.3092), (8, 0.2802), (9, 0.294), (10, 0.3509), (11, 0.359), (12, 0.4021), (13, 0.4147), (14, 0.4302), (15, 0.4415), (16, 0.4586), (17, 0.4411), (18, 0.424), (19, 0.4662), (20, 0.478), (21, 0.4862), (22, 0.4996), (23, 0.4802), (24, 0.4813), (25, 0.5113), (26, 0.5169), (27, 0.5363), (28, 0.538), (29, 0.5443), (30, 0.5437), (31, 0.5594), (32, 0.5556), (33, 0.5523), (34, 0.5646), (35, 0.5649), (36, 0.5658), (37, 0.5652), (38, 0.5843), (39, 0.585), (40, 0.5816), (41, 0.5752), (42, 0.5863), (43, 0.5791), (44, 0.5912), (45, 0.5915), (46, 0.5956), (47, 0.5905), (48, 0.59), (49, 0.5911), (50, 0.6123), (51, 0.6025), (52, 0.5962), (53, 0.5986), (54, 0.6135), (55, 0.5901), (56, 0.5957), (57, 0.6166), (58, 0.6152), (59, 0.6107), (60, 0.6198), (61, 0.6255), (62, 0.6162), (63, 0.6166), (64, 0.6171), (65, 0.6199), (66, 0.6158), (67, 0.627), (68, 0.6209), (69, 0.6294), (70, 0.6321), (71, 0.6303), (72, 0.6246), (73, 0.633), (74, 0.6346), (75, 0.632), (76, 0.6285), (77, 0.6378), (78, 0.6385), (79, 0.6407), (80, 0.6317), (81, 0.6231), (82, 0.637), (83, 0.6444), (84, 0.6394), (85, 0.6382), (86, 0.6391), (87, 0.6319), (88, 0.6375), (89, 0.6322), (90, 0.6428), (91, 0.6427), (92, 0.6368), (93, 0.6391), (94, 0.6413), (95, 0.6328), (96, 0.6337), (97, 0.6444), (98, 0.6456), (99, 0.6342), (100, 0.6164), (101, 0.6326), (102, 0.6417), (103, 0.6459), (104, 0.6338), (105, 0.6477), (106, 0.6552), (107, 0.6419), (108, 0.6272), (109, 0.6393), (110, 0.6446), (111, 0.648), (112, 0.6409), (113, 0.6405), (114, 0.6384), (115, 0.6357), (116, 0.6349), (117, 0.6297), (118, 0.6357), (119, 0.6379), (120, 0.6343), (121, 0.641), (122, 0.6477), (123, 0.6535), (124, 0.6414), (125, 0.6411), (126, 0.6408), (127, 0.6501), (128, 0.6531), (129, 0.6473), (130, 0.6417), (131, 0.645), (132, 0.647), (133, 0.6505), (134, 0.6525), (135, 0.6464), (136, 0.6486), (137, 0.6511), (138, 0.6521), (139, 0.6493), (140, 0.6459), (141, 0.6563), (142, 0.6545), (143, 0.6494), (144, 0.6435), (145, 0.6504), (146, 0.6554), (147, 0.6592), (148, 0.6506), (149, 0.6507), (150, 0.6531), (151, 0.6506), (152, 0.6571), (153, 0.6525), (154, 0.6535), (155, 0.6514), (156, 0.6505), (157, 0.6567), (158, 0.6562), (159, 0.6578), (160, 0.6622), (161, 0.658), (162, 0.6563), (163, 0.657), (164, 0.6641), (165, 0.6591), (166, 0.6601), (167, 0.6602), (168, 0.6648), (169, 0.6655), (170, 0.6607), (171, 0.6558), (172, 0.6526), (173, 0.6585), (174, 0.6649), (175, 0.6637), (176, 0.6577), (177, 0.6586), (178, 0.6571), (179, 0.6573), (180, 0.6604), (181, 0.6657), (182, 0.6667), (183, 0.6683), (184, 0.6665), (185, 0.664), (186, 0.6588), (187, 0.6559), (188, 0.6592), (189, 0.6607), (190, 0.6666), (191, 0.6718), (192, 0.6685), (193, 0.6657), (194, 0.6638), (195, 0.6619), (196, 0.663), (197, 0.6652), (198, 0.6663), (199, 0.6664), (200, 0.6688), (201, 0.6696), (202, 0.6684), (203, 0.6674), (204, 0.6653), (205, 0.665), (206, 0.6662), (207, 0.6662), (208, 0.6662), (209, 0.6663), (210, 0.6653), (211, 0.6676), (212, 0.6681), (213, 0.668), (214, 0.6683), (215, 0.6678), (216, 0.6698), (217, 0.6729), (218, 0.6704), (219, 0.6676), (220, 0.6652), (221, 0.6647), (222, 0.665), (223, 0.6673), (224, 0.6686), (225, 0.669), (226, 0.6648), (227, 0.6629), (228, 0.6635), (229, 0.6629), (230, 0.664), (231, 0.664), (232, 0.6631), (233, 0.6662), (234, 0.6648), (235, 0.6636), (236, 0.6631), (237, 0.6653), (238, 0.6614), (239, 0.6625), (240, 0.6656), (241, 0.6669), (242, 0.6671), (243, 0.6725), (244, 0.6707), (245, 0.6706), (246, 0.6704), (247, 0.6675), (248, 0.6671), (249, 0.6667), (250, 0.6677), (251, 0.667), (252, 0.6688), (253, 0.6707), (254, 0.6682), (255, 0.6656), (256, 0.666), (257, 0.6658), (258, 0.6642), (259, 0.668), (260, 0.67), (261, 0.6716), (262, 0.6695), (263, 0.6674), (264, 0.6691), (265, 0.6682), (266, 0.671), (267, 0.6721), (268, 0.671), (269, 0.6694), (270, 0.6662), (271, 0.6658), (272, 0.6665), (273, 0.6665), (274, 0.6671), (275, 0.6662), (276, 0.6669), (277, 0.6662), (278, 0.6655), (279, 0.6651), (280, 0.6664), (281, 0.6666), (282, 0.6665), (283, 0.666), (284, 0.6666), (285, 0.6671), (286, 0.6672), (287, 0.6694), (288, 0.6699), (289, 0.6694), (290, 0.6694), (291, 0.6699), (292, 0.67), (293, 0.6702), (294, 0.6708), (295, 0.6701), (296, 0.6704), (297, 0.6709), (298, 0.6707), (299, 0.6702), (300, 0.6712)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000), (11, 10000), (12, 10000), (13, 10000), (14, 10000), (15, 10000), (16, 10000), (17, 10000), (18, 10000), (19, 10000), (20, 10000), (21, 10000), (22, 10000), (23, 10000), (24, 10000), (25, 10000), (26, 10000), (27, 10000), (28, 10000), (29, 10000), (30, 10000), (31, 10000), (32, 10000), (33, 10000), (34, 10000), (35, 10000), (36, 10000), (37, 10000), (38, 10000), (39, 10000), (40, 10000), (41, 10000), (42, 10000), (43, 10000), (44, 10000), (45, 10000), (46, 10000), (47, 10000), (48, 10000), (49, 10000), (50, 10000), (51, 10000), (52, 10000), (53, 10000), (54, 10000), (55, 10000), (56, 10000), (57, 10000), (58, 10000), (59, 10000), (60, 10000), (61, 10000), (62, 10000), (63, 10000), (64, 10000), (65, 10000), (66, 10000), (67, 10000), (68, 10000), (69, 10000), (70, 10000), (71, 10000), (72, 10000), (73, 10000), (74, 10000), (75, 10000), (76, 10000), (77, 10000), (78, 10000), (79, 10000), (80, 10000), (81, 10000), (82, 10000), (83, 10000), (84, 10000), (85, 10000), (86, 10000), (87, 10000), (88, 10000), (89, 10000), (90, 10000), (91, 10000), (92, 10000), (93, 10000), (94, 10000), (95, 10000), (96, 10000), (97, 10000), (98, 10000), (99, 10000), (100, 10000), (101, 10000), (102, 10000), (103, 10000), (104, 10000), (105, 10000), (106, 10000), (107, 10000), (108, 10000), (109, 10000), (110, 10000), (111, 10000), (112, 10000), (113, 10000), (114, 10000), (115, 10000), (116, 10000), (117, 10000), (118, 10000), (119, 10000), (120, 10000), (121, 10000), (122, 10000), (123, 10000), (124, 10000), (125, 10000), (126, 10000), (127, 10000), (128, 10000), (129, 10000), (130, 10000), (131, 10000), (132, 10000), (133, 10000), (134, 10000), (135, 10000), (136, 10000), (137, 10000), (138, 10000), (139, 10000), (140, 10000), (141, 10000), (142, 10000), (143, 10000), (144, 10000), (145, 10000), (146, 10000), (147, 10000), (148, 10000), (149, 10000), (150, 10000), (151, 10000), (152, 10000), (153, 10000), (154, 10000), (155, 10000), (156, 10000), (157, 10000), (158, 10000), (159, 10000), (160, 10000), (161, 10000), (162, 10000), (163, 10000), (164, 10000), (165, 10000), (166, 10000), (167, 10000), (168, 10000), (169, 10000), (170, 10000), (171, 10000), (172, 10000), (173, 10000), (174, 10000), (175, 10000), (176, 10000), (177, 10000), (178, 10000), (179, 10000), (180, 10000), (181, 10000), (182, 10000), (183, 10000), (184, 10000), (185, 10000), (186, 10000), (187, 10000), (188, 10000), (189, 10000), (190, 10000), (191, 10000), (192, 10000), (193, 10000), (194, 10000), (195, 10000), (196, 10000), (197, 10000), (198, 10000), (199, 10000), (200, 10000), (201, 10000), (202, 10000), (203, 10000), (204, 10000), (205, 10000), (206, 10000), (207, 10000), (208, 10000), (209, 10000), (210, 10000), (211, 10000), (212, 10000), (213, 10000), (214, 10000), (215, 10000), (216, 10000), (217, 10000), (218, 10000), (219, 10000), (220, 10000), (221, 10000), (222, 10000), (223, 10000), (224, 10000), (225, 10000), (226, 10000), (227, 10000), (228, 10000), (229, 10000), (230, 10000), (231, 10000), (232, 10000), (233, 10000), (234, 10000), (235, 10000), (236, 10000), (237, 10000), (238, 10000), (239, 10000), (240, 10000), (241, 10000), (242, 10000), (243, 10000), (244, 10000), (245, 10000), (246, 10000), (247, 10000), (248, 10000), (249, 10000), (250, 10000), (251, 10000), (252, 10000), (253, 10000), (254, 10000), (255, 10000), (256, 10000), (257, 10000), (258, 10000), (259, 10000), (260, 10000), (261, 10000), (262, 10000), (263, 10000), (264, 10000), (265, 10000), (266, 10000), (267, 10000), (268, 10000), (269, 10000), (270, 10000), (271, 10000), (272, 10000), (273, 10000), (274, 10000), (275, 10000), (276, 10000), (277, 10000), (278, 10000), (279, 10000), (280, 10000), (281, 10000), (282, 10000), (283, 10000), (284, 10000), (285, 10000), (286, 10000), (287, 10000), (288, 10000), (289, 10000), (290, 10000), (291, 10000), (292, 10000), (293, 10000), (294, 10000), (295, 10000), (296, 10000), (297, 10000), (298, 10000), (299, 10000), (300, 10000)]}
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.6712
wandb:     loss 3.29762
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240610_162629-zvygnl17
wandb: Find logs at: ./wandb/offline-run-20240610_162629-zvygnl17/logs
[2m[36m(pid=2909554)[0m 2024-06-10 16:26:36.939782: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=2909554)[0m 2024-06-10 16:26:37.025399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=2909554)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=2909554)[0m 2024-06-10 16:26:38.173403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[1m[33m(autoscaler +10h54m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h54m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h55m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h55m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h56m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h56m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h56m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h56m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h57m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h57m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h57m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h58m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h59m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10h59m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
2024-06-11 03:27:53.643294: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 03:27:53.702186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 03:27:54.882297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 03:28:22,849 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 03:28:22,884 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-11 03:28:24,432 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 03:28:24,446 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 03:28:38,252 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-06-11 03:28:38,256 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAvg
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 03:28:38,263 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 03:28:38,364 | Config.py:72 | No previous federated learning simulation found with hash 3dda1d960d86602a827554c0fcb07d542640155a876087bf82ace2d6bfc2db5d, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 03:28:44,481 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 03:28:44,481 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 03:28:44,532	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 03:28:44,546	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 03:28:44,561 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 03:28:44,655	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 03:28:44,656 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 2.0, 'CPU': 4.0, 'GPU': 2.0, 'accelerator_type:G': 2.0, 'memory': 34359738368.0, 'object_store_memory': 36316086681.0, 'node:10.20.240.18': 2.0}
INFO flwr 2024-06-11 03:28:44,656 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 03:28:44,657 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 03:28:44,671 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 03:28:44,672 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 03:28:44,673 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 03:28:44,673 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 03:28:46,766 | server.py:94 | initial parameters (loss, other metrics): 2.5065833984375, {'accuracy': 0.0978, 'data_size': 10000}
INFO flwr 2024-06-11 03:28:46,767 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 03:28:46,767 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[1m[36m(autoscaler +1m4s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +1m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m2s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m37s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m43s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[36m(pid=3839164)[0m 2024-06-11 03:44:10.108527: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3839164)[0m 2024-06-11 03:44:10.944954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3839164)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3839164)[0m 2024-06-11 03:44:15.453584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[33m(raylet)[0m [2024-06-11 03:44:57,527 E 2906522 2906522] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[1m[33m(autoscaler +8m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +9m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +10m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +11m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m29s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +12m58s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +13m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +13m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +13m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +14m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +15m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +15m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m4s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +16m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[33m(raylet)[0m [2024-06-11 03:47:57,530 E 2906522 2906522] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 03:47:57,944 E 2906523 2906523] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: baccd8655287851f0509ddef8187bf7d920d56b081f1f711a92be0a1, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3843263)[0m 2024-06-11 03:48:15.267178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3843263)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
ERROR flwr 2024-06-11 03:48:32,941 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:48:32,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:48:32,943 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:48:32,944 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3843263)[0m 2024-06-11 03:48:44.576301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ERROR flwr 2024-06-11 03:49:08,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:49:08,336 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:49:08,337 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:49:08,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:49:08,338 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:49:08,338 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 03:49:54,940 | server.py:236 | fit_round 1 received 5 results and 5 failures
INFO flwr 2024-06-11 03:51:29,654 | server.py:125 | fit progress: (1, 2.239556640625, {'accuracy': 0.1807, 'data_size': 10000}, 1362.8869581031613)
INFO flwr 2024-06-11 03:51:29,655 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 03:51:29,657 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 03:53:51,891 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:53:51,891 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:54:17,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:54:17,785 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:54:17,785 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:54:17,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:54:38,493 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:54:38,493 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:54:38,495 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:54:38,500 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:54:58,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:54:58,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:55:16,154 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:55:16,155 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:55:16,170 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:55:16,170 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 03:55:44,498 | server.py:236 | fit_round 2 received 2 results and 8 failures
INFO flwr 2024-06-11 03:56:02,214 | server.py:125 | fit progress: (2, 2.0889224609375, {'accuracy': 0.251, 'data_size': 10000}, 1635.4471364701167)
INFO flwr 2024-06-11 03:56:02,215 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-11 03:56:02,216 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
[2m[1m[33m(autoscaler +17m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +17m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +18m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +18m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +19m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +23m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +23m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +24m25s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m39s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +25m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m54s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +26m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +27m35s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +27m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +28m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
ERROR flwr 2024-06-11 03:57:05,654 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:57:05,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:58:34,280 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:58:34,280 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:58:34,281 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:58:34,281 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:58:34,281 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:58:34,281 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:58:58,572 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:58:58,572 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:58:58,573 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:58:58,573 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:59:19,063 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:59:19,064 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:59:34,551 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:59:34,551 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 03:59:34,552 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 03:59:34,552 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 03:59:34,554 | server.py:236 | fit_round 3 received 1 results and 9 failures
INFO flwr 2024-06-11 03:59:45,903 | server.py:125 | fit progress: (3, 2.1198572265625, {'accuracy': 0.256, 'data_size': 10000}, 1859.1354744578712)
INFO flwr 2024-06-11 03:59:45,903 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-11 03:59:45,904 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 04:01:09,301 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:01:09,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:01:35,635 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:01:35,636 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:02:44,453 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:02:44,453 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:02:44,454 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:02:44,455 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:02:44,460 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:02:44,461 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 04:02:57,549 E 2906522 2906522] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 04:02:57,973 E 2906523 2906523] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: baccd8655287851f0509ddef8187bf7d920d56b081f1f711a92be0a1, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:03:19,322 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:03:19,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:03:19,324 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:03:19,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:03:19,324 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:03:19,350 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:03:19,325 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:03:19,351 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 04:03:19,412 | server.py:236 | fit_round 4 received 1 results and 9 failures
INFO flwr 2024-06-11 04:03:28,987 | server.py:125 | fit progress: (4, 2.0480107421875, {'accuracy': 0.2854, 'data_size': 10000}, 2082.2200370240025)
INFO flwr 2024-06-11 04:03:28,988 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-11 04:03:28,989 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 04:04:38,971 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:04:38,972 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:04:38,973 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:04:38,973 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:05:12,944 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:05:12,944 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:05:31,128 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:05:31,129 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:05:46,644 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:05:46,644 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:06:21,391 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:06:21,392 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:06:21,392 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:06:21,392 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:06:21,393 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:06:21,393 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:06:21,395 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:06:21,395 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b793fe4483d9fc8377dfd93c04000000, name=DefaultActor.__init__, pid=3843263, memory used=3.21GB) was running was 238.94GB / 251.51GB (0.950035), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-8bd7e36efd0b7bfee8c6c4ed0a58497c2dee38f649da53a6ef84084a*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
3822259	7.05	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3845950	4.70	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_n...
3843263	3.21	ray::DefaultActor
2906523	0.53	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906522	0.36	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
2905910	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 04:06:21,396 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 04:06:21,396 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13) where the task (actor ID: b5014b97cf0f39819c7c28f104000000, name=DefaultActor.__init__, pid=3839164, memory used=3.42GB) was running was 238.97GB / 251.51GB (0.950154), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0d2f6c708c528dc16e38a057134e0287ef70e77b28c1b0c01329dd0c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
2909640	8.06	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
3822259	5.20	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
3822181	3.46	
3839164	3.42	ray::DefaultActor
2906523	0.54	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2906522	0.48	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
2905333	0.38	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
2906612	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2906615	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
2905909	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 04:06:21,413 | server.py:236 | fit_round 5 received 0 results and 10 failures
ERROR flwr 2024-06-11 04:06:21,454 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 04:06:21,474 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 04:06:21,475 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 04:06:21,475 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2854
wandb:     loss 2.04801
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_032840-s15fwon9
wandb: Find logs at: ./wandb/offline-run-20240611_032840-s15fwon9/logs
[2m[1m[33m(autoscaler +29m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +29m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +29m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +29m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +30m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +31m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +31m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +31m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +32m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +32m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +32m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +33m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +34m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +34m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
2024-06-11 04:07:36.778617: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 04:07:37.459660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 04:07:39.177984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 04:07:46,867 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 04:07:46,868 | Data.py:111 | Found preprocessed data for the given preprocess function with hash eda5b80526d0a893cb27d74de67f453d190ef90188f6c8963c1161cbb5a526ce, returning
INFO flwr 2024-06-11 04:07:47,693 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 04:07:47,694 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 04:07:57,706 | main.py:110 | Loaded 1 configs with name CIFAR10-RESNET18-FEDNAG, running...
INFO flwr 2024-06-11 04:07:57,709 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedNag
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
				momentum: 0.9
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				lr: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 04:07:57,715 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 04:07:57,730 | Config.py:72 | No previous federated learning simulation found with hash 34a8be47a4ea137dd430b718a2caa64d7341558dc6b6a5bdf24494dc629b6290, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 04:07:59,483 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 04:07:59,483 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 04:07:59,528	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 04:07:59,555	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 04:07:59,646 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 04:07:59,741	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 04:07:59,742 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 2.0, 'GPU': 2.0, 'CPU': 4.0, 'object_store_memory': 36316086681.0, 'node:10.20.240.18': 2.0, 'memory': 34359738368.0, 'node:__internal_head__': 2.0}
INFO flwr 2024-06-11 04:07:59,743 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 04:07:59,743 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 04:07:59,764 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 04:07:59,765 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 04:07:59,766 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 04:07:59,766 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 04:08:02,211 | server.py:94 | initial parameters (loss, other metrics): 2.5065833984375, {'accuracy': 0.0978, 'data_size': 10000}
INFO flwr 2024-06-11 04:08:02,211 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 04:08:02,213 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3864250)[0m 2024-06-11 04:08:08.237294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[1m[36m(autoscaler +30s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m6s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +1m41s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +3m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m12s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +4m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m27s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +5m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m8s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +6m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m10s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m45s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +7m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +8m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[36m(pid=3873505)[0m 2024-06-11 04:17:17.416726: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=3873505)[0m 2024-06-11 04:17:18.069421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3873505)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3873505)[0m 2024-06-11 04:17:22.827404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[33m(raylet)[0m [2024-06-11 04:17:57,567 E 2906522 2906522] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 8040f2f6f337106e2bafe961461b06a0a32795d404c6a86ba721fb13, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 04:17:57,995 E 2906523 2906523] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: baccd8655287851f0509ddef8187bf7d920d56b081f1f711a92be0a1, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 04:18:13,247 | server.py:236 | fit_round 1 received 10 results and 0 failures
[2024-06-11 04:19:33,973 E 3863186 3864237] core_worker.cc:587: :info_message: Attempting to recover 2 lost objects by resubmitting their tasks. To disable object reconstruction, set @ray.remote(max_retries=0).
[2024-06-11 04:20:35,745 E 3863186 3864237] gcs_rpc_client.h:547: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure. The program will terminate.
srun: error: ctit088: task 0: Exited with exit code 1
2024-06-11 04:20:39,589	INFO scripts.py:1139 -- Did not find any active Ray processes.
