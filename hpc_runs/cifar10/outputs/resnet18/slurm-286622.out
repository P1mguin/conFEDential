ctit080
2024-05-19 18:40:59,133	ERROR services.py:1207 -- Failed to start the dashboard 
2024-05-19 18:40:59,134	ERROR services.py:1232 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.
2024-05-19 18:40:59,134	ERROR services.py:1242 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: '/local/ray/session_2024-05-19_18-40-34_633252_2102948/logs/dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2024-05-19 18:40:59,134	ERROR services.py:1276 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues
2024-05-19 18:40:34,506	INFO usage_lib.py:407 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-05-19 18:40:34,507	INFO scripts.py:722 -- Local node IP: 10.20.240.10
2024-05-19 18:41:00,946	SUCC scripts.py:759 -- --------------------
2024-05-19 18:41:00,946	SUCC scripts.py:760 -- Ray runtime started.
2024-05-19 18:41:00,946	SUCC scripts.py:761 -- --------------------
2024-05-19 18:41:00,946	INFO scripts.py:763 -- Next steps
2024-05-19 18:41:00,946	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-05-19 18:41:00,946	INFO scripts.py:769 --   ray start --address='10.20.240.10:6379'
2024-05-19 18:41:00,946	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-05-19 18:41:00,947	INFO scripts.py:780 -- import ray
2024-05-19 18:41:00,947	INFO scripts.py:781 -- ray.init()
2024-05-19 18:41:00,947	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-05-19 18:41:00,947	INFO scripts.py:813 --   ray stop
2024-05-19 18:41:00,948	INFO scripts.py:816 -- To view the status of the cluster, use
2024-05-19 18:41:00,948	INFO scripts.py:817 --   ray status
2024-05-19 18:42:28.061563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-19 18:42:53.021422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-19 18:44:20,775 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-19 18:44:20,777 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-19 18:44:52,323 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-19 18:44:52,324 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-19 18:45:04,470 | main.py:103 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-05-19 18:45:04,470 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 200
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.1, 'betas': [0.9, 0.999], 'eps': 1e-09, 'weight_decay': 0.9999}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-19 18:45:04,470 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-19 18:45:04,472 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-19 18:45:35,604 | Simulation.py:395 | Created 2 clients with resources 8 CPUs, 0.5 GPUs, and 5.3GB for the total available 16 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-19 18:45:35,604 | Simulation.py:160 | Starting federated learning simulation
ERROR flwr 2024-05-19 18:45:35,668 | Simulation.py:183 | Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240519_184526-85z5b0yh
wandb: Find logs at: ./wandb/offline-run-20240519_184526-85z5b0yh/logs
2024-05-19 18:46:30.637327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-19 18:46:32.838540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-19 18:46:42,600 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-19 18:46:42,601 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-19 18:46:44,396 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-19 18:46:44,397 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-19 18:46:45,336 | main.py:103 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-05-19 18:46:45,336 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 200
			local_rounds: 1
		Model:
			optimizer_name: FedAvg
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-19 18:46:45,336 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-19 18:46:45,338 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-19 18:46:54,999 | Simulation.py:395 | Created 2 clients with resources 8 CPUs, 0.5 GPUs, and 5.3GB for the total available 16 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-19 18:46:54,999 | Simulation.py:160 | Starting federated learning simulation
ERROR flwr 2024-05-19 18:46:55,058 | Simulation.py:183 | Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240519_184654-e3bd6k9t
wandb: Find logs at: ./wandb/offline-run-20240519_184654-e3bd6k9t/logs
2024-05-19 18:47:13.859959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-19 18:47:15.743863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-19 18:47:22,562 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-19 18:47:22,563 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-19 18:47:24,304 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-19 18:47:24,304 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-19 18:47:25,217 | main.py:103 | Loaded 1 configs with name CIFAR10-RESNET18-FEDNAG, running...
INFO flwr 2024-05-19 18:47:25,217 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar10
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": (element["img"] / 255.).transpose(2, 0, 1),
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 200
			local_rounds: 1
		Model:
			optimizer_name: FedNag
			model_name: ResNet18
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
				momentum: 0.9
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet18
					pretrained: False
					out_features: 10
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-19 18:47:25,217 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-19 18:47:25,218 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-ctit080: error: *** JOB 286622 ON ctit080 CANCELLED AT 2024-05-19T18:47:38 ***
slurmstepd-ctit080: error: *** STEP 286622.2 ON ctit080 CANCELLED AT 2024-05-19T18:47:38 ***
