ctit082
2024-05-13 20:57:12.657270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-13 20:57:14.844029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-13 21:01:07,934 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-13 21:01:07,936 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-13 21:02:12,086 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-13 21:02:12,087 | Simulation.py:243 | Found previously split dataloaders, loading them
INFO flwr 2024-05-13 21:02:57,181 | main.py:67 | Loaded 1 configs with name CIFAR10-RESNET18-FEDADAM, running...
INFO flwr 2024-05-13 21:02:57,182 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-13 21:02:57,183 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2024-05-13 21:03:51,271	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-13 21:03:59,652	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-13 21:03:59,906	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ddf338ac90a05a92.zip' (1.30MiB) to Ray cluster...
2024-05-13 21:03:59,911	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ddf338ac90a05a92.zip'.
INFO flwr 2024-05-13 21:04:11,276 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs forthe total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-13 21:04:11,277 | Simulation.py:158 | Starting federated learning simulation
INFO flwr 2024-05-13 21:04:11,278 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-13 21:04:11,370	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-13 21:04:11,370 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77770459545.0, 'accelerator_type:TITAN': 1.0, 'memory': 171464405607.0, 'node:10.20.240.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 64.0}
INFO flwr 2024-05-13 21:04:11,370 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-13 21:04:11,371 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-13 21:04:11,388 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-13 21:04:11,389 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-13 21:04:11,389 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-13 21:04:11,390 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-05-13 21:04:16,018 | app.py:313 | Given groups=1, weight of size [64, 3, 7, 7], expected input[10000, 32, 32, 3] to have 3 channels, but got 32 channels instead
ERROR flwr 2024-05-13 21:04:16,269 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/training/Server.py", line 60, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, simulation)
  File "/home/s2240084/conFEDential/src/training/learning_methods/Strategy.py", line 83, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[10000, 32, 32, 3] to have 3 channels, but got 32 channels instead

ERROR flwr 2024-05-13 21:04:16,270 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-13 21:04:16,270 | Simulation.py:169 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
[2m[36m(pid=75910)[0m 2024-05-13 21:04:17.387841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=75910)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240513_210259-ogrk57sh
wandb: Find logs at: ./wandb/offline-run-20240513_210259-ogrk57sh/logs
[2m[36m(pid=75912)[0m 2024-05-13 21:04:19.532742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=75907)[0m 2024-05-13 21:04:17.558017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=75907)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[2m[36m(pid=75903)[0m 2024-05-13 21:04:19.532807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 7x across cluster][0m
2024-05-13 21:07:26.685944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-13 21:07:27.984891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-13 21:07:34,430 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-13 21:07:34,431 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-13 21:07:35,952 | Simulation.py:35 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-13 21:07:35,953 | Simulation.py:243 | Found previously split dataloaders, loading them
INFO flwr 2024-05-13 21:07:36,899 | main.py:67 | Loaded 1 configs with name CIFAR10-RESNET18-FEDAVG, running...
INFO flwr 2024-05-13 21:07:36,899 | Config.py:58 | Starting conFEDential simulation
INFO flwr 2024-05-13 21:07:36,960 | Config.py:62 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2024-05-13 21:07:41,497	INFO worker.py:1621 -- Started a local Ray instance.
2024-05-13 21:07:41,779	INFO packaging.py:518 -- Creating a file package for local directory '/home/s2240084/conFEDential'.
2024-05-13 21:07:41,916	INFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_ddf338ac90a05a92.zip' (1.30MiB) to Ray cluster...
2024-05-13 21:07:41,920	INFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_ddf338ac90a05a92.zip'.
INFO flwr 2024-05-13 21:07:54,513 | Simulation.py:367 | Created 8 clients with resources 8 CPUs and 0.125 GPUs forthe total available 64 CPUs and 1 GPUs
INFO flwr 2024-05-13 21:07:54,513 | Simulation.py:158 | Starting federated learning simulation
INFO flwr 2024-05-13 21:07:54,514 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
2024-05-13 21:07:54,578	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-13 21:07:54,579 | app.py:213 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 77459150438.0, 'node:__internal_head__': 1.0, 'node:10.20.240.12': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 170738017690.0, 'CPU': 64.0}
INFO flwr 2024-05-13 21:07:54,579 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-13 21:07:54,579 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.125}
INFO flwr 2024-05-13 21:07:54,595 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
INFO flwr 2024-05-13 21:07:54,596 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-13 21:07:54,596 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-13 21:07:54,596 | server.py:91 | Evaluating initial parameters
ERROR flwr 2024-05-13 21:07:55,800 | app.py:313 | Given groups=1, weight of size [64, 3, 7, 7], expected input[10000, 32, 32, 3] to have 3 channels, but got 32 channels instead
ERROR flwr 2024-05-13 21:07:55,875 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/s2240084/conFEDential/src/training/Server.py", line 60, in evaluate
    loss, accuracy, data_size = Strategy.test(parameters, test_loader, simulation)
  File "/home/s2240084/conFEDential/src/training/learning_methods/Strategy.py", line 83, in test
    outputs = net(features)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[10000, 32, 32, 3] to have 3 channels, but got 32 channels instead

ERROR flwr 2024-05-13 21:07:55,876 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 8, 'num_gpus': 0.125} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 8, 'num_gpus': 0.125}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-13 21:07:55,876 | Simulation.py:169 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240513_210738-vh8qy7yk
wandb: Find logs at: ./wandb/offline-run-20240513_210738-vh8qy7yk/logs
2024-05-13 21:09:59.607544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-13 21:10:00.898321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-13 21:10:05,995 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-13 21:10:05,996 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
Traceback (most recent call last):
  File "/home/s2240084/conFEDential/src/main.py", line 72, in <module>
    main()
  File "/home/s2240084/conFEDential/src/main.py", line 60, in main
    configs = batch_config.generate_configs_from_yaml_file(str(Path(args.yaml_file).resolve()))
  File "/home/s2240084/conFEDential/src/utils/batch_config.py", line 77, in generate_configs_from_yaml_file
    configs = [Config.from_dict(raw_config) for raw_config in raw_configs]
  File "/home/s2240084/conFEDential/src/utils/batch_config.py", line 77, in <listcomp>
    configs = [Config.from_dict(raw_config) for raw_config in raw_configs]
  File "/home/s2240084/conFEDential/src/experiment/Config.py", line 45, in from_dict
    simulation=Simulation.from_dict(config['simulation']),
  File "/home/s2240084/conFEDential/src/experiment/Simulation.py", line 58, in from_dict
    model=Model.from_dict(config['model'])
  File "/home/s2240084/conFEDential/src/experiment/Model.py", line 61, in from_dict
    return Model(
  File "/home/s2240084/conFEDential/src/experiment/Model.py", line 32, in __init__
    self._prepare_learning_method()
  File "/home/s2240084/conFEDential/src/experiment/Model.py", line 172, in _prepare_learning_method
    learning_method_class = getattr(training.learning_methods, self._optimizer_name)
AttributeError: module 'src.training.learning_methods' has no attribute 'FedNag'. Did you mean: 'FedNAG'?
srun: error: ctit082: task 0: Exited with exit code 1
