ctit081
2024-05-23 13:13:57,012	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-05-23 13:13:57,012	INFO scripts.py:722 -- Local node IP: 10.20.240.11
2024-05-23 13:14:13,886	SUCC scripts.py:759 -- --------------------
2024-05-23 13:14:13,886	SUCC scripts.py:760 -- Ray runtime started.
2024-05-23 13:14:13,886	SUCC scripts.py:761 -- --------------------
2024-05-23 13:14:13,886	INFO scripts.py:763 -- Next steps
2024-05-23 13:14:13,886	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-05-23 13:14:13,886	INFO scripts.py:769 --   ray start --address='10.20.240.11:6379'
2024-05-23 13:14:13,886	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-05-23 13:14:13,886	INFO scripts.py:780 -- import ray
2024-05-23 13:14:13,886	INFO scripts.py:781 -- ray.init()
2024-05-23 13:14:13,886	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-05-23 13:14:13,886	INFO scripts.py:813 --   ray stop
2024-05-23 13:14:13,887	INFO scripts.py:816 -- To view the status of the cluster, use
2024-05-23 13:14:13,887	INFO scripts.py:817 --   ray status
2024-05-23 13:14:42.579560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 13:14:44.895591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-23 13:14:54,301 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-23 13:14:54,319 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-23 13:15:03,778 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-23 13:15:03,814 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-23 13:15:29,617 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDADAM, running...
INFO flwr 2024-05-23 13:15:29,620 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 1
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 2
			local_rounds: 8
		Model:
			optimizer_name: FedAdam
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.1, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.9999}
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-23 13:15:29,635 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-23 13:15:29,638 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-23 13:15:36,123 | Simulation.py:395 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-23 13:15:36,123 | Simulation.py:160 | Starting federated learning simulation
2024-05-23 13:15:36,186	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.11:6379...
2024-05-23 13:15:36,200	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-05-23 13:15:42,408 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)
2024-05-23 13:15:42,538	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-23 13:15:42,538 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'GPU': 1.0, 'node:10.20.240.11': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 64163791257.0, 'memory': 17179869184.0, 'accelerator_type:TITAN': 1.0}
INFO flwr 2024-05-23 13:15:42,539 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-23 13:15:42,539 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-05-23 13:15:42,552 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-05-23 13:15:42,552 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-23 13:15:42,553 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-23 13:15:42,553 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=661658)[0m 2024-05-23 13:15:47.070712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=661658)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=661658)[0m 2024-05-23 13:15:49.010036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-23 13:15:54,030 | server.py:94 | initial parameters (loss, other metrics): 0.00015558641163649327, {'accuracy': 0.008750295618095205, 'data_size': 29599}
INFO flwr 2024-05-23 13:15:54,031 | server.py:104 | FL starting
DEBUG flwr 2024-05-23 13:15:54,031 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-23 13:26:29,593 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-23 13:26:30,084 | server.py:125 | fit progress: (1, 0.00015081443143513578, {'accuracy': 0.15987026588736106, 'data_size': 29599}, 636.0530527047813)
INFO flwr 2024-05-23 13:26:30,084 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-23 13:26:30,085 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-23 13:37:00,018 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-23 13:37:00,195 | server.py:125 | fit progress: (2, 0.0001523345462164084, {'accuracy': 0.11290921990607791, 'data_size': 29599}, 1266.1640984229743)
INFO flwr 2024-05-23 13:37:00,195 | server.py:171 | evaluate_round 2: no clients selected, cancel
INFO flwr 2024-05-23 13:37:00,195 | server.py:153 | FL finished in 1266.1647038110532
INFO flwr 2024-05-23 13:37:00,196 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-23 13:37:00,196 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-23 13:37:00,197 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-23 13:37:00,197 | app.py:229 | app_fit: losses_centralized [(0, 0.00015558641163649327), (1, 0.00015081443143513578), (2, 0.0001523345462164084)]
INFO flwr 2024-05-23 13:37:00,197 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.008750295618095205), (1, 0.15987026588736106), (2, 0.11290921990607791)], 'data_size': [(0, 29599), (1, 29599), (2, 29599)]}
wandb: 
wandb: Run summary:
wandb: accuracy 0.11291
wandb:     loss 0.00015
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240523_131535-rpo3hvou
wandb: Find logs at: ./wandb/offline-run-20240523_131535-rpo3hvou/logs
2024-05-23 13:38:48.115426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 13:38:53.061598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-23 13:39:16,683 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-23 13:39:16,685 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-23 13:39:26,725 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-23 13:39:26,729 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-23 13:39:58,572 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDAVG, running...
INFO flwr 2024-05-23 13:39:58,574 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 1
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 2
			local_rounds: 20
		Model:
			optimizer_name: FedAvg
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.1
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-23 13:39:58,587 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-23 13:39:59,113 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-23 13:40:09,042 | Simulation.py:395 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-23 13:40:09,043 | Simulation.py:160 | Starting federated learning simulation
2024-05-23 13:40:09,103	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.11:6379...
2024-05-23 13:40:09,357	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-05-23 13:40:09,373 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)
2024-05-23 13:40:09,540	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-23 13:40:09,541 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 64163791257.0, 'CPU': 2.0, 'node:10.20.240.11': 1.0, 'GPU': 1.0, 'accelerator_type:TITAN': 1.0, 'memory': 17179869184.0}
INFO flwr 2024-05-23 13:40:09,541 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-23 13:40:09,541 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-05-23 13:40:09,551 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-05-23 13:40:09,552 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-23 13:40:09,552 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-23 13:40:09,552 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-23 13:40:10,506 | server.py:94 | initial parameters (loss, other metrics): 0.00015558641163649327, {'accuracy': 0.008750295618095205, 'data_size': 29599}
INFO flwr 2024-05-23 13:40:10,507 | server.py:104 | FL starting
DEBUG flwr 2024-05-23 13:40:10,507 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=665840)[0m 2024-05-23 13:40:21.691252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=665840)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=665840)[0m 2024-05-23 13:40:28.950819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-05-23 13:55:07,830 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-23 13:55:08,956 | server.py:125 | fit progress: (1, 0.00015269026908847446, {'accuracy': 0.14274130882800096, 'data_size': 29599}, 898.4487490910105)
INFO flwr 2024-05-23 13:55:08,956 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-23 13:55:08,957 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-23 14:09:29,945 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-23 14:09:31,407 | server.py:125 | fit progress: (2, 0.00014895829619892208, {'accuracy': 0.29582080475691747, 'data_size': 29599}, 1760.8998718308285)
INFO flwr 2024-05-23 14:09:31,408 | server.py:171 | evaluate_round 2: no clients selected, cancel
INFO flwr 2024-05-23 14:09:31,408 | server.py:153 | FL finished in 1760.9006417761557
INFO flwr 2024-05-23 14:09:31,409 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-23 14:09:31,409 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2024-05-23 14:09:31,409 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-23 14:09:31,409 | app.py:229 | app_fit: losses_centralized [(0, 0.00015558641163649327), (1, 0.00015269026908847446), (2, 0.00014895829619892208)]
INFO flwr 2024-05-23 14:09:31,410 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.008750295618095205), (1, 0.14274130882800096), (2, 0.29582080475691747)], 'data_size': [(0, 29599), (1, 29599), (2, 29599)]}
wandb: 
wandb: Run summary:
wandb: accuracy 0.29582
wandb:     loss 0.00015
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240523_134007-wnwuhlfo
wandb: Find logs at: ./wandb/offline-run-20240523_134007-wnwuhlfo/logs
2024-05-23 14:11:05.735458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 14:11:11.024669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-23 14:11:28,146 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-23 14:11:28,147 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-23 14:11:28,277 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-23 14:11:28,278 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-23 14:11:53,414 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDNAG, running...
INFO flwr 2024-05-23 14:11:53,416 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 2
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 2
			local_rounds: 20
		Model:
			optimizer_name: FedNAG
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.1
				momentum: 0.85
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-23 14:11:53,429 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-23 14:11:53,448 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-23 14:12:01,325 | Simulation.py:395 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-23 14:12:01,325 | Simulation.py:160 | Starting federated learning simulation
2024-05-23 14:12:01,384	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.11:6379...
2024-05-23 14:12:01,400	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-05-23 14:12:01,416 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)
2024-05-23 14:12:01,481	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-23 14:12:01,481 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 2.0, 'memory': 17179869184.0, 'accelerator_type:TITAN': 1.0, 'node:10.20.240.11': 1.0, 'GPU': 1.0, 'object_store_memory': 64163791257.0}
INFO flwr 2024-05-23 14:12:01,482 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-23 14:12:01,482 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-05-23 14:12:01,586 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-05-23 14:12:01,588 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-23 14:12:01,588 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-23 14:12:01,589 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-23 14:12:02,403 | server.py:94 | initial parameters (loss, other metrics): 0.00015558641163649327, {'accuracy': 0.008750295618095205, 'data_size': 29599}
INFO flwr 2024-05-23 14:12:02,403 | server.py:104 | FL starting
DEBUG flwr 2024-05-23 14:12:02,404 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=671544)[0m 2024-05-23 14:12:08.026716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=671544)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=671544)[0m 2024-05-23 14:12:10.775578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-05-23 14:23:41,776 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-05-23 14:23:50,308 | server.py:125 | fit progress: (1, 0.0001510403728907187, {'accuracy': 0.1903780533126119, 'data_size': 29599}, 707.904045351781)
INFO flwr 2024-05-23 14:23:50,308 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-05-23 14:23:50,309 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-05-23 14:35:12,229 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-05-23 14:35:16,716 | server.py:125 | fit progress: (2, 0.00014645396271623177, {'accuracy': 0.33960606777255986, 'data_size': 29599}, 1394.3125548381358)
INFO flwr 2024-05-23 14:35:16,717 | server.py:171 | evaluate_round 2: no clients selected, cancel
INFO flwr 2024-05-23 14:35:16,717 | server.py:153 | FL finished in 1394.313275211025
INFO flwr 2024-05-23 14:35:16,718 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-05-23 14:35:16,776 | app.py:227 | app_fit: metrics_distributed_fit {'velocity': [(1, [array([[ 0.00000000e+00,  2.87234712e-07, -9.82118920e-09, ...,
        -6.95468572e-10, -2.29999997e-09, -4.27775015e-09],
       [ 0.00000000e+00,  4.38533760e-07, -1.65025185e-08, ...,
         6.29576336e-10, -5.54859355e-07, -4.15284802e-08],
       [ 0.00000000e+00,  1.08685846e-04,  3.51476192e-05, ...,
         1.78718055e-05,  2.70032833e-05, -1.61194766e-05],
       ...,
       [ 0.00000000e+00, -3.13590746e-04,  3.22193705e-06, ...,
        -8.65361289e-06, -2.75889848e-04, -1.12832572e-06],
       [ 0.00000000e+00,  7.88371326e-05, -1.20159975e-05, ...,
        -9.57808152e-06,  1.06560088e-04, -1.42752742e-05],
       [ 0.00000000e+00, -6.24644890e-05, -1.81151408e-04, ...,
        -8.58290969e-06, -1.80118557e-04, -1.94491022e-05]], dtype=float32), array([ 2.37786622e-07, -5.81991799e-07, -3.26410154e-05,  6.63802837e-07,
        3.94613307e-05,  8.36913096e-05,  1.56642549e-04, -1.65379170e-04,
        2.39091605e-07,  2.44899525e-07,  2.24681808e-05,  2.28502145e-07,
        2.36430466e-07,  3.39603986e-07, -3.80286522e-08, -2.20830261e-05,
        4.06739531e-07,  1.39924438e-07,  9.16726494e-05, -2.83842655e-06,
        2.78049015e-07, -3.45440594e-06,  1.66976752e-05,  2.59058282e-07,
        5.70724915e-05,  2.17984038e-07,  1.30401713e-05,  3.29776810e-07,
        1.16975507e-07,  2.24595269e-07,  4.00711899e-04, -1.22308920e-05,
       -4.50068228e-07, -1.39282523e-08, -2.66613199e-07, -5.11620128e-05,
        2.90165235e-06, -1.10452138e-05, -2.70835858e-06,  4.02062651e-05,
        3.03773760e-07, -7.37164919e-06,  7.21505700e-08, -9.20206185e-06,
       -4.02799487e-05, -4.79803430e-06,  9.41460243e-08, -1.80165116e-05,
       -1.52601842e-06, -8.39636923e-05, -2.30915699e-04, -3.15101752e-05,
       -1.68486280e-04,  1.73515655e-04, -1.84880686e-04,  3.29641363e-04,
        1.82832646e-05, -7.62634954e-05, -3.15143916e-05, -2.68643944e-05,
       -6.47724373e-04,  8.30181816e-05, -1.61771750e-04,  3.11162444e-06,
       -3.00105239e-05,  2.28897591e-07,  6.69553701e-05, -1.25317345e-03,
        5.35265834e-04,  7.33018038e-04,  8.37193511e-05,  3.35739810e-06,
        4.38127718e-05,  5.99827908e-05,  9.31397954e-05, -8.50131182e-05,
       -4.51239903e-05,  4.56710288e-04,  2.01929957e-04, -8.62535956e-07,
       -4.25545513e-05,  3.30086696e-05, -6.80414523e-05, -1.02623171e-05,
       -2.16479725e-06,  1.13322012e-05, -2.50454068e-05, -4.30049986e-05,
       -3.65600954e-07,  1.91549339e-06,  7.55022484e-05,  1.09171174e-06,
       -4.20145398e-06,  1.03837898e-07,  1.03843864e-04, -6.22889456e-06,
       -4.89085214e-05, -3.23186105e-04,  7.07838044e-05, -9.43161067e-05],
      dtype=float32)]), (2, [array([[ 0.0000000e+00,  4.3150417e-09,  4.0119534e-09, ...,
        -1.5047735e-10, -2.0587911e-09,  6.0412768e-09],
       [ 0.0000000e+00,  5.4364304e-09,  6.8977908e-09, ...,
        -1.2927638e-10, -2.6458611e-09,  1.0076275e-08],
       [ 0.0000000e+00,  7.8288314e-05,  2.8214741e-05, ...,
         2.2388416e-05, -3.5607020e-05,  6.1398823e-05],
       ...,
       [ 0.0000000e+00, -1.4691889e-04, -7.1369950e-06, ...,
         1.9002596e-07, -3.7403299e-06, -4.8743482e-06],
       [ 0.0000000e+00, -5.8924688e-05,  2.2854409e-05, ...,
        -4.7080772e-05, -3.6745339e-05, -9.6688700e-07],
       [ 0.0000000e+00, -2.1677461e-05,  1.4112662e-05, ...,
        -1.6730678e-06, -2.7812442e-07,  6.9810912e-06]], dtype=float32), array([ 1.04565592e-08,  1.09737197e-08,  1.39501703e-04, -9.72815251e-09,
       -4.14407396e-07,  5.29109129e-05, -1.60927716e-10, -1.78912378e-05,
        1.05819415e-08, -1.64271907e-09,  2.45637912e-05,  1.20643673e-08,
        1.06848770e-08, -8.33580955e-08, -9.49767021e-09,  1.11660265e-04,
       -8.14595467e-08,  8.93692054e-09, -4.01451653e-05, -5.74853679e-04,
       -1.28838195e-07, -4.38242864e-07, -6.11761209e-07, -3.88151946e-08,
       -3.91389294e-05,  6.98074976e-09, -1.27637526e-04, -3.33034293e-08,
        1.06092060e-08,  1.00183808e-08,  8.02216877e-04, -1.62704077e-06,
       -6.09435613e-10, -3.40392425e-08, -2.92961673e-08, -5.65004666e-06,
       -1.54083937e-05, -4.55155167e-08, -9.38592393e-07,  3.45346576e-04,
       -5.71866053e-07,  9.86202722e-05, -3.48175444e-09, -2.95821337e-06,
        3.31480915e-05, -1.02897693e-05, -4.45127057e-07, -2.00173508e-06,
       -1.36886513e-06,  4.72890073e-03, -1.03371986e-03, -4.82340693e-05,
       -5.73107684e-07, -2.95068421e-05,  4.94493761e-05,  1.44324331e-05,
       -7.21051720e-07,  3.72286813e-05,  2.05524484e-04, -5.38719934e-04,
        1.52655790e-04, -1.10580688e-04,  3.56843084e-05, -2.19051003e-07,
       -4.80402559e-05,  8.19606605e-09, -5.26708391e-06,  1.41298588e-05,
       -5.30921342e-03,  6.97373616e-05,  1.57510280e-04, -7.45814521e-09,
       -2.17175125e-06, -1.69484956e-05,  4.25585404e-06, -1.75107889e-05,
        5.31632220e-04,  1.62951874e-05, -8.69691348e-07, -2.00882596e-07,
        4.20736469e-05,  2.24742354e-04,  1.21236835e-05,  7.29854537e-06,
        1.31889203e-04, -3.46104098e-05,  2.54657411e-04, -1.95810368e-04,
       -8.81981634e-08, -1.64770017e-06,  5.42177222e-05, -2.96715007e-06,
        8.09527410e-05,  1.46425805e-08, -8.48537866e-07, -5.39693417e-07,
       -2.75993425e-05, -1.68389015e-04,  1.59182164e-05, -1.14223731e-05],
      dtype=float32)])]}
INFO flwr 2024-05-23 14:35:16,776 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-05-23 14:35:16,777 | app.py:229 | app_fit: losses_centralized [(0, 0.00015558641163649327), (1, 0.0001510403728907187), (2, 0.00014645396271623177)]
INFO flwr 2024-05-23 14:35:16,777 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.008750295618095205), (1, 0.1903780533126119), (2, 0.33960606777255986)], 'data_size': [(0, 29599), (1, 29599), (2, 29599)]}
wandb: 
wandb: Run summary:
wandb: accuracy 0.33961
wandb:     loss 0.00015
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240523_141159-0dh1rq00
wandb: Find logs at: ./wandb/offline-run-20240523_141159-0dh1rq00/logs
2024-05-23 14:41:50.226447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-ctit081: error: *** JOB 287395 ON ctit081 CANCELLED AT 2024-05-23T14:43:00 ***
slurmstepd-ctit081: error: *** STEP 287395.3 ON ctit081 CANCELLED AT 2024-05-23T14:43:00 ***
srun: error: ctit081: task 0: Terminated
