ctit083
2024-05-20 11:47:57,598	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-05-20 11:47:57,598	INFO scripts.py:722 -- Local node IP: 10.20.240.13
2024-05-20 11:48:09,966	SUCC scripts.py:759 -- --------------------
2024-05-20 11:48:09,967	SUCC scripts.py:760 -- Ray runtime started.
2024-05-20 11:48:09,967	SUCC scripts.py:761 -- --------------------
2024-05-20 11:48:09,967	INFO scripts.py:763 -- Next steps
2024-05-20 11:48:09,967	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-05-20 11:48:09,967	INFO scripts.py:769 --   ray start --address='10.20.240.13:6379'
2024-05-20 11:48:09,967	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-05-20 11:48:09,967	INFO scripts.py:780 -- import ray
2024-05-20 11:48:09,967	INFO scripts.py:781 -- ray.init()
2024-05-20 11:48:09,967	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-05-20 11:48:09,967	INFO scripts.py:813 --   ray stop
2024-05-20 11:48:09,968	INFO scripts.py:816 -- To view the status of the cluster, use
2024-05-20 11:48:09,968	INFO scripts.py:817 --   ray status
2024-05-20 11:50:28.101102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-20 11:51:21.609243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-20 11:54:25,207 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-20 11:54:25,208 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-20 11:54:42,114 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-20 11:54:42,115 | Simulation.py:260 | Found no previously split dataloaders, splitting the data now
INFO flwr 2024-05-20 12:12:48,457 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDADAM, running...
INFO flwr 2024-05-20 12:12:48,457 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 1
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 50
			local_rounds: 8
		Model:
			optimizer_name: FedAdam
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.1, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.9999}
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-20 12:12:48,458 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-20 12:12:49,079 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-20 12:12:53,579 | Simulation.py:395 | Created 4 clients with resources 4 CPUs, 0.25 GPUs, and 4.0GB for the total available 16 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-20 12:12:53,579 | Simulation.py:160 | Starting federated learning simulation
2024-05-20 12:12:53,633	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.13:6379...
2024-05-20 12:12:53,646	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-05-20 12:12:53,661 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=50, round_timeout=None)
2024-05-20 12:12:53,720	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-05-20 12:12:53,721 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 16.0, 'node:__internal_head__': 1.0, 'object_store_memory': 79781750784.0, 'memory': 17179869184.0, 'node:10.20.240.13': 1.0}
INFO flwr 2024-05-20 12:12:53,721 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-05-20 12:12:53,721 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.25, 'memory': 4294967296}
INFO flwr 2024-05-20 12:12:53,730 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
INFO flwr 2024-05-20 12:12:53,731 | server.py:89 | Initializing global parameters
INFO flwr 2024-05-20 12:12:53,732 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-05-20 12:12:53,732 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-05-20 12:12:54,455 | server.py:94 | initial parameters (loss, other metrics): 0.00015558291578652097, {'accuracy': 0.009155714720091895, 'data_size': 29599}
INFO flwr 2024-05-20 12:12:54,455 | server.py:104 | FL starting
DEBUG flwr 2024-05-20 12:12:54,455 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=3529035)[0m 2024-05-20 12:16:42.881430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=3529035)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=3529035)[0m 2024-05-20 12:16:51.282420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=3529036)[0m 2024-05-20 12:16:42.930765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(pid=3529036)[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
ERROR flwr 2024-05-20 12:17:38,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in process_unordered_future
    if self._check_actor_fits_in_pool():
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 361, in _check_actor_fits_in_pool
    num_actors_updated = pool_size_from_resources(self.client_resources)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 155, in pool_size_from_resources
    num_cpus,
UnboundLocalError: local variable 'num_cpus' referenced before assignment

ERROR flwr 2024-05-20 12:17:38,010 | ray_client_proxy.py:162 | local variable 'num_cpus' referenced before assignment
2024-05-20 12:17:38,011	WARNING worker.py:2037 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py", line 720, in <module>
    monitor.run()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py", line 595, in run
    self._run()
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py", line 449, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

ERROR flwr 2024-05-20 12:17:38,011 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in process_unordered_future
    if self._check_actor_fits_in_pool():
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 361, in _check_actor_fits_in_pool
    num_actors_updated = pool_size_from_resources(self.client_resources)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 155, in pool_size_from_resources
    num_cpus,
UnboundLocalError: local variable 'num_cpus' referenced before assignment

ERROR flwr 2024-05-20 12:17:38,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in process_unordered_future
    if self._check_actor_fits_in_pool():
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 361, in _check_actor_fits_in_pool
    num_actors_updated = pool_size_from_resources(self.client_resources)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 155, in pool_size_from_resources
    num_cpus,
UnboundLocalError: local variable 'num_cpus' referenced before assignment

ERROR flwr 2024-05-20 12:17:38,013 | ray_client_proxy.py:162 | local variable 'num_cpus' referenced before assignment
ERROR flwr 2024-05-20 12:17:38,013 | ray_client_proxy.py:162 | local variable 'num_cpus' referenced before assignment
ERROR flwr 2024-05-20 12:17:38,014 | ray_actor.py:310 | The actor died unexpectedly before finishing this task.
	class_name: DefaultActor
	actor_id: c7cc46402a0c03fa5130f80002000000
	pid: 3529034
	namespace: 2f9a04f2-9ed2-4435-9788-5b4155e26e71
	ip: 10.20.240.13
The actor is dead because its node has died. Node Id: 29ce1052e02a8793ec8b403d3ac38e1c0f0cc24a5a70c59cc034fd25
ERROR flwr 2024-05-20 12:17:38,014 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in process_unordered_future
    if self._check_actor_fits_in_pool():
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 361, in _check_actor_fits_in_pool
    num_actors_updated = pool_size_from_resources(self.client_resources)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 155, in pool_size_from_resources
    num_cpus,
UnboundLocalError: local variable 'num_cpus' referenced before assignment

WARNING flwr 2024-05-20 12:17:38,016 | ray_actor.py:331 | Actor(c7cc46402a0c03fa5130f80002000000) will be remove from pool.
ERROR flwr 2024-05-20 12:17:38,016 | ray_client_proxy.py:162 | local variable 'num_cpus' referenced before assignment
ERROR flwr 2024-05-20 12:17:38,027 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

ERROR flwr 2024-05-20 12:17:38,027 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

ERROR flwr 2024-05-20 12:17:38,028 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

ERROR flwr 2024-05-20 12:17:38,029 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

ERROR flwr 2024-05-20 12:17:38,029 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 315, in _fetch_future_result
    raise ex
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: DefaultActor
	actor_id: c7cc46402a0c03fa5130f80002000000
	pid: 3529034
	namespace: 2f9a04f2-9ed2-4435-9788-5b4155e26e71
	ip: 10.20.240.13
The actor is dead because its node has died. Node Id: 29ce1052e02a8793ec8b403d3ac38e1c0f0cc24a5a70c59cc034fd25

ERROR flwr 2024-05-20 12:17:38,029 | ray_client_proxy.py:162 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
ERROR flwr 2024-05-20 12:17:38,030 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

ERROR flwr 2024-05-20 12:17:38,030 | ray_client_proxy.py:162 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
ERROR flwr 2024-05-20 12:17:38,031 | ray_client_proxy.py:162 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
ERROR flwr 2024-05-20 12:17:38,031 | ray_client_proxy.py:162 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
ERROR flwr 2024-05-20 12:17:38,031 | ray_client_proxy.py:162 | The actor died unexpectedly before finishing this task.
	class_name: DefaultActor
	actor_id: c7cc46402a0c03fa5130f80002000000
	pid: 3529034
	namespace: 2f9a04f2-9ed2-4435-9788-5b4155e26e71
	ip: 10.20.240.13
The actor is dead because its node has died. Node Id: 29ce1052e02a8793ec8b403d3ac38e1c0f0cc24a5a70c59cc034fd25
ERROR flwr 2024-05-20 12:17:38,032 | ray_client_proxy.py:162 | 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
DEBUG flwr 2024-05-20 12:17:38,034 | server.py:236 | fit_round 1 received 0 results and 10 failures
ERROR flwr 2024-05-20 12:17:38,038 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-05-20 12:17:38,052 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 81, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 101, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 162, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-05-20 12:17:38,053 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 4, 'num_gpus': 0.25, 'memory': 4294967296} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 4, 'num_gpus': 0.25, 'memory': 4294967296}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-05-20 12:17:38,053 | Simulation.py:183 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.00916
wandb:     loss 0.00016
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240520_121252-2rfzbuo0
wandb: Find logs at: ./wandb/offline-run-20240520_121252-2rfzbuo0/logs
[2m[36m(pid=3529036)[0m 2024-05-20 12:16:51.281948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT[32m [repeated 3x across cluster][0m
[2m[1m[36m(autoscaler +19m34s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +19m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +20m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +21m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +21m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +22m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +22m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'memory': 4294967296.0, 'GPU': 0.25}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
2024-05-20 12:18:47.666777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-20 12:18:59.259280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-20 12:19:07,758 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-20 12:19:07,762 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-20 12:19:07,831 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-20 12:19:07,833 | Simulation.py:255 | Found previously split dataloaders, loading them
INFO flwr 2024-05-20 12:19:33,310 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDAVG, running...
INFO flwr 2024-05-20 12:19:33,311 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 1
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 50
			local_rounds: 20
		Model:
			optimizer_name: FedAvg
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.1
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-20 12:19:33,324 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-20 12:19:33,337 | Config.py:77 | Found previous federated learning simulation, continuing to attack simulation...
2024-05-20 12:19:38.723880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-20 12:19:40.634314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-05-20 12:19:52,261 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-05-20 12:19:52,264 | Data.py:107 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-05-20 12:19:52,299 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-05-20 12:19:52,304 | Simulation.py:260 | Found no previously split dataloaders, splitting the data now
INFO flwr 2024-05-20 12:22:38,265 | main.py:103 | Loaded 1 configs with name PURCHASE-LOG_RES-FEDNAG, running...
INFO flwr 2024-05-20 12:22:38,265 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: purchase
			batch_size: 2
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["features"],
				    "y": element["label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 50
			local_rounds: 20
		Model:
			optimizer_name: FedNAG
			model_name: Logistic Regression
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.1
				momentum: 0.85
			model_architecture:
				type: Linear
					in_features: 600
					out_features: 100
				type: Softmax
					dim: -1
	Attack:
		data_access: all
		message_access: server
		repetitions: 10
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv3d
						out_channels: 1000
						kernel_size: [1, 1]
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-05-20 12:22:38,266 | Config.py:68 | Starting conFEDential simulation
INFO flwr 2024-05-20 12:22:38,412 | Config.py:72 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-05-20 12:22:43,105 | Simulation.py:395 | Created 4 clients with resources 4 CPUs, 0.25 GPUs, and 4.0GB for the total available 16 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-05-20 12:22:43,105 | Simulation.py:160 | Starting federated learning simulation
ERROR flwr 2024-05-20 12:22:43,157 | Simulation.py:183 | Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240520_122242-csmqo52l
wandb: Find logs at: ./wandb/offline-run-20240520_122242-csmqo52l/logs
2024-05-20 12:22:51,708	INFO scripts.py:1139 -- Did not find any active Ray processes.
