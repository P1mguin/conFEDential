ctit084
2024-06-05 11:18:46,288	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-06-05 11:18:46,289	INFO scripts.py:722 -- Local node IP: 10.20.240.14
2024-06-05 11:18:52,477	SUCC scripts.py:759 -- --------------------
2024-06-05 11:18:52,478	SUCC scripts.py:760 -- Ray runtime started.
2024-06-05 11:18:52,478	SUCC scripts.py:761 -- --------------------
2024-06-05 11:18:52,478	INFO scripts.py:763 -- Next steps
2024-06-05 11:18:52,478	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-06-05 11:18:52,478	INFO scripts.py:769 --   ray start --address='10.20.240.14:6379'
2024-06-05 11:18:52,478	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-06-05 11:18:52,478	INFO scripts.py:780 -- import ray
2024-06-05 11:18:52,478	INFO scripts.py:781 -- ray.init()
2024-06-05 11:18:52,478	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-06-05 11:18:52,478	INFO scripts.py:813 --   ray stop
2024-06-05 11:18:52,478	INFO scripts.py:816 -- To view the status of the cluster, use
2024-06-05 11:18:52,479	INFO scripts.py:817 --   ray status
2024-06-05 11:19:39.573435: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-05 11:19:50.460208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 11:20:19.492071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-05 11:22:26,107 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:22:26,108 | Data.py:109 | No preprocessed data found for the given preprocess function, preprocessing now
Using the latest cached version of the dataset since cifar100 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'cifar100' at .cache/data/cifar100/cifar100/0.0.0/aadb3af77e9048adbea6b47c21a81e47dd092ae5 (last modified on Tue Jun  4 17:10:51 2024).
INFO flwr 2024-06-05 11:29:23,690 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:29:23,691 | Simulation.py:267 | Found no previously split dataloaders, splitting the data now
INFO flwr 2024-06-05 11:38:07,227 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:07,228 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:07,939 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:07,941 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:38:16,164 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:16,176 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:16,389 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:16,390 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:38:23,573 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:23,577 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:23,807 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:23,807 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:38:30,930 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:30,934 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:31,204 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:31,205 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:38:37,912 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:37,916 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:38,150 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:38,151 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:38:45,138 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:38:45,143 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:38:45,414 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:38:45,415 | Simulation.py:267 | Found no previously split dataloaders, splitting the data now
INFO flwr 2024-06-05 11:41:09,013 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:41:09,014 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:41:09,218 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:41:09,219 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:41:16,958 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:41:16,963 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:41:17,237 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:41:17,238 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:41:24,934 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:41:24,940 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:41:25,215 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:41:25,216 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:41:33,195 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:41:33,201 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:41:33,478 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:41:33,478 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:41:39,729 | Data.py:31 | Preprocessing the federated learning dataset
INFO flwr 2024-06-05 11:41:39,736 | Data.py:104 | Found preprocessed data for the given preprocess function, returning
INFO flwr 2024-06-05 11:41:40,009 | Simulation.py:38 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-05 11:41:40,010 | Simulation.py:262 | Found previously split dataloaders, loading them
INFO flwr 2024-06-05 11:41:48,301 | main.py:103 | Loaded 12 configs with name CIFAR100-RESNET34-FEDADAM, running...
INFO flwr 2024-06-05 11:41:48,306 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 11:41:48,317 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 11:41:48,480 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 11:42:18,900 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 11:42:18,900 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 11:42:18,944	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 11:42:19,272	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 11:42:27,859 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 11:42:27,913	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 11:42:27,914 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 17179869184.0, 'accelerator_type:RTX': 1.0, 'object_store_memory': 24908054937.0, 'CPU': 2.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0}
INFO flwr 2024-06-05 11:42:27,914 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 11:42:27,915 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 11:42:28,821 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 11:42:28,823 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 11:42:28,823 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 11:42:28,823 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=1001823)[0m 2024-06-05 11:42:40.782513: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1001823)[0m 2024-06-05 11:42:41.808815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1001823)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2024-06-05 11:42:44,466 | server.py:94 | initial parameters (loss, other metrics): 0.0005039959907531739, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-05 11:42:44,467 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 11:42:44,467 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1001823)[0m 2024-06-05 11:43:04.680827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 11:47:37,951 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 11:51:14,420 | server.py:125 | fit progress: (1, 0.0012388747215270996, {'accuracy': 0.0107, 'data_size': 10000}, 509.95334223425016)
INFO flwr 2024-06-05 11:51:14,421 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 11:51:14,422 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 11:53:48,606 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 11:54:07,246 | server.py:125 | fit progress: (2, 0.000496140718460083, {'accuracy': 0.0103, 'data_size': 10000}, 682.778661143966)
INFO flwr 2024-06-05 11:54:07,246 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 11:54:07,247 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 11:56:44,351 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 11:57:17,822 | server.py:125 | fit progress: (3, 0.0005454538822174072, {'accuracy': 0.0103, 'data_size': 10000}, 873.355309819337)
INFO flwr 2024-06-05 11:57:17,823 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 11:57:17,823 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 11:59:55,789 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-05 12:00:35,099 | server.py:125 | fit progress: (4, 0.0005027716636657715, {'accuracy': 0.0116, 'data_size': 10000}, 1070.6323393103667)
INFO flwr 2024-06-05 12:00:35,100 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 12:00:35,101 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:03:10,694 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-05 12:03:56,409 | server.py:125 | fit progress: (5, 0.0004740520477294922, {'accuracy': 0.0112, 'data_size': 10000}, 1271.9419775241986)
INFO flwr 2024-06-05 12:03:56,410 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-05 12:03:56,410 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:06:31,142 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-05 12:07:01,486 | server.py:125 | fit progress: (6, 0.0004670713901519775, {'accuracy': 0.0101, 'data_size': 10000}, 1457.018514088355)
INFO flwr 2024-06-05 12:07:01,486 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-05 12:07:01,486 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:09:37,251 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-05 12:10:52,976 | server.py:125 | fit progress: (7, 0.0004641904354095459, {'accuracy': 0.01, 'data_size': 10000}, 1688.5087111690082)
INFO flwr 2024-06-05 12:10:52,976 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-05 12:10:52,976 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:13:27,957 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-05 12:13:59,367 | server.py:125 | fit progress: (8, 0.0004662651538848877, {'accuracy': 0.0101, 'data_size': 10000}, 1874.9001490832306)
INFO flwr 2024-06-05 12:13:59,368 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-05 12:13:59,368 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:16:35,149 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-06-05 12:17:40,402 | server.py:125 | fit progress: (9, 0.00046806859970092773, {'accuracy': 0.0099, 'data_size': 10000}, 2095.9346235492267)
INFO flwr 2024-06-05 12:17:40,402 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-05 12:17:40,403 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:20:13,491 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-06-05 12:20:47,551 | server.py:125 | fit progress: (10, 0.00046689434051513674, {'accuracy': 0.0101, 'data_size': 10000}, 2283.0844882773235)
INFO flwr 2024-06-05 12:20:47,552 | server.py:171 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2024-06-05 12:20:47,552 | server.py:222 | fit_round 11: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:23:23,053 | server.py:236 | fit_round 11 received 10 results and 0 failures
INFO flwr 2024-06-05 12:24:03,318 | server.py:125 | fit progress: (11, 0.00046914634704589846, {'accuracy': 0.01, 'data_size': 10000}, 2478.850628259126)
INFO flwr 2024-06-05 12:24:03,318 | server.py:171 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2024-06-05 12:24:03,318 | server.py:222 | fit_round 12: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:26:35,080 | server.py:236 | fit_round 12 received 10 results and 0 failures
INFO flwr 2024-06-05 12:27:09,352 | server.py:125 | fit progress: (12, 0.00048637275695800783, {'accuracy': 0.0103, 'data_size': 10000}, 2664.8845362612046)
INFO flwr 2024-06-05 12:27:09,352 | server.py:171 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2024-06-05 12:27:09,352 | server.py:222 | fit_round 13: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:29:43,696 | server.py:236 | fit_round 13 received 10 results and 0 failures
INFO flwr 2024-06-05 12:30:22,793 | server.py:125 | fit progress: (13, 0.0005205715179443359, {'accuracy': 0.01, 'data_size': 10000}, 2858.3263821741566)
INFO flwr 2024-06-05 12:30:22,794 | server.py:171 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2024-06-05 12:30:22,794 | server.py:222 | fit_round 14: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:32:57,654 | server.py:236 | fit_round 14 received 10 results and 0 failures
INFO flwr 2024-06-05 12:33:41,079 | server.py:125 | fit progress: (14, 0.0005645780563354493, {'accuracy': 0.0102, 'data_size': 10000}, 3056.611531239003)
INFO flwr 2024-06-05 12:33:41,079 | server.py:171 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2024-06-05 12:33:41,080 | server.py:222 | fit_round 15: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:36:15,972 | server.py:236 | fit_round 15 received 10 results and 0 failures
INFO flwr 2024-06-05 12:36:44,302 | server.py:125 | fit progress: (15, 0.0005976134300231934, {'accuracy': 0.0092, 'data_size': 10000}, 3239.8347725220956)
INFO flwr 2024-06-05 12:36:44,302 | server.py:171 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2024-06-05 12:36:44,303 | server.py:222 | fit_round 16: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:39:18,567 | server.py:236 | fit_round 16 received 10 results and 0 failures
INFO flwr 2024-06-05 12:39:56,114 | server.py:125 | fit progress: (16, 0.0006069337844848633, {'accuracy': 0.0095, 'data_size': 10000}, 3431.646728152409)
INFO flwr 2024-06-05 12:39:56,114 | server.py:171 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2024-06-05 12:39:56,115 | server.py:222 | fit_round 17: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:42:29,054 | server.py:236 | fit_round 17 received 10 results and 0 failures
INFO flwr 2024-06-05 12:43:03,703 | server.py:125 | fit progress: (17, 0.0006007436752319336, {'accuracy': 0.0094, 'data_size': 10000}, 3619.236488183029)
INFO flwr 2024-06-05 12:43:03,704 | server.py:171 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2024-06-05 12:43:03,704 | server.py:222 | fit_round 18: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:45:38,129 | server.py:236 | fit_round 18 received 10 results and 0 failures
INFO flwr 2024-06-05 12:46:13,169 | server.py:125 | fit progress: (18, 0.0005815980434417724, {'accuracy': 0.0101, 'data_size': 10000}, 3808.701725547202)
INFO flwr 2024-06-05 12:46:13,169 | server.py:171 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2024-06-05 12:46:13,170 | server.py:222 | fit_round 19: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:48:48,166 | server.py:236 | fit_round 19 received 10 results and 0 failures
INFO flwr 2024-06-05 12:49:15,833 | server.py:125 | fit progress: (19, 0.0005642912864685059, {'accuracy': 0.0101, 'data_size': 10000}, 3991.365719963331)
INFO flwr 2024-06-05 12:49:15,833 | server.py:171 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2024-06-05 12:49:15,834 | server.py:222 | fit_round 20: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 12:51:48,092 | server.py:236 | fit_round 20 received 10 results and 0 failures
INFO flwr 2024-06-05 12:52:23,195 | server.py:125 | fit progress: (20, 0.0005517903327941895, {'accuracy': 0.0101, 'data_size': 10000}, 4178.727679630276)
INFO flwr 2024-06-05 12:52:23,195 | server.py:171 | evaluate_round 20: no clients selected, cancel
INFO flwr 2024-06-05 12:52:23,195 | server.py:153 | FL finished in 4178.728513575159
INFO flwr 2024-06-05 12:52:23,212 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-06-05 12:52:23,213 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-06-05 12:52:23,213 | app.py:229 | app_fit: losses_centralized [(0, 0.0005039959907531739), (1, 0.0012388747215270996), (2, 0.000496140718460083), (3, 0.0005454538822174072), (4, 0.0005027716636657715), (5, 0.0004740520477294922), (6, 0.0004670713901519775), (7, 0.0004641904354095459), (8, 0.0004662651538848877), (9, 0.00046806859970092773), (10, 0.00046689434051513674), (11, 0.00046914634704589846), (12, 0.00048637275695800783), (13, 0.0005205715179443359), (14, 0.0005645780563354493), (15, 0.0005976134300231934), (16, 0.0006069337844848633), (17, 0.0006007436752319336), (18, 0.0005815980434417724), (19, 0.0005642912864685059), (20, 0.0005517903327941895)]
INFO flwr 2024-06-05 12:52:23,213 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0116), (1, 0.0107), (2, 0.0103), (3, 0.0103), (4, 0.0116), (5, 0.0112), (6, 0.0101), (7, 0.01), (8, 0.0101), (9, 0.0099), (10, 0.0101), (11, 0.01), (12, 0.0103), (13, 0.01), (14, 0.0102), (15, 0.0092), (16, 0.0095), (17, 0.0094), (18, 0.0101), (19, 0.0101), (20, 0.0101)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000), (11, 10000), (12, 10000), (13, 10000), (14, 10000), (15, 10000), (16, 10000), (17, 10000), (18, 10000), (19, 10000), (20, 10000)]}
INFO flwr 2024-06-05 12:52:23,766 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 12:52:23,767 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 12:52:23,885 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
INFO flwr 2024-06-05 12:52:24,424 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 12:52:24,425 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 12:52:24,470	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 12:52:24,476	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 12:52:24,491 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 12:52:24,528	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 12:52:24,529 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'object_store_memory': 24908054937.0, 'memory': 17179869184.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-06-05 12:52:24,529 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 12:52:24,530 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 12:52:24,539 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 12:52:24,540 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 12:52:24,541 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 12:52:24,541 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 12:52:25,294 | server.py:94 | initial parameters (loss, other metrics): 0.0005021286487579345, {'accuracy': 0.0088, 'data_size': 10000}
INFO flwr 2024-06-05 12:52:25,294 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 12:52:25,295 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1052599)[0m 2024-06-05 12:53:11.946978: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1052599)[0m 2024-06-05 12:53:12.036388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1052599)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1052599)[0m 2024-06-05 12:53:40.346578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 12:58:13,108 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 13:00:17,831 | server.py:125 | fit progress: (1, 0.0005198300838470459, {'accuracy': 0.0114, 'data_size': 10000}, 472.53654003189877)
INFO flwr 2024-06-05 13:00:17,832 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 13:00:17,832 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:02:51,845 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 13:03:12,642 | server.py:125 | fit progress: (2, 0.00046417994499206545, {'accuracy': 0.0095, 'data_size': 10000}, 647.3469464727677)
INFO flwr 2024-06-05 13:03:12,642 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 13:03:12,642 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:05:50,679 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 13:06:14,453 | server.py:125 | fit progress: (3, 0.0004577149391174316, {'accuracy': 0.0223, 'data_size': 10000}, 829.1584727521986)
INFO flwr 2024-06-05 13:06:14,454 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 13:06:14,454 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:08:51,281 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-05 13:09:24,259 | server.py:125 | fit progress: (4, 0.0004417734146118164, {'accuracy': 0.0263, 'data_size': 10000}, 1018.9639077167958)
INFO flwr 2024-06-05 13:09:24,259 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 13:09:24,260 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:11:56,869 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-05 13:12:29,588 | server.py:125 | fit progress: (5, 0.00043357129096984864, {'accuracy': 0.0278, 'data_size': 10000}, 1204.29301515501)
INFO flwr 2024-06-05 13:12:29,588 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-05 13:12:29,589 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:15:03,877 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-05 13:15:49,738 | server.py:125 | fit progress: (6, 0.00042912449836730957, {'accuracy': 0.0344, 'data_size': 10000}, 1404.443246700801)
INFO flwr 2024-06-05 13:15:49,738 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-05 13:15:49,739 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:18:26,428 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-05 13:18:55,327 | server.py:125 | fit progress: (7, 0.0004271974563598633, {'accuracy': 0.042, 'data_size': 10000}, 1590.0320631577633)
INFO flwr 2024-06-05 13:18:55,327 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-05 13:18:55,327 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:21:27,397 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-05 13:22:02,917 | server.py:125 | fit progress: (8, 0.0004223899841308594, {'accuracy': 0.0465, 'data_size': 10000}, 1777.6219222391956)
INFO flwr 2024-06-05 13:22:02,917 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-05 13:22:02,917 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:24:39,181 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-06-05 13:25:07,642 | server.py:125 | fit progress: (9, 0.0004221391201019287, {'accuracy': 0.0418, 'data_size': 10000}, 1962.346968786791)
INFO flwr 2024-06-05 13:25:07,642 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-05 13:25:07,642 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:27:43,222 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-06-05 13:28:13,793 | server.py:125 | fit progress: (10, 0.0004243398666381836, {'accuracy': 0.0471, 'data_size': 10000}, 2148.4986941120587)
INFO flwr 2024-06-05 13:28:13,794 | server.py:171 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2024-06-05 13:28:13,794 | server.py:222 | fit_round 11: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:30:45,582 | server.py:236 | fit_round 11 received 10 results and 0 failures
INFO flwr 2024-06-05 13:31:25,042 | server.py:125 | fit progress: (11, 0.0004243566989898682, {'accuracy': 0.0527, 'data_size': 10000}, 2339.746992073953)
INFO flwr 2024-06-05 13:31:25,042 | server.py:171 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2024-06-05 13:31:25,043 | server.py:222 | fit_round 12: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:33:59,334 | server.py:236 | fit_round 12 received 10 results and 0 failures
INFO flwr 2024-06-05 13:34:27,374 | server.py:125 | fit progress: (12, 0.0004208687782287598, {'accuracy': 0.0543, 'data_size': 10000}, 2522.0797680858523)
INFO flwr 2024-06-05 13:34:27,375 | server.py:171 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2024-06-05 13:34:27,432 | server.py:222 | fit_round 13: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:36:58,589 | server.py:236 | fit_round 13 received 10 results and 0 failures
INFO flwr 2024-06-05 13:37:46,655 | server.py:125 | fit progress: (13, 0.000421565055847168, {'accuracy': 0.0485, 'data_size': 10000}, 2721.3604785697535)
INFO flwr 2024-06-05 13:37:46,656 | server.py:171 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2024-06-05 13:37:46,656 | server.py:222 | fit_round 14: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:40:21,001 | server.py:236 | fit_round 14 received 10 results and 0 failures
INFO flwr 2024-06-05 13:40:55,106 | server.py:125 | fit progress: (14, 0.0004195662021636963, {'accuracy': 0.0514, 'data_size': 10000}, 2909.811011339072)
INFO flwr 2024-06-05 13:40:55,106 | server.py:171 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2024-06-05 13:40:55,107 | server.py:222 | fit_round 15: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:43:29,232 | server.py:236 | fit_round 15 received 10 results and 0 failures
INFO flwr 2024-06-05 13:44:06,932 | server.py:125 | fit progress: (15, 0.00041712517738342285, {'accuracy': 0.056, 'data_size': 10000}, 3101.637624195777)
INFO flwr 2024-06-05 13:44:06,933 | server.py:171 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2024-06-05 13:44:06,933 | server.py:222 | fit_round 16: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:46:39,711 | server.py:236 | fit_round 16 received 10 results and 0 failures
INFO flwr 2024-06-05 13:47:51,571 | server.py:125 | fit progress: (16, 0.0004121912956237793, {'accuracy': 0.0633, 'data_size': 10000}, 3326.2767977537587)
INFO flwr 2024-06-05 13:47:51,572 | server.py:171 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2024-06-05 13:47:51,572 | server.py:222 | fit_round 17: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:50:24,774 | server.py:236 | fit_round 17 received 10 results and 0 failures
INFO flwr 2024-06-05 13:51:34,185 | server.py:125 | fit progress: (17, 0.00041389126777648924, {'accuracy': 0.0685, 'data_size': 10000}, 3548.8903347421438)
INFO flwr 2024-06-05 13:51:34,185 | server.py:171 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2024-06-05 13:51:34,199 | server.py:222 | fit_round 18: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 13:54:09,162 | server.py:236 | fit_round 18 received 10 results and 0 failures
INFO flwr 2024-06-05 13:56:27,254 | server.py:125 | fit progress: (18, 0.00040973119735717773, {'accuracy': 0.0754, 'data_size': 10000}, 3841.9591498370282)
INFO flwr 2024-06-05 13:56:27,254 | server.py:171 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2024-06-05 13:56:27,255 | server.py:222 | fit_round 19: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 13:57:50,373 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:57:50,383 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:57:50,394 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:57:50,399 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-05 13:57:52,694 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:58:03,257 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:58:03,257 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:58:03,272 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:58:03,298 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:58:11,761 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:58:11,761 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:58:11,761 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:58:11,762 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 13:58:11,788 | server.py:236 | fit_round 19 received 4 results and 6 failures
INFO flwr 2024-06-05 13:58:42,686 | server.py:125 | fit progress: (19, 0.0004079054832458496, {'accuracy': 0.0668, 'data_size': 10000}, 3977.3914288240485)
INFO flwr 2024-06-05 13:58:42,687 | server.py:171 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2024-06-05 13:58:42,687 | server.py:222 | fit_round 20: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 13:59:25,957 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:25,958 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:25,958 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:25,965 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:25,972 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:25,972 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:25,993 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:26,061 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:26,136 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:26,177 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:39,160 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:39,161 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:39,189 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:39,199 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:54,404 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:54,406 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:54,405 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:54,407 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 13:59:54,405 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 13:59:54,409 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6fa324117630288813eb0b7902000000, name=DefaultActor.__init__, pid=1052599, memory used=3.60GB) was running was 238.94GB / 251.51GB (0.950019), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-ef46a7d3315f0eabeb4f1c2ef7606e1213072a4a5000a8e54d2ed4cb*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.61	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1052599	3.60	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 13:59:54,432 | server.py:236 | fit_round 20 received 0 results and 10 failures
ERROR flwr 2024-06-05 13:59:54,500 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 13:59:55,100 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 13:59:55,101 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 13:59:55,102 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0668
wandb:     loss 0.00041
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_114212-lzdczu9s
wandb: Find logs at: ./wandb/offline-run-20240605_114212-lzdczu9s/logs
INFO flwr 2024-06-05 13:59:59,197 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 13:59:59,201 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 13:59:59,261 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 14:00:19,495 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 14:00:19,496 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 14:00:19,550	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 14:00:19,554	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 14:00:19,571 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 14:00:19,609	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 14:00:19,610 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 17179869184.0, 'accelerator_type:RTX': 1.0, 'node:__internal_head__': 1.0, 'CPU': 2.0, 'node:10.20.240.14': 1.0, 'GPU': 1.0, 'object_store_memory': 24908054937.0}
INFO flwr 2024-06-05 14:00:19,610 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 14:00:19,610 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 14:00:19,619 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 14:00:19,620 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 14:00:19,621 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 14:00:19,621 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 14:00:20,420 | server.py:94 | initial parameters (loss, other metrics): 0.0004987221240997314, {'accuracy': 0.0098, 'data_size': 10000}
INFO flwr 2024-06-05 14:00:20,420 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 14:00:20,421 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1104048)[0m 2024-06-05 14:01:46.938075: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1104048)[0m 2024-06-05 14:01:47.029551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1104048)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1104048)[0m 2024-06-05 14:02:44.744477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 14:10:44,945 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 14:18:11,626 | server.py:125 | fit progress: (1, 0.0018381824493408202, {'accuracy': 0.0124, 'data_size': 10000}, 1071.2052018409595)
INFO flwr 2024-06-05 14:18:11,627 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 14:18:11,627 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 14:20:43,833 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 14:22:02,415 | server.py:125 | fit progress: (2, 0.0016087959289550781, {'accuracy': 0.0101, 'data_size': 10000}, 1301.9940305030905)
INFO flwr 2024-06-05 14:22:02,415 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 14:22:02,415 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 14:24:02,216 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:24:02,230 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:24:08,578 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:24:08,579 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:24:08,580 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:24:08,579 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 14:24:08,606 | server.py:236 | fit_round 3 received 7 results and 3 failures
[2m[33m(raylet)[0m [2024-06-05 14:24:52,747 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-05 14:25:07,090 | server.py:125 | fit progress: (3, 0.0010016497611999512, {'accuracy': 0.0107, 'data_size': 10000}, 1486.6693184701726)
INFO flwr 2024-06-05 14:25:07,090 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 14:25:07,091 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 14:25:42,937 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:42,954 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:42,961 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:25:43,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:25:51,591 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:51,591 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:25:51,593 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:51,593 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:25:51,594 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:51,599 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:25:58,533 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:25:58,534 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:26:05,522 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:26:05,523 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:26:05,544 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:26:05,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:26:21,257 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:26:21,257 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:26:21,258 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:26:21,259 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 54dbc7883b6fd32575e908f503000000, name=DefaultActor.__init__, pid=1104048, memory used=3.71GB) was running was 239.25GB / 251.51GB (0.951242), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-37fe7419e661dfcbfdc719e44240289a1e4280053ca47762cad6e5a7*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	18.86	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1104048	3.71	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
1001731	0.06	wandb-service(2-998775-s-47831)
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 14:26:21,288 | server.py:236 | fit_round 4 received 0 results and 10 failures
ERROR flwr 2024-06-05 14:26:21,348 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 14:26:21,451 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 14:26:21,452 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 14:26:21,453 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0107
wandb:     loss 0.001
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_135959-5qy6dasg
wandb: Find logs at: ./wandb/offline-run-20240605_135959-5qy6dasg/logs
INFO flwr 2024-06-05 14:26:25,161 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 14:26:25,163 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 14:26:25,299 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 14:26:35,844 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 14:26:35,844 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 14:26:35,891	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 14:26:35,896	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 14:26:35,908 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 14:26:35,948	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 14:26:35,949 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:10.20.240.14': 1.0, 'GPU': 1.0, 'object_store_memory': 24908054937.0, 'memory': 17179869184.0, 'accelerator_type:RTX': 1.0, 'node:__internal_head__': 1.0, 'CPU': 2.0}
INFO flwr 2024-06-05 14:26:35,949 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 14:26:35,949 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 14:26:35,959 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 14:26:35,960 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 14:26:35,960 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 14:26:35,960 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 14:26:36,811 | server.py:94 | initial parameters (loss, other metrics): 0.0004961407661437989, {'accuracy': 0.0089, 'data_size': 10000}
INFO flwr 2024-06-05 14:26:36,812 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 14:26:36,812 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1115448)[0m 2024-06-05 14:26:59.944371: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1115448)[0m 2024-06-05 14:27:00.005209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1115448)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1115448)[0m 2024-06-05 14:27:02.479276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 14:30:13,337 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 14:36:25,674 | server.py:125 | fit progress: (1, 0.0005313389778137207, {'accuracy': 0.0155, 'data_size': 10000}, 588.861793519929)
INFO flwr 2024-06-05 14:36:25,675 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 14:36:25,675 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 14:39:01,084 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 14:39:51,652 | server.py:125 | fit progress: (2, 0.0004971821308135987, {'accuracy': 0.021, 'data_size': 10000}, 794.8399228178896)
INFO flwr 2024-06-05 14:39:51,653 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 14:39:51,653 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 14:42:23,027 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 14:42:59,972 | server.py:125 | fit progress: (3, 0.0004670738697052002, {'accuracy': 0.0295, 'data_size': 10000}, 983.1594322090968)
INFO flwr 2024-06-05 14:42:59,972 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 14:42:59,972 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 14:45:29,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:45:29,538 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 14:45:29,560 | server.py:236 | fit_round 4 received 9 results and 1 failures
[2m[33m(raylet)[0m [2024-06-05 14:45:52,781 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-05 14:45:54,193 | server.py:125 | fit progress: (4, 0.0004499779224395752, {'accuracy': 0.0395, 'data_size': 10000}, 1157.3811381589621)
INFO flwr 2024-06-05 14:45:54,194 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 14:45:54,194 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 14:46:07,767 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:07,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:16,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:16,347 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:23,109 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:23,110 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:38,891 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:38,891 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:38,891 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:38,892 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:52,893 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:52,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:59,306 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:59,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:46:59,307 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:46:59,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:47:08,402 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:47:08,402 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:47:08,404 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:47:08,404 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: c8cc1a071ea43790999ad58404000000, name=DefaultActor.__init__, pid=1115448, memory used=3.71GB) was running was 239.08GB / 251.51GB (0.950596), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-a7a53a58ebb5bacc08b668a6336a19e5313d71f05b959813fdafcfc9*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.44	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1115448	3.71	ray::DefaultActor.run
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 14:47:08,428 | server.py:236 | fit_round 5 received 0 results and 10 failures
ERROR flwr 2024-06-05 14:47:08,448 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 14:47:08,564 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 14:47:08,565 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 14:47:08,566 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0395
wandb:     loss 0.00045
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_142625-rtba6qem
wandb: Find logs at: ./wandb/offline-run-20240605_142625-rtba6qem/logs
INFO flwr 2024-06-05 14:47:12,386 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.001}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 14:47:12,389 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 14:47:12,417 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 14:47:18,721 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 14:47:18,722 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 14:47:18,768	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 14:47:18,772	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 14:47:18,786 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 14:47:18,823	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 14:47:18,823 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 24908054937.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'memory': 17179869184.0, 'accelerator_type:RTX': 1.0, 'CPU': 2.0}
INFO flwr 2024-06-05 14:47:18,824 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 14:47:18,824 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 14:47:18,832 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 14:47:18,833 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 14:47:18,833 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 14:47:18,833 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 14:47:19,606 | server.py:94 | initial parameters (loss, other metrics): 0.0004988436222076416, {'accuracy': 0.0109, 'data_size': 10000}
INFO flwr 2024-06-05 14:47:19,607 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 14:47:19,607 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1127667)[0m 2024-06-05 14:47:34.324745: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1127667)[0m 2024-06-05 14:47:34.413407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1127667)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1127667)[0m 2024-06-05 14:47:40.895208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 14:50:57,598 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 14:53:25,075 | server.py:125 | fit progress: (1, 0.0007730506420135498, {'accuracy': 0.0134, 'data_size': 10000}, 365.4681945107877)
INFO flwr 2024-06-05 14:53:25,076 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 14:53:25,097 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 14:55:55,932 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 14:56:56,986 | server.py:125 | fit progress: (2, 0.0008657674789428711, {'accuracy': 0.0135, 'data_size': 10000}, 577.3788040718064)
INFO flwr 2024-06-05 14:56:56,986 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 14:56:56,986 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 14:59:14,592 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:59:14,593 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 14:59:14,592 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 14:59:14,594 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 14:59:14,609 | server.py:236 | fit_round 3 received 8 results and 2 failures
[2m[33m(raylet)[0m [2024-06-05 14:59:52,803 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-05 14:59:52,908 | server.py:125 | fit progress: (3, 0.0010344503402709962, {'accuracy': 0.0143, 'data_size': 10000}, 753.3006440000609)
INFO flwr 2024-06-05 14:59:52,908 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 14:59:52,908 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:00:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:00:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:00:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:00:29,598 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:00:29,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:00:29,570 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:00:29,544 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:00:29,722 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:00:38,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:00:38,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:01:01,514 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:01:01,519 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:01:01,520 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:01:01,525 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:01:08,505 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:01:08,506 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:01:08,506 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:01:08,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:01:08,507 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:01:08,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: f049ed0756579ec14ddaf0d605000000, name=DefaultActor.__init__, pid=1127667, memory used=3.67GB) was running was 238.99GB / 251.51GB (0.950208), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-0efb3e8a515bb44af27a895064b377c9fd5d42b11e115fcad026fd1d*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1127667	3.67	ray::DefaultActor
998565	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:01:08,530 | server.py:236 | fit_round 4 received 0 results and 10 failures
ERROR flwr 2024-06-05 15:01:08,538 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 15:01:08,541 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 15:01:08,541 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 15:01:08,541 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0143
wandb:     loss 0.00103
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_144712-7wngea1q
wandb: Find logs at: ./wandb/offline-run-20240605_144712-7wngea1q/logs
INFO flwr 2024-06-05 15:01:12,217 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 32
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.001}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 15:01:12,219 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 15:01:12,264 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 15:01:13,813 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 15:01:13,813 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 15:01:13,850	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 15:01:13,856	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 15:01:13,868 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 15:01:13,905	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 15:01:13,906 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 24908054937.0, 'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:RTX': 1.0}
INFO flwr 2024-06-05 15:01:13,906 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 15:01:13,907 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 15:01:13,918 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 15:01:13,919 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 15:01:13,920 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 15:01:13,920 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 15:01:14,831 | server.py:94 | initial parameters (loss, other metrics): 0.0005034099102020264, {'accuracy': 0.0098, 'data_size': 10000}
INFO flwr 2024-06-05 15:01:14,831 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 15:01:14,831 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1136623)[0m 2024-06-05 15:01:16.438306: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1136623)[0m 2024-06-05 15:01:16.535896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1136623)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1136623)[0m 2024-06-05 15:01:19.151097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 15:04:40,500 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 15:06:42,944 | server.py:125 | fit progress: (1, 0.0004815310955047607, {'accuracy': 0.0115, 'data_size': 10000}, 328.11260507581756)
INFO flwr 2024-06-05 15:06:42,945 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 15:06:42,945 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
[2m[33m(raylet)[0m [2024-06-05 15:07:52,813 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:09,812 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:09,823 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:09,828 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:09,828 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:09,828 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:09,864 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:09,885 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:09,905 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:16,165 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:16,165 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:23,141 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:23,141 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:08:23,143 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:08:23,143 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:08:23,179 | server.py:236 | fit_round 2 received 3 results and 7 failures
INFO flwr 2024-06-05 15:08:35,314 | server.py:125 | fit progress: (2, 0.00048450241088867187, {'accuracy': 0.0176, 'data_size': 10000}, 440.4829931608401)
INFO flwr 2024-06-05 15:08:35,315 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 15:08:35,315 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:09:14,318 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:14,413 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:30,295 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:30,295 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:30,296 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:30,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:30,297 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:30,297 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:30,298 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:30,298 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:30,299 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:30,299 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:37,595 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:37,596 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:44,619 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:44,620 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:53,689 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:53,689 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:09:53,690 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:09:53,691 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 74f5d893f081f3e1a4e2c35b06000000, name=DefaultActor.__init__, pid=1136623, memory used=3.71GB) was running was 238.95GB / 251.51GB (0.950064), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-59651ea304933180e839af6d0fda2e20c3bd6862c03557fbb9dd9502*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.11	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1136623	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:09:53,699 | server.py:236 | fit_round 3 received 0 results and 10 failures
ERROR flwr 2024-06-05 15:09:53,700 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 15:09:53,701 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 15:09:53,702 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 15:09:53,702 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0176
wandb:     loss 0.00048
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_150112-21cpxz2o
wandb: Find logs at: ./wandb/offline-run-20240605_150112-21cpxz2o/logs
INFO flwr 2024-06-05 15:09:57,420 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 15:09:57,421 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 15:09:57,439 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 15:09:58,888 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 15:09:58,888 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 15:09:58,934	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 15:09:58,938	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 15:09:58,951 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 15:09:58,989	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 15:09:58,990 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 24908054937.0, 'memory': 17179869184.0, 'CPU': 2.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:RTX': 1.0}
INFO flwr 2024-06-05 15:09:58,990 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 15:09:58,991 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 15:09:58,999 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 15:09:59,000 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 15:09:59,000 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 15:09:59,000 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 15:09:59,851 | server.py:94 | initial parameters (loss, other metrics): 0.000504190731048584, {'accuracy': 0.0094, 'data_size': 10000}
INFO flwr 2024-06-05 15:09:59,852 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 15:09:59,852 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1142842)[0m 2024-06-05 15:10:01.897524: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1142842)[0m 2024-06-05 15:10:01.994648: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1142842)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1142842)[0m 2024-06-05 15:10:04.329504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 15:12:41,921 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 15:14:57,018 | server.py:125 | fit progress: (1, 0.0015493685722351074, {'accuracy': 0.01, 'data_size': 10000}, 297.1659712018445)
INFO flwr 2024-06-05 15:14:57,018 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 15:14:57,019 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 15:17:28,287 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 15:17:58,475 | server.py:125 | fit progress: (2, 0.001765117073059082, {'accuracy': 0.0113, 'data_size': 10000}, 478.62306176079437)
INFO flwr 2024-06-05 15:17:58,475 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 15:17:58,476 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 15:20:28,070 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 15:21:02,484 | server.py:125 | fit progress: (3, 0.0005861918926239013, {'accuracy': 0.0115, 'data_size': 10000}, 662.6322558657266)
INFO flwr 2024-06-05 15:21:02,485 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 15:21:02,485 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:22:19,412 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:19,423 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:22:33,764 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:33,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:22:42,252 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:42,253 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:22:42,253 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:42,254 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:22:48,700 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:48,701 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:22:48,702 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:22:48,702 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:22:48,727 | server.py:236 | fit_round 4 received 4 results and 6 failures
[2m[33m(raylet)[0m [2024-06-05 15:22:52,834 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-05 15:23:03,923 | server.py:125 | fit progress: (4, 0.0005731534004211426, {'accuracy': 0.0096, 'data_size': 10000}, 784.0707756979391)
INFO flwr 2024-06-05 15:23:03,923 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 15:23:03,923 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:24:05,490 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:05,587 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,505 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,507 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,507 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,508 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,508 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,509 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,509 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,509 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:12,515 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,569 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,596 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,623 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:12,723 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:21,482 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:21,482 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:24:21,483 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:24:21,483 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 373a1a68b7a7383992d0350f07000000, name=DefaultActor.__init__, pid=1142842, memory used=3.71GB) was running was 239.22GB / 251.51GB (0.951127), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-47724832e439deedd56fa84ee8df8978522880962af7b438a358dd00*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.43	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1142842	3.71	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:24:21,491 | server.py:236 | fit_round 5 received 0 results and 10 failures
ERROR flwr 2024-06-05 15:24:21,510 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 15:24:21,541 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 15:24:21,541 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 15:24:21,542 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0096
wandb:     loss 0.00057
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_150957-8kjkx12p
wandb: Find logs at: ./wandb/offline-run-20240605_150957-8kjkx12p/logs
INFO flwr 2024-06-05 15:24:25,323 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.1}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 15:24:25,325 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 15:24:25,370 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 15:24:26,214 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 15:24:26,214 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 15:24:26,251	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 15:24:26,256	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 15:24:26,269 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 15:24:26,306	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 15:24:26,307 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'object_store_memory': 24908054937.0, 'accelerator_type:RTX': 1.0, 'memory': 17179869184.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-06-05 15:24:26,308 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 15:24:26,308 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 15:24:26,317 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 15:24:26,318 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 15:24:26,318 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 15:24:26,318 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 15:24:27,149 | server.py:94 | initial parameters (loss, other metrics): 0.0005011575222015381, {'accuracy': 0.011, 'data_size': 10000}
INFO flwr 2024-06-05 15:24:27,150 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 15:24:27,150 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1153462)[0m 2024-06-05 15:24:28.569829: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1153462)[0m 2024-06-05 15:24:28.673606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1153462)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1153462)[0m 2024-06-05 15:24:30.699593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 15:27:11,112 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 15:29:11,238 | server.py:125 | fit progress: (1, 0.0005506963729858398, {'accuracy': 0.0096, 'data_size': 10000}, 284.08827881282195)
INFO flwr 2024-06-05 15:29:11,239 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 15:29:11,239 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:30:40,529 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:30:40,540 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-05 15:30:52,844 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:03,654 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:03,654 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:03,655 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:03,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:03,656 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:03,656 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:03,656 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:03,657 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:03,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:03,657 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:31:03,686 | server.py:236 | fit_round 2 received 4 results and 6 failures
INFO flwr 2024-06-05 15:31:17,901 | server.py:125 | fit progress: (2, 0.0004764719009399414, {'accuracy': 0.0137, 'data_size': 10000}, 410.7514030979946)
INFO flwr 2024-06-05 15:31:17,902 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 15:31:17,902 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:31:58,594 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:58,600 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:58,600 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:58,600 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:58,599 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:31:58,632 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:58,661 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:31:58,690 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:20,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:20,545 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:29,511 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:29,522 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:36,609 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:36,609 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:36,613 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:36,613 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:36,614 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:36,614 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:32:36,614 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:32:36,615 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 6b96017b97f445018267ccf308000000, name=DefaultActor.__init__, pid=1153462, memory used=3.69GB) was running was 239.11GB / 251.51GB (0.950703), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-d3368ffe8abe1c223ca79eb664488b68992ef481be850d2398e28475*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.16	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1153462	3.69	ray::DefaultActor.run
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:32:36,628 | server.py:236 | fit_round 3 received 0 results and 10 failures
ERROR flwr 2024-06-05 15:32:36,629 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 15:32:36,631 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 15:32:36,632 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 15:32:36,632 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0137
wandb:     loss 0.00048
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_152425-gtf91uan
wandb: Find logs at: ./wandb/offline-run-20240605_152425-gtf91uan/logs
INFO flwr 2024-06-05 15:32:40,430 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 15:32:40,432 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 15:32:40,478 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 15:32:41,137 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 15:32:41,137 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 15:32:41,175	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 15:32:41,179	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 15:32:41,213 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 15:32:41,250	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 15:32:41,251 | app.py:213 | Flower VCE: Ray initialized with resources: {'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'node:10.20.240.14': 1.0, 'CPU': 2.0, 'memory': 17179869184.0, 'object_store_memory': 24908054937.0, 'node:__internal_head__': 1.0}
INFO flwr 2024-06-05 15:32:41,252 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 15:32:41,252 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 15:32:41,261 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 15:32:41,262 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 15:32:41,262 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 15:32:41,262 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 15:32:42,091 | server.py:94 | initial parameters (loss, other metrics): 0.0004976737976074219, {'accuracy': 0.0098, 'data_size': 10000}
INFO flwr 2024-06-05 15:32:42,091 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 15:32:42,092 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1159408)[0m 2024-06-05 15:32:43.553419: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1159408)[0m 2024-06-05 15:32:43.656227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1159408)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1159408)[0m 2024-06-05 15:32:45.692732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 15:35:28,240 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 15:37:44,235 | server.py:125 | fit progress: (1, 0.0014758360862731934, {'accuracy': 0.0118, 'data_size': 10000}, 302.1431256989017)
INFO flwr 2024-06-05 15:37:44,235 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 15:37:44,236 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
[2m[33m(raylet)[0m [2024-06-05 15:38:52,855 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:27,554 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:27,555 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:27,555 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:27,607 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:27,565 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:27,586 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:27,560 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:27,607 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:34,598 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:34,598 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:39:34,599 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:39:34,600 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:39:34,643 | server.py:236 | fit_round 2 received 4 results and 6 failures
INFO flwr 2024-06-05 15:39:48,715 | server.py:125 | fit progress: (2, 0.0009552956581115723, {'accuracy': 0.0121, 'data_size': 10000}, 426.6236367239617)
INFO flwr 2024-06-05 15:39:48,716 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 15:39:48,716 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 15:40:27,401 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:27,401 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:27,402 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:27,402 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:27,403 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:27,403 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:27,403 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:27,404 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:34,789 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:34,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:43,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:43,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:58,044 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:58,045 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:40:58,046 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:40:58,046 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:41:07,017 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:41:07,017 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 15:41:07,018 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 15:41:07,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: ac0a75b35bd1660ac52a3ed009000000, name=DefaultActor.__init__, pid=1159408, memory used=3.62GB) was running was 238.98GB / 251.51GB (0.950175), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-cb5b07ed620518950b11d38767fa1b9bc03619fc4418ef2c35ceed25*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	17.94	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1159408	3.62	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 15:41:07,041 | server.py:236 | fit_round 3 received 0 results and 10 failures
ERROR flwr 2024-06-05 15:41:07,054 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 15:41:07,061 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 15:41:07,061 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 15:41:07,061 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0121
wandb:     loss 0.00096
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_153240-epdhts28
wandb: Find logs at: ./wandb/offline-run-20240605_153240-epdhts28/logs
INFO flwr 2024-06-05 15:41:10,719 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 15:41:10,720 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 15:41:10,731 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 15:41:11,410 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 15:41:11,410 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 15:41:11,447	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 15:41:11,452	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 15:41:11,465 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 15:41:11,502	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 15:41:11,503 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 2.0, 'object_store_memory': 24908054937.0, 'memory': 17179869184.0, 'node:10.20.240.14': 1.0, 'GPU': 1.0, 'accelerator_type:RTX': 1.0}
INFO flwr 2024-06-05 15:41:11,503 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 15:41:11,503 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 15:41:11,515 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 15:41:11,517 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 15:41:11,517 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 15:41:11,517 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 15:41:12,391 | server.py:94 | initial parameters (loss, other metrics): 0.0004982635974884034, {'accuracy': 0.008, 'data_size': 10000}
INFO flwr 2024-06-05 15:41:12,391 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 15:41:12,392 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1165619)[0m 2024-06-05 15:41:14.206412: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1165619)[0m 2024-06-05 15:41:14.317147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1165619)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1165619)[0m 2024-06-05 15:41:17.106624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 15:43:56,689 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 15:47:17,257 | server.py:125 | fit progress: (1, 0.0004961993217468261, {'accuracy': 0.0135, 'data_size': 10000}, 364.86483455915004)
INFO flwr 2024-06-05 15:47:17,258 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 15:47:17,317 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 15:50:02,623 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 15:50:52,623 | server.py:125 | fit progress: (2, 0.0005008602142333985, {'accuracy': 0.0155, 'data_size': 10000}, 580.2312712469138)
INFO flwr 2024-06-05 15:50:52,623 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 15:50:52,624 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 15:57:16,145 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 15:58:13,958 | server.py:125 | fit progress: (3, 0.0004697490692138672, {'accuracy': 0.0324, 'data_size': 10000}, 1021.5667701852508)
INFO flwr 2024-06-05 15:58:13,959 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 15:58:13,961 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 16:00:29,638 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:00:29,638 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:00:29,639 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:00:29,639 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 16:00:29,665 | server.py:236 | fit_round 4 received 8 results and 2 failures
INFO flwr 2024-06-05 16:00:56,880 | server.py:125 | fit progress: (4, 0.0004529854774475098, {'accuracy': 0.0337, 'data_size': 10000}, 1184.4882635241374)
INFO flwr 2024-06-05 16:00:56,880 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 16:00:56,881 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
[2m[33m(raylet)[0m [2024-06-05 16:00:59,544 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:49,836 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:49,852 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,886 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,886 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,887 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,887 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,888 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,889 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,889 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,890 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,890 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,890 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:01:58,916 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:01:58,937 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:02:05,862 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:02:05,863 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:02:12,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:02:12,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:02:12,778 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:02:12,778 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 8a3508a48ec1319dd6e101fd0a000000, name=DefaultActor.__init__, pid=1165619, memory used=3.63GB) was running was 239.11GB / 251.51GB (0.95068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-bdc0456b1c09cda4f50ce8c0d0bd94df32b04db1a0851801067d32be*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.30	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1165619	3.63	ray::DefaultActor
998565	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 16:02:12,800 | server.py:236 | fit_round 5 received 0 results and 10 failures
ERROR flwr 2024-06-05 16:02:12,801 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 16:02:12,803 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 16:02:12,803 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 16:02:12,804 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0337
wandb:     loss 0.00045
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_154110-x4jrx1jf
wandb: Find logs at: ./wandb/offline-run-20240605_154110-x4jrx1jf/logs
INFO flwr 2024-06-05 16:02:16,591 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.001}
				global: {'lr': 0.1, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 16:02:16,594 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 16:02:16,595 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 16:02:21,966 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 16:02:21,966 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 16:02:22,008	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 16:02:22,015	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 16:02:22,045 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 16:02:22,083	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 16:02:22,084 | app.py:213 | Flower VCE: Ray initialized with resources: {'object_store_memory': 24908054937.0, 'node:__internal_head__': 1.0, 'node:10.20.240.14': 1.0, 'GPU': 1.0, 'CPU': 2.0, 'memory': 17179869184.0, 'accelerator_type:RTX': 1.0}
INFO flwr 2024-06-05 16:02:22,085 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 16:02:22,085 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 16:02:22,094 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 16:02:22,096 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 16:02:22,096 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 16:02:22,097 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 16:02:23,042 | server.py:94 | initial parameters (loss, other metrics): 0.0004983395576477051, {'accuracy': 0.0093, 'data_size': 10000}
INFO flwr 2024-06-05 16:02:23,043 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 16:02:23,043 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1177664)[0m 2024-06-05 16:02:42.500983: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1177664)[0m 2024-06-05 16:02:47.914121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1177664)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1177664)[0m 2024-06-05 16:02:55.611745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 16:06:53,548 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 16:10:38,504 | server.py:125 | fit progress: (1, 0.0005658043384552002, {'accuracy': 0.0135, 'data_size': 10000}, 495.46119996206835)
INFO flwr 2024-06-05 16:10:38,505 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 16:10:38,505 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:13:11,829 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 16:13:35,509 | server.py:125 | fit progress: (2, 0.0005974864959716797, {'accuracy': 0.0152, 'data_size': 10000}, 672.4662927552126)
INFO flwr 2024-06-05 16:13:35,510 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 16:13:35,510 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 16:14:55,112 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:14:55,130 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-05 16:14:59,567 E 998708 998708] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e, IP: 10.20.240.14) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.14`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:15:09,434 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:15:09,445 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:15:18,766 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:15:18,766 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:15:25,939 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:15:25,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:15:25,941 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:15:25,941 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:15:25,941 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:15:25,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 16:15:25,947 | server.py:236 | fit_round 3 received 4 results and 6 failures
INFO flwr 2024-06-05 16:15:48,458 | server.py:125 | fit progress: (3, 0.0005839571475982666, {'accuracy': 0.0152, 'data_size': 10000}, 805.4147807471454)
INFO flwr 2024-06-05 16:15:48,458 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 16:15:48,459 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-05 16:16:24,592 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:24,593 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:24,593 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:24,626 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:24,671 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:24,694 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:33,310 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:33,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:40,532 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:40,596 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:46,823 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:46,823 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:46,825 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:46,825 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:16:54,197 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:16:54,213 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:17:02,940 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:17:02,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-05 16:17:02,941 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-05 16:17:02,941 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.14, ID: 6de0e256c553179a081c420876ba48f0afc1b87fb40ffa22d4e5f61e) where the task (actor ID: 76b62da251000de093da2f770b000000, name=DefaultActor.__init__, pid=1177664, memory used=3.96GB) was running was 239.03GB / 251.51GB (0.950378), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.14`. To see the logs of the worker, use `ray logs worker-3ad6db7b421e974c3c113573462e52c7c41d47258c3bc5d653c1b27a*out -ip 10.20.240.14. Top 10 memory users:
PID	MEM(GB)	COMMAND
998775	19.21	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
1177664	3.96	ray::DefaultActor
998565	0.25	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
1001731	0.06	wandb-service(2-998775-s-47831)
998753	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998650	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
998648	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998710	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
998649	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --...
998708	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-05 16:17:02,943 | server.py:236 | fit_round 4 received 0 results and 10 failures
ERROR flwr 2024-06-05 16:17:02,975 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-05 16:17:03,053 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-05 16:17:03,056 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-05 16:17:03,057 | Simulation.py:184 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0152
wandb:     loss 0.00058
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_160216-jmovyd51
wandb: Find logs at: ./wandb/offline-run-20240605_160216-jmovyd51/logs
INFO flwr 2024-06-05 16:17:07,162 | main.py:105 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 20
			local_rounds: 1
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.001}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 1
		Attack Simulation:
			batch_size: 64
			optimizer_name: FedAvg
			optimizer_parameters:
				lr: 0.1
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-05 16:17:07,165 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-05 16:17:07,218 | Config.py:71 | No previous federated learning simulation found, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-05 16:17:19,072 | Simulation.py:402 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-05 16:17:19,072 | Simulation.py:161 | Starting federated learning simulation
2024-06-05 16:17:19,121	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.14:6379...
2024-06-05 16:17:19,126	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-05 16:17:19,139 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)
2024-06-05 16:17:19,177	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-05 16:17:19,178 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 17179869184.0, 'CPU': 2.0, 'accelerator_type:RTX': 1.0, 'object_store_memory': 24908054937.0, 'node:__internal_head__': 1.0, 'node:10.20.240.14': 1.0, 'GPU': 1.0}
INFO flwr 2024-06-05 16:17:19,178 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-05 16:17:19,178 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-05 16:17:19,191 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-05 16:17:19,192 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-05 16:17:19,192 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-05 16:17:19,192 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-05 16:17:20,003 | server.py:94 | initial parameters (loss, other metrics): 0.0004983960151672363, {'accuracy': 0.0087, 'data_size': 10000}
INFO flwr 2024-06-05 16:17:20,003 | server.py:104 | FL starting
DEBUG flwr 2024-06-05 16:17:20,004 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=1186921)[0m 2024-06-05 16:18:04.790610: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=1186921)[0m 2024-06-05 16:18:32.239235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=1186921)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=1186921)[0m 2024-06-05 16:19:15.893741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-05 16:27:25,591 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-05 16:32:18,695 | server.py:125 | fit progress: (1, 0.0004833984375, {'accuracy': 0.0116, 'data_size': 10000}, 898.6916809943505)
INFO flwr 2024-06-05 16:32:18,696 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-05 16:32:18,696 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:35:44,079 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-05 16:36:42,921 | server.py:125 | fit progress: (2, 0.0004753939628601074, {'accuracy': 0.0153, 'data_size': 10000}, 1162.9177222852595)
INFO flwr 2024-06-05 16:36:42,922 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-05 16:36:42,924 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:39:18,944 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-05 16:39:45,092 | server.py:125 | fit progress: (3, 0.0004736001014709473, {'accuracy': 0.0156, 'data_size': 10000}, 1345.0880394671112)
INFO flwr 2024-06-05 16:39:45,092 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-05 16:39:45,092 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:42:17,715 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-05 16:42:52,510 | server.py:125 | fit progress: (4, 0.00047174201011657716, {'accuracy': 0.0169, 'data_size': 10000}, 1532.5067165922374)
INFO flwr 2024-06-05 16:42:52,511 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-05 16:42:52,511 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:45:26,487 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-05 16:46:05,828 | server.py:125 | fit progress: (5, 0.0004651450157165527, {'accuracy': 0.0266, 'data_size': 10000}, 1725.8242616332136)
INFO flwr 2024-06-05 16:46:05,829 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-05 16:46:05,829 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:48:38,893 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-05 16:49:24,695 | server.py:125 | fit progress: (6, 0.0004546999454498291, {'accuracy': 0.033, 'data_size': 10000}, 1924.691450572107)
INFO flwr 2024-06-05 16:49:24,695 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-05 16:49:24,696 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:51:55,600 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-05 16:52:55,773 | server.py:125 | fit progress: (7, 0.00044340806007385253, {'accuracy': 0.0399, 'data_size': 10000}, 2135.769941524137)
INFO flwr 2024-06-05 16:52:55,774 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-05 16:52:55,775 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:55:28,433 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-05 16:56:18,033 | server.py:125 | fit progress: (8, 0.0004385779857635498, {'accuracy': 0.0463, 'data_size': 10000}, 2338.029215130955)
INFO flwr 2024-06-05 16:56:18,033 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-05 16:56:18,034 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 16:58:49,458 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-06-05 16:59:23,105 | server.py:125 | fit progress: (9, 0.0004309121608734131, {'accuracy': 0.0519, 'data_size': 10000}, 2523.101763747167)
INFO flwr 2024-06-05 16:59:23,106 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-05 16:59:23,106 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:01:53,404 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-06-05 17:02:38,913 | server.py:125 | fit progress: (10, 0.00042364859580993654, {'accuracy': 0.0651, 'data_size': 10000}, 2718.9091463219374)
INFO flwr 2024-06-05 17:02:38,913 | server.py:171 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2024-06-05 17:02:38,914 | server.py:222 | fit_round 11: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:05:11,792 | server.py:236 | fit_round 11 received 10 results and 0 failures
INFO flwr 2024-06-05 17:05:44,183 | server.py:125 | fit progress: (11, 0.00041789703369140625, {'accuracy': 0.0686, 'data_size': 10000}, 2904.1789979343303)
INFO flwr 2024-06-05 17:05:44,183 | server.py:171 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2024-06-05 17:05:44,188 | server.py:222 | fit_round 12: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:08:17,282 | server.py:236 | fit_round 12 received 10 results and 0 failures
INFO flwr 2024-06-05 17:08:52,543 | server.py:125 | fit progress: (12, 0.0004117518424987793, {'accuracy': 0.0772, 'data_size': 10000}, 3092.5395177220926)
INFO flwr 2024-06-05 17:08:52,544 | server.py:171 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2024-06-05 17:08:52,545 | server.py:222 | fit_round 13: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:11:25,818 | server.py:236 | fit_round 13 received 10 results and 0 failures
INFO flwr 2024-06-05 17:12:05,068 | server.py:125 | fit progress: (13, 0.00040731892585754394, {'accuracy': 0.0787, 'data_size': 10000}, 3285.064342220314)
INFO flwr 2024-06-05 17:12:05,068 | server.py:171 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2024-06-05 17:12:05,069 | server.py:222 | fit_round 14: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:14:37,607 | server.py:236 | fit_round 14 received 10 results and 0 failures
INFO flwr 2024-06-05 17:15:17,637 | server.py:125 | fit progress: (14, 0.00040367965698242186, {'accuracy': 0.0801, 'data_size': 10000}, 3477.6332383812405)
INFO flwr 2024-06-05 17:15:17,637 | server.py:171 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2024-06-05 17:15:17,638 | server.py:222 | fit_round 15: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:17:51,109 | server.py:236 | fit_round 15 received 10 results and 0 failures
INFO flwr 2024-06-05 17:18:29,733 | server.py:125 | fit progress: (15, 0.00040165395736694334, {'accuracy': 0.0861, 'data_size': 10000}, 3669.729840436019)
INFO flwr 2024-06-05 17:18:29,734 | server.py:171 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2024-06-05 17:18:29,734 | server.py:222 | fit_round 16: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:21:02,468 | server.py:236 | fit_round 16 received 10 results and 0 failures
INFO flwr 2024-06-05 17:21:45,418 | server.py:125 | fit progress: (16, 0.00039771647453308107, {'accuracy': 0.0886, 'data_size': 10000}, 3865.4144504410215)
INFO flwr 2024-06-05 17:21:45,419 | server.py:171 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2024-06-05 17:21:45,419 | server.py:222 | fit_round 17: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:24:16,463 | server.py:236 | fit_round 17 received 10 results and 0 failures
INFO flwr 2024-06-05 17:24:47,298 | server.py:125 | fit progress: (17, 0.0003955229997634888, {'accuracy': 0.0971, 'data_size': 10000}, 4047.294923592359)
INFO flwr 2024-06-05 17:24:47,299 | server.py:171 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2024-06-05 17:24:47,299 | server.py:222 | fit_round 18: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:27:20,431 | server.py:236 | fit_round 18 received 10 results and 0 failures
INFO flwr 2024-06-05 17:27:53,784 | server.py:125 | fit progress: (18, 0.0003918752431869507, {'accuracy': 0.1004, 'data_size': 10000}, 4233.780169463251)
INFO flwr 2024-06-05 17:27:53,784 | server.py:171 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2024-06-05 17:27:53,784 | server.py:222 | fit_round 19: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:30:26,820 | server.py:236 | fit_round 19 received 10 results and 0 failures
INFO flwr 2024-06-05 17:31:05,319 | server.py:125 | fit progress: (19, 0.00038896551132202147, {'accuracy': 0.1068, 'data_size': 10000}, 4425.315141796134)
INFO flwr 2024-06-05 17:31:05,319 | server.py:171 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2024-06-05 17:31:05,320 | server.py:222 | fit_round 20: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-05 17:33:39,029 | server.py:236 | fit_round 20 received 10 results and 0 failures
INFO flwr 2024-06-05 17:34:06,573 | server.py:125 | fit progress: (20, 0.0003887128829956055, {'accuracy': 0.1054, 'data_size': 10000}, 4606.569716711063)
INFO flwr 2024-06-05 17:34:06,574 | server.py:171 | evaluate_round 20: no clients selected, cancel
INFO flwr 2024-06-05 17:34:06,574 | server.py:153 | FL finished in 4606.570527493954
INFO flwr 2024-06-05 17:34:06,617 | app.py:226 | app_fit: losses_distributed []
INFO flwr 2024-06-05 17:34:06,618 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2024-06-05 17:34:06,618 | app.py:229 | app_fit: losses_centralized [(0, 0.0004983960151672363), (1, 0.0004833984375), (2, 0.0004753939628601074), (3, 0.0004736001014709473), (4, 0.00047174201011657716), (5, 0.0004651450157165527), (6, 0.0004546999454498291), (7, 0.00044340806007385253), (8, 0.0004385779857635498), (9, 0.0004309121608734131), (10, 0.00042364859580993654), (11, 0.00041789703369140625), (12, 0.0004117518424987793), (13, 0.00040731892585754394), (14, 0.00040367965698242186), (15, 0.00040165395736694334), (16, 0.00039771647453308107), (17, 0.0003955229997634888), (18, 0.0003918752431869507), (19, 0.00038896551132202147), (20, 0.0003887128829956055)]
INFO flwr 2024-06-05 17:34:06,618 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.0087), (1, 0.0116), (2, 0.0153), (3, 0.0156), (4, 0.0169), (5, 0.0266), (6, 0.033), (7, 0.0399), (8, 0.0463), (9, 0.0519), (10, 0.0651), (11, 0.0686), (12, 0.0772), (13, 0.0787), (14, 0.0801), (15, 0.0861), (16, 0.0886), (17, 0.0971), (18, 0.1004), (19, 0.1068), (20, 0.1054)], 'data_size': [(0, 10000), (1, 10000), (2, 10000), (3, 10000), (4, 10000), (5, 10000), (6, 10000), (7, 10000), (8, 10000), (9, 10000), (10, 10000), (11, 10000), (12, 10000), (13, 10000), (14, 10000), (15, 10000), (16, 10000), (17, 10000), (18, 10000), (19, 10000), (20, 10000)]}
wandb: 
wandb: Run summary:
wandb: accuracy 0.1054
wandb:     loss 0.00039
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240605_161707-52mc6van
wandb: Find logs at: ./wandb/offline-run-20240605_161707-52mc6van/logs
2024-06-05 17:36:31,327	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --store_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.14 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.14,1.0,node:__internal_head__,1.0,accelerator_type:RTX,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,24908054937 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.14 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=62064 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.14:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.14 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs -Dray.session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --resource_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --metrics-agent-port=62064 --metrics_export_port=61629 --object_store_memory=24908054937 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal"` (via SIGTERM)
2024-06-05 17:36:31,330	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py --logs-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --monitor-ip=10.20.240.14` (via SIGTERM)
2024-06-05 17:36:31,330	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/log_monitor.py --logs-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --gcs-address=10.20.240.14:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2024-06-05 17:36:31,334	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.14:6379 --host=0.0.0.0 --port=10001 --mode=proxy --metrics-agent-port=62064` (via SIGTERM)
2024-06-05 17:36:31,339	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --store_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.14 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.14,1.0,node:__internal_head__,1.0,accelerator_type:RTX,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,24908054937 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.14 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=62064 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.14:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.14 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs -Dray.session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --resource_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --metrics-agent-port=62064 --metrics_export_port=61629 --object_store_memory=24908054937 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal"` (via SIGTERM)
2024-06-05 17:36:31,344	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --store_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.14 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.14,1.0,node:__internal_head__,1.0,accelerator_type:RTX,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,24908054937 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.14 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=62064 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.14:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.14 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs -Dray.session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --resource_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --metrics-agent-port=62064 --metrics_export_port=61629 --object_store_memory=24908054937 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal"` (via SIGTERM)
2024-06-05 17:36:31,349	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --store_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.14 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.14,1.0,node:__internal_head__,1.0,accelerator_type:RTX,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,24908054937 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.14 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=62064 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.14:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.14 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs -Dray.session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --resource_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --metrics-agent-port=62064 --metrics_export_port=61629 --object_store_memory=24908054937 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal"` (via SIGTERM)
2024-06-05 17:36:31,353	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/log_monitor.py --logs-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --gcs-address=10.20.240.14:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2024-06-05 17:36:31,361	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --store_socket_name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.14 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.14,1.0,node:__internal_head__,1.0,accelerator_type:RTX,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,24908054937 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.14 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=62064 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.14:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.14 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs -Dray.session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --resource_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --metrics-agent-port=62064 --metrics_export_port=61629 --object_store_memory=24908054937 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.14:6379 --session-name=session_2024-06-05_11-18-46_330645_998564 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal"` (via SIGTERM)
2024-06-05 17:36:31,361	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.14 --metrics-export-port=61629 --dashboard-agent-port=62064 --listen-port=52365 --node-manager-port=37557 --object-store-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-05_11-18-46_330645_998564/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564 --runtime-env-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/runtime_resources --log-dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-05_11-18-46_330645_998564 --gcs-address=10.20.240.14:6379 --minimal --agent-id 424238335` (via SIGTERM)
2024-06-05 17:36:31,583	INFO scripts.py:1098 -- 1/5 stopped.2024-06-05 17:36:31,583	INFO scripts.py:1098 -- 2/5 stopped.2024-06-05 17:36:32,009	INFO scripts.py:1098 -- 3/5 stopped.2024-06-05 17:36:32,262	INFO scripts.py:1098 -- 4/5 stopped.2024-06-05 17:36:32,262	INFO scripts.py:1098 -- 5/5 stopped.2024-06-05 17:36:32,368	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/local/ray/session_2024-06-05_11-18-46_330645_998564/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL2xvY2FsL3JheS9zZXNzaW9uXzIwMjQtMDYtMDVfMTEtMTgtNDZfMzMwNjQ1Xzk5ODU2NFwifX0iLCAiaXNfZXh0ZXJuYWxfc3RvcmFnZV90eXBlX2ZzIjogdHJ1ZX0= --gcs_server_port=6379 --metrics-agent-port=62064 --node-ip-address=10.20.240.14 --session-name=session_2024-06-05_11-18-46_330645_998564` (via SIGTERM)
2024-06-05 17:36:32,825	INFO scripts.py:1098 -- 1/1 stopped.2024-06-05 17:36:32,825	SUCC scripts.py:1142 -- Stopped all 6 Ray processes.
