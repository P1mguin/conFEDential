ctit088
2024-06-11 13:04:24,874	INFO scripts.py:1139 -- Did not find any active Ray processes.
2024-06-11 13:04:26,495	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-06-11 13:04:26,496	INFO scripts.py:722 -- Local node IP: 10.20.240.18
2024-06-11 13:04:32,592	SUCC scripts.py:759 -- --------------------
2024-06-11 13:04:32,593	SUCC scripts.py:760 -- Ray runtime started.
2024-06-11 13:04:32,593	SUCC scripts.py:761 -- --------------------
2024-06-11 13:04:32,593	INFO scripts.py:763 -- Next steps
2024-06-11 13:04:32,593	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-06-11 13:04:32,593	INFO scripts.py:769 --   ray start --address='10.20.240.18:6379'
2024-06-11 13:04:32,593	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-06-11 13:04:32,593	INFO scripts.py:780 -- import ray
2024-06-11 13:04:32,593	INFO scripts.py:781 -- ray.init()
2024-06-11 13:04:32,593	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-06-11 13:04:32,593	INFO scripts.py:813 --   ray stop
2024-06-11 13:04:32,594	INFO scripts.py:816 -- To view the status of the cluster, use
2024-06-11 13:04:32,594	INFO scripts.py:817 --   ray status
2024-06-11 13:05:30.598083: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 13:05:44.833230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 13:06:13.849585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 13:07:40,812 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 13:07:40,831 | Data.py:111 | Found preprocessed data for the given preprocess function with hash efddb1ca4fe67eba076ca51cfd1766212af1fce46847913eececd421a82a7a21, returning
INFO flwr 2024-06-11 13:07:47,122 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 13:07:47,132 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 13:08:00,218 | main.py:110 | Loaded 1 configs with name CIFAR100-RESNET34-FEDADAM, running...
INFO flwr 2024-06-11 13:08:00,221 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 13:08:00,227 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 13:08:00,253 | Config.py:72 | No previous federated learning simulation found with hash caa47aafb7ac0f49d834ae52c066512e979437b77978603c35f56eaf479e01b8, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 13:08:06,380 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 13:08:06,380 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 13:08:06,422	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 13:08:06,435	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 13:08:07,825 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 13:08:07,876	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 13:08:07,877 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'memory': 17179869184.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 12628287897.0, 'node:10.20.240.18': 1.0}
INFO flwr 2024-06-11 13:08:07,877 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 13:08:07,878 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 13:08:07,887 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-11 13:08:07,888 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 13:08:07,889 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 13:08:07,889 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=156090)[0m 2024-06-11 13:08:09.728198: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=156090)[0m 2024-06-11 13:08:09.790185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=156090)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=156090)[0m 2024-06-11 13:08:11.076390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 13:08:20,566 | server.py:94 | initial parameters (loss, other metrics): 5.039958984375, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-11 13:08:20,566 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 13:08:20,567 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:10:56,081 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-11 13:13:51,669 | server.py:125 | fit progress: (1, 5.63133359375, {'accuracy': 0.0147, 'data_size': 10000}, 331.10240281699225)
INFO flwr 2024-06-11 13:13:51,670 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 13:13:51,671 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:16:13,577 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-11 13:16:56,071 | server.py:125 | fit progress: (2, 5.029721484375, {'accuracy': 0.0355, 'data_size': 10000}, 515.5046167359687)
INFO flwr 2024-06-11 13:16:56,072 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-11 13:16:56,072 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:19:18,709 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-11 13:19:58,662 | server.py:125 | fit progress: (3, 5.553808984375, {'accuracy': 0.0156, 'data_size': 10000}, 698.0950307729654)
INFO flwr 2024-06-11 13:19:58,663 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-11 13:19:58,663 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:22:21,520 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-11 13:23:01,622 | server.py:125 | fit progress: (4, 5.30846015625, {'accuracy': 0.0373, 'data_size': 10000}, 881.0557246659882)
INFO flwr 2024-06-11 13:23:01,623 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-11 13:23:01,624 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:25:21,664 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-11 13:26:04,036 | server.py:125 | fit progress: (5, 6.09784765625, {'accuracy': 0.0261, 'data_size': 10000}, 1063.4691653558984)
INFO flwr 2024-06-11 13:26:04,037 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-11 13:26:04,037 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:28:25,945 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-11 13:29:08,077 | server.py:125 | fit progress: (6, 6.339403515625, {'accuracy': 0.0282, 'data_size': 10000}, 1247.510103361681)
INFO flwr 2024-06-11 13:29:08,078 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-11 13:29:08,078 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:31:30,169 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-11 13:32:12,309 | server.py:125 | fit progress: (7, 6.0886515625, {'accuracy': 0.0285, 'data_size': 10000}, 1431.7426923615858)
INFO flwr 2024-06-11 13:32:12,310 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-11 13:32:12,310 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:34:36,000 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-11 13:35:18,883 | server.py:125 | fit progress: (8, 5.726282421875, {'accuracy': 0.0357, 'data_size': 10000}, 1618.3163025947288)
INFO flwr 2024-06-11 13:35:18,884 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-11 13:35:18,884 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:37:40,863 | server.py:236 | fit_round 9 received 10 results and 0 failures
INFO flwr 2024-06-11 13:38:21,852 | server.py:125 | fit progress: (9, 5.473203515625, {'accuracy': 0.0414, 'data_size': 10000}, 1801.2848009467125)
INFO flwr 2024-06-11 13:38:21,852 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-11 13:38:21,853 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:40:41,286 | server.py:236 | fit_round 10 received 10 results and 0 failures
INFO flwr 2024-06-11 13:41:21,178 | server.py:125 | fit progress: (10, 5.1745640625, {'accuracy': 0.0426, 'data_size': 10000}, 1980.6114459307864)
INFO flwr 2024-06-11 13:41:21,179 | server.py:171 | evaluate_round 10: no clients selected, cancel
DEBUG flwr 2024-06-11 13:41:21,179 | server.py:222 | fit_round 11: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:43:44,408 | server.py:236 | fit_round 11 received 10 results and 0 failures
INFO flwr 2024-06-11 13:44:26,512 | server.py:125 | fit progress: (11, 4.82874453125, {'accuracy': 0.0641, 'data_size': 10000}, 2165.9452888416126)
INFO flwr 2024-06-11 13:44:26,513 | server.py:171 | evaluate_round 11: no clients selected, cancel
DEBUG flwr 2024-06-11 13:44:26,513 | server.py:222 | fit_round 12: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:46:49,543 | server.py:236 | fit_round 12 received 10 results and 0 failures
INFO flwr 2024-06-11 13:47:36,042 | server.py:125 | fit progress: (12, 4.905291015625, {'accuracy': 0.0657, 'data_size': 10000}, 2355.4755007405765)
INFO flwr 2024-06-11 13:47:36,043 | server.py:171 | evaluate_round 12: no clients selected, cancel
DEBUG flwr 2024-06-11 13:47:36,043 | server.py:222 | fit_round 13: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:49:58,134 | server.py:236 | fit_round 13 received 10 results and 0 failures
INFO flwr 2024-06-11 13:50:38,912 | server.py:125 | fit progress: (13, 4.6165546875, {'accuracy': 0.0674, 'data_size': 10000}, 2538.345704748761)
INFO flwr 2024-06-11 13:50:38,913 | server.py:171 | evaluate_round 13: no clients selected, cancel
DEBUG flwr 2024-06-11 13:50:38,913 | server.py:222 | fit_round 14: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:53:01,525 | server.py:236 | fit_round 14 received 10 results and 0 failures
INFO flwr 2024-06-11 13:53:46,679 | server.py:125 | fit progress: (14, 4.515333984375, {'accuracy': 0.0773, 'data_size': 10000}, 2726.1126286578365)
INFO flwr 2024-06-11 13:53:46,680 | server.py:171 | evaluate_round 14: no clients selected, cancel
DEBUG flwr 2024-06-11 13:53:46,680 | server.py:222 | fit_round 15: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:56:09,161 | server.py:236 | fit_round 15 received 10 results and 0 failures
INFO flwr 2024-06-11 13:56:51,795 | server.py:125 | fit progress: (15, 4.34610859375, {'accuracy': 0.0826, 'data_size': 10000}, 2911.228109380696)
INFO flwr 2024-06-11 13:56:51,795 | server.py:171 | evaluate_round 15: no clients selected, cancel
DEBUG flwr 2024-06-11 13:56:51,796 | server.py:222 | fit_round 16: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 13:59:15,124 | server.py:236 | fit_round 16 received 10 results and 0 failures
INFO flwr 2024-06-11 14:00:10,663 | server.py:125 | fit progress: (16, 4.11407890625, {'accuracy': 0.1058, 'data_size': 10000}, 3110.095930110663)
INFO flwr 2024-06-11 14:00:10,663 | server.py:171 | evaluate_round 16: no clients selected, cancel
DEBUG flwr 2024-06-11 14:00:10,664 | server.py:222 | fit_round 17: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:02:33,273 | server.py:236 | fit_round 17 received 10 results and 0 failures
INFO flwr 2024-06-11 14:03:40,884 | server.py:125 | fit progress: (17, 4.148422265625, {'accuracy': 0.1122, 'data_size': 10000}, 3320.317402275745)
INFO flwr 2024-06-11 14:03:40,885 | server.py:171 | evaluate_round 17: no clients selected, cancel
DEBUG flwr 2024-06-11 14:03:40,885 | server.py:222 | fit_round 18: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:06:03,609 | server.py:236 | fit_round 18 received 10 results and 0 failures
INFO flwr 2024-06-11 14:06:57,095 | server.py:125 | fit progress: (18, 3.92234375, {'accuracy': 0.1096, 'data_size': 10000}, 3516.5285556456074)
INFO flwr 2024-06-11 14:06:57,096 | server.py:171 | evaluate_round 18: no clients selected, cancel
DEBUG flwr 2024-06-11 14:06:57,096 | server.py:222 | fit_round 19: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:09:18,561 | server.py:236 | fit_round 19 received 10 results and 0 failures
INFO flwr 2024-06-11 14:10:02,425 | server.py:125 | fit progress: (19, 3.906206640625, {'accuracy': 0.1319, 'data_size': 10000}, 3701.8581519797444)
INFO flwr 2024-06-11 14:10:02,426 | server.py:171 | evaluate_round 19: no clients selected, cancel
DEBUG flwr 2024-06-11 14:10:02,426 | server.py:222 | fit_round 20: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:12:25,373 | server.py:236 | fit_round 20 received 10 results and 0 failures
INFO flwr 2024-06-11 14:13:09,723 | server.py:125 | fit progress: (20, 3.755587890625, {'accuracy': 0.1326, 'data_size': 10000}, 3889.1559531176463)
INFO flwr 2024-06-11 14:13:09,723 | server.py:171 | evaluate_round 20: no clients selected, cancel
DEBUG flwr 2024-06-11 14:13:09,724 | server.py:222 | fit_round 21: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:15:31,393 | server.py:236 | fit_round 21 received 10 results and 0 failures
INFO flwr 2024-06-11 14:16:17,161 | server.py:125 | fit progress: (21, 3.71667421875, {'accuracy': 0.158, 'data_size': 10000}, 4076.5938886217773)
INFO flwr 2024-06-11 14:16:17,161 | server.py:171 | evaluate_round 21: no clients selected, cancel
DEBUG flwr 2024-06-11 14:16:17,161 | server.py:222 | fit_round 22: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:18:40,653 | server.py:236 | fit_round 22 received 10 results and 0 failures
INFO flwr 2024-06-11 14:19:28,003 | server.py:125 | fit progress: (22, 3.5812140625, {'accuracy': 0.1558, 'data_size': 10000}, 4267.4359900127165)
INFO flwr 2024-06-11 14:19:28,003 | server.py:171 | evaluate_round 22: no clients selected, cancel
DEBUG flwr 2024-06-11 14:19:28,004 | server.py:222 | fit_round 23: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:21:51,030 | server.py:236 | fit_round 23 received 10 results and 0 failures
INFO flwr 2024-06-11 14:22:38,342 | server.py:125 | fit progress: (23, 3.646578125, {'accuracy': 0.1641, 'data_size': 10000}, 4457.775220522657)
INFO flwr 2024-06-11 14:22:38,343 | server.py:171 | evaluate_round 23: no clients selected, cancel
DEBUG flwr 2024-06-11 14:22:38,343 | server.py:222 | fit_round 24: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:25:01,111 | server.py:236 | fit_round 24 received 10 results and 0 failures
INFO flwr 2024-06-11 14:25:50,476 | server.py:125 | fit progress: (24, 3.5073609375, {'accuracy': 0.1699, 'data_size': 10000}, 4649.908798934892)
INFO flwr 2024-06-11 14:25:50,476 | server.py:171 | evaluate_round 24: no clients selected, cancel
DEBUG flwr 2024-06-11 14:25:50,477 | server.py:222 | fit_round 25: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:28:14,192 | server.py:236 | fit_round 25 received 10 results and 0 failures
INFO flwr 2024-06-11 14:29:00,267 | server.py:125 | fit progress: (25, 3.502256640625, {'accuracy': 0.1704, 'data_size': 10000}, 4839.700108664576)
INFO flwr 2024-06-11 14:29:00,267 | server.py:171 | evaluate_round 25: no clients selected, cancel
DEBUG flwr 2024-06-11 14:29:00,268 | server.py:222 | fit_round 26: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:31:21,761 | server.py:236 | fit_round 26 received 10 results and 0 failures
INFO flwr 2024-06-11 14:32:04,843 | server.py:125 | fit progress: (26, 3.46740234375, {'accuracy': 0.1843, 'data_size': 10000}, 5024.276190730743)
INFO flwr 2024-06-11 14:32:04,844 | server.py:171 | evaluate_round 26: no clients selected, cancel
DEBUG flwr 2024-06-11 14:32:04,844 | server.py:222 | fit_round 27: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:34:27,437 | server.py:236 | fit_round 27 received 10 results and 0 failures
INFO flwr 2024-06-11 14:35:11,768 | server.py:125 | fit progress: (27, 3.479096875, {'accuracy': 0.1897, 'data_size': 10000}, 5211.201739161741)
INFO flwr 2024-06-11 14:35:11,769 | server.py:171 | evaluate_round 27: no clients selected, cancel
DEBUG flwr 2024-06-11 14:35:11,769 | server.py:222 | fit_round 28: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:37:33,040 | server.py:236 | fit_round 28 received 10 results and 0 failures
INFO flwr 2024-06-11 14:38:15,242 | server.py:125 | fit progress: (28, 3.44102890625, {'accuracy': 0.1797, 'data_size': 10000}, 5394.674995297566)
INFO flwr 2024-06-11 14:38:15,242 | server.py:171 | evaluate_round 28: no clients selected, cancel
DEBUG flwr 2024-06-11 14:38:15,243 | server.py:222 | fit_round 29: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:40:36,889 | server.py:236 | fit_round 29 received 10 results and 0 failures
INFO flwr 2024-06-11 14:41:19,835 | server.py:125 | fit progress: (29, 3.526532421875, {'accuracy': 0.1961, 'data_size': 10000}, 5579.267842834815)
INFO flwr 2024-06-11 14:41:19,835 | server.py:171 | evaluate_round 29: no clients selected, cancel
DEBUG flwr 2024-06-11 14:41:19,836 | server.py:222 | fit_round 30: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:43:41,976 | server.py:236 | fit_round 30 received 10 results and 0 failures
INFO flwr 2024-06-11 14:44:26,518 | server.py:125 | fit progress: (30, 3.45433515625, {'accuracy': 0.1941, 'data_size': 10000}, 5765.951221687719)
INFO flwr 2024-06-11 14:44:26,519 | server.py:171 | evaluate_round 30: no clients selected, cancel
DEBUG flwr 2024-06-11 14:44:26,519 | server.py:222 | fit_round 31: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:46:48,939 | server.py:236 | fit_round 31 received 10 results and 0 failures
INFO flwr 2024-06-11 14:47:36,352 | server.py:125 | fit progress: (31, 3.372825390625, {'accuracy': 0.1925, 'data_size': 10000}, 5955.784921966959)
INFO flwr 2024-06-11 14:47:36,352 | server.py:171 | evaluate_round 31: no clients selected, cancel
DEBUG flwr 2024-06-11 14:47:36,353 | server.py:222 | fit_round 32: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:49:58,881 | server.py:236 | fit_round 32 received 10 results and 0 failures
INFO flwr 2024-06-11 14:50:47,111 | server.py:125 | fit progress: (32, 3.358110546875, {'accuracy': 0.1988, 'data_size': 10000}, 6146.543917570729)
INFO flwr 2024-06-11 14:50:47,111 | server.py:171 | evaluate_round 32: no clients selected, cancel
DEBUG flwr 2024-06-11 14:50:47,112 | server.py:222 | fit_round 33: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:53:09,504 | server.py:236 | fit_round 33 received 10 results and 0 failures
INFO flwr 2024-06-11 14:53:57,760 | server.py:125 | fit progress: (33, 3.3349328125, {'accuracy': 0.2116, 'data_size': 10000}, 6337.193035817705)
INFO flwr 2024-06-11 14:53:57,761 | server.py:171 | evaluate_round 33: no clients selected, cancel
DEBUG flwr 2024-06-11 14:53:57,761 | server.py:222 | fit_round 34: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:56:18,723 | server.py:236 | fit_round 34 received 10 results and 0 failures
INFO flwr 2024-06-11 14:56:59,083 | server.py:125 | fit progress: (34, 3.380571484375, {'accuracy': 0.217, 'data_size': 10000}, 6518.516644978896)
INFO flwr 2024-06-11 14:56:59,084 | server.py:171 | evaluate_round 34: no clients selected, cancel
DEBUG flwr 2024-06-11 14:56:59,084 | server.py:222 | fit_round 35: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 14:59:21,421 | server.py:236 | fit_round 35 received 10 results and 0 failures
INFO flwr 2024-06-11 15:00:03,005 | server.py:125 | fit progress: (35, 3.28241015625, {'accuracy': 0.214, 'data_size': 10000}, 6702.438726133667)
INFO flwr 2024-06-11 15:00:03,006 | server.py:171 | evaluate_round 35: no clients selected, cancel
DEBUG flwr 2024-06-11 15:00:03,006 | server.py:222 | fit_round 36: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:02:25,778 | server.py:236 | fit_round 36 received 10 results and 0 failures
INFO flwr 2024-06-11 15:03:09,681 | server.py:125 | fit progress: (36, 3.2506052734375, {'accuracy': 0.2107, 'data_size': 10000}, 6889.114610312972)
INFO flwr 2024-06-11 15:03:09,682 | server.py:171 | evaluate_round 36: no clients selected, cancel
DEBUG flwr 2024-06-11 15:03:09,682 | server.py:222 | fit_round 37: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:05:30,554 | server.py:236 | fit_round 37 received 10 results and 0 failures
INFO flwr 2024-06-11 15:06:12,440 | server.py:125 | fit progress: (37, 3.510175, {'accuracy': 0.2198, 'data_size': 10000}, 7071.873069034889)
INFO flwr 2024-06-11 15:06:12,440 | server.py:171 | evaluate_round 37: no clients selected, cancel
DEBUG flwr 2024-06-11 15:06:12,440 | server.py:222 | fit_round 38: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:08:34,408 | server.py:236 | fit_round 38 received 10 results and 0 failures
INFO flwr 2024-06-11 15:09:22,005 | server.py:125 | fit progress: (38, 3.224846875, {'accuracy': 0.2247, 'data_size': 10000}, 7261.438351447694)
INFO flwr 2024-06-11 15:09:22,006 | server.py:171 | evaluate_round 38: no clients selected, cancel
DEBUG flwr 2024-06-11 15:09:22,006 | server.py:222 | fit_round 39: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:11:43,381 | server.py:236 | fit_round 39 received 10 results and 0 failures
INFO flwr 2024-06-11 15:12:29,276 | server.py:125 | fit progress: (39, 3.21170234375, {'accuracy': 0.229, 'data_size': 10000}, 7448.7096004099585)
INFO flwr 2024-06-11 15:12:29,277 | server.py:171 | evaluate_round 39: no clients selected, cancel
DEBUG flwr 2024-06-11 15:12:29,277 | server.py:222 | fit_round 40: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:14:52,841 | server.py:236 | fit_round 40 received 10 results and 0 failures
INFO flwr 2024-06-11 15:15:36,116 | server.py:125 | fit progress: (40, 3.314878515625, {'accuracy': 0.2276, 'data_size': 10000}, 7635.548918431625)
INFO flwr 2024-06-11 15:15:36,116 | server.py:171 | evaluate_round 40: no clients selected, cancel
DEBUG flwr 2024-06-11 15:15:36,117 | server.py:222 | fit_round 41: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:17:57,913 | server.py:236 | fit_round 41 received 10 results and 0 failures
INFO flwr 2024-06-11 15:18:44,607 | server.py:125 | fit progress: (41, 3.346187109375, {'accuracy': 0.2277, 'data_size': 10000}, 7824.040314022917)
INFO flwr 2024-06-11 15:18:44,608 | server.py:171 | evaluate_round 41: no clients selected, cancel
DEBUG flwr 2024-06-11 15:18:44,608 | server.py:222 | fit_round 42: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:21:05,547 | server.py:236 | fit_round 42 received 10 results and 0 failures
INFO flwr 2024-06-11 15:21:45,853 | server.py:125 | fit progress: (42, 3.24534140625, {'accuracy': 0.2316, 'data_size': 10000}, 8005.285855077673)
INFO flwr 2024-06-11 15:21:45,853 | server.py:171 | evaluate_round 42: no clients selected, cancel
DEBUG flwr 2024-06-11 15:21:45,854 | server.py:222 | fit_round 43: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:24:06,818 | server.py:236 | fit_round 43 received 10 results and 0 failures
INFO flwr 2024-06-11 15:24:50,428 | server.py:125 | fit progress: (43, 3.215898828125, {'accuracy': 0.2299, 'data_size': 10000}, 8189.8610563557595)
INFO flwr 2024-06-11 15:24:50,428 | server.py:171 | evaluate_round 43: no clients selected, cancel
DEBUG flwr 2024-06-11 15:24:50,428 | server.py:222 | fit_round 44: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:27:12,333 | server.py:236 | fit_round 44 received 10 results and 0 failures
INFO flwr 2024-06-11 15:28:01,011 | server.py:125 | fit progress: (44, 3.2023529296875, {'accuracy': 0.2379, 'data_size': 10000}, 8380.444200497586)
INFO flwr 2024-06-11 15:28:01,012 | server.py:171 | evaluate_round 44: no clients selected, cancel
DEBUG flwr 2024-06-11 15:28:01,012 | server.py:222 | fit_round 45: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:30:23,878 | server.py:236 | fit_round 45 received 10 results and 0 failures
INFO flwr 2024-06-11 15:31:04,902 | server.py:125 | fit progress: (45, 3.163780859375, {'accuracy': 0.2421, 'data_size': 10000}, 8564.335164777935)
INFO flwr 2024-06-11 15:31:04,902 | server.py:171 | evaluate_round 45: no clients selected, cancel
DEBUG flwr 2024-06-11 15:31:04,903 | server.py:222 | fit_round 46: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:33:27,911 | server.py:236 | fit_round 46 received 10 results and 0 failures
INFO flwr 2024-06-11 15:34:11,344 | server.py:125 | fit progress: (46, 3.21241171875, {'accuracy': 0.2436, 'data_size': 10000}, 8750.777666758746)
INFO flwr 2024-06-11 15:34:11,345 | server.py:171 | evaluate_round 46: no clients selected, cancel
DEBUG flwr 2024-06-11 15:34:11,345 | server.py:222 | fit_round 47: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:36:33,672 | server.py:236 | fit_round 47 received 10 results and 0 failures
INFO flwr 2024-06-11 15:37:15,761 | server.py:125 | fit progress: (47, 3.1695197265625, {'accuracy': 0.2497, 'data_size': 10000}, 8935.193908114918)
INFO flwr 2024-06-11 15:37:15,761 | server.py:171 | evaluate_round 47: no clients selected, cancel
DEBUG flwr 2024-06-11 15:37:15,762 | server.py:222 | fit_round 48: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:39:37,381 | server.py:236 | fit_round 48 received 10 results and 0 failures
INFO flwr 2024-06-11 15:40:17,352 | server.py:125 | fit progress: (48, 3.1211806640625, {'accuracy': 0.2563, 'data_size': 10000}, 9116.785184830893)
INFO flwr 2024-06-11 15:40:17,352 | server.py:171 | evaluate_round 48: no clients selected, cancel
DEBUG flwr 2024-06-11 15:40:17,353 | server.py:222 | fit_round 49: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:42:39,600 | server.py:236 | fit_round 49 received 10 results and 0 failures
INFO flwr 2024-06-11 15:43:21,492 | server.py:125 | fit progress: (49, 3.1161705078125, {'accuracy': 0.2543, 'data_size': 10000}, 9300.925267182756)
INFO flwr 2024-06-11 15:43:21,493 | server.py:171 | evaluate_round 49: no clients selected, cancel
DEBUG flwr 2024-06-11 15:43:21,493 | server.py:222 | fit_round 50: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:45:44,019 | server.py:236 | fit_round 50 received 10 results and 0 failures
INFO flwr 2024-06-11 15:46:26,324 | server.py:125 | fit progress: (50, 3.233056640625, {'accuracy': 0.2447, 'data_size': 10000}, 9485.757275215816)
INFO flwr 2024-06-11 15:46:26,325 | server.py:171 | evaluate_round 50: no clients selected, cancel
DEBUG flwr 2024-06-11 15:46:26,325 | server.py:222 | fit_round 51: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:48:49,481 | server.py:236 | fit_round 51 received 10 results and 0 failures
INFO flwr 2024-06-11 15:49:35,258 | server.py:125 | fit progress: (51, 3.182640625, {'accuracy': 0.2498, 'data_size': 10000}, 9674.691174007952)
INFO flwr 2024-06-11 15:49:35,259 | server.py:171 | evaluate_round 51: no clients selected, cancel
DEBUG flwr 2024-06-11 15:49:35,259 | server.py:222 | fit_round 52: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:52:04,861 | server.py:236 | fit_round 52 received 10 results and 0 failures
INFO flwr 2024-06-11 15:52:50,021 | server.py:125 | fit progress: (52, 3.127319140625, {'accuracy': 0.2623, 'data_size': 10000}, 9869.454413324594)
INFO flwr 2024-06-11 15:52:50,022 | server.py:171 | evaluate_round 52: no clients selected, cancel
DEBUG flwr 2024-06-11 15:52:50,022 | server.py:222 | fit_round 53: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:55:13,094 | server.py:236 | fit_round 53 received 10 results and 0 failures
INFO flwr 2024-06-11 15:55:55,909 | server.py:125 | fit progress: (53, 3.1184455078125, {'accuracy': 0.261, 'data_size': 10000}, 10055.342116487678)
INFO flwr 2024-06-11 15:55:55,909 | server.py:171 | evaluate_round 53: no clients selected, cancel
DEBUG flwr 2024-06-11 15:55:55,910 | server.py:222 | fit_round 54: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 15:58:17,619 | server.py:236 | fit_round 54 received 10 results and 0 failures
INFO flwr 2024-06-11 15:59:01,727 | server.py:125 | fit progress: (54, 3.141020703125, {'accuracy': 0.2589, 'data_size': 10000}, 10241.160066684708)
INFO flwr 2024-06-11 15:59:01,727 | server.py:171 | evaluate_round 54: no clients selected, cancel
DEBUG flwr 2024-06-11 15:59:01,728 | server.py:222 | fit_round 55: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:01:27,750 | server.py:236 | fit_round 55 received 10 results and 0 failures
INFO flwr 2024-06-11 16:02:14,792 | server.py:125 | fit progress: (55, 3.1821076171875, {'accuracy': 0.266, 'data_size': 10000}, 10434.225527021568)
INFO flwr 2024-06-11 16:02:14,793 | server.py:171 | evaluate_round 55: no clients selected, cancel
DEBUG flwr 2024-06-11 16:02:14,793 | server.py:222 | fit_round 56: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:04:42,426 | server.py:236 | fit_round 56 received 10 results and 0 failures
INFO flwr 2024-06-11 16:05:28,194 | server.py:125 | fit progress: (56, 3.106248046875, {'accuracy': 0.2694, 'data_size': 10000}, 10627.627758443821)
INFO flwr 2024-06-11 16:05:28,195 | server.py:171 | evaluate_round 56: no clients selected, cancel
DEBUG flwr 2024-06-11 16:05:28,195 | server.py:222 | fit_round 57: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:07:49,196 | server.py:236 | fit_round 57 received 10 results and 0 failures
INFO flwr 2024-06-11 16:08:35,277 | server.py:125 | fit progress: (57, 3.073021875, {'accuracy': 0.2674, 'data_size': 10000}, 10814.710007018875)
INFO flwr 2024-06-11 16:08:35,278 | server.py:171 | evaluate_round 57: no clients selected, cancel
DEBUG flwr 2024-06-11 16:08:35,278 | server.py:222 | fit_round 58: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 16:09:07,795 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:07,811 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:09:15,096 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:15,113 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:09:22,617 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:22,633 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 16:09:32,737 E 155296 155296] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:09:38,582 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:38,593 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:09:46,859 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:46,875 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:09:53,732 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:09:53,749 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:10:01,769 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:01,770 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:10:01,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:01,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:10:01,772 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:01,772 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 16:10:01,806 | server.py:236 | fit_round 58 received 1 results and 9 failures
INFO flwr 2024-06-11 16:10:10,123 | server.py:125 | fit progress: (58, 3.443056640625, {'accuracy': 0.2337, 'data_size': 10000}, 10909.556105252821)
INFO flwr 2024-06-11 16:10:10,124 | server.py:171 | evaluate_round 58: no clients selected, cancel
DEBUG flwr 2024-06-11 16:10:10,124 | server.py:222 | fit_round 59: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 16:10:49,365 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:49,366 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:49,366 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:10:49,386 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:10:49,412 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:10:49,440 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:06,220 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:06,236 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:13,204 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:13,205 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:13,206 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:13,206 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:13,207 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:13,208 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:20,526 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:20,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:27,898 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:27,899 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:11:27,900 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:11:27,900 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 69104df8022f264107be0bce01000000, name=DefaultActor.__init__, pid=156090, memory used=3.56GB) was running was 239.01GB / 251.51GB (0.950298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-082bd75cca0b1166e30ea894608d87c487bd139cec86f41af6fa832c*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
155350	8.54	python src/main.py --yaml-file examples/cifar100/resnet34/fed_adam.yaml --ray --memory 16 --num-cpus...
156090	3.56	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155999	0.06	wandb-service(2-155350-s-39317)
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 16:11:27,945 | server.py:236 | fit_round 59 received 0 results and 10 failures
ERROR flwr 2024-06-11 16:11:27,954 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 16:11:28,127 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 16:11:28,127 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 16:11:28,128 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.2337
wandb:     loss 3.44306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_130801-4v8v5mnx
wandb: Find logs at: ./wandb/offline-run-20240611_130801-4v8v5mnx/logs
2024-06-11 16:13:41.922422: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 16:13:42.003340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 16:14:08.127373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 16:16:06,211 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 16:16:06,254 | Data.py:111 | Found preprocessed data for the given preprocess function with hash efddb1ca4fe67eba076ca51cfd1766212af1fce46847913eececd421a82a7a21, returning
INFO flwr 2024-06-11 16:16:07,589 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 16:16:07,609 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 16:16:21,074 | main.py:110 | Loaded 1 configs with name CIFAR100-RESNET34-FEDAVG, running...
INFO flwr 2024-06-11 16:16:21,077 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAvg
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				lr: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 16:16:21,084 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 16:16:21,110 | Config.py:72 | No previous federated learning simulation found with hash b3b9198efd6ce563a44ef7ab149d79fbaa826077dedf48204d6e81f2596f72b8, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 16:16:33,608 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 16:16:33,608 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 16:16:33,651	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 16:16:33,754	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 16:16:33,773 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 16:16:33,819	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 16:16:33,820 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 17179869184.0, 'node:__internal_head__': 1.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 12628287897.0, 'CPU': 2.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-06-11 16:16:33,820 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 16:16:33,820 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 16:16:33,833 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-11 16:16:33,834 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 16:16:33,834 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 16:16:33,835 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 16:16:35,801 | server.py:94 | initial parameters (loss, other metrics): 5.039958984375, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-11 16:16:35,801 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 16:16:35,802 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=307680)[0m 2024-06-11 16:16:35.869914: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=307680)[0m 2024-06-11 16:16:35.928619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=307680)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=307680)[0m 2024-06-11 16:16:37.238112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
DEBUG flwr 2024-06-11 16:19:09,778 | server.py:236 | fit_round 1 received 10 results and 0 failures
INFO flwr 2024-06-11 16:22:24,637 | server.py:125 | fit progress: (1, 4.778652734375, {'accuracy': 0.0124, 'data_size': 10000}, 348.8350852509029)
INFO flwr 2024-06-11 16:22:24,637 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 16:22:24,639 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:24:48,817 | server.py:236 | fit_round 2 received 10 results and 0 failures
INFO flwr 2024-06-11 16:25:46,389 | server.py:125 | fit progress: (2, 4.699158984375, {'accuracy': 0.017, 'data_size': 10000}, 550.5869277999736)
INFO flwr 2024-06-11 16:25:46,389 | server.py:171 | evaluate_round 2: no clients selected, cancel
DEBUG flwr 2024-06-11 16:25:46,390 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:28:08,573 | server.py:236 | fit_round 3 received 10 results and 0 failures
INFO flwr 2024-06-11 16:29:05,777 | server.py:125 | fit progress: (3, 4.632502734375, {'accuracy': 0.019, 'data_size': 10000}, 749.9750039097853)
INFO flwr 2024-06-11 16:29:05,777 | server.py:171 | evaluate_round 3: no clients selected, cancel
DEBUG flwr 2024-06-11 16:29:05,778 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:31:29,228 | server.py:236 | fit_round 4 received 10 results and 0 failures
INFO flwr 2024-06-11 16:32:28,444 | server.py:125 | fit progress: (4, 4.534499609375, {'accuracy': 0.0303, 'data_size': 10000}, 952.6421663858928)
INFO flwr 2024-06-11 16:32:28,444 | server.py:171 | evaluate_round 4: no clients selected, cancel
DEBUG flwr 2024-06-11 16:32:28,445 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:34:52,367 | server.py:236 | fit_round 5 received 10 results and 0 failures
INFO flwr 2024-06-11 16:35:49,627 | server.py:125 | fit progress: (5, 4.40205390625, {'accuracy': 0.0387, 'data_size': 10000}, 1153.8254710999317)
INFO flwr 2024-06-11 16:35:49,628 | server.py:171 | evaluate_round 5: no clients selected, cancel
DEBUG flwr 2024-06-11 16:35:49,628 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:38:13,584 | server.py:236 | fit_round 6 received 10 results and 0 failures
INFO flwr 2024-06-11 16:39:09,540 | server.py:125 | fit progress: (6, 4.292596484375, {'accuracy': 0.0505, 'data_size': 10000}, 1353.7379878577776)
INFO flwr 2024-06-11 16:39:09,540 | server.py:171 | evaluate_round 6: no clients selected, cancel
DEBUG flwr 2024-06-11 16:39:09,541 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:41:31,454 | server.py:236 | fit_round 7 received 10 results and 0 failures
INFO flwr 2024-06-11 16:42:27,627 | server.py:125 | fit progress: (7, 4.199998828125, {'accuracy': 0.0649, 'data_size': 10000}, 1551.8254091315903)
INFO flwr 2024-06-11 16:42:27,628 | server.py:171 | evaluate_round 7: no clients selected, cancel
DEBUG flwr 2024-06-11 16:42:27,628 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 100)
DEBUG flwr 2024-06-11 16:44:50,781 | server.py:236 | fit_round 8 received 10 results and 0 failures
INFO flwr 2024-06-11 16:45:51,408 | server.py:125 | fit progress: (8, 4.1385984375, {'accuracy': 0.0744, 'data_size': 10000}, 1755.6067652385682)
INFO flwr 2024-06-11 16:45:51,409 | server.py:171 | evaluate_round 8: no clients selected, cancel
DEBUG flwr 2024-06-11 16:45:51,409 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 100)
[2m[33m(raylet)[0m [2024-06-11 16:46:32,787 E 155296 155296] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:46:44,240 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:46:44,255 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:46:58,526 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:46:58,526 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:46:58,547 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:46:58,573 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:47:21,065 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:47:21,065 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:47:21,066 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:47:21,064 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:47:21,066 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 16:47:21,088 | server.py:236 | fit_round 9 received 2 results and 8 failures
INFO flwr 2024-06-11 16:47:38,409 | server.py:125 | fit progress: (9, 4.202167578125, {'accuracy': 0.0726, 'data_size': 10000}, 1862.6073779757135)
INFO flwr 2024-06-11 16:47:38,409 | server.py:171 | evaluate_round 9: no clients selected, cancel
DEBUG flwr 2024-06-11 16:47:38,410 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 16:48:00,989 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:01,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:24,160 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:24,175 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:31,763 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:31,779 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:39,207 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:39,213 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:46,886 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:46,901 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:54,655 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:54,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:54,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:54,657 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:54,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:54,658 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:54,658 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:54,659 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:48:54,658 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:48:54,659 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: f7231cd95cd90f58791001bd02000000, name=DefaultActor.__init__, pid=307680, memory used=3.50GB) was running was 238.99GB / 251.51GB (0.950223), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-6b04d676a4e0927d5c6906a7452dad425af5651ae70143537675cd35*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
307025	7.42	python src/main.py --yaml-file examples/cifar100/resnet34/fed_avg.yaml --ray --memory 16 --num-cpus ...
307680	3.50	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
307628	0.06	wandb-service(2-307025-s-56781)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 16:48:54,687 | server.py:236 | fit_round 10 received 0 results and 10 failures
ERROR flwr 2024-06-11 16:48:54,720 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 16:48:54,942 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 16:48:54,942 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 16:48:54,944 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.0726
wandb:     loss 4.20217
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_161625-n4xtz7fq
wandb: Find logs at: ./wandb/offline-run-20240611_161625-n4xtz7fq/logs
2024-06-11 16:50:48.684249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 16:50:48.747684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 16:51:09.321895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 16:52:41,004 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 16:52:41,005 | Data.py:111 | Found preprocessed data for the given preprocess function with hash efddb1ca4fe67eba076ca51cfd1766212af1fce46847913eececd421a82a7a21, returning
INFO flwr 2024-06-11 16:52:43,129 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 16:52:43,130 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 16:52:57,372 | main.py:110 | Loaded 1 configs with name CIFAR100-RESNET34-FEDNAG, running...
INFO flwr 2024-06-11 16:52:57,375 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedNag
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
				momentum: 0.9
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				lr: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 16:52:57,382 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 16:52:57,400 | Config.py:72 | No previous federated learning simulation found with hash d3453dc869cf16ea7829424fbda52f8fff1d34cb0ed8efbc8926106d89775e3f, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 16:53:05,140 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 16:53:05,140 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 16:53:05,187	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 16:53:05,296	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 16:53:05,314 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 16:53:05,368	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 16:53:05,369 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:10.20.240.18': 1.0, 'object_store_memory': 12628287897.0, 'node:__internal_head__': 1.0, 'memory': 17179869184.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
INFO flwr 2024-06-11 16:53:05,369 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 16:53:05,369 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 16:53:05,381 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
INFO flwr 2024-06-11 16:53:05,382 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 16:53:05,383 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 16:53:05,383 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 16:53:07,305 | server.py:94 | initial parameters (loss, other metrics): 5.039958984375, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-11 16:53:07,306 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 16:53:07,306 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=313080)[0m 2024-06-11 16:53:08.151195: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=313080)[0m 2024-06-11 16:53:08.224331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=313080)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=313080)[0m 2024-06-11 16:53:10.260161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ERROR flwr 2024-06-11 16:55:49,907 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:55:49,908 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 16:55:49,908 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 16:55:49,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 16:55:49,921 | server.py:236 | fit_round 1 received 8 results and 2 failures
[2m[33m(raylet)[0m [2024-06-11 16:56:32,803 E 155296 155296] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-11 17:01:48,056 | server.py:125 | fit progress: (1, 4.7783, {'accuracy': 0.012, 'data_size': 10000}, 520.7499538846314)
INFO flwr 2024-06-11 17:01:48,057 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 17:01:48,058 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 17:02:10,552 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:10,552 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:10,570 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:10,662 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:32,403 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:32,408 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:32,414 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:32,424 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:32,435 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:32,502 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:39,328 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:39,330 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:54,159 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:54,159 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:02:54,160 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:02:54,160 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:03:01,822 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:03:01,822 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 17:03:01,824 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 17:03:01,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: b22ddb01ddbf2e70c7b568eba28d3dfecb1a790f9239d013ece62100) where the task (actor ID: 0d7846eebed2c0f9568e1e2203000000, name=DefaultActor.__init__, pid=313080, memory used=4.19GB) was running was 239.00GB / 251.51GB (0.950243), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-d4d2a088dc92b5e71b0ef6d3de9230e780f98b8c143d0a1861839e66*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
312999	5.75	python src/main.py --yaml-file examples/cifar100/resnet34/fed_nag.yaml --ray --memory 16 --num-cpus ...
313080	4.19	ray::DefaultActor
155153	0.24	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
155346	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155238	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
313026	0.06	wandb-service(2-312999-s-51877)
155236	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155297	0.05	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
155237	0.05	/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --...
155296	0.02	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 17:03:01,850 | server.py:236 | fit_round 2 received 0 results and 10 failures
ERROR flwr 2024-06-11 17:03:01,934 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 17:03:02,013 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 17:03:02,014 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 17:03:02,014 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.012
wandb:     loss 4.7783
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_165300-ty9d4rqz
wandb: Find logs at: ./wandb/offline-run-20240611_165300-ty9d4rqz/logs
2024-06-11 17:04:37,834	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --store_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.18 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.18,1.0,node:__internal_head__,1.0,accelerator_type:G,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,12628287897 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.18 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=57327 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.18:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.18 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs -Dray.session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --resource_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --metrics-agent-port=57327 --metrics_export_port=59613 --object_store_memory=12628287897 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal"` (via SIGTERM)
2024-06-11 17:04:37,837	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py --logs-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --monitor-ip=10.20.240.18` (via SIGTERM)
2024-06-11 17:04:37,838	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/log_monitor.py --logs-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --gcs-address=10.20.240.18:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2024-06-11 17:04:37,843	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -m ray.util.client.server --address=10.20.240.18:6379 --host=0.0.0.0 --port=10001 --mode=proxy --metrics-agent-port=57327` (via SIGTERM)
2024-06-11 17:04:37,848	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --store_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.18 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.18,1.0,node:__internal_head__,1.0,accelerator_type:G,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,12628287897 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.18 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=57327 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.18:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.18 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs -Dray.session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --resource_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --metrics-agent-port=57327 --metrics_export_port=59613 --object_store_memory=12628287897 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal"` (via SIGTERM)
2024-06-11 17:04:37,853	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --store_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.18 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.18,1.0,node:__internal_head__,1.0,accelerator_type:G,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,12628287897 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.18 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=57327 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.18:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.18 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs -Dray.session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --resource_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --metrics-agent-port=57327 --metrics_export_port=59613 --object_store_memory=12628287897 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal"` (via SIGTERM)
2024-06-11 17:04:37,858	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --store_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.18 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.18,1.0,node:__internal_head__,1.0,accelerator_type:G,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,12628287897 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.18 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=57327 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.18:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.18 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs -Dray.session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --resource_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --metrics-agent-port=57327 --metrics_export_port=59613 --object_store_memory=12628287897 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal"` (via SIGTERM)
2024-06-11 17:04:37,862	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/log_monitor.py --logs-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --gcs-address=10.20.240.18:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2024-06-11 17:04:37,870	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --store_socket_name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.20.240.18 --maximum_startup_concurrency=2 --static_resource_list=node:10.20.240.18,1.0,node:__internal_head__,1.0,accelerator_type:G,1,CPU,2,GPU,1,memory,17179869184,object_store_memory,12628287897 "--python_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.20.240.18 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --redis-address=None --temp-dir=/local/ray --metrics-agent-port=57327 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --temp-dir=/local/ray --webui= RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/workers/setup_worker.py -Dray.address=10.20.240.18:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store -Dray.raylet.socket-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet -Dray.redis.password= -Dray.node-ip=10.20.240.18 -Dray.home=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/../.. -Dray.logging.dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs -Dray.session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/cpp/lib --temp_dir=/local/ray --session_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --resource_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --metrics-agent-port=57327 --metrics_export_port=59613 --object_store_memory=12628287897 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.20.240.18:6379 --session-name=session_2024-06-11_13-04-26_556312_155152 --labels= --head --num_prestart_python_workers=2 "--agent_command=/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal"` (via SIGTERM)
2024-06-11 17:04:37,871	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-address=10.20.240.18 --metrics-export-port=59613 --dashboard-agent-port=57327 --listen-port=52365 --node-manager-port=39319 --object-store-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/plasma_store --raylet-name=/local/ray/session_2024-06-11_13-04-26_556312_155152/sockets/raylet --temp-dir=/local/ray --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --runtime-env-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/runtime_resources --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-06-11_13-04-26_556312_155152 --gcs-address=10.20.240.18:6379 --minimal --agent-id 424238335` (via SIGTERM)
2024-06-11 17:04:37,877	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=127.0.0.1 --port=8265 --port-retries=0 --temp-dir=/local/ray --log-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --session-dir=/local/ray/session_2024-06-11_13-04-26_556312_155152 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.20.240.18:6379 --node-ip-address=10.20.240.18 --minimal` (via SIGTERM)
2024-06-11 17:04:38,062	INFO scripts.py:1098 -- 1/6 stopped.2024-06-11 17:04:38,114	INFO scripts.py:1098 -- 2/6 stopped.2024-06-11 17:04:38,287	INFO scripts.py:1098 -- 3/6 stopped.2024-06-11 17:04:38,288	INFO scripts.py:1098 -- 4/6 stopped.2024-06-11 17:04:38,742	INFO scripts.py:1098 -- 5/6 stopped.2024-06-11 17:04:38,794	INFO scripts.py:1098 -- 6/6 stopped.2024-06-11 17:04:38,912	VINFO scripts.py:1068 -- Send termination request to `/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/local/ray/session_2024-06-11_13-04-26_556312_155152/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL2xvY2FsL3JheS9zZXNzaW9uXzIwMjQtMDYtMTFfMTMtMDQtMjZfNTU2MzEyXzE1NTE1MlwifX0iLCAiaXNfZXh0ZXJuYWxfc3RvcmFnZV90eXBlX2ZzIjogdHJ1ZX0= --gcs_server_port=6379 --metrics-agent-port=57327 --node-ip-address=10.20.240.18 --session-name=session_2024-06-11_13-04-26_556312_155152` (via SIGTERM)
2024-06-11 17:04:39,367	INFO scripts.py:1098 -- 1/1 stopped.2024-06-11 17:04:39,368	SUCC scripts.py:1142 -- Stopped all 7 Ray processes.
