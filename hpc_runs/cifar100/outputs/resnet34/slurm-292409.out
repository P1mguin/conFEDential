ctit088
2024-06-11 11:20:37,818	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-06-11 11:20:37,820	INFO scripts.py:722 -- Local node IP: 10.20.240.18
2024-06-11 11:21:04,046	SUCC scripts.py:759 -- --------------------
2024-06-11 11:21:04,046	SUCC scripts.py:760 -- Ray runtime started.
2024-06-11 11:21:04,046	SUCC scripts.py:761 -- --------------------
2024-06-11 11:21:04,046	INFO scripts.py:763 -- Next steps
2024-06-11 11:21:04,046	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-06-11 11:21:04,046	INFO scripts.py:769 --   ray start --address='10.20.240.18:6379'
2024-06-11 11:21:04,046	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-06-11 11:21:04,046	INFO scripts.py:780 -- import ray
2024-06-11 11:21:04,046	INFO scripts.py:781 -- ray.init()
2024-06-11 11:21:04,046	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-06-11 11:21:04,046	INFO scripts.py:813 --   ray stop
2024-06-11 11:21:04,047	INFO scripts.py:816 -- To view the status of the cluster, use
2024-06-11 11:21:04,047	INFO scripts.py:817 --   ray status
2024-06-11 11:23:06.455167: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 11:23:12.504635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 11:23:45.960133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:25:48,328 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 11:25:48,449 | Data.py:111 | Found preprocessed data for the given preprocess function with hash efddb1ca4fe67eba076ca51cfd1766212af1fce46847913eececd421a82a7a21, returning
INFO flwr 2024-06-11 11:25:56,969 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 11:25:57,004 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 11:26:09,952 | main.py:110 | Loaded 1 configs with name CIFAR100-RESNET34-FEDADAM, running...
INFO flwr 2024-06-11 11:26:09,957 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAdam
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				local: {'lr': 0.01}
				global: {'lr': 0.01, 'betas': [0.9, 0.99], 'eps': 0.0001}
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				eps: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 11:26:09,967 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 11:26:10,035 | Config.py:72 | No previous federated learning simulation found with hash caa47aafb7ac0f49d834ae52c066512e979437b77978603c35f56eaf479e01b8, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 11:26:17,319 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 11:26:17,319 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 11:26:17,384	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 11:26:17,399	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 11:26:18,778 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 11:26:18,853	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 11:26:18,854 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 4.0, 'node:__internal_head__': 2.0, 'memory': 34359738368.0, 'object_store_memory': 22894210252.0, 'node:10.20.240.18': 2.0, 'GPU': 2.0, 'accelerator_type:G': 2.0}
INFO flwr 2024-06-11 11:26:18,855 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 11:26:18,855 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 11:26:18,869 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 11:26:18,869 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 11:26:18,870 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 11:26:18,870 | server.py:91 | Evaluating initial parameters
[2m[36m(pid=96831)[0m 2024-06-11 11:26:20.956934: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=96831)[0m 2024-06-11 11:26:21.015500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=96831)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=96831)[0m 2024-06-11 11:26:23.470674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:26:37,799 | server.py:94 | initial parameters (loss, other metrics): 5.040051953125, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-11 11:26:37,799 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 11:26:37,800 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 11:27:58,876 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:27:58,914 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:28:04,053 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:28:04,053 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:18,545 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:18,555 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:30,907 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:30,924 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:37,695 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:37,696 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:37,712 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:37,729 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:44,922 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:44,922 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:44,923 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:44,924 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:28:44,924 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:28:44,925 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:28:44,931 | server.py:236 | fit_round 1 received 2 results and 8 failures
[2m[33m(raylet)[0m [2024-06-11 11:29:04,054 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:29:04,054 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO flwr 2024-06-11 11:31:51,082 | server.py:125 | fit progress: (1, 5.77900390625, {'accuracy': 0.017, 'data_size': 10000}, 313.28210789570585)
INFO flwr 2024-06-11 11:31:51,083 | server.py:171 | evaluate_round 1: no clients selected, cancel
DEBUG flwr 2024-06-11 11:31:51,084 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 100)
ERROR flwr 2024-06-11 11:32:31,108 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:32:31,114 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:32:39,038 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:32:39,044 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:32:46,761 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:32:46,762 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:32:54,772 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:32:54,772 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:32:54,778 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:32:54,789 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:33:02,417 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:33:02,418 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:33:02,418 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:33:02,425 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:33:10,155 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:33:10,156 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:33:10,157 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: 8db62dc67b04e4fe4a2331ec01000000, name=DefaultActor.__init__, pid=96832, memory used=3.04GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-0507e884652b121fc58d1d64e80121d5e6a20a93a6edc6e8e94afc48*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.42	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:33:10,157 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:33:10,157 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:33:10,158 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: 8f77719ea46879bbcb15ed4501000000, name=DefaultActor.__init__, pid=96831, memory used=2.51GB) was running was 238.95GB / 251.51GB (0.950051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-c8746499de64871cbcd46cb69bbdd2bd72acbad1318e2484e0cc0587*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
92062	4.75	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
96968	4.41	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_av...
96832	3.04	ray::DefaultActor.run
96831	2.51	ray::DefaultActor
91576	0.84	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91577	0.34	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
90689	0.23	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:33:10,201 | server.py:236 | fit_round 2 received 0 results and 10 failures
ERROR flwr 2024-06-11 11:33:10,203 | app.py:313 | 'NoneType' object has no attribute 'tensors'
ERROR flwr 2024-06-11 11:33:10,208 | app.py:314 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 109, in fit
    res_fit = self.fit_round(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/server/server.py", line 248, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 78, in aggregate_fit
    self._capture_results(server_round, results, aggregated_parameters, config)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 98, in _capture_results
    self._capture_aggregates(server_round, aggregated_parameters, metrics)
  File "/home/s2240084/conFEDential/src/training/Server.py", line 124, in _capture_aggregates
    aggregated_parameters = parameters_to_ndarrays(aggregated_parameters)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/common/parameter.py", line 34, in parameters_to_ndarrays
    return [bytes_to_ndarray(tensor) for tensor in parameters.tensors]
AttributeError: 'NoneType' object has no attribute 'tensors'

ERROR flwr 2024-06-11 11:33:10,209 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
ERROR flwr 2024-06-11 11:33:10,210 | Simulation.py:185 | Simulation crashed.
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run summary:
wandb: accuracy 0.017
wandb:     loss 5.779
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/s2240084/conFEDential/wandb/offline-run-20240611_112612-2wa0ije2
wandb: Find logs at: ./wandb/offline-run-20240611_112612-2wa0ije2/logs
[2m[36m(pid=96832)[0m 2024-06-11 11:26:21.229753: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=96832)[0m 2024-06-11 11:26:21.296928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=96832)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[36m(pid=96832)[0m 2024-06-11 11:26:23.645933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[1m[36m(autoscaler +2m46s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[2m[1m[33m(autoscaler +2m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[2m[1m[33m(autoscaler +2m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0, 'memory': 17179869184.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
2024-06-11 11:34:17.824058: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-11 11:34:17.894441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-11 11:34:19.569847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
INFO flwr 2024-06-11 11:34:27,160 | Data.py:36 | Preprocessing the federated learning dataset
INFO flwr 2024-06-11 11:34:27,162 | Data.py:111 | Found preprocessed data for the given preprocess function with hash efddb1ca4fe67eba076ca51cfd1766212af1fce46847913eececd421a82a7a21, returning
INFO flwr 2024-06-11 11:34:27,694 | Simulation.py:37 | Preparing datasets for federated learning simulation
INFO flwr 2024-06-11 11:34:27,695 | Simulation.py:266 | Found previously split dataloaders with hash bd65c6a03831e38c587d483e1fe6a818e6bdcb912c586e58c4dda52c558b4a79, loading them
INFO flwr 2024-06-11 11:34:36,226 | main.py:110 | Loaded 1 configs with name CIFAR100-RESNET34-FEDAVG, running...
INFO flwr 2024-06-11 11:34:36,231 | main.py:112 | Config:
	Simulation:
		Data:
			dataset_name: cifar100
			batch_size: 64
			preprocess_fn:
				def preprocess_fn(element):
				  return {
				    "x": element["img"],
				    "y": element["fine_label"]
				  }
		Federation:
			client_count: 100
			fraction_fit: 0.1
			global_rounds: 300
			local_rounds: 4
		Model:
			optimizer_name: FedAvg
			model_name: ResNet34
			criterion_name: CrossEntropyLoss
			optimizer_parameters: 
				lr: 0.01
			model_architecture:
				repo_or_dir: pytorch/vision:v0.10.0
					model: resnet34
					pretrained: False
					out_features: 100
	Attack:
		data_access: 1.0
		message_access: server
		repetitions: 0
		Attack Simulation:
			batch_size: 64
			optimizer_name: Adam
			optimizer_parameters:
				lr: 0.0001
			ModelArchitecture:
				gradient_component:
					type: Dropout
						p: 0.2
					type: Conv2d
						out_channels: 1000
						kernel_size: None
						stride: 1
					type: ReLU
				fcn_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
				encoder_component:
					type: Dropout
						p: 0.2
					type: Linear
						out_features: 256
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 256
						out_features: 128
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 128
						out_features: 64
					type: ReLU
					type: Dropout
						p: 0.2
					type: Linear
						in_features: 64
						out_features: 1
					type: ReLU
					type: Sigmoid
INFO flwr 2024-06-11 11:34:36,241 | Config.py:67 | Starting conFEDential simulation
INFO flwr 2024-06-11 11:34:36,260 | Config.py:72 | No previous federated learning simulation found with hash b3b9198efd6ce563a44ef7ab149d79fbaa826077dedf48204d6e81f2596f72b8, starting training simulation...
wandb: Tracking run with wandb version 0.16.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
INFO flwr 2024-06-11 11:34:38,178 | Simulation.py:406 | Created 1 clients with resources 2 CPUs, 1.0 GPUs, and 16.0GB for the total available 2 CPUs, 1 GPUs and 16.0GB.
INFO flwr 2024-06-11 11:34:38,178 | Simulation.py:161 | Starting federated learning simulation
2024-06-11 11:34:38,231	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.20.240.18:6379...
2024-06-11 11:34:38,246	INFO worker.py:1621 -- Connected to Ray cluster.
INFO flwr 2024-06-11 11:34:38,262 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=300, round_timeout=None)
2024-06-11 11:34:38,335	INFO worker.py:1453 -- Calling ray.init() again after it has already been called.
INFO flwr 2024-06-11 11:34:38,336 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 34359738368.0, 'GPU': 2.0, 'accelerator_type:G': 2.0, 'object_store_memory': 22894210252.0, 'node:10.20.240.18': 2.0, 'CPU': 4.0, 'node:__internal_head__': 2.0}
INFO flwr 2024-06-11 11:34:38,336 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
INFO flwr 2024-06-11 11:34:38,337 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0, 'memory': 17179869184}
INFO flwr 2024-06-11 11:34:38,363 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
INFO flwr 2024-06-11 11:34:38,364 | server.py:89 | Initializing global parameters
INFO flwr 2024-06-11 11:34:38,364 | server.py:272 | Using initial parameters provided by strategy
INFO flwr 2024-06-11 11:34:38,365 | server.py:91 | Evaluating initial parameters
INFO flwr 2024-06-11 11:34:40,350 | server.py:94 | initial parameters (loss, other metrics): 5.040051953125, {'accuracy': 0.0116, 'data_size': 10000}
INFO flwr 2024-06-11 11:34:40,351 | server.py:104 | FL starting
DEBUG flwr 2024-06-11 11:34:40,352 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 100)
[2m[36m(pid=106964)[0m 2024-06-11 11:34:46.532136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2m[36m(pid=106962)[0m 2024-06-11 11:34:54.806547: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2m[36m(pid=106962)[0m 2024-06-11 11:34:55.133589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2m[36m(pid=106962)[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2m[33m(raylet)[0m [2024-06-11 11:35:04,062 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:35:04,066 E 91577 91577] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=106962)[0m 2024-06-11 11:35:24.638268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ERROR flwr 2024-06-11 11:36:58,481 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:36:58,497 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2024-06-11 11:37:04,065 E 91576 91576] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc, IP: 10.20.240.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.20.240.18`
[2m[33m(raylet)[0m 
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:07,793 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:07,804 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:19,394 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:19,394 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:19,395 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:19,396 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:19,397 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:19,397 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:19,396 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

ERROR flwr 2024-06-11 11:37:19,398 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 82368ecf485f399cbe71e12f70429c776b9d0827d87624f23dede1cc) where the task (actor ID: bf811a4afe7b06f07f726b4d04000000, name=DefaultActor.__init__, pid=106964, memory used=3.55GB) was running was 238.95GB / 251.51GB (0.95005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-451b0efd5ad870acbad35dfe79fde593981a150030f408007fd90d93*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
106964	3.55	ray::DefaultActor
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:19,398 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR flwr 2024-06-11 11:37:19,397 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 10.20.240.18, ID: 85e550ac56cf0499f9cb5ceaa1a9836bd5b91d2594fa0c2849ddba84) where the task (actor ID: c35c1ff14b247c077564fd6804000000, name=DefaultActor.__init__, pid=106962, memory used=0.35GB) was running was 238.94GB / 251.51GB (0.950007), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.20.240.18`. To see the logs of the worker, use `ray logs worker-4c6b46e240a0c12fb89241d3af08f2a3cc261cf6c2cf0a01906f4f97*out -ip 10.20.240.18. Top 10 memory users:
PID	MEM(GB)	COMMAND
106889	4.69	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar100/resnet34/fed_a...
105408	2.85	/home/s2240084/conFEDential/venv/bin/python src/main.py --yaml-file examples/cifar10/resnet18/fed_na...
91577	1.26	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
91576	0.59	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --rayle...
106962	0.35	ray::DefaultActor.run
90689	0.27	/home/s2240084/conFEDential/venv/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
92026	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
92029	0.06	/home/s2240084/conFEDential/venv/bin/python -u /home/s2240084/conFEDential/venv/lib/python3.10/site-...
90825	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
90824	0.06	/home/s2240084/conFEDential/venv/bin/python /home/s2240084/conFEDential/venv/lib/python3.10/site-pac...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
DEBUG flwr 2024-06-11 11:37:19,400 | server.py:236 | fit_round 1 received 3 results and 7 failures
